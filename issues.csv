,author,closedAt,number,publishedAt,resourcePath,source,state,text,label
0,{'login': 'jklaiho'},2019-03-25T11:40:21Z,16068,2016-05-31T12:00:14Z,/ansible/ansible/issues/16068,ansible,CLOSED,"Magic variable role_names to also list roles belonged to via dependencies?
Feature Idea
ansible 2.0.2.0 (not that relevant here though)
magic variables
The magic variable role_names, containing the list of all role names that the current target of the playbook belongs to, is quite useful in certain scenarios. For example, I use the same playbook to set up development vagrant VMs as well as test and production servers. A few tasks in certain roles are conditional on role membership, e.g. having when: ""'dev' in role_names"".
However, it looks like roles that are included via role dependencies in meta/main.yml files are not included in the role_names list. You could make an argument either way, but it seems to me that they should be there for completeness' sake, since role membership via dependency is still a type of role membership.
Seeing as role_names isn't really documented (""role_names"" site:docs.ansible.com on Google returns nothing, at least), it probably isn't very widely used right now, and changing the behaviour probably won't break much. Is this something you could consider?",NONE
1,{'login': 'jsmartin'},2014-09-29T20:46:26Z,4578,2013-10-18T02:51:50Z,/ansible/ansible/issues/4578,ansible,CLOSED,"SmartOS IP address fact
On SmartOS, in contrast with the other OS's, it appears that the IP address object is an actual list of IPs. Is that to be expected?
for example, in a smartos, I have to address it like this:
where on linux
will suffice.",NONE
2,{'login': 'tumbl3w33d'},2017-08-19T23:13:23Z,22337,2017-03-06T23:01:55Z,/ansible/ansible/issues/22337,ansible,CLOSED,"maven_artifact module should allow preserving the name of the downloaded artifact
Feature Idea
maven_artifact
N/A
N/A
When downloading an artifact you can ask the module to determine the latest version automatically (it gets extracted from the repo's metadata). The artifact's name in the repository comes with a version string which currently gets overwritten with 'latest' when downloading.
The problem with this approach is that you sometimes need to know what version the downloaded artifact has for further processing, My use case is transferring the version to the RPM that I build from it.
The implementation should introduce an additional parameter which causes what I described above. This is only relevant for the constellation version: latest and dest being a directory, because else the filename chosen by dest or the version explicitly selected win.
The downloaded artifact contains the dynamically determined version string. In this case, the last version found in this list. At the moment of creating this issue this is /tmp/spring-core-4.3.7.RELEASE.jar
The downloaded artifact is renamed to /tmp/spring-core-latest.jar",NONE
3,{'login': 'zerkms'},2015-05-04T23:42:47Z,10915,2015-05-04T23:39:55Z,/ansible/ansible/issues/10915,ansible,CLOSED,"SSH connection is established for every command
Here is the log of the run
The deply.yml is as simple as
followed by 2 tasks to install packages via apt-get.
The OS on both machines is ubuntu trusty, the requiretty does not exist in sudoers (on the target machine) and setting Defaults !requiretty does not change anything.
In the ansible config I have made only these 2 changes:
uncommented
ssh_args = -o ControlMaster=auto -o ControlPersist=60s
enabled
pipelining = True
It's for ansible 1.9.1 installed from the PPA.
What am I doing wrong?",SECURITY
4,{'login': 'claco'},2014-09-29T20:46:57Z,7860,2014-06-20T02:52:32Z,/ansible/ansible/issues/7860,ansible,CLOSED,"rax_identity stacktraces with pyrax >= 1.8.0
Starting with pyrax 1.8.0 and greater installed, the rax_identity module now dies with a stacktrace trying to serialize the output:
As a workaround, rolling back to < 1.8.0 fixes the problem.",NONE
5,{'login': 'ansible-bug-reporter'},2017-05-12T00:45:18Z,24460,2017-05-10T19:03:23Z,/ansible/ansible/issues/24460,ansible,CLOSED,"Force_handlers with_items does not execute handlers
Bug Report
Force handlers with_items
ansible 2.3.0.0
config file = /etc/ansible/ansible.cfg
configured module search path = configured module search path = Default w/o overrides
python version = 2.7.10 (default, May 1 2017, 19:24:18) [GCC 4.4.7 20120313 (Red Hat 4.4.7-16)]
We expect handlers to run when at least one item is changed when using with_items and force_handlers.
PLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************
TASK [command] ****************************************************************************************************************************************************************************************************************************************************************
changed: [localhost] => (item=ls)
changed: [localhost] => (item=pwd)
failed: [localhost] (item=asdf) => {""changed"": true, ""cmd"": ""asdf"", ""delta"": ""0:00:00.001902"", ""end"": ""2017-05-10 13:59:28.650743"", ""failed"": true, ""item"": ""asdf"", ""rc"": 127, ""start"": ""2017-05-10 13:59:28.648841"", ""stderr"": ""/bin/sh: asdf: command not found"", ""stderr_lines"": [""/bin/sh: asdf: command not found""], ""stdout"": """", ""stdout_lines"": []}
RUNNING HANDLER [hello_world] *************************************************************************************************************************************************************************************************************************************************
ok: [localhost] => {
""changed"": false,
""msg"": ""Hello world!""
}
PLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************
localhost : ok=1 changed=1 unreachable=0 failed=1
PLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************
TASK [command] ****************************************************************************************************************************************************************************************************************************************************************
changed: [localhost] => (item=ls)
changed: [localhost] => (item=pwd)
failed: [localhost] (item=asdf) => {""changed"": true, ""cmd"": ""asdf"", ""delta"": ""0:00:00.001916"", ""end"": ""2017-05-10 14:00:00.284031"", ""failed"": true, ""item"": ""asdf"", ""rc"": 127, ""start"": ""2017-05-10 14:00:00.282115"", ""stderr"": ""/bin/sh: asdf: command not found"", ""stderr_lines"": [""/bin/sh: asdf: command not found""], ""stdout"": """", ""stdout_lines"": []}
PLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************
localhost : ok=0 changed=0 unreachable=0 failed=1",FAULT TOLERANCE
6,{'login': 'phillipuniverse'},2013-10-19T18:02:28Z,4518,2013-10-15T13:57:16Z,/ansible/ansible/issues/4518,ansible,CLOSED,"Stack trace in YAML Validation Code
From the ansible-project list, the stack trace is as follows:
The issue ended up being that I missed a ':' between a variable key and value. I assume that this can be reproduced by leaving off a colon anywhere in a variable definition (or maybe even tasks). I specifically created this it by creating a vars section and putting:
I fixed it by instead doing:
@mpdehaan mentioned that this would be a pretty quick fix.",NONE
7,{'login': 'jlec'},2017-02-27T15:14:03Z,15326,2016-04-07T15:55:25Z,/ansible/ansible/issues/15326,ansible,CLOSED,"password_hash/get_encrypted_password uses passlib default of rounds=656000 which is 131 times glibc default
In the password_hash filter function the underlying passlib call uses the default rounds parameter.
The default for glibc is 5000, the passlib default for sha512 is 656000. This means on a login in a linux account the hash calculation will take significantly longer.
Actually you get basically no rounds parameter when setting it to 5000
Could a simple parameter be added to get_encrypted_password to set the value which has a preferable default what glibc does?",PERFORMANCE
8,{'login': 'webarchitect609'},2017-01-22T17:48:50Z,20547,2017-01-22T10:15:57Z,/ansible/ansible/issues/20547,ansible,CLOSED,"Broken 'Edit on GitHub' link at http://docs.ansible.com/ansible/intro_adhoc.html
Documentation Report
'Edit on GitHub' http://joxi.ru/l2Z6VxFwlbVL2J?d=1 link leads to 404 error page http://joxi.ru/eAO14Wfxv1Gdmo?d=1
1 Go to http://docs.ansible.com/ansible/intro_adhoc.html
2 Press 'Edit on GitHub' at top right corner.
3 See 404 error page.
Some kind of valid git hub page for editing related page.
404 error page",NONE
9,{'login': 'tzookb'},2015-11-19T22:41:48Z,13103,2015-11-09T22:18:55Z,/ansible/ansible/issues/13103,ansible,CLOSED,"ansible fails with an exception in python
I sinply run command: ""forever stopall""
but I get this output from stdout when it fails:
fatal: [node1] => Traceback (most recent call last):
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 586, in _executor
exec_rc = self._executor_internal(host, new_stdin)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 789, in _executor_internal
return self._executor_internal_inner(host, self.module_name, self.module_args, inject, port, complex_args=complex_args)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 1005, in _executor_internal_inner
num_args_post = self._count_module_args(module_args)
File ""/Library/Python/2.7/site-packages/ansible/runner/init.py"", line 433, in _count_module_args
vargs = split_args(args)
File ""/Library/Python/2.7/site-packages/ansible/module_utils/splitter.py"", line 73, in split_args
args = args.strip()
AttributeError: 'dict' object has no attribute 'strip'",NONE
10,{'login': 'blakfeld'},2016-03-01T04:18:53Z,14659,2016-02-25T15:39:17Z,/ansible/ansible/issues/14659,ansible,CLOSED,"--diff Flag crashes on win_copy
Issue Type:
Bug Report
Ansible Version:
Ansible Configuration:
Stock
Environment:
Running from: Ubuntu 14.04
Targeting: Windows 2k12
Summary:
The --diff flag fails when using the win_copy module.
Steps To Reproduce:
Run any playbook that contains the win_copy module while using the --diff flag against a Windows box.
Expected Results:
I expect to see a diff between the file that was already on disk, and the one I just copied over.
Actual Results:
Here's the entire output
https://gist.github.com/blakfeld/542b191c85f4385e50bf
Here's the relevant output
The problem appears to be because the --diff command is relying upon the existence of a 'state' key in the returned JSON. This appears to be a key added in the module_utils.basic Python module, specifically the add_path_info method, which seems to be called by load_common_file_arguments. I was going to just hack this into win_copy, but it seems to me that the most reasonable solution would be to add load_common_file_arguments to powershell_common.ps1. I'm more than happy to start work on that if the community agrees that is the best answer.",FAULT TOLERANCE
11,{'login': 'sebi-hgdata'},2015-08-12T14:50:19Z,11881,2015-08-06T19:53:25Z,/ansible/ansible/issues/11881,ansible,CLOSED,"Variable overding during nested includes issue
It seems variables are not overridden when using nested includes (this works ok in v1 ) .This might be related to #11353 . To reproduce the issue you can follow the steps from #11353 ...
In build.yml I have a variable docker_tags what is overriden in tasks/docker/base_build.yml... instead of printing the overridden value it prints the value from build.yml. See debug statements after running 'ansible-playbook -v build_admin_ui.yml -i inventories/local/hosts --extra-vars ""admin_ui_version=1234""'",MAINTAINABILITY
12,{'login': 'BenMo158'},2016-04-29T00:13:51Z,13390,2015-12-02T06:13:11Z,/ansible/ansible/issues/13390,ansible,CLOSED,"[v2] ""win_iis_webbinding"" module require additional parameters to run?
I was run
I need run this,it's work
",MAINTAINABILITY
13,{'login': 'jpcarey'},2015-06-24T18:42:38Z,11362,2015-06-23T23:33:17Z,/ansible/ansible/issues/11362,ansible,CLOSED,"ec2 dynamic_inventory tag issue
The special characters escape to underscores in ec2.py does not appear to work poperly.
I have the following tag for an ec2 instance:
aws:cloudformation:stack-name => imdev-flask-3-2-298-l1FiTlA
Per the docs, this should translate to an underscore between stack and name: 'ec2_tag_aws_cloudformation_stack_name'
This does not work, and produced the following var undefined error:
One or more undefined variables: 'ec2_tag_aws_cloudformation_stack_name' is undefined
running ec2.py, the actual output is ""ec2_tag_aws_cloudformation_stack-name"": ""imdev-flask-3-2-298-l1FiTlA""
If I try to use in the ec2.py form, ansible will throw an error for the invalid character:
Failed to template msg=""{{ ec2_tag_aws_cloudformation_stack-name == removed_version }}"": Unable to look up a name or access an attribute in template string. Make sure your variable name does not contain invalid characters like '-'.
However, accessing through the hostvars allows reference to the item with the ""-"". Ex:
""{{ hostvars[inventory_hostname]['ec2_tag_aws_cloudformation_stack-name'] }}""",FAULT TOLERANCE
14,{'login': 'tartansandal'},2013-10-22T16:01:21Z,4608,2013-10-20T23:05:27Z,/ansible/ansible/issues/4608,ansible,CLOSED,"documented used of 'lookup' now generates variable undefined error 
The following documented use of the lookup plugin
now generates the following error:
I've traced this back to commit 5031104.",FAULT TOLERANCE
15,{'login': 'evgkrsk'},2019-01-25T16:25:15Z,13175,2015-11-16T01:06:20Z,/ansible/ansible/issues/13175,ansible,CLOSED,"Wrong diagnostics in ansible-galaxy on certificate issues
Issue Type:
Bug Report
Component Name
ansible-galaxy
Ansible Version:
Ansible Configuration:
Clean out-of-box configuration (on SOME OS).
Environment:
N/A. Generic, non-mainstream OS. In my case, ALT Linux Sisypus (unstable branch).
Summary:
On non-mainstream OS with non-standard SSL/TLS CA certificate paths (not in '/etc/ssl/certs', '/etc/pki/ca-trust/extracted/pem', '/etc/pki/tls/certs', '/usr/share/ca-certificates/cacert.org', '/etc/ansible' in my case) diagnostics on any problem with certificate check in ansible-galaxy is just plain wrong (ERROR! Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED).
Steps To Reproduce:
Try to install any galaxy role just after clean ansible install (ansible-galaxy will fail to check correct SSL/TLS certificate of galaxy.ansible.com with wrong error message).
Expected Results:
Actual Results:
Sanity check:
Workarounds:
None. Old workaround for ansible-1.9.x NOT working anymore:
",SECURITY
16,,,22531,2017-03-11T19:19:29Z,/ansible/ansible/issues/22531,ansible,OPEN,"Trailing new lines aren't kept by default by template module
Bug Report
Jinja
(referring to commit 1998edd)
When trying to wrap a conditional inside a template in an one liner, the line break will be skipped if there's no trailing character (e.g. like a whitespace to workaround this issue)
Create a template like this:
",FAULT TOLERANCE
17,{'login': 'ahes'},2016-02-02T15:23:36Z,14243,2016-02-01T15:55:33Z,/ansible/ansible/issues/14243,ansible,CLOSED,"pre_tasks not working while gathering=smart in ansible.cfg
Issue Type:
Bug report
Ansible Version:
Ansible Configuration:
Environment:
Mac OS X
Summary:
When gathering=smart used in ansible.cfg pre_tasks are not running.
Steps To Reproduce:
ansible.cfg:
play.yml:
Expected Results:
Actual Results:
",NONE
18,{'login': 'dp4qb'},2016-06-01T18:46:20Z,10828,2015-04-24T09:18:17Z,/ansible/ansible/issues/10828,ansible,CLOSED,"Issue with 'include' statements and role path
My ansible's 'include' statement was working fine, but recently after including ymls in subfolder, it somehow brokes the role's path.
Here's the tree of my project:
site.yml:
roles/webservers/tasks/main.yml:
Prior to this point, it works just fine. But the next step, after including the ""profile""/main.yml, brokes the role's path.
roles/webservers/tasks/dev/main.yml:
results an error:
So, after this 'include', it somehow thinks that role's root is located in 'webservers/tasks/', instead of 'webservers/'. But in the same time, it sees the variables in 'webservers/vars/' path.
Moreover, it was working fine and recently just broke. I haven't updated ansible, nor edited it's parameters, just updated the playbook.
If I specify the 'src=../../templates/httpd_conf.j2' it works fine, but since this behaviour just appeared like from nowhere, I can't rely on this path.
I've managed to use this workaround: template: src=""{{ role_path }}/templates/httpd_conf.j2"" dest=/etc/httpd/conf/httpd.conf But that's a workaround. The reason of auto-pathing is broke is still undiscovered.
Ansilbe's version: 1.9.0.1",FAULT TOLERANCE
19,{'login': 'ankitmth'},2016-11-16T09:21:57Z,14151,2016-01-27T11:08:52Z,/ansible/ansible/issues/14151,ansible,CLOSED,"Ansible 2.0.0.2 : ""ERROR! file or module does not exist"" while running a playbook with script module
After installing ansible 2.0.0.2 from rpm:
I get the below error on running the playbook which uses the script module:
The Playbook in question: -
",FAULT TOLERANCE
20,{'login': 'ktosiek'},2015-06-28T01:05:40Z,8213,2014-07-20T18:52:22Z,/ansible/ansible/issues/8213,ansible,CLOSED,"Variable interpolation in hostvars
Issue Type: Feature Idea
Ansible Version: ansible 1.6.6
Environment: N/A
Summary:
When variables are accessed through hostvars, jinja2 expressions inside those variables should be interpolated.
There was discussion about this issue on ansible-project mailing list: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/ansible-project/pdWFR2Q8U8E/m6sgB1C5K1YJ
And here is a filter that shows simple implementation (but might be pretty slow, and I think it should be in hostvars not a filter): https://groups.google.com/group/ansible-project/attach/16318558e74073ae/hostvars.py?part=0.1&view=1
Steps To Reproduce:
Given vars like:
The values for both {{ bar }} and {{ hostvars[some_host].bar }} should be ""x is y"".
Currently the value of {{ hostvars[some_host].bar }} would be (literally) ""x is {{ foo }}"".",NONE
21,{'login': 'basictheprogram'},2017-04-04T17:31:07Z,23249,2017-04-04T02:23:33Z,/ansible/ansible/issues/23249,ansible,CLOSED,"win_chocolatey installing powershell when powershell4 is installed and state: latest
Bug Report
win_chocolatey
Ansible configuration from git clone
Control host macOS 10.12.3
Managed host Windows 7
Attempting to upgrade powershell4 to powershell5 using the win_chocolatey module with state: latest
Might be related to #21873
Related to #22892
Have chocolatey installed
Install powershell4
Powershell4 would be upgraded to Powershell5
https://gist.github.com/basictheprogram/74b48320e386d308a69f82f7c90019b5
choco_summary.log
https://gist.github.com/basictheprogram/de146516fe292a43630c10e7df205f38
chocolatey.log
https://gist.github.com/basictheprogram/5cc240c1f89fba6ef7ac769ff5e41a01",MAINTAINABILITY
22,{'login': 'michalzubkowicz'},2016-05-30T15:43:10Z,16045,2016-05-30T08:07:08Z,/ansible/ansible/issues/16045,ansible,CLOSED,"Docker module: argument memory_limit is of type <type 'str'> and we were unable to convert to int"" on Ansible 2.0.2.0-1.el7
Bug Report
Centos 7
After upgrade to ansible 2.0.2.0 it's not possible to enter memory_limit as human readable string (ie. 265MB) only bytes are accepted.
try set memory_limit: 256MB
Should accept string as early versions
Is showing error
argument memory_limit is of type <type 'str'> and we were unable to convert to int",USABILITY
23,{'login': 'landryb'},2017-06-23T16:25:42Z,25910,2017-06-20T15:19:14Z,/ansible/ansible/issues/25910,ansible,CLOSED,"openbsd_pkg: add support for pkgname%branch syntax
Bug Report
openbsd_pkg
OpenBSD -current or 6.1
Trying to use the pkgname%branch syntax fails
installing openldap-server 2.44 from the databases/openldap branch. works fine in a shell:
The openbsd_pkg module errors out.
",NONE
24,{'login': 'jean-christophe-manciot'},2017-09-15T14:49:22Z,26146,2017-06-27T15:21:12Z,/ansible/ansible/issues/26146,ansible,CLOSED,"Error with paramiko 2.1.0 <--> 2.2.1: Unicode-objects must be encoded before hashing
Bug Report
ios_command
but I suspect all commands are impacted
Both latest stable & unstable versions:
inventory = ./hosts
library = /home/actionmystique/Ansible/git-yang-networkop/ansible-101/library
forks = 1000
gathering = explicit
gather_timeout = 30
roles_path = /home/actionmystique/Ansible/Roles/roles
private_role_vars = yes
hash_behaviour = merge
log_path = /var/log/ansible.log
retry_files_enabled = False
show_custom_stats = True
timeout = 60
pipelining = True
connect_timeout = 60
connect_retries = 30
connect_interval = 1
host: Ubuntu 17.04 4.10
target:
IOS-XEv 16.4.1
paramiko: 2.2.1
cf. title, for instance with a show running-config command.
I am able to manually ssh into the remote device.
Is the paramiko version too recent?
Structure passed as ""provider"": connections.ssh
Role: ios_pull_config:
Playbook:
Startup & running configurations from the target IOSv node.
ACTUAL RESULTS: CLI
ACTUAL RESULTS: log
ACTUAL RESULTS: Manual SSH
WORKAROUND: older paramiko
The latest paramiko version is 2.2.1.
When downgrading to paramiko 2.0.6, this issue is ... gone.
I recommend to put a more stringent requirement on paramiko in requirements.txt until this issue is solved with more recent paramiko, for instance:
The issue begins with paramiko 2.1.0+",NONE
25,{'login': 'svpace'},2016-05-16T23:18:13Z,15864,2016-05-15T16:19:55Z,/ansible/ansible/issues/15864,ansible,CLOSED,"[modules-core: file] Allow for ""directory"" or ""link"" in place of ""file"" with state 
Feature Idea
using ""file"" to check for directories (or links) is not very intuitive.
It would be great if
could be used as an alias for
",USABILITY
26,{'login': 'psi-4ward'},2017-11-22T00:36:04Z,13887,2016-01-14T13:32:19Z,/ansible/ansible/issues/13887,ansible,CLOSED,"GATHERING FACTS fails
Guest is CentOS 7 Vagrant BOX
When i run vagrant provision or ansible-playbook ... gathering facts fails MOST time:
anyone konws why?
For me the output looks like incomplete JSON",FAULT TOLERANCE
27,{'login': 'philltomlinson'},2015-11-11T12:00:28Z,13062,2015-11-06T16:24:37Z,/ansible/ansible/issues/13062,ansible,CLOSED,"Shell with_items Prompts for ssh key password on second command
I have seen something when running the shell module along with a with_items where it now prompts for a key pass phrase. It runs the first command with no password prompt but then on the second one it prompts for a key password and fails the task:
When running the playbook you get prompted for a key passphrase:
This previously worked on the v2.0.0-0.3.beta1 tag but no longer works on later tags and on the dev branch.",SECURITY
28,{'login': 'gholms'},2014-02-13T20:42:30Z,3978,2013-08-30T23:20:12Z,/ansible/ansible/issues/3978,ansible,CLOSED,"Add a way to make boto not verify SSL certs
Starting a year ago with version 2.6.0, boto began verifying servers' SSL certificates by default. Since most Eucalyptus and Nova installs have self-signed certs, this breaks Ansible's AWS-related modules when they're pointed at one of those clouds using a relatively current version of boto.
They added a validate_certs=False arg to calls like boto.connect_ec2_endpoint one can use to disable this behavior, but right now Ansible doesn't have a way to trigger that. Modules like ec2, ec2_elb, and so on would benefit from a parameter that lets one turn cert verification off when they need to talk to services with self-signed certs.
EPEL bug that triggered this report: https://bugzilla.redhat.com/show_bug.cgi?id=1003105",SECURITY
29,{'login': 'deimosfr'},2016-07-12T18:17:33Z,9811,2014-12-14T18:46:45Z,/ansible/ansible/issues/9811,ansible,CLOSED,"ansible-galaxy file parameter full path
Hi,
It would be nice when you setup a galaxy dependencies file like this:
to be able to specify the name of the final module path. The goal is when i launch it like this:
I get a folder 'roles/nginx' which is better than having 'roles/jdauphant.nginx'
Thanks",NONE
30,{'login': 'bobrik'},2015-04-13T17:38:43Z,10673,2015-04-13T10:01:05Z,/ansible/ansible/issues/10673,ansible,CLOSED,"JSON output gets truncated on spaces
Issue Type:
Bug report
Ansible Version:
and
Environment:
N/A
Summary:
copy: content=""{{ some_data|to_json }}\n"" dest=/tmp/wtf.yaml results in truncated json.
Steps To Reproduce:
Example playbook:
Resulting file:
Expected Results:
File should not be truncated.
Actual Results:
File gets truncated.",FAULT TOLERANCE
31,{'login': 'omame'},2017-04-12T07:10:59Z,14148,2016-01-27T09:50:24Z,/ansible/ansible/issues/14148,ansible,CLOSED,"Misleading error when a file isn't found in a role
Issue Type:
Bug Report
Ansible Version:
ansible 2.0.0.2
config file = /home/user/ansible/ansible.cfg
configured module search path = Default w/o overrides
Ansible Configuration:
Clean.
Environment:
Ubuntu 14.04.
Summary:
When a copy or assemble operation can't access the src file in a role a misleading error suggests to look at the playbooks directory.
Steps To Reproduce:
Expected Results:
Actual Results:
",USABILITY
32,{'login': 'profhase'},2017-04-25T14:49:20Z,23955,2017-04-25T09:35:05Z,/ansible/ansible/issues/23955,ansible,CLOSED,"Copy module shows 'changed' in check mode
Bug Report
copy
NA
NA
When using the copy module to copy a file into a directory on the target system, the check mode yields changed though there is no change on the target system. The problem only occurs if the path
is given without trailing slash.
Call in check mode after file exists:
changed: false
changed: true
This effect does not occur if dest is given with a trailing slash:
",MAINTAINABILITY
33,{'login': 'sascha-egerer'},2015-09-17T15:13:35Z,12412,2015-09-17T08:42:11Z,/ansible/ansible/issues/12412,ansible,CLOSED,"Ansible fails to load Yaml file with equal-sign as value
Ansible fails when loading a Yaml file like this:
The Yaml file is valid (see http://www.yamllint.com/) but ansible fails with ERROR: Syntax Error while loading YAML script ...
Putting quotes around the equal-sign works but i can't do that in my case as this file will be generated by a Yaml parser. So using a workaround is not an option.",FAULT TOLERANCE
34,{'login': 'fulminemizzega'},2018-09-25T20:09:44Z,22320,2017-03-06T16:06:43Z,/ansible/ansible/issues/22320,ansible,CLOSED,"fedora 25 dnf installed packages not marked as user installed
Bug Report
dnf
ansible.cfg
Fedora 25
When running the playbook below not every package is marked as user installed, this means for example that running ""dnf autoremove"" afterwards will remove packages installed by the playbook.
Run the following playbook, then run ""dnf history userinstalled"":
dnf history userinstalled output:
All the packages in the playbook should be listed as user installed, so that 'dnf autoremove' will not remove them. A possible workaround is running 'dnf mark '.
Plus, if you support also dnf autoremove #20333, then interesting things will happen.",MAINTAINABILITY
35,{'login': 'bcomnes'},2015-07-02T13:38:15Z,10077,2015-01-23T20:12:23Z,/ansible/ansible/issues/10077,ansible,CLOSED,"`defaults/main.yml` overriding variables `include_vars:` in roles
In defaults/main.yml:
In vars/Darwin.yml:
In tasks/main.yml:
Assume this is run on a Darwin machine we get
where I would expect to get baz.
The work around is to ditch the defaults folder and instead load it with a defaults file with with_first_found:
Maybe I am doing something backwards here, but the defaults folder seem to take higher precedence over including variables selectively included in the task.",NONE
36,{'login': 'samvarankashyap'},2016-08-10T18:35:42Z,17033,2016-08-10T17:33:25Z,/ansible/ansible/issues/17033,ansible,CLOSED,"ansible os_server module doesnot work with async_status
Bug Report
module : os_server
N/A
Ansible is unable to parse the os_server module output using async_status .
The following tasks are to be written
name: ""Async:: provision/deprovision os_server resources by looping on count""
os_server:
state: ""{{ instance.4 }}""
auth:
auth_url: ""{{ instance.0 }}""
username: ""{{ instance.1 }}""
password: ""{{ instance.2 }}""
project_name: ""{{ instance.3 }}""
name: ""{{ instance.9 }}{{ instance.10 }}{{ instance.11 }}""
image: ""{{ instance.5 }}""
key_name: ""{{ instance.6 }}""
api_timeout: 99999
flavor: ""{{ instance.7 }}""
network: ""{{ instance.8 }}""
with_nested:
[""{{ endpoint }}""]
[""{{ username }}""]
[""{{ password }}""]
[""{{ project }}""]
[""{{ state }}""]
[""{{ res_def['image'] }}""]
[""{{ res_def['keypair'] }}""]
[""{{ res_def['flavor'] }}""]
[""{{ res_def['networks'][0] }}""]
[""{{ res_grp_name }}""]
[""{{ res_def['res_name'] }}""]
""{{ res_count.stdout }}""
loop_control:
loop_var: instance
async: 1000
poll: 0
register: res_def_output
when: async == true
name: 'check on fire and forget task'
async_status: jid={{ item.ansible_job_id }}
register: job_result
until: job_result.finished
retries: 30
with_items: ""{{ res_def_output['results'] }}""
Gives following error :
expected results are the output of the booted instance
Ansible is unable to parse the output of the job
",NONE
37,{'login': 'asad-at-srt'},2013-04-06T21:42:10Z,2585,2013-04-06T14:39:44Z,/ansible/ansible/issues/2585,ansible,CLOSED,"Please enhance when: to accept a list
The semantics should be an AND of the items in the list
when:
expression-1
expression-2
should be equivalent to:
when: expression-1 and expression-2
Justification:
Long/complex expressions are made eminently more readable by breaking them down into components.
This is quite general, any boolean expression can be transformed into an AND-form with negation: a or b = ^(^a and ^b)",NONE
38,{'login': 'mmoya'},2013-03-04T04:39:39Z,2290,2013-03-03T20:48:14Z,/ansible/ansible/issues/2290,ansible,CLOSED,"include fails to expand host variables
The playbook:
this works as long as othertasks is defined inline.
When othertasks is removed from playbook and defined in the inventory file or in host_vars, debug continues working but include fails.
Error is ERROR: file not found: ./tasks/$othertasks.yml.",FAULT TOLERANCE
39,{'login': 'garyrutland'},2015-11-20T13:13:37Z,13225,2015-11-20T09:27:24Z,/ansible/ansible/issues/13225,ansible,CLOSED,"Copying files with mode 400 uploads with a mode of 620
Hi,
I did post this in the Google forum, but my post never seemed to appear so asking here now instead.
This is probably something that isn't recommended but I'm trying to copy some PEM keys to a destination server so that they can be used by that server at a later time.
I need to make sure that they are set to a mode of 400 as well, so using the following code:
I would hope and expect to see the following when listing the files on the destination server:
But instead I'm seeing the following:
Any particular reason why the files being uploaded are actually getting the incorrect permissions?
Or is there a different approach to making sure these files have the correct permissions?
Thanks,
Gary",NONE
40,{'login': 'zshamrock'},2017-11-22T00:36:51Z,13674,2015-12-26T12:50:57Z,/ansible/ansible/issues/13674,ansible,CLOSED,"tags are not inherented by multiple include levels
Issue Type:
Bug Report (or feature request, depends on how you see it)
Ansible Version:
1.9.4
Ansible Configuration:
Default
Environment:
Debian 8.2 (jessie)
Summary:
See the project structure below
Steps To Reproduce:
I have the following project structure:
[1] main.yml has content:
- include: media/main.yml tags=media
[2] main.yml has content:
Expected Results:
I expect all tasks from media/main.yml will inherit all tags specified in tasks/main.yml. So, if I run ansible-playbook -i hosts site.yml --tags media, it will run all tasks specified in media/main.yml, and $ ansible-playbook --list-tags -i hosts site.yml reports this tag as well
Actual Results:
It fails with the following error:
P.S.1: The whole source code of the project is available here https://github.com/zshamrock/ididitagain
P.S.2: As a workaround (or the right way to do?), move specific subdirectories, like media, dev, dot-files, etc, in its own roles, and assign tags with role instead, as mentioned here http://docs.ansible.com/ansible/playbooks_tags.html in my site.yml?
",FAULT TOLERANCE
41,{'login': 'zentavr'},2015-12-14T21:00:05Z,13547,2015-12-14T20:52:06Z,/ansible/ansible/issues/13547,ansible,CLOSED,"Seems like default() filter does not work with ansible 2.0.0@devel
Issue Type: Bug Report
Ansible Version: 2.0.0 devel@8d16638
Ansible Configuration:
Environment: Mac OS X El Capitan 10.11.1
Summary
I have a task like this:
...where ec2_instances is:
When I execute the task - I face an error when we reach the block without defined (that redis cluster in my case):
The error is:
Expected Results: workeable default() filter",NONE
42,{'login': 'jkramarz'},2014-03-19T15:35:49Z,6314,2014-03-06T15:15:34Z,/ansible/ansible/issues/6314,ansible,CLOSED,"Playbooks are runned over other group of hosts than listed by list-hosts
Issue Type:
Bug report
Ansible Version:
starting from commit ae9843f
ansible 1.5 is affected
Environment:
at last Debian 6
Summary:
Please summarize your request in this space. You will earn bonus points for being succinct, but please add enough detail so we can understand the request.
Steps To Reproduce:
run
ansible-playbook -C site.yml -i hosts --list-hosts
ansible-playbook -C site.yml -i hosts
using files
hosts:
site.yml:
Expected Results:
Groups not declared as children of group 'physical' should not be involved.
Actual Results:
Hosts from groups named after hosts included in group 'physical' (not declared as group's children) are involved by play.
",MAINTAINABILITY
43,{'login': 'dagwieers'},2012-11-24T23:19:22Z,1664,2012-11-23T15:16:29Z,/ansible/ansible/issues/1664,ansible,CLOSED,"Variables from group_vars/all are not being picked up when using inventory script
In our case we have seen the above, I still need a minimal test-case to demonstrate this.",NONE
44,{'login': 'pfigue'},2014-10-13T22:55:22Z,9321,2014-10-13T11:33:44Z,/ansible/ansible/issues/9321,ansible,CLOSED,"Duplicated newline in Jinja2 variable when copy module is used
Issue Type: Bug Report
Ansible Version: ansible 1.7.2
Environment: Arch Linux
Summary: Variable content gets newlines duplicated when pasted into a file via the copy module
Steps To Reproduce:
Add a step copy: content=""{{ ssl_private_key }}"" dest=/etc/ssl/private/foo.pem in a playbook
Write the content of ssl_private_key in an ansible-vault vars file
Run the playbook
Check if /etc/ssl/private/foo.pem will have an extra newline for each line in the ssl_private_key variable
Expected Results: if the variable ssl_private_key is defined like this:
I expect to have this in /etc/ssl/private/foo.pem:
And this is what I actually get with Ansible 1.7.1 and some previous versions.
Actual Results: But instead, with Ansible 1.7.2 the result will have duplicated \n:
",NONE
45,{'login': 'jorgenschaefer'},2014-09-29T20:36:03Z,6840,2014-04-03T11:02:56Z,/ansible/ansible/issues/6840,ansible,CLOSED,"synchronize action should maybe force the connection for hosts named localhost (edited title)
Issue Type:
Bug Report
Ansible Version:
ansible 1.6 (devel 317c2f4) last updated 2014/04/03 12:41:29 (GMT +200)
Environment:
openSUSE 12.2 (x86_64)
Summary:
Using the synchronize action while deploying to a VM listening on localhost confuses ansible as it tries to unnecessarily delegate to localhost, and also forgets to clear the port.
Steps To Reproduce:
I'm running a simple playbook to deploy some software to a VM running on localhost:2222.
Inventory file:
tasks/main.yml:
This fails because it tries to connect to 127.0.0.1:2222 with my current username. What is happening, apparently, is that it assumes I'm doing a delegate action and tries to connect to localhost with my current username, but retains the faulty port. There is no need to delegate at all.
Expected Results:
I'd assume it would simply run rsync locally, without delegation, to deploy to the VM.
Actual Results:
It fails because it tries to run an ssh connection to <username>@127.0.0.1 port 2222, which does not work because <username> does not exist on the VM, and port 2222 is the VM. It would succeed on port 22, but that's not necessary.
The following change also makes the above work for me, so it is indeed some bogus delegation business. (The second hunk is only necessary because the first change removes the conn.delegate attribute.)
",NONE
46,{'login': 'huguesalary'},2014-08-09T02:06:16Z,7360,2014-05-11T04:48:06Z,/ansible/ansible/issues/7360,ansible,CLOSED,"Docker - dns argument not working
Issue Type:
Bug Report
Ansible Version:
Ansible 1.7 (but had the same issue with 1.6.1 and 1.6)
Environment:
Running ansible from Mac OS
Managing Debian
Summary:
With the docker module, using the dns argument does not add the dns to the container. (nothing's added to the container's /etc/resolv.conf)
Steps To Reproduce:
Expected Results:
nameserver 127.0.0.1 should appear at the top of the file
Actual Results:
nameserver 127.0.0.1 does not appear",NONE
47,{'login': 'ThStock'},2014-05-06T04:13:03Z,7281,2014-05-05T17:50:41Z,/ansible/ansible/issues/7281,ansible,CLOSED,"--limit does not work with intersecting hostgroups
Issue Type:
Bug Report, Feature Idea
Ansible Version:
ansible 1.5.5
Environment:
N/A
Summary:
I'll be nice, if the --limit switch, will limit task execution to the given hostgroup, also when both hostgroups consists of the same hosts
Steps To Reproduce:
Inventory:
Playbook:
Expected Results:
Actual Results:
",NONE
48,{'login': 'nobita2041'},2012-05-29T12:43:02Z,417,2012-05-28T07:37:46Z,/ansible/ansible/issues/417,ansible,CLOSED,"sudo options in connection.py (CentOS 5.4, -u and -k incompatible?)
「sudo: the `-u' and '-k' options may not be used together」
I modified below, and fine.
connection.py
",NONE
49,{'login': 'tonk'},2012-11-13T12:46:00Z,1610,2012-11-13T10:38:54Z,/ansible/ansible/issues/1610,ansible,CLOSED,"Template variable stuff broken
Yesterday everything was working fine, but after a git pull; make install my run borked big time.
I got:
Investigating this pointed me to my issue playbook, containing
And the template containing
When I roll back to the code of yesterday, things work again.
It does look as if an undefined variable is used things break. But that's what the is defined is for, so there seems to be a bug in the template stuff of last night.
Could you look into that, please?",MAINTAINABILITY
50,{'login': 'ThoTischner'},2016-08-08T15:04:06Z,16946,2016-08-04T09:39:11Z,/ansible/ansible/issues/16946,ansible,CLOSED,"Hipchat Callback not working
Bug Report
CentOS7
Hipchat Callback: https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/callback/hipchat.py
is not working.
Vars can not be set.
Enable hipchat callback via ansible.cfg whitelisting.
Configure the required Hipchat ENV-Vars.
Run any playbook, following error occurs:
Message send to hipchat room.
Hipchat message not working
MISC
The display error can be solved by changing the callback from:
self.display.warning('
to
self._display.warning('",NONE
51,{'login': 'nickhammond'},2016-05-27T14:19:26Z,10881,2015-04-29T21:10:49Z,/ansible/ansible/issues/10881,ansible,CLOSED,"Add documentation for privilege_escalation/become to ansible.cfg
By looking at the example file here it looks as though the new group would look similar to this:
I can create a pull request to add that section to the docs, just wanted to confirm that will be the correct setup moving forward.
Docs:
Become docs
Ansible configuration file docs
",NONE
52,{'login': 'piffey'},2014-08-14T01:26:29Z,8615,2014-08-14T00:22:49Z,/ansible/ansible/issues/8615,ansible,CLOSED,"Delegated Hosts Don't Use Inventory Details or Confirm Existence In Inventory
I wasn't sure if this was a bug or intended behavior. When I think about the issue, it seems more intuitive that you'd use the same name that you would for a host in your inventory and expect Ansible to use the same connection details when running the task.
Issue Type: Bug Report
Ansible Version: 1.7
Environment: Arch to drive, managing Ubuntu 12.04
Summary:
When running a task with delegate_to it doesn't look the provided host up in the inventory. You can pass it anything and it will try to SSH to it. Since that's the case it also doesn't properly look up the ansible_ssh_port for the delegated host.
Steps To Reproduce:
Hosts File:
(Using 127.0.0.1 as an example, but assume that's a WAN IP with a bunch of NAT one-to-many forwards.)
Playbook:
Expected Results:
Actual Results:
Edit: Seems likely related to Issue 8224.
Edit2: Made the title better. Seemed ambiguous/not as explanatory as it could be.",NONE
53,{'login': 'jyrkiput'},2016-11-01T16:20:21Z,10975,2015-05-11T09:52:19Z,/ansible/ansible/issues/10975,ansible,CLOSED,"SSH connection got stuck when IP and host have different keys in known_hosts
In following situatation, I'd like to have some kind of error report, but now the connection just got stuck.
I reinstalled one machine, and tried to use same IP and host after installation. The initial ansible connection reported nicely
fatal: [node2-jenkins-slave.dev.sysart.fi] => Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this. Please add this host's fingerprint to your known_hosts file to manage this host.
After adding doing this and trying to connect, the connection got stuck
ansible -i hosts node2-jenkins-slave.dev.sysart.fi -m ping -u root -k -vvvv
SSH password:
<node2-jenkins-slave.dev.sysart.fi> ESTABLISH CONNECTION FOR USER: root
<node2-jenkins-slave.dev.sysart.fi> REMOTE_MODULE ping
<node2-jenkins-slave.dev.sysart.fi> EXEC sshpass -d6 ssh -C -tt -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=""/home/jyrki/.ansible/cp/ansible-ssh-%h-%p-%r"" -o GSSAPIAuthentication=no -o PubkeyAuthentication=no -o User=root -o ConnectTimeout=10 node2-jenkins-slave.dev.sysart.fi /bin/sh -c 'mkdir -p $HOME/.ansible/tmp/ansible-tmp-1431337315.21-244074993784678 && echo $HOME/.ansible/tmp/ansible-tmp-1431337315.21-244074993784678'
I think that the reason for this had something to do with the warning I got with ssh
ssh node2-jenkins-slave.dev.sysart.fi
Warning: the ECDSA host key for 'node2-jenkins-slave.dev.sysart.fi' differs from the key for the IP >address '192.168.179.43'
Offending key for IP in /home/jyrki/.ssh/known_hosts:107
Matching host key in /home/jyrki/.ssh/known_hosts:133
After removing both keys and adding the host to known_hosts, everything worked.",SECURITY
54,{'login': 'elouanKeryell-Even'},2015-05-21T02:03:27Z,11039,2015-05-20T12:20:36Z,/ansible/ansible/issues/11039,ansible,CLOSED,"Single * pattern not working
Issue Type:
Bug Report
Ansible Version:
$ ansible --version
ansible 1.9.1
configured module search path = None
Ansible Configuration:
I think I changed nothing to the base configuration.
Environment:
Centos 7
Summary:
In the docs it says it is possible to use a single * pattern to target every host, but it doesn't seem to work for me.
Steps To Reproduce:
$ ansible * -m ping
Expected Results:
Same result as $ ansible all -m ping, which is working fine for me.
Actual Results:
Usage: ansible <host-pattern> [options]",NONE
55,{'login': 'buster'},2014-08-08T17:12:27Z,8512,2014-08-08T09:25:55Z,/ansible/ansible/issues/8512,ansible,CLOSED,"ansible 1.7 passing an argument with newlines to the shell module eats newlines (edited title)
I'm using quite a lot of multiline strings, and according to http://stackoverflow.com/questions/3790454/in-yaml-how-do-i-break-a-string-over-multiple-lines the way this works in YAML is to use | isntead of > to preserve newlines.
This worked in ansible 1.6.x without problems.
Now, newlines are stripped away..
Example:
This should print hi and hi2 in two lines but it doesn't anymore.
It breaks a lot of ansible code for me which, unfortunately, relies on shell scripts.",NONE
56,{'login': 'anaselbouhali'},2017-06-05T12:13:46Z,25350,2017-06-05T09:58:58Z,/ansible/ansible/issues/25350,ansible,CLOSED,"C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.
Bug Report
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.
C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.
",NONE
57,{'login': 'tbielawa'},2013-06-02T21:20:30Z,3099,2013-06-02T20:26:03Z,/ansible/ansible/issues/3099,ansible,CLOSED,"Docs ansible-1.1: ""register"" example broken?
Description:
The example [1] for register appears to be broken.
Steps to reproduce:
Copy example from [1] into a yaml file
Run the file with ansible-playbook
Expected Result:
Ansible prints the message ""motd contains the word hi""
Observed result:
This is failing when it gets to the final 'else' clause in compile_when_to_only_if. I added some debugging statements:
which outputs:
[1] http://ansible.cc/docs/playbooks2.html#register-variables",NONE
58,{'login': 'maxalbert'},2015-08-07T04:10:04Z,10403,2015-03-06T21:03:46Z,/ansible/ansible/issues/10403,ansible,CLOSED,"New keyword for 'with_sequence' in order to skip task if count < 0 or start > end
Issue Type: Feature request
Ansible Version: 1.8.4
Environment: N/A
Summary:
When using with_sequence ""programmatically"" (i.e., in combination with variables), it would be useful to allow a negative value for the count argument, or a start value that is larger than end. Currently this results in an error (""can't count backwards""), but it could be useful to simply skip the task if the range is empty. This could be achieved by adding an extra keyword, e.g. skip_with_emtpy_range (although it should probably be less verbose), which would be False by default for backwards compatibility.
Example:
Expected Results:
The task should be skipped because skip_with_empty_range is True:
Actual Results:
Currently the task fails with the error message ""can't count backwards"" (of course, skip_with_emtpy_range needs to be omitted when running the example above because it is not supported yet).
",FAULT TOLERANCE
59,{'login': 'kustodian'},2016-09-20T13:57:33Z,16365,2016-06-20T07:33:58Z,/ansible/ansible/issues/16365,ansible,CLOSED,"Could not create retry file '*.retry'. [Errno 2] No such file or directory: ''
Bug Report
N/A
Ubuntu
If retry_files_save_path isn't set in ansible.cfg, when a playbook fails, a retry files tries to be created in an empty directory.
Make any playbook fail.
A try file should be created in the user home directory.
A warning message is printed and a retry file is not created:
It looks like the default value for the retry files isn't set to be ~/ like it's mentioned in the documentation.",MAINTAINABILITY
60,{'login': '0mmariano'},2017-04-13T00:40:39Z,23489,2017-04-11T11:18:27Z,/ansible/ansible/issues/23489,ansible,CLOSED,"Unable to use some environment variables
ISSUE TYPE: Bug report
COMPONENT NAME: ansible_env
ANSIBLE VERSION:
ansible 2.2.1.0
config file = /etc/ansible/ansible.cfg
configured module search path = ['/usr/share/my_modules/', '/etc/ansible/roles/glassfish/library']
CONFIGURATION:
defined values under [default]
OS: Debian Jessie
SUMMARY: Ansible is unable to interpolate an environment variable when it is called from a hosts file or a group_vars/all file.
STEPS TO REPRODUCE:
In /etc/profile.d/required_env_vars.sh --
DEPLOY_DIR=/myhome/deployments
In testPlaybook.yml
deploy_dir = ""{{ ansible_env.DEPLOY_DIR }}/RT{{rt_number}}""
where rt_number is defined via vars_prompt
This fails because DEPLOY_DIR is not found in the output of the setup command for the host (looked in the ansible_env), but HOME is. So it looks like Ansible is unable to use environment variables defined in a profile.d script.
As a work-around, I added an [all:vars] section the inventories/poc/hosts file --
[all:vars]
deploy_dir = $HOME/deployments --> did not work
deploy_dir = /myhome/deployments --> worked
deploy_dir = $DEPLOY_DIR --> did not work
deploy_dir = ""{{ ansible_env.DEPLOY_DIR }}"" --> did not work
Since the second one worked (hardcoded value), I removed the [all:vars] section and used an inventories/poc/group_vars/all file --
deploy_dir = /myhome/deployments
This resulted in the error below --
ERROR! Unexpected Exception: dictionary update sequence element #0 has length 1; 2 is required
the full traceback was:
Traceback (most recent call last):
File ""/usr/bin/ansible-playbook"", line 103, in 
exit_code = cli.run()
File ""/usr/lib/python2.7/dist-packages/ansible/cli/playbook.py"", line 132, in run
inventory = Inventory(loader=loader, variable_manager=variable_manager, host_list=self.options.inventory)
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 98, in init
self.parse_inventory(host_list)
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 165, in parse_inventory
group.vars = combine_vars(group.vars, self.get_group_variables(group.name))
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 555, in get_group_variables
self._vars_per_group[groupname] = self._get_group_variables(groupname, vault_password=vault_password)
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 573, in _get_group_variables
vars = combine_vars(vars, self.get_group_vars(group))
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 775, in get_group_vars
return self._get_hostgroup_vars(host=None, group=group, new_pb_basedir=new_pb_basedir, return_results=return_results)
File ""/usr/lib/python2.7/dist-packages/ansible/inventory/init.py"", line 839, in _get_hostgroup_vars
host_results = self._variable_manager.add_group_vars_file(base_path, self._loader)
File ""/usr/lib/python2.7/dist-packages/ansible/vars/init.py"", line 619, in add_group_vars_file
data = self._load_inventory_file(path, loader)
File ""/usr/lib/python2.7/dist-packages/ansible/vars/init.py"", line 577, in _load_inventory_file
rval.update(data)
ValueError: dictionary update sequence element #0 has length 1; 2 is required
On my target server, I also tried to set up DEPLOY_DIR as an environment variable in various places --
/etc/environment -- which is not preferred because it can't do variable interpolation (only key=val pairs, no variables on the right hand side). The output of the env command showed the variables I defined as expected.
/etc/profile -- The output of the env command showed the variables defined also.
/etc/profile.d/required_env_vars.sh -- this is preferred because I'd like to keep /etc/profile the same across all servers, and just add custom environment variables via profile.d script. The output of the env command DID NOT show the variables defined, but doing ""echo $variable"" showed the expected value
None of these affected the content of ansible_env (I thought ansible_env would get the variables if the were returned by the ""env"" command?).
So it looks like Ansible does not include all the environment variables defined in its ""environment"" (ansible_env).
UPDATE 1
Okay. My bad. I changed the content of group_vars/all to use a colon instead of an equal and it worked.
deploy_dir : /home/b013000915/deployments
Got confused because in some cases (e.g. facts.d content, some modules), I had to use an equal sign.
But the other issue (not getting some of the environment variables defined) still persists.
UPDATE 2
I also tried using lookup to get the value of the following variables on the target node but it did not work for custom variables (defined via /etc/profile)
In /etc/profile
DEPLOY_DIR=/myhome/deployments
In testPlaybook.yml
name: check using lookup env
debug:
msg: ""deploy_dir is {{ lookup('env', 'DEPLOY_DIR') }}, home is {{ lookup('env', 'HOME') }}""
Output was
ok: [IP address] => {
""msg"": ""deploy_dir is , home is /myhome""
}",NONE
61,{'login': 'muffl0n'},2015-09-23T12:29:45Z,12473,2015-09-22T14:45:14Z,/ansible/ansible/issues/12473,ansible,CLOSED,"--limit isn't honored at all
Issue Type:
Bug Report
Ansible Version:
Ansible Configuration:
n/a
Environment:
n/a
Summary:
I have a simple inventory with four hosts (master1, master2, slave1, slave2) in two different groups (app-server, app-slave). When I use a host-pattern to select one group (e.g. app-server) and specify a host with --limit, ansible just selects all hosts. It does not matter if the host is contained in the selected group.
Steps To Reproduce:
hosts:
Run some module with this inventory, use a host-pattern and limit it with a host. E.g:
ansible -i hosts -m debug -a ""msg=foo"" app-slave -l master1
ansible -i hosts -m debug -a ""msg=foo"" app-slave -l slave1
Expected Results:
Actual Results:
",PERFORMANCE
62,{'login': 'sivel'},2019-02-08T17:26:45Z,21028,2017-02-03T22:06:56Z,/ansible/ansible/issues/21028,ansible,CLOSED,"Update, remove or migrate CODING_GUIDELINES.md
Documentation Report
CODING_GUIDELINES.md
N/A
N/A
The CODING_GUIDELINES.md file is extremely out of date. It contains information about coding style, tests, etc.
I bean looking into this document, but found the amount of information needing updating a bit daunting for me to undertake at the moment.
This file should either be updated, removed and potentially migrated into the docsite.
N/A
N/A
N/A",MAINTAINABILITY
63,{'login': 'ogai'},2019-06-24T17:43:19Z,12186,2015-09-01T15:02:42Z,/ansible/ansible/issues/12186,ansible,CLOSED,"search filter on an undefined variable returns a non-descriptive error
Bug Report
core
2.1
In templates
Trying to use this template fails with a non-descriptive error when STRINGG is undefined:
The error is:
The error should be:
In plays
The error is:
",NONE
64,{'login': 'maedox'},2017-01-23T17:41:47Z,9966,2015-01-09T14:12:53Z,/ansible/ansible/issues/9966,ansible,CLOSED,"apt_repository: Failed to validate the SSL certificate for launchpad.net:443
Issue Type:
Bug Report
Ansible Version:
ansible 1.8.2
configured module search path = /usr/share/ansible
Environment:
Running from: Linux Mint 17.1 (based on Ubuntu 14.04)
Managing: Ubuntu 10.04, 12.04
Summary:
Adding a PPA with the apt_repository module fails with certificate validation problems.
Manually running add-apt-repository on the host works.
Steps To Reproduce:
Expected Results:
PPA added under /etc/apt/sources.list.d/
Actual Results:
",SECURITY
65,{'login': 'Brian-Williams'},2016-03-28T15:16:47Z,14668,2016-02-25T21:03:52Z,/ansible/ansible/issues/14668,ansible,CLOSED,"scp_if_ssh parameter ignored in ansible 2.0.1.0
Issue Type:
Bug Report
Ansible Version:
ansible 2.0.1.0
config file = /etc/ansible/ansible.cfg
configured module search path = /home/share/library
Ansible Configuration:
$ cat /etc/ansible/ansible.cfg | grep scp
if True, make ansible use scp if the connection type is ssh
scp_if_ssh = True
Environment:
N/A
Summary:
Parameter scp_if_ssh is set to True in ansible.cfg. It fails to connect to host with error message ""unable to open an sftp connection"". It shouldn't be attempting an sftp connection.
Steps To Reproduce:
Run the following on a system that rejects sftp connections:
ansible -m setup
When I uninstall ansible and reinstall 1.9.4 with no configuration changes setup works.
Expected Results:
It uses scp to succeed.
Actual Results:
It attempts to connect with SFTP.
",SECURITY
66,{'login': 'bcoca'},2017-07-28T19:26:17Z,13243,2015-11-20T23:59:27Z,/ansible/ansible/issues/13243,ansible,CLOSED,"Allow multiple vault passwords/files
vault password could keep prompting until empty password is supplied, vault file could take a list of files
This allows for having multiple vault files with different keys, good for ops team having access to all vaults but qa or dev having access only to specific ones",SECURITY
67,{'login': 'mpdehaan'},2013-02-23T18:24:58Z,1461,2012-10-26T23:55:31Z,/ansible/ansible/issues/1461,ansible,CLOSED,"Let the inventory file location be a directory
If the file is a directory, run all items within it, whether script or INI file, and blend the results.
This will allow the inventory directory to be used in conf.d form, or even one group per file.
It will also allow for hybrid EC2/local inventory, etc.",NONE
68,{'login': 'luiseterc'},2015-03-18T00:50:54Z,10471,2015-03-16T12:10:10Z,/ansible/ansible/issues/10471,ansible,CLOSED,"Apt module does not work well when specifying version along with with_items loop
When trying to install several packages with this task:
Only package2 (last one in the loop) is forced to install with 1.0 version. Package1 is installed with the latest version found on the repository. Running the playbook with ""-vvvv"" I can se how the command is eventually executed:
REMOTE_MODULE apt name=package1,package2=1.0* update_cache=yes state=present force=yes
I got it working by rewriting the task as following:
",MAINTAINABILITY
69,{'login': 'agaffney'},2015-11-13T22:39:54Z,13161,2015-11-13T19:35:36Z,/ansible/ansible/issues/13161,ansible,CLOSED,"Debug task is run even though dependent task is skipped
I found a case in ansible 1.9.4 (also present in 1.9.2) where a debug task with when: whatever|success runs even though the task that registers whatever was skipped.
I was able to reproduce with this playbook:
which results in the following output:
",FAULT TOLERANCE
70,{'login': 'wgj'},2012-07-02T22:55:18Z,527,2012-07-02T21:43:58Z,/ansible/ansible/issues/527,ansible,CLOSED,"yum: ansible not able to find package
ansible-playbook/yum module isn't able to find a package that I can find manually. This issue 'goes away' if I install the package manually.
[wes@mgmt001 ~]$ ssh root@host.domain yum clean all
Loaded plugins: fastestmirror
Cleaning up Everything
Cleaning up list of fastest mirrors
[wes@mgmt001 ~]$ ssh root@host.domain yum list epel-release
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Available Packages
epel-release.noarch 5-4 private-repository
[wes@mgmt001 ~]$ ansible-playbook -i /tmp/all_hosts_badtz -f 30 ~/ansible/playbooks/company/company.yml
TASK: [yum: install epel-release] *********************
failed: [host.domain] => {""changed"": false, ""failed"": true, ""msg"": ""No Package matching 'epel-release' found available, installed or updated""}
From playbook:
name: ""yum: install epel-release""
action: yum pkg=epel-release state=latest
",MAINTAINABILITY
71,{'login': 'doiim'},2017-08-02T13:09:54Z,24844,2017-05-19T19:21:13Z,/ansible/ansible/issues/24844,ansible,CLOSED,"Wrong temp path OSX
Bug Report
core
ask_become_pass=True
ask_sudo_pass=True
local_tmp = /Users/mateus/.ansible/tmp
macOS 101.12.5
For some reason Ansible is trying to create a temp file on the wrong directory, since macOS uses /Users/ instead of /home/.
My $HOME variable is fine (/Users/mateus/)
Already tried setting local_tmp and remote_tmp.
As you can see below, the ControlPath is right, only the mkdir is wrong
",NONE
72,{'login': 'yew011'},2016-01-22T22:01:07Z,14081,2016-01-22T19:24:08Z,/ansible/ansible/issues/14081,ansible,CLOSED,"async with command not working
Issue Type:
bug
Ansible Version:
Ansible Configuration:
default
Environment:
Summary:
When doing async reboot using ansible, if I call /sbin/shutdown directly, it works.
However if I ran /bin/sleep first then, nothing happens.
Steps To Reproduce:
syslog:
",NONE
73,{'login': 'hachaboob'},2016-11-11T13:52:54Z,17786,2016-09-27T23:47:50Z,/ansible/ansible/issues/17786,ansible,CLOSED,"Allow group_vars and host_vars to override role variables
Feature Idea
Variable Precedence
N/A
N/A
Change Ansible's variable precedence so that groups_vars and host_vars can override all role variables. Roles should be considered standalone or isolated modules and maintain there own variable precedence. Roles can use conditions to set OS specific variables in their vars folder. All variables defined in a role should be able to be overridden outside of the role by group_vars and host_vars. This would take care of the use case for being able to set OS specific variable defaults in a role and being able to override them.
",NONE
74,{'login': 'maximede'},2019-02-22T16:30:38Z,16775,2016-07-20T17:18:49Z,/ansible/ansible/issues/16775,ansible,CLOSED,"Different behavior for variables used in an included task in a dependent role since 2.0.2.0
Bug Report
roles
N/A
This one is really similar to #16729
When a dependency role (configured with allow_duplicates ) uses a variable set by a dependent role inside an included task and multiple role are using the same dependency role, the dependency role always use the variable defined in the last called role
Create a playbook
Add two role (role1 and role2) with a dependency on another role configured with allow_duplicates: yes (common-role)
Use a variable ( app_name) in an included task in the common role which is defined in role1/vars/main.yml and role2/vars/main.yml
Display the var value in a debug task inside the common role
Directory structure
playbook.yml
role/common-role/meta/main.yml
roles/common-role/tasks/included.yml
roles/common-role/tasks/main.yml
roles/role1/meta/main.yml
roles/role1/vars/main.yml
roles/role2/meta/main.yml
roles/role2/vars/main.yml
ansible 2.0.1.0 ( and the previous versions) was producing this :
ansible 2.0.2+ produces this :
tested with ansible 2.0.2.0 , the stable-2.1 branch and the devel branch, same results.",NONE
75,{'login': 'tidzo'},2018-11-20T19:19:17Z,25020,2017-05-25T10:06:40Z,/ansible/ansible/issues/25020,ansible,CLOSED,"Wait_for should return matches to groups in its search_regex
Feature Idea
wait_for
n/a
n/a
When using the wait_for module to monitor, say, a file for a string matching a particular search_regex, it would be helpful if the matches to that regex were available in the module's output.
In my current use case, I'm monitoring a log file from a remote process which was started asynchronously and want to display certain pertinent information from it in the Ansible output.
The wait_for task will pause until text matching the search_regex is found in somefile.log (or timeout occurs). The text between the parens in the regex will be available somewhere in the variable registered in wait_for task.
The matches are not available in the registered variable.",NONE
76,{'login': 'jctanner'},2016-08-04T15:45:58Z,16951,2016-08-04T15:09:05Z,/ansible/ansible/issues/16951,ansible,CLOSED,"broken plugins cause UnboundLocalError: local variable 'obj' referenced before assignment
Bug Report
N/A
N/A
PluginLoader.all() creates a variable named ""obj"" in try/except, raises a warning if failed and then attempts to use the variable later even though it may not exist.
Create a bad plugin file in ""filter_plugins"" and run a playbook with a debug: var=
No traceback, just a warning.
",NONE
77,{'login': 'dapf73'},2017-05-19T10:16:59Z,23079,2017-03-29T16:27:13Z,/ansible/ansible/issues/23079,ansible,CLOSED,"vmware_guest doesn't create a properly formatted /etc/resolv.conf file
I'm trying Ansible 2.3 RC2
Bug Report
vmware_guest: customization: dns_servers
I want to provision a VMWare VM and populate /etc/resolv.conf, so I've puth this in my playbook:
And defined this variable in a group_vars file
But I get a wrongly formatted /etc/resolv.conf file:
Instead of getting one nameserver line per server, I get all of them in a single line.
Provision the VM using the aforementioned playbook/group_vars file
Get a resolv.conf file like this:
Got a resolv.conf file like this:
",NONE
78,{'login': 'ruimcfreitas'},2017-02-21T16:38:23Z,21725,2017-02-21T16:13:01Z,/ansible/ansible/issues/21725,ansible,CLOSED,"Unable to manage windows server after binding an IIS https site with option ""Require Server Name Identification""
Bug Report
inventory = ./hosts
running Ansible from: Linux ubuntu16_Ansible 4.4.0-59-generic
managing: Windows 2016 with IIS
Unable to manage windows server after binding an IIS https site with option ""Require Server Name Identification""
On the Windows 2016 machine there is a IIS web site with binding for https on port 443 using a certificate.
The ansible commands and playbooks are able to manage this machine.
After I change the binding and select the option ""Require Server Name Identification"" (required to use a single IP address to service multiple sites with certificates using ""host name"") ansible stops communication with server.
I can only restore communication after I remove the binding and restart the windows server.
win2016gui2 | SUCCESS => {
""changed"": false,
""ping"": ""pong""
}
",FAULT TOLERANCE
79,{'login': 'defionscode'},2014-07-10T18:29:55Z,8035,2014-07-03T18:22:59Z,/ansible/ansible/issues/8035,ansible,CLOSED,"ansible_memfree_mb fact combines disk-cache use of memory
Issue Type: Bug Report
Ansible Version: ansible 1.6.3
Environment: N/A
Summary:
I setup a play to alert whenever memory utilization is dangerously high. I immediately received a handful of alerts and was a bit concerned until I realized it takes into account memory being used by the disk cache and not 'real' memory utilization.
Steps To Reproduce:
To reproduce just compare the ansible_memfree_mb fact, which in my example is
with the return value of running
Expected Results: I expected 'real' memory available
Actual Results:
In my case returns
As you can see, the ansible_facts are correct but misleading, in this case saying only 10gb are free when in fact I have 13gb that are useable.
Perhaps this could be added as a ansible_nocache_memfree_mb fact?",PERFORMANCE
80,{'login': 'slyall'},2016-02-27T16:37:37Z,9686,2014-12-01T22:48:38Z,/ansible/ansible/issues/9686,ansible,CLOSED,"Include inventory modules in distribution
Currently inventory modules are only available on github.
It would be cool if they could be bundled in with the release so they can be used without needing to be separately downloaded",NONE
81,{'login': 'Val'},2015-11-12T16:09:39Z,13140,2015-11-12T15:08:23Z,/ansible/ansible/issues/13140,ansible,CLOSED,"Missing `xsltproc` Debian packaging README
packaging/debian/README.md does not include installation of xsltproc which provides missing local eponym command.
",NONE
82,{'login': 'willthames'},2017-03-13T21:41:26Z,6545,2014-03-18T06:42:32Z,/ansible/ansible/issues/6545,ansible,CLOSED,"Ansible inventory allows groups to have same name as hosts
Issue Type
Bug Report
Ansible Version:
ansible 1.6 (devel 9da26da) last updated 2014/03/18 11:15:04 (GMT +1000)
Environment:
N/A
Summary:
If you define a group containing another group without the :children, then the intended group is a host, but will also become a group when defining its contents.
The net result is that the host that should inherit characteristics of several layers of parents will not inherit those characteristics as the definitions will be:
grandparent -> parent(host)
parent(group) -> host
rather than
grandparent -> parent -> host
I believe that it should not be possible to define a group with the same name as a host (or at least a warning suggesting it might not be what you want should happen)
Steps To Reproduce:
See https://gist.github.com/willthames/9614054
Expected Results:
See https://gist.github.com/willthames/9614054
Actual Results:
See https://gist.github.com/willthames/9614054",FAULT TOLERANCE
83,{'login': 'rhaido'},2014-03-16T15:50:13Z,5994,2014-02-13T13:33:10Z,/ansible/ansible/issues/5994,ansible,CLOSED,"Inconsistent expansion of the variables in the lookup 'with_items'
Issue Type:
Bug Report
Ansible Version:
Production:
Fresh clone (13.02.2014)
Environment:
Summary:
Dear Ansible Dev Team,
While migrating code from old ${...} variable expansion to new Jinja2 style {{...}}, I've noticed some incompatibilities with the previous syntax, while I would expect {{...}} be a full equivalent of ${...}, i.e the following case should work just fine:
where jdk_next is a list of values:
Unfortunately, it does not work.
Steps To Reproduce:
Create the playbook, which uses roles, for example server.yml:
And put the mentioned vars in role's ""base"" vars/ and mentioned task in role's ""base"" tasks/, as it should be.
Execute ansible playbook.
Expected Results:
With previous syntax:
- the result is just fine:
Actual Results:
With the code written in Jinja2 style, I've received with the bad result:
Then, even more interesting: I found, that it's not possible anymore specify variable enclosed in {{..}} as a first member of iteration sequence like this:
The result is a syntax error:
While the old syntax was ok:
Quick debug & hacking showed the following things:
if you use original anisble syntax, the file expansion string, which arrives in the variable varname of the template() function of utils/template.py, is [u'jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45'], which is correct
if you use jinja2 syntax, the final expansion string, which arrives in the variable varname of the template() function of utils/template.py is [u""jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']""] which is incorrect.
I do not have comments about SyntaxError I've provided earlier as I have no time to debug it :(
Workaround
This hack is acceptable but still ugly - the idea is to use the join() Jinja2 filter, i.e. the following code:
works:
",MAINTAINABILITY
84,{'login': 'dagwieers'},2012-11-23T16:15:23Z,1657,2012-11-22T02:48:47Z,/ansible/ansible/issues/1657,ansible,CLOSED,"Implement RHEL5 python-dmidecode support
For older systems lacking sysfs I plan to implement python-dmidecode support in setup as a backup.",SCALABILITY
85,{'login': 'maxwo'},2018-04-26T21:44:39Z,24373,2017-05-08T13:23:49Z,/ansible/ansible/issues/24373,ansible,CLOSED,"include_once is skipped when required roles are partially skipped
I use an include_role task in a role, but it is skipped when I use a conditional depency role.
I tried to find more infos in the documentation, but there is nothing I can find to explain it if I do anything wrong...
Bug Report
include_role or meta/dependencies
Default configuration
Mac OS X / Debian
I use a include_role task which is skipped because I use conditional required roles.
roles/shell/meta/main.yml
roles/shell/tasks/main.yml
In the dotfiles role, nothing special, no required role, just some unconditional tasks:
roles/dotfiles/tasks/main.yml
The last include_role should be played.
The last include_role is skipped, whether it is played on a Debian machine or a Mac OS machine.
When I remove the role dependencies, everything is fine.
",MAINTAINABILITY
86,{'login': 'dan-mcdonald'},2014-09-29T20:21:30Z,8547,2014-08-11T16:44:19Z,/ansible/ansible/issues/8547,ansible,CLOSED,"`postgres_user` module doesn't work with AWS RDS databases
Issue Type:
Bug report
Ansible Version:
ansible 1.7.0
Environment:
OSX Mavericks 10.9.4
Summary:
When running against an AWS RDS Postgresql instance, the postgres_user module can't set up new users.
Steps To Reproduce:
Run this task:
Expected Results:
Actual Results:
Apparently the pg_authid relation is not available in RDS.
Possible workaround: if access denied for pg_authid, then always set the password.",MAINTAINABILITY
87,{'login': 'walterdolce'},2018-01-18T21:08:34Z,22411,2017-03-08T15:08:36Z,/ansible/ansible/issues/22411,ansible,CLOSED,"letsencrypt module does not create any cert when using DNS-based validation
Bug Report
letsencrypt
Ansible version is 2.2.1.0
It looks like the Let's Encrypt Ansible module doesn't create any cert when running with the DNS-based validation.
No errors are being thrown or anything, it provisions just fine
There should be a cert generated",SECURITY
88,{'login': 'hyperized'},2017-05-11T21:33:17Z,23579,2017-04-13T15:14:48Z,/ansible/ansible/issues/23579,ansible,CLOSED,"Memory load increased in 2.3.0 compared to 2.2.0.1 (high memory use, high ram use)
Bug Report
Ansible
Ubuntu 14.04.5 (exclusively) hosts & clients
Increased memory usage when 2.3.0 is rolled out: http://imgur.com/a/ZgUCe
Recovery takes place when 2.2.0.1 is reverted.
Install Ansible 2.3.0
Run regular job schedule (6x a day a full run of ~100 roles, several irregular jobs like backups), started from the Rundeck job scheduler over regular SSH.
Similar memory usage.
http://imgur.com/a/ZgUCe",PERFORMANCE
89,{'login': 'dghubble'},2014-04-02T20:09:20Z,6694,2014-03-26T07:39:51Z,/ansible/ansible/issues/6694,ansible,CLOSED,"Homebrew module fails silently after fresh Homebrew installs
Reproduce (done on a fresh OSX Mavericks install + Command line tools)
Run the homebrew installer script.
ruby -e ""$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)""
Completes. Run brew doctor. Everything looks good.
Note that the installer does not (and should not) create /usr/local/Cellar, this is created when the first brew package is installed. So right now no Cellar exists.
Try running a simple playbook with localhost inventory:
ansible-playbook myplaybook -i localhost_inventory
PLAY [My playbook] ******************************************************
GATHERING FACTS ***************************************************************
ok: [localhost]
TASK: [Install brew packages] *********************************************
ok: [localhost]
PLAY RECAP ********************************************************************
localhost : ok=2 changed=0 unreachable=0 failed=0
You'll get something that completes instantly, seems to indicate ack was installed(ok), but it wasn't. No Cellar exists. ack is seriously not installed.
Now go ahead and make the /usr/local/Cellar directory. Run the config again. Boom, suddenly it actually works. There is a noticeable delay as ack is installed and ack immediately works after the playbook completes.
Try deleting the Cellar completely and you're back to the original broken behavior.
A clean install seems like a pretty simple edge case that should be supported. What's up with this? Lemme know if I'm going crazy.",MAINTAINABILITY
90,{'login': 'nirik'},2014-03-01T00:17:09Z,6228,2014-03-01T00:11:15Z,/ansible/ansible/issues/6228,ansible,CLOSED,"ansible 1.5 breaks gather_facts+accelerate
Issue Type:·
Bug report
Ansible Version:·
ansible 1.5
Environment:
Both RHEL6 and Fedora rawhide.·
Summary:
Any plays with both:·
gather_facts: true
accelerate: true
Fail with:·
PLAY [foo] ********************************************************************·
GATHERING FACTS ***************************************************************·
fatal: [td] => Incorrect permissions on ACCELERATE_KEYS_FILE (/home/kevin/.fireball.keys/td)
ansible never actually connects to the host that I can tell,·
perhaps it's thinking accelerate is already started when it's not?
The directory it refers to doesn't exist (since it never connected to the host)
Steps To Reproduce:
Create a playbook with:·
gather_facts: true
accelerate: true
and at least one host. Run it.·
Expected Results:
Facts are gathered and rest of playbook runs.·
Actual Results:
Ansible-playbook fails at gathering facts and errors out.",NONE
91,{'login': 'Yajo'},2018-07-23T07:10:45Z,24295,2017-05-04T13:23:30Z,/ansible/ansible/issues/24295,ansible,CLOSED,"Clearing facts (to refresh them) does not work
Bug Report
meta
setup
None.
If you need to reload some facts (in my case I need the VPN IP after I configured it), it does not work
Option A:
Option B:
Facts reloaded, VPN ip data available.
Option A:
... but later another task fails with:
Option B:
WORKAROUND
Run the playbook again 😞",NONE
92,{'login': 'greg-hellings'},2017-08-29T00:58:58Z,26199,2017-06-28T20:53:31Z,/ansible/ansible/issues/26199,ansible,CLOSED,"jenkins_plugin - incorrect ""changed"" and silent install failures
Bug Report
jenkins_plugin
default configuration
This seems independent of versions, but I see it when running from Fedora while controlling Fedora, CentOS 7, and RHEL 7 machines with Jenkins 1.651.3 running on them.
Some plugins, after being installed, are either not installed to the latest version (despite no value being specified for the version) or are not installed at all, despite the module reporting success. On subsequent runs, these same plugins continue to report a changed/updated edition despite there being no change in the version being installed.
An example of some plugins where this behavior has been noticed:
antisamy-markup-formatter (version 1.1 installed, despite 1.5 being available)
scriptler (reports installed, but the plugin fails to be installed)
dynamic-parameter (same as scriptler)
And many others. However, the behavior does not affect all plugins.
On clean CentOS system, run the following playbook: https://gist.github.com/greg-hellings/14f58eb19a4992b910f27e53f63573b4
Plugins are installed to the latest version if version is specified as latest.
Spurious ""changed"" values are not reported from the module when nothing gets updated.
The module errors when a plugin install error occurs.
No version specified results in both plugins reporting back ""changed"" when neither the version updates (antisamy-markup-formatter) or the plugin is not installed at all (scriptler).
With the line version: latest added to that final task in the sample file, the scriptler install fails while the antisamy-markup-formatter still reports ""changed"" without actually updating anything.",MAINTAINABILITY
93,{'login': 'brandond'},2017-08-22T15:11:38Z,22471,2017-03-10T00:47:19Z,/ansible/ansible/issues/22471,ansible,CLOSED,"ec2_group: add tags
From @jbrockett on December 4, 2014 15:15
Issue Type:
Feature Idea
Component name:
ec2_group
Ansible Version:
ansible 2.3
Environment:
N/A
Summary:
Please add the ability to create and modify tags associated with the security group. At least being able to set the Name tag would be helpful.",SECURITY
94,{'login': 'oppianmatt'},2014-10-30T11:51:00Z,9456,2014-10-30T11:45:51Z,/ansible/ansible/issues/9456,ansible,CLOSED,"service: nginx enabled=yes not enabling service ansible 1.7.2
Issue Type:
Bug Report
Ansible Version:
1.7.2
1.6.6
Environment:
Destination: Ubuntu 12.04.4 LTS
Control hosts: Mac OSX and Ubuntu 12.04.4 LTS
Summary:
When trying to enable nginx, which is a typical init.d startup script. It appears to not have any affect.
Have tried with both ansible versions and with Mac OSX and Ubuntu 12.04 control hosts. Destination host is a Ubuntu 12.04
Steps To Reproduce:
Install nginx from deb and it will create a /etc/init.d/nginx script
In a task write:
Run the playbook.
Expected Results:
Service enabled
Actual Results:
Reports changed every time:
chkconfig and rc.d directories unchanged:
",NONE
95,{'login': 'peterjanes'},,20231,2017-01-13T15:44:50Z,/ansible/ansible/issues/20231,ansible,OPEN,"Include all dependency roles in include search path for role
Feature Idea
 - Bug Report
include
roles_path set to both my local directory and left as default.
Fedora 25
Files present in a parent role's tasks directory are not found by child tasks' include statements.
Unzip ansible-test.zip and execute ansible-playbook test.yml. Note that
works, but
fails.
parent-task.yml should be found as if it were part of the child task that inherits from it. This would follow the standard (single) role behaviour where ""Any copy, script, template or include tasks (in the role) can reference files in roles/x/{files,templates,tasks}/ (dir depends on task) without having to path them relatively or absolutely"".
Adding static: yes to avoid the warning:
",NONE
96,{'login': 'abadger'},2016-09-27T16:30:05Z,17246,2016-08-25T16:30:59Z,/ansible/ansible/issues/17246,ansible,CLOSED,"Vault bytes <=> text string API
Bug Report
lib/ansible/parsing/vault/init.py
unittests for vault
The new vault API is mixing bytes and text in inappropriate ways.
In working on enabling unittests for Python3, I came across the fact that vault unittests were skipping a lot of tests depending on whether they were running on python3 or python2. Closer examination revealed that the tests were either giving different values to the API or receiving different values back depending on the version of Python that we are running on. This is problematic API design that we need to change before release.
Good API should follow one of these rules for input and output:
Take bytes and return bytes
The old vault API attempted to use this strategy for ""internal"" functions. Many of the functions inside of vault were not called from outside of parsing/vault/init.py. Since they also dealt with operations involving byte strings (encoding, decoding, hexlifying, writing to disk, etc) it made sense that they dealt solely in bytes.
Take text and return text
It is relatively easy to create this sort of API in python3 as combining text and bytes leads to immediate tracebacks. In python2, it is easy to mix this up with one of the other strategies below as ascii-only bytes and text strings will combine. Most of ansible's current API does not follow this strategy because we don't trust that the data coming in is going to be the string type we expect. As we secure our borders (and with python3 tests throwing errors when these are combined inappropriately) we should be able to move more API to this model.
Take either bytes or text, normalize internally, and return text
This is the strategy that a lot of the old external Vault API was taking. We didn't trust that the rest of Ansible was properly sending us text strings so as the first step we used to_unicode and to_bytes to make sure that the string we were dealing with was the correct type.
Take either bytes or text, normalize internally, and return bytes
Old internal vault API may have taken this strategy. For internal API we should know that our inputs are only text or only bytes but we may have been paranoid. Since the data we passed around was often pure-ascii, this couldn't cause tracebacks.
Take either bytes or text. If bytes were input then output bytes. If text was input then output text
I would not recommend this for any of our Vault API. You'll see this in some Python stdlib API like os.path.abspath() (a good usage of the strategy) or os.listdir() (a bad usage of the strategy). It is appropriate when the purpose of the function is a straightforward text transformation and there is a need to give callers a version that works with text and a version that works with bytes. This is not the case for Vault's internal API (where we can adapt the callers to use just one API) and it is not needed for Vault's external API (where we should probably always be returning text). This strategy can also be used for functions which are operating inside of a larger ""native string"" data model as the code run on python2 should be taking in bytes and outputting bytes while the code on python3 is taking in text and outputting text. Note, though, that this strategy does not validate the input or output while technically the native string model could validate that only bytes was accepted on python2 and only text was accepted on python3.
Whichever strategy is followed, be sure that the same strategy is being applied for both Python2 and Python3. Most projects (including Ansible) are working towards a single code base that runs on both python2 and python3. Writing API that expects different types and returns different types on one or the other hampers this overall design.
Also note, it is tempting to do validation of the strings you receive. I'd hesitate to do that because all checks have a cost. Even asserts which are really meant for this purpose invokes the cost because no one really runs ansible in python optimized mode where asserts are stripped out. My general stance has been if you trust the data coming in, there's no need to validate it. If you don't trust the data, you should use to_bytes() or to_unicode() to accept either bytes or text and normalize to the type you want to operate on.",NONE
97,{'login': 't2d'},2013-01-18T23:26:15Z,1861,2013-01-12T13:50:52Z,/ansible/ansible/issues/1861,ansible,CLOSED,"hacking/module_formatter build error in Debian [0.9]
I tried to build ansible 0.9 on Debian Squeeze, but the make script fails.
",NONE
98,{'login': 'nathanhruby'},2013-12-15T16:33:31Z,5270,2013-12-12T17:56:19Z,/ansible/ansible/issues/5270,ansible,CLOSED,"Add support for ""build-dep"" to apt module
As a system administrator I want to install build dependency packages for developer and buildhost machines using the same syntax I install normal packages with.
It would be useful if one could install build-deps via the apt module instead of mixing and matching apt module and commands for ""apt-get build-dep """,NONE
99,{'login': 'berlic'},2017-03-29T23:11:20Z,22988,2017-03-27T07:49:11Z,/ansible/ansible/issues/22988,ansible,CLOSED,"sequence lookup shortcut syntax doesn't work and wrong docs
Bug Report
Documentation Report
sequence lookup plugin
2.1, 2.3 and 2.4 have the same bug.
Standard config
N/A
with_sequence shortcut syntax [start-]end[/stride][:format] is not honoured.
Loop over [5,6]:
Parameter error:
Also there is wrong parameters passing in the docs:
Parameters can be passed to with_sequence only as string, not as list or dict.",FAULT TOLERANCE
100,{'login': 'VeryVito'},,14452,2018-02-03T20:48:49Z,/flutter/flutter/issues/14452,flutter,OPEN,"Dragging a list that is currently handling an animateTo animation throws exception
The end goal is to create a scrolling list that automatically ""snaps"" to a given location based on the actual scroll position at which the user stops scrolling. This may not be the best solution (I'm new to Flutter), but here's the process by which I get the crash:
Wrap a ListView within a NotificationListener. Upon receiving a UserScrollNotification with ScrollDirection.idle, call the ScrollController's animateTo() method to scroll to another location (in the example here, I arbitrarily scroll to 10000.0 pixels).
Sample code
It works great as long as the user only interacts with the ListView when it is idle, but if the user attempts to scroll/drag/touch the list during the animateTo() process, the following exception is raised within the Flutter Scrollable package:
After this, the ListView becomes unresponsive.
I'm sure there's a better way to achieve what I'm trying to do, but the platform itself doesn't seem to recover from this. Figured I'd make a note of it.
Thanks!
Run your application with flutter run and attach all the log output.
Run flutter analyze and attach any output of that command also.
Flutter Doctor
Hope this helps! Thanks!",FAULT TOLERANCE
101,{'login': 'sethladd'},2017-03-03T00:28:44Z,6651,2016-11-02T17:03:45Z,/flutter/flutter/issues/6651,flutter,CLOSED,"Request for a ""Thinking in Flutter"" doc
We seem to have a documentation gap, between ""Getting Started"" and ""Tour of the Widget Framework"". In a recent UX study, we noticed that the high-level concepts and the unique bits of Flutter aren't clearly called out, which made it more challenging for a new user to wrap their heads around what Flutter is and how it thinks.
Initial thoughts on this doc:
Audience is brand new users for Flutter
Intended for the ""ok, what's this flutter thing? what's different?""
Intended to be the next doc you read, after getting started, and before you start diving in
Fairly short, high level
Initial thoughts on table of contents:
What is Flutter?
what problem(s) does it solve
who is it for
what's special?
Functional-reactive framework
widget lifecycle
setState
Only rebuilding what needs to be rebuilt (efficient)
Customizable/extensible
Material design
full set of widgets out of the box
Dev cycle
Hot reload
Fast restart
Tooling
CLI
IntelliJ
Plugins/interop
coming soon!
Once a new user scans through this doc, they should know
What's special about flutter?
What does functional-reactive mean, for Flutter?
setState is a thing, and why not to be scared of it
How to dev cycle
Where to find additional info on widgets, layout, plugins, etc
Thanks!
(this came up in a recent UX study)",USABILITY
102,{'login': 'sethladd'},2016-04-05T20:05:37Z,3070,2016-04-04T16:17:13Z,/flutter/flutter/issues/3070,flutter,CLOSED,"Tab underline not moving on first swipe
Please check out this video for an example: https://dl.dropboxusercontent.com/u/316685/video.mp4
In the Buttons page, in the gallery app, swipe left slowly. Notice how the tab underline under FLOATING doesn't animate during swipe.
Then, complete the swipe, and RAISED will be underlined. Then, swipe left and right, and the underline is animated.",NONE
103,{'login': 'Kleak'},2019-10-01T11:24:49Z,30690,2019-04-08T13:02:34Z,/flutter/flutter/issues/30690,flutter,CLOSED,"firebase_ml_vision scanning datamatrix is really not accurate.
clone flutter/plugins
cd packages/firebase_ml_vision/example
flutter run (on a real device)
scan some datamatrix
Here is an album with 4 almost exactly same photo but only one work :
https://photos.google.com/share/AF1QipOUTCX6xK77XqMfz4yBki2d3eygi_0GQqNwfiHgDkwl1x_eAodtMVSucjXLgBOIsA?key=Z2lUcnA5bzM5ZWtYOFAwaUNRNzNDOXNGLUlnZUFn
You can use it for testing
What is expected
The scanning should work and return the data in the datamatrix
What happen
The plugin doesn't find the datamatrix in the image most of the time.
Remark
On the emulator the plugins work as expected
Flutter doctor
",NONE
104,{'login': 'sethladd'},2016-03-10T23:37:00Z,2586,2016-03-10T22:07:39Z,/flutter/flutter/issues/2586,flutter,CLOSED,"App is white, then quits, on iPod
I created a brand-new project with flutter create, and then flutter run to my iPod. The app is installed onto the device, but when I run it, it's a pure white screen, and then approx 10s later it quite.
Output of the logs:
",NONE
105,{'login': 'PeterKW'},2019-02-26T13:46:44Z,28512,2019-02-26T13:41:30Z,/flutter/flutter/issues/28512,flutter,CLOSED,"Grid View Builder Catch RenderFlex Overflow
I am a student who has only recently started to learn Flutter.
I have a GridView which builds a dynamic number of grids. If the devices screen is too small sometimes the number of grids overflow and this comes through to the console:
Is there any way I could add a try / catch block somewhere which could remove cards from the buttonsList until there aren't enough cards on the screen to overflow?
I couldn't find anywhere to catch the error message outputted in the terminal during runtime.
Any ideas on how I could fix this?
Thank you all very much!",NONE
106,{'login': 'vithanichandresh'},2018-06-25T11:36:50Z,18792,2018-06-25T11:28:43Z,/flutter/flutter/issues/18792,flutter,CLOSED,"is flutter have concept like android “addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK)”
here is flow thats i want to manage back
1 - on click home page's button start store list page.
2 - on click store list page's button start filter page.
3 - on click store filter page's button store list page.
4 - on press back go to home page.",NONE
107,{'login': 'collinjackson'},2019-10-13T18:18:39Z,32181,2019-05-07T01:18:35Z,/flutter/flutter/issues/32181,flutter,CLOSED,"firebase_storage Content-Type autodetection not working on Android
The integration test passes on iOS and fails on Android.
",NONE
108,{'login': 'sethladd'},2017-02-01T00:36:56Z,7766,2017-01-31T23:53:34Z,/flutter/flutter/issues/7766,flutter,CLOSED,"`flutter run --release` stuck when building for iOS: ""library not found for -lPods-Runner""
My flutter run --release for my iPhone is stuck at:
Any ideas?
and
",NONE
109,{'login': 'Hixie'},2017-04-08T00:14:55Z,8888,2017-03-20T05:57:14Z,/flutter/flutter/issues/8888,flutter,CLOSED,"Rename BlockBody and fix references to RenderBlockBase
I thought we had decided to rename BlockBody, but I can't find a bug on it anymore.",NONE
110,{'login': 'jy03773322'},2017-02-24T12:29:37Z,8373,2017-02-23T17:38:47Z,/flutter/flutter/issues/8373,flutter,CLOSED,"FormatException: Could not parse ""EAP AI-162.3715353""?
log:
Flutter crash report; please file at https://github.com/flutter/flutter/issues.
command
flutter doctor
exception
FormatException: Could not parse ""EAP AI-162.3715353"".
flutter doctor
",NONE
111,{'login': 'SuriahKamaruddin'},2019-11-07T13:17:03Z,29031,2019-03-08T06:20:16Z,/flutter/flutter/issues/29031,flutter,CLOSED,"The parameter 'options' isnt defined and the method GoogleMapOptions isnt define for class
I have problem with the ""options: GoogleMapOptions"" part, where the options is not define. Am i missing something? Im new with flutter.
This is my main.dart
**import 'package:flutter/material.dart';
import 'package:google_maps_flutter/google_maps_flutter.dart';
import 'dart:async';
void main() => runApp(MyApp());
class MyApp extends StatelessWidget {
// This widget is the root of your application.
@override
Widget build(BuildContext context) {
return MaterialApp(
title: 'Google Map',
theme: ThemeData(
primarySwatch: Colors.blue,
),
home: MyHomePage(title: 'Google Map'),
);
}
}
class MyHomePage extends StatefulWidget {
MyHomePage({Key key, this.title}) : super(key: key);
final String title;
@override
_MyHomePageState createState() => _MyHomePageState();
}
class _MyHomePageState extends State {
@override
Widget build(BuildContext context) {
}
}**
where the pubspec i had include
google_maps_flutter:
git:
url: git://github.com/flutter/plugins
path: packages/google_maps_flutter",NONE
112,{'login': 'jhionan'},2018-06-27T09:22:31Z,18763,2018-06-24T04:57:18Z,/flutter/flutter/issues/18763,flutter,CLOSED,"Tabview error
have a bottom navigation bar, and the 2 tab is have appbar + 2 tabs.
get error when clicking bottomNavigationBar 3 to 1 without clicking at 2
https://github.com/jhionan/flutterError
Error:
",NONE
113,{'login': 'vy0592'},2017-01-23T22:52:33Z,7203,2016-12-08T19:13:31Z,/flutter/flutter/issues/7203,flutter,CLOSED,"Calling popAndPushNamed throws exception
If there's more than one route in the current stack and when popAndPushNamed is called(or calling pop and then push), the following exception will be thrown:
",FAULT TOLERANCE
114,{'login': 'tvolkert'},2019-04-24T05:38:42Z,10656,2017-06-13T15:25:03Z,/flutter/flutter/issues/10656,flutter,CLOSED,"Make `flutter test` verify that target package depends on `flutter_test`
Example test failures visible at https://travis-ci.org/flutter/plugins/jobs/242412801
",NONE
115,{'login': 'goderbauer'},2017-10-27T19:47:24Z,11204,2017-07-13T18:45:27Z,/flutter/flutter/issues/11204,flutter,CLOSED,"a11y: Semantics Tree doesn't update fast enough for scrolling
On the Gallery App homepage in Android, select an item in the list of demos and start swiping right multiple times really fast. Continue swiping right when the end of the list is reached. Android will scroll and attempt to focus the next item in the list. However, the Semantics Tree has not been updated yet, focusing the next item fails (because from Android's a11y perspective there is no element) and the boing sound is played. After a couple of swipes, the semantic tree has been updated and everything works as expected.",PERFORMANCE
116,{'login': 'xster'},2019-10-25T00:46:33Z,32946,2019-05-18T00:04:41Z,/flutter/flutter/issues/32946,flutter,CLOSED,"Offer options that balances embeddable Flutter load time with resource consumption
We should identify the right balance point between loading the embedding / vm / isolate / framework / userland element tree / resources in the tree based on each part’s load time vs its memory use.
This involves #32945 to audit cold init time and also auditing memory consumption.
Our recommended embedding strategy + default behaviors from APIs should be reasonably balanced between fast loading and memory usage.
Document alterations users can make given those APIs such as prewarming/keeping the vm only, prewarming/keeping the isolate. Flutter side APIs to flush parts of the tree when window is not visible.",PERFORMANCE
117,{'login': 'mxdwjcty'},2019-01-05T13:59:18Z,26063,2019-01-04T08:56:12Z,/flutter/flutter/issues/26063,flutter,CLOSED,"app run crash after plugin image picker installed
my device is Redmi note 5",NONE
118,{'login': 'timbergus'},2019-10-15T22:33:56Z,25167,2018-12-10T20:50:39Z,/flutter/flutter/issues/25167,flutter,CLOSED,"Empty application after publishing it in iTunes Connect (Firebase related)
I'm pretty sorry for the lack of information, and I'm sure this is something I'm doing wrong, but I don't know where else I can ask. I have a pretty small application that shows a list of values from a static file. I uploaded it to iTunes Connect and downloaded it using TestFlight. Everything works just fine in my iPhone X.
I have modified the application to add a Firebase connection with a Firestore DDBB. If I run the application from Android Studio directly in my phone, it works, and it shows the data from the database. If I archive the app and upload it to iTunes Connect, and install it through TestFlight, when opening the application, it just appears as a white screen. The application works but shows nothing, but an empty white screen.
Do you know what is happening or how I can get information that can help to solve my problem?
Regards",NONE
119,{'login': 'xster'},2018-07-09T19:41:50Z,14076,2018-01-12T20:23:58Z,/flutter/flutter/issues/14076,flutter,CLOSED,"Extract modal bottom action sheet concept from Material to Widgets
There are multiple Cupertino concepts of a modal dismissable action sheet at the bottom of the screen too such as
https://developer.apple.com/ios/human-interface-guidelines/views/action-sheets/ and https://developer.apple.com/ios/human-interface-guidelines/controls/pickers/. The current showModalBottomSheet helper mechanism is in bottom_sheet.dart and has a lot of Material concepts baked in. Extract into common utility.",NONE
120,{'login': 'sdivakarrajesh'},,24969,2018-12-05T05:08:01Z,/flutter/flutter/issues/24969,flutter,OPEN,"Should the modal barrier of an Alert also change the color of the status bar?
Create a custom Status Bar color with
Then create an AlertDialog that shows up on an event
When the AlertDialog is fired up the custom status bar color stays bright and its opacity is not affected
",NONE
121,{'login': 'cvolzke4'},2019-05-13T01:06:50Z,32445,2019-05-09T23:51:53Z,/flutter/flutter/issues/32445,flutter,CLOSED,"webview_flutter: showDialog displays the dialog behind any webviews
Create an app with a widget that contains a WebView.
In that widget, add a WillPopScope widget.
In the WillPopScope.onWillPop event handler, call showDialog() to show a dialog.
Flutter doctor output
[✓] Flutter (Channel beta, v0.11.9, on Mac OS X 10.14.4 18E227, locale en-US)
• Flutter version 0.11.9 at /Users/cvolzke/development/flutter
• Framework revision d48e6e4 (6 months ago), 2018-11-20 22:05:23 -0500
• Engine revision 5c81474
• Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)
[!] Android toolchain - develop for Android devices (Android SDK 28.0.3)
• Android SDK at /Users/cvolzke/Library/Android/sdk
• Android NDK location not configured (optional; useful for native profiling support)
• Platform android-stable, build-tools 28.0.3
• Java binary at: /Library/Java/JavaVirtualMachines/jdk-10-latest/Contents/Home/bin/java
• Java version Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)
✗ Android license status unknown.
[!] iOS toolchain - develop for iOS devices (Xcode 10.0)
• Xcode at /Applications/Xcode.app/Contents/Developer
• Xcode 10.0, Build version 10A255
✗ Verify that all connected devices have been paired with this computer in Xcode.
If all devices have been paired, libimobiledevice and ideviceinstaller may require updating.
To update with Brew, run:
brew update
brew uninstall --ignore-dependencies libimobiledevice
brew uninstall --ignore-dependencies usbmuxd
brew install --HEAD usbmuxd
brew unlink usbmuxd
brew link usbmuxd
brew install --HEAD libimobiledevice
brew install ideviceinstaller
• ios-deploy 1.9.4
• CocoaPods version 1.5.3
[!] Android Studio (not installed)
• Android Studio not found; download from https://developer.android.com/studio/index.html
(or visit https://flutter.io/setup/#android-setup for detailed instructions).
[!] IntelliJ IDEA Ultimate Edition (version 2018.1.4)
• IntelliJ at /Applications/IntelliJ IDEA.app
✗ Flutter plugin not installed; this adds Flutter specific functionality.
✗ Dart plugin not installed; this adds Dart specific functionality.
• For information about installing plugins, see
https://flutter.io/intellij-setup/#installing-the-plugins
[!] Connected device
! No devices available",NONE
122,{'login': 'fzhangtj'},,30434,2019-04-03T09:15:48Z,/flutter/flutter/issues/30434,flutter,OPEN,"assertion in material/input_decorator.dart seems not correct
The last line assert(!renderObject.slotToChild.keys.contains(slot)) seems incorrect, since slot is for _RenderDecorationElement. It should assert !renderObject.slotToChild.keys.contains(renderObject.childToSlot[child]));",NONE
123,{'login': 'collinjackson'},2016-02-01T18:42:50Z,839,2015-12-09T19:09:49Z,/flutter/flutter/issues/839,flutter,CLOSED,"iOS simulator builds have clear toolbar
Ran flutter init and launched on an iOS simulator using https://github.com/flutter/engine/wiki/Flutter-Apps-on-iOS
Got an invisible toolbar background (you can faintly see the white text overlaid on it):
Running on an iOS device works fine.
Other sample apps are also having issues with the simulator, but this is probably the simplest reduced test case.",NONE
124,{'login': 'Hixie'},,1147,2016-01-08T19:22:07Z,/flutter/flutter/issues/1147,flutter,OPEN,"MaterialApp should have a debugShowKeylines mode
We should have a mode where we show the keylines from this picture, exactly the way they're shown in this picture:
https://www.google.com/design/spec/layout/metrics-keylines.html#metrics-keylines-ratio-keylines",MAINTAINABILITY
125,{'login': 'pcomans'},2017-08-29T18:56:26Z,11760,2017-08-23T18:07:01Z,/flutter/flutter/issues/11760,flutter,CLOSED,"firebase_database: FirebaseAnimatedList should expose index to itemBuilder
This is regarding firebase_database 0.0.14
I noticed that the itemBuilder for FirebaseAnimatedList has the following typedef:
The itemBuilder of AnimatedList has the following typedef:
The index of the current element being rendered is essentially being hidden by FirebaseAnimatedList. It would be very useful for creating numbered lists / striped lists etc.
Is this omission by design? Can we add index to FirebaseAnimatedListItemBuilder?
I am aware that this is a breaking change so I'm open to feedback.",MAINTAINABILITY
126,{'login': 'cbracken'},,32156,2019-05-06T19:01:22Z,/flutter/flutter/issues/32156,flutter,OPEN,"Improve Flutter's memory consumption
This is a tracking bug for the collection of sub-issues around improving Flutter's memory consumption, and offering developers more flexibility in choosing where to set the dial on the time performance vs memory consumption spectrum.
Related issues:
 #13493: Flutter should provide more control over image caching
 #26187: Flutter should be smarter about memory limits for images
 #16995: Free resources after a Flutter view is disposed
 #19358: Unmount everything and dispose states when host activity dies
 #23231: Memory leaks on iOS
 #20690: More deterministic measurement of memory consumption.
 #26081: Disable in-memory decoded frame cache by default
 #26443: Consider mipmapping ui.Images generated by the engine
 #15479: OnMemoryPressure is unreliable on Android
 #25155: Raster cache images may be much bigger than the visible/clipped area
 #19558: IO thread GrContext memory needs to be cleaned up
 #44013: Persist the memory profile timeline
",PERFORMANCE
127,{'login': 'Hixie'},2018-06-01T07:30:45Z,3112,2016-04-05T19:02:08Z,/flutter/flutter/issues/3112,flutter,CLOSED,"Support progress bar ""query indeterminate"" mode
https://www.google.com/design/spec/components/progress-activity.html#progress-activity-types-of-indicators",NONE
128,{'login': 'angel1st'},,28647,2019-02-28T12:48:54Z,/flutter/flutter/issues/28647,flutter,OPEN,"[firebase_admob] - app compilation crashed after the plugin has been added
Plugin added to pubspec.yaml. Below is the list of dependencies section:
Used version 0.7.0 according to AndroidX compatibility list
Added to AndroidManifest.xml
where:
APP_ID is my app id e.g. com.program.myapp and
ADMOB_APP_ID is the AdMob id (I have registered AdMob account)
Note: My app is NOT yet published on Google play
Added to android level build.gradle
Added to app level build.gradle
Start building. Below is the output
flutter run --verbose log
flutter doctor -v log
Side notes:
In case step 4 is omitted, I can compile the app, but then I get The Google Mobile Ads SDK was initialized incorrectly. although I believe all other settings are just fine.
I also tried the most recent plugin version and it has the identical behavior.
",NONE
129,{'login': 'alevittoria'},2018-06-21T15:28:54Z,18675,2018-06-21T13:32:01Z,/flutter/flutter/issues/18675,flutter,CLOSED,"firebase_messaging notification doesn't show if app is killed
#Hi! I copy the example from this link:
https://github.com/flutter/plugins/tree/master/packages/firebase_messaging/example
But it works only with app in foreground or in background.
If I send a notification with the app killed from task manager, the notification doesn't show.
This is what I send on body :
Someone can help me?",NONE
130,{'login': 'miguelpruivo'},2018-05-29T09:44:59Z,17978,2018-05-28T18:59:20Z,/flutter/flutter/issues/17978,flutter,CLOSED,"Add iOS dependency to Flutter Plugin 
So far I've the plugin (standalone) and the app created. Now I need to (somehow) use and consume a static framework (obj-c) in iOS plugin.
I've the file plugin.h and plugin.m and I added the mylib.framework to the root of iOS folder, next to podspec file. Also tried to add the s.dependency 'mylib' to it. Still, it doesn't seem to work at all. Everytime I try to flutter build ios on my main app (using the plugin) I get the #import <myplugin/myplugin.h> not found
PS: The library works if I add to the iOS project on the Flutter app, but I MUST use it on a plugin.",NONE
131,{'login': 'OrionWambert'},,26292,2019-01-09T14:59:46Z,/flutter/flutter/issues/26292,flutter,OPEN,"The site don't work fine
Hi guys, it's normal for the mobile version of the site to show up like that in the widget catalog and the Widget index
",NONE
132,{'login': 'lrd7512369'},2019-11-08T10:37:51Z,32472,2019-05-10T07:44:26Z,/flutter/flutter/issues/32472,flutter,CLOSED,"Can not make project when open for editing in android studio
l am writing an application with flutter and it works well at my android device ,but it builds failed when l click ""open for editing in android studio"" ,hope someone can help me,
flutter doctor:
here are the error logs:
",NONE
133,{'login': 'zhanghao19920218'},2018-08-20T15:54:38Z,18746,2018-06-23T05:35:57Z,/flutter/flutter/issues/18746,flutter,CLOSED,"Running flutter create myapp by VSCode always have problems
When I try to create a flutter project by visual studio code, lib and test file always get a red signal and it stay in creating Running ""flutter packages get"" in myapp... status",FAULT TOLERANCE
134,{'login': 'abarth'},2017-01-05T19:09:00Z,7204,2016-12-08T19:19:22Z,/flutter/flutter/issues/7204,flutter,CLOSED,"Remove ignoreTransform and highQuality options from MaskFilter
The ignoreTransform doesn't make much sense in a composited system because the local transform (which you're attempting to ignore) will depend on the compositing strategy, which means the ouput will depend on the compositing strategy.
Apparently the quality is ignored in Skia's GPU backend.",NONE
135,{'login': 'sarah-fernandez'},2019-03-08T00:39:54Z,28984,2019-03-07T08:38:44Z,/flutter/flutter/issues/28984,flutter,CLOSED,"InputEventReceiver exception dispatching input event with Flutter 1.2.1 and Android JellyBean 4.1.2
Using Android Studio. I have two different applications (one is a game, and the other is a set of widgets to better manage the space in tablets).
They were running smoothly in flutter 1.1.2 beta for devices from API 16 up to API 27, and emulators from API 21 up to API 28.
After flutter upgrade to flutter 1.2.1 beta, both applications in my device with API 16 fails at the moment of touching anything inside the application with the same error.
Then the application dies and a dialog that reads Unfortunately, <application_name> has stopped and an exception is thrown through the log file.
Both applications works fine in a Samsung Galaxy Nexus API 17 Jelly Bean MR1 and up to a Moto G5 Plus API 27, so this is a problem only in Jelly Bean API 16.
Both applications start fine. But touching anything in the screen we get the following log error:
flutter analyze results:
flutter doctor -v:
",FAULT TOLERANCE
136,{'login': 'mpetricone'},2018-11-23T13:54:41Z,24676,2018-11-23T13:54:17Z,/flutter/flutter/issues/24676,flutter,CLOSED,"missing localization data != null
...
...
...
",NONE
137,{'login': 'aam'},2018-12-05T14:31:58Z,14497,2018-02-06T18:33:18Z,/flutter/flutter/issues/14497,flutter,CLOSED,"Request to hot reload fails (on Windows)
This is on be0c488 running on Pixel.",NONE
138,{'login': 'sethladd'},2017-02-02T14:45:46Z,5385,2016-08-15T16:22:21Z,/flutter/flutter/issues/5385,flutter,CLOSED,"google_sign_in: auto-publish API docs
We should probably update http://flutter.github.io/google_sign_in/ manually. It would be super cool to automate the publishing of that site.
Our users are finding these sites and asking questions about them :)",USABILITY
139,{'login': 'jonahwilliams'},2018-07-30T17:45:25Z,19963,2018-07-29T22:13:10Z,/flutter/flutter/issues/19963,flutter,CLOSED,"DataTable checkbox tap target is too small
The checkbox is only given the minimum possible width, currently 18. it should be at least 48 by 48.",USABILITY
140,{'login': 'wmleler'},2018-12-17T14:59:35Z,10159,2017-05-18T00:49:09Z,/flutter/flutter/issues/10159,flutter,CLOSED,"Documentation for keytool
On https://flutter.io/android-release/ we should add some documentation that explains what keytool does and especially say where it leaves the key.jks file. It wasn't in the current directory, nor in the app directory. I had to search and finally found it in my home directory (ick!).",USABILITY
141,{'login': 'sethladd'},2016-04-07T20:36:06Z,2208,2016-02-26T16:47:07Z,/flutter/flutter/issues/2208,flutter,CLOSED,"remove ""flutter start""
Which is the real one? If start is deprecated in favor of run, maybe we can print out ""start is replaced by run. Please update your scripts, as we will soon remove start.""",NONE
142,{'login': 'FunnyDevs'},2017-02-24T12:27:52Z,8388,2017-02-24T09:42:00Z,/flutter/flutter/issues/8388,flutter,CLOSED,"Host OS not supported on Windows
Hi! I'm trying Flutter on Windows platform and i have installed everything with no problem. I created the app from command line but when i try to launch it, (from command line or even from IntellijIDEA plugin) i get this error.
UnimplementedError: Host OS not supported.",NONE
143,{'login': 'lukaselmer'},,12137,2017-09-17T12:31:41Z,/flutter/flutter/issues/12137,flutter,OPEN,"_googleSignIn.signIn() deadlock?
_googleSignIn.signIn() does not seem to work correctly?
This may be related to
flutter/plugins#94
#10552
You can reproduce it doing the flutter-firebase codelab (https://codelabs.developers.google.com/codelabs/flutter-firebase/#5), using Android 8 (SDK 26). There seems to be some kind of deadlock when calling this method.
It also does not seem to work using the following versions:
We also found out that you can sign in like this:
call _googleSignIn.signIn()
select the user
we noticed a very short flicker at the top of the screen, but that's it
if you use await _googleSignIn.signIn(), the code below is never executed
there is also no exception thrown
(deadlock in async call?)
close the app, so that it stops running in the background
open the app again
sign in using _googleSignIn.signInSilently(): this works now because the account has been selected already
",NONE
144,{'login': 'eseidelGoogle'},,5232,2016-08-04T21:43:18Z,/flutter/flutter/issues/5232,flutter,OPEN,"AppBar Hero Transition could crossfade 
https://youtu.be/KbyuPMZ-9_A
@abarth says we should fix this, but to do so would require re-writing the Hero system (which has to happen for other reasons). Just noting down one example of where being able to fade/animate-color would be nice.
",MAINTAINABILITY
145,{'login': 'Sun3'},,16474,2018-04-11T20:55:35Z,/flutter/flutter/issues/16474,flutter,OPEN,"New Feature Request - TabController disable Slide Transition
New Feature - TabController disable Slide Transition
Ability to Disable or change the Horizontal Slide Transition when changing Tabs. I would like to switch pages by Fading instead of sliding left or right.
Thank you.",MAINTAINABILITY
146,{'login': 'OrionWambert'},2019-05-27T10:05:46Z,32556,2019-05-11T15:36:11Z,/flutter/flutter/issues/32556,flutter,CLOSED,"Flutter_web initialization error
Hello ! Please, I'm trying to create a new project flutter_web but I have an error in pub get :
I reinstalled git but the problem persists , I also launched the pub cache repair but to no avail
Flutter doctor output :
",NONE
147,{'login': 'rachelward9'},2017-06-28T18:23:10Z,10731,2017-06-15T18:14:43Z,/flutter/flutter/issues/10731,flutter,CLOSED,"Building Beautiful UIs Tutorial - Silly Link Display
On step six under Place the message list, there is a chopped up link on, ""Naming the argument...""
",NONE
148,{'login': 'yyoon'},2019-02-05T00:54:52Z,11183,2017-07-13T00:02:51Z,/flutter/flutter/issues/11183,flutter,CLOSED,"Text cursor doesn't blink when the focus is requested programmatically within onSubmitted callback
I wanted to retain keyboard focus in a TextField even after the text is submitted, so that I can type in multiple messages in a row in a chat app. The default behavior is to unfocus when the text is submitted, so I added a FocusScope.of(context).requestFocus() call in my handleSubmit callback.
With this change, after submitting the text, the text field goes into a weird state where it has focus (i.e. I can type text) but the cursor isn't blinking.
Below is the minimal example code that reproduces this issue.
",NONE
149,{'login': 'danong'},2019-01-07T17:28:37Z,19582,2018-07-19T21:24:00Z,/flutter/flutter/issues/19582,flutter,CLOSED,"Flutter Gallery app crashes when switching between app bar options in the ""Contact profile"" study
Switching between the different app bar display options in the ""Contact profile"" study can cause the Flutter Gallery app to crash. I tested this on a OnePlus3T using the production version on the PlayStore and locally using the version in v0.5.1.
@HansMuller I think this is behind the error in #17598
Open the ""Contact profile"" study
Scroll the bottom of the page
Select the drop down menu in top right of the app bar and choose the ""App bar snaps"" option
Scroll up until the full app bar is displayed (screenshot attached)
Select the ""App bar floats"" option
The app will not immediately crash but it is in a bad state and further actions will cause various exceptions and crashes. Logs attached below.
Error when scrolling:
Error when trying to switch to the 'App bar snaps' option:
Error when clicking the back button
Error when trying to re-open the Contact profile study after the last crash
",NONE
150,{'login': 'KuoGao'},2018-10-08T08:59:31Z,20249,2018-08-06T07:22:45Z,/flutter/flutter/issues/20249,flutter,CLOSED,"Command “flutter create –t module xxx” is not work
I want to Add Flutter to existing apps , but command ""flutter create –t module xxx"" is not work,
exec command ""flutter create –t module xxx"" , and output ""Multiple output directories specified.""
Doctor summary (to see all details, run flutter doctor -v):
[✓] Flutter (Channel master, v0.5.8-pre.241, on Mac OS X 10.12.6 16G1510, locale zh-Hans-CN)
[✓] Android toolchain - develop for Android devices (Android SDK 27.0.3)
[!] iOS toolchain - develop for iOS devices
✗ Xcode installation is incomplete; a full installation is necessary for iOS development.
Download at: https://developer.apple.com/xcode/download/
Or install Xcode via the App Store.
Once installed, run:
sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer
✗ libimobiledevice and ideviceinstaller are not installed. To install, run:
brew install --HEAD libimobiledevice
brew install ideviceinstaller
✗ ios-deploy not installed. To install:
brew install ios-deploy
✗ CocoaPods not installed.
CocoaPods is used to retrieve the iOS platform side's plugin code that responds to your plugin usage on the Dart side.
Without resolving iOS dependencies with CocoaPods, plugins will not work on iOS.
For more info, see https://flutter.io/platform-plugins
To install:
brew install cocoapods
pod setup
[✓] Android Studio (version 3.1)
[!] IntelliJ IDEA Community Edition (version 2018.2)
✗ Flutter plugin not installed; this adds Flutter specific functionality.
✗ Dart plugin not installed; this adds Dart specific functionality.
[✓] Connected devices (1 available)
! Doctor found issues in 2 categories.",NONE
151,{'login': 'BerndWessels'},2020-01-31T20:53:04Z,18099,2018-06-01T01:54:58Z,/flutter/flutter/issues/18099,flutter,CLOSED,"NestedScrollView cannot scroll inner list view
I have a NestedScrollView with a SliverAppBar with an expandedHeight and a SliverFixedExtentList inside. Basically this example.
Now I want to jumpTo position 5000.0 in the list.
So I add a ScrollController to the NestedScrollView. But the range that I can jumpTo seems to be limited to the height of the header.
I am not able to programatically scroll the SliverFixedExtentList to any position.
I believe this is because the NestedScrollView doesn't allow access to the innerController, but that is just a guess.
Is there a way to achieve this any other way?
Why, because I try to have a TabBar and each tab scrolls the ListView underneath to a specific section - basically like in the UeberEats app's menu page.",NONE
152,{'login': 'sethladd'},2016-02-24T20:58:41Z,2130,2016-02-24T20:45:10Z,/flutter/flutter/issues/2130,flutter,CLOSED,"broken link to android studio
(reported by user)
on https://flutter.io/setup/
broken link
https://flutter.io/setup/https//developer.android.com/sdk/index.html
correct link
https://developer.android.com/sdk/index.html",NONE
153,{'login': 'jimbeveridge'},2016-03-06T19:24:57Z,2326,2016-03-02T18:26:37Z,/flutter/flutter/issues/2326,flutter,CLOSED,"Non-actionable assert: '_placeholderSize == null' is not true.
The assertion '_placeholderSize == null' is not true. actually means that liftToOverlay was called twice. The text in the assertion should include this helpful hint.
Steps to Reproduce
Use a Mimicable
Call liftToOverlay a second time before the first has finished.
Flutter Version
Flutter from https://github.com/flutter/flutter.git (on master)
Framework: 9ce6bff (2 days ago)
Engine: 006a702
Logs
",NONE
154,{'login': 'zoechi'},2017-12-13T18:37:54Z,13510,2017-12-12T15:56:27Z,/flutter/flutter/issues/13510,flutter,CLOSED,"Querying for a non-existent location waits forever
As far as I know this should return immediately in any case, not only when a value is found,
but in Flutter it only returns after the first event which may never come.
This makes it impossible to check if a location exists.
firebase_database 0.1.2
Flutter Doctor
For more information about diagnosing and reporting Flutter bugs, please see https://flutter.io/bug-reports/.
See also https://stackoverflow.com/questions/37910008/check-if-value-exists-in-firebase-db",NONE
155,{'login': 'gggustafson'},2018-12-06T13:28:54Z,19956,2018-07-29T19:13:07Z,/flutter/flutter/issues/19956,flutter,CLOSED,"Issue from website page Building Layouts in Flutter
From URL: https://flutter.io/tutorials/layout/
In the second figure, the caption to the right that reads ""Column"" should read ""Rows""",NONE
156,{'login': 'Hixie'},2020-01-06T20:13:25Z,17360,2018-05-07T22:04:41Z,/flutter/flutter/issues/17360,flutter,CLOSED,"When generating golden files, catch the case of orphan files
Sometimes when writing a test you change your mind about what file names to use, but you do so after having run it with the old names, and then check in the unused files. We should be able to catch that case since when generating goldens we know every test we run and know every file it generated.
This might require that we put files from tests in a directory specific to the test.
cc @tvolkert",MAINTAINABILITY
157,{'login': 'dikeboy'},2018-12-30T16:11:51Z,25875,2018-12-29T09:55:13Z,/flutter/flutter/issues/25875,flutter,CLOSED,"canvas.drawImageRect can't output the image
When i use the drawRect, The out byte can display on the image widget,
But when i use the drawImageRect, I found the out byte always be the same code , and it can't display on the image widget. But I can draw it on the CustomPainter
If I change the image2 to the source image, It will be ok.
How can i do to get the cut image
",NONE
158,{'login': 'vijayredme'},2019-11-03T20:25:59Z,29992,2019-03-26T19:16:08Z,/flutter/flutter/issues/29992,flutter,CLOSED,"Unable to debug / hit breakpoints in flutter app in the visual studio code
I am using vs code and trying to debug the application, but when i hit 'Start Debugging' button in vs code, app simply launches and never hits breakpoint. anyone has a clue asto what could be wrong ?",NONE
159,{'login': 'AskYous'},2018-09-22T21:07:45Z,22175,2018-09-22T20:43:51Z,/flutter/flutter/issues/22175,flutter,CLOSED,"Could not find a file named ""pubspec.yaml"" in ""...pub.dartlang.org/archive-2.0.4"".
Navigate to a flutter project
run flutter doctor (or any flutter command)
Experience this output:
Can't run anything with flutter to get some logs. It just gives me the above response.",NONE
160,{'login': 'chandeepadissanayake'},2018-09-07T12:03:54Z,21545,2018-09-07T10:13:11Z,/flutter/flutter/issues/21545,flutter,CLOSED,"Orientation(Rotation) Issue of Camera Plugin
Even with the provided example, I'm experiencing an issue with camera plugin's orientation when device is rotated. I confirmed this error across different android versions. When the orientation is portrait, everything is fine, working so well. But when we turn around the device, the camera view is 90 degrees off the correct view in clockwise direction, that it should provide. I will attach few screenshots which would help to describe the issue more precisely.
Portrait View
Landscape View
",NONE
161,{'login': 'goderbauer'},2018-09-10T09:50:52Z,21632,2018-09-10T08:32:55Z,/flutter/flutter/issues/21632,flutter,CLOSED,"Engine roll caused regression
With engine roll 6a8a73c the following benchmark regressed:
futter_gallery_ios32__transition_perf/99th_percentile_frame_rasterizer_time_millis
/cc @bkonyi",NONE
162,{'login': 'daniel-v'},2018-12-10T15:39:39Z,16421,2018-04-10T17:36:28Z,/flutter/flutter/issues/16421,flutter,CLOSED,"Request: firebase_storage to support metadata
I could use support reading/writing metadata, especially custom metadata field.",MAINTAINABILITY
163,{'login': 'yczhangsjtu'},2020-01-03T06:01:33Z,31045,2019-04-15T05:37:55Z,/flutter/flutter/issues/31045,flutter,CLOSED,"Is this a typo? assert(transform.determinant != 0.0);
I found this line of code in flutter/lib/src/painting/matrix_utils.dart, line 166, commit: 27b058a
I guess it is expected to be:
However, if my guess is correct, there is another issue which causes this assertion to fail. Followed are the steps to reproduce
Replace determinant with determinant() as above;
Open project flutter_gallery in Android Studio;
Start running main.dart in Android simulator;
As the application is launched, tap 'Material -> Bottom app bar' to open the bottom app bar demo;
Tap 'None' radio in FAB shape, the Floating Action Button should disappear;
Tap 'Circular' or 'Diamond' radio in FAB shape.
The console output looks as follows
",NONE
164,{'login': 'xster'},2018-03-08T10:10:56Z,14373,2018-01-31T00:57:14Z,/flutter/flutter/issues/14373,flutter,CLOSED,"Unclear how to add/edit platform view code to an existing Flutter app
More or less the reverse of #8945.
Given a new Flutter project, if some code needs to be done in the platform side:
 How to open Xcode, how is the iOS code structured
 How to open Android Studio, how is the Android code structured
 How to bring in existing local iOS/Android source code
 How to refer to existing iOS/Android packages using gradle / CocoaPods
cc @sethladd, @Sfshaza",NONE
165,{'login': 'lesnitsky'},,26829,2019-01-21T00:45:28Z,/flutter/flutter/issues/26829,flutter,OPEN,"firebase_admob pauses video when app goes to background
When user switches app during rewarding video playback and brings it back to active state, video is paused. Yes, if user clicks on ""close"" button he/she is prompted to resume the video, but it would be nice to have an API to automatically continue playing video",NONE
166,{'login': 'sethladd'},2016-04-07T19:46:10Z,2832,2016-03-22T16:51:41Z,/flutter/flutter/issues/2832,flutter,CLOSED,"Double shadow flash when moving component list underneath app bar
Scrolling the component list up under the app bar flashes the shadow at the border between the app bar and the component list.
Movie: https://dl.dropboxusercontent.com/u/316685/RECORDING.mp4",NONE
167,,2018-09-06T08:07:08Z,21476,2018-09-06T02:00:24Z,/flutter/flutter/issues/21476,flutter,CLOSED,"VScode autoformat intending spaces
I tried to auto-format my code in vscode with a flutter extension. And this gave me this result
Sometimes it uses 2 spaces, and sometimes it uses 4 spaces. Is this working correctly?",NONE
168,{'login': 'Tricky12321'},,31049,2019-04-15T07:45:43Z,/flutter/flutter/issues/31049,flutter,OPEN,"Testing buttons using tester.tap to show a dialog gives Framework Error
Have a method open an AlertDialog
Have a button execute the method
In a test, use await tester.tap(find.byKey(const Key('LoginBtnKey'))); to tap the button
Then verify that the dialog is shown using expect(find.byType(AlertDialog), findsOneWidget);
This gives a framework error.
",NONE
169,{'login': 'xorrior'},2019-04-25T18:15:42Z,31617,2019-04-25T18:07:12Z,/flutter/flutter/issues/31617,flutter,CLOSED,"firebase_admob ^0.8.0+4 Android crashes on startup (Release version only)
So I'm having issues getting the flutter firebase_admob plugin to work on Android (Release). It initializes and loads ads perfectly for the debug build on Android and the debug/release builds on iOS. I've already made the necessary changes to the AndroidManifest.xml file and included my admob app id (See screenshot). I'm really lost at this point. I've looked at all of the available solutions for the issue I'm having and none have worked. Please help!
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/TwitterApi.dart:30:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/TwitterApi.dart:35:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/TwitterApi.dart:41:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/TwitterApi.dart:47:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/TwitterApi.dart:58:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/TwitterApi.dart:65:3 • sdk_version_async_exported_from_core
info • This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final:
DeleteProgressDialog.progress, DeleteProgressDialog.timeRemaining • lib/ui/DeleteProgressDialog.dart:5:7 • must_be_immutable
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/ui/Login.dart:22:1 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/ui/Login.dart:76:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/ui/Login.dart:130:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/ui/Login.dart:138:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/ui/Login.dart:146:3 • sdk_version_async_exported_from_core
info • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •
lib/ui/Login.dart:245:3 • sdk_version_async_exported_from_core
info • This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final: UserInfoHeader.user •
lib/ui/UserInfoHeader.dart:5:7 • must_be_immutable
[✓] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale en-US)
• Flutter version 1.2.1 at /Users/xorrior/flutter
• Framework revision 8661d8a (2 months ago), 2019-02-14 19:19:53 -0800
• Engine revision 3757390
• Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)
[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)
• Android SDK at /Users/xorrior/Library/Android/sdk
• Android NDK location not configured (optional; useful for native profiling support)
• Platform android-28, build-tools 28.0.3
• Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java
• Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)
• All Android licenses accepted.
[✓] iOS toolchain - develop for iOS devices (Xcode 10.2.1)
• Xcode at /Applications/Xcode.app/Contents/Developer
• Xcode 10.2.1, Build version 10E1001
• ios-deploy 1.9.4
• CocoaPods version 1.5.3
[✓] Android Studio (version 3.4)
• Android Studio at /Applications/Android Studio.app/Contents
• Flutter plugin version 34.0.2
• Dart plugin version 183.5901
• Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)
[✓] IntelliJ IDEA Ultimate Edition (version 2019.1.1)
• IntelliJ at /Applications/IntelliJ IDEA.app
• Flutter plugin version 34.0.4
• Dart plugin version 191.7019
[✓] VS Code (version 1.33.1)
• VS Code at /Applications/Visual Studio Code.app/Contents
• Flutter extension version 2.25.1
[✓] Connected device (1 available)
• Pixel 3 XL • 93VY18Y3R • android-arm64 • Android 9 (API 28)
",NONE
170,{'login': 'd-silveira'},2018-10-08T09:14:32Z,21068,2018-08-27T16:37:59Z,/flutter/flutter/issues/21068,flutter,CLOSED,"Hot reloading a page with streambuilder bugs out
I have a Streambuilder listening to a behaviourSubject broadcastStream, but whenever I Hot Reload or Hot Restart the code for any small change, The streambuilder stops seing what's on the tip of the stream, forcing me to re-do what I originally did to fill the stream",NONE
171,{'login': 'DavidMihola'},2015-11-09T20:13:32Z,53,2015-11-08T20:33:37Z,/flutter/flutter/issues/53,flutter,CLOSED,"`flutter start` doesn't give good error messages when it can't find main.dart
When I try to flutter start any of the example programs widgets, fitness or rendering I get an error like the following:
Downloading Sky Snapshot from the cloud, one moment please...
Downloading Sky Shell from the cloud, one moment please...
[1108/213053:FATAL:loader.cc(21)] Check failed: base::ReadFileToString(path, &source). /home/dmta/flutter/flutter/examples/rendering/./lib/main.dart
The other examples I have tried worked fine.",NONE
172,{'login': 'skytz'},2016-03-23T17:19:49Z,2844,2016-03-23T17:17:23Z,/flutter/flutter/issues/2844,flutter,CLOSED,"ARM Support
Not exactly an issue but a question.
Im working for a small dev company and we're talking about using flutter in production.
There is no deadline in the next 6 months.
Will it be an arm version by then?
And as a secondary question, will it be stable enough?
Thank you",NONE
173,{'login': 'jibeex'},2019-11-06T15:42:10Z,20912,2018-08-22T13:49:24Z,/flutter/flutter/issues/20912,flutter,CLOSED,"Issue on new instructions for integrating Flutter into existing iOS project. 
The previous version worked fine. However after I have followed the new instructions(updated yesterday) for integrating Flutter into existing iOS project. This error came up when I push a FlutterViewController
",NONE
174,{'login': 'jonahwilliams'},2019-03-13T07:32:18Z,27446,2019-02-02T22:52:12Z,/flutter/flutter/issues/27446,flutter,CLOSED,"Update DevFS to use a Watcher
Instead of scanning the project directory on hot reload, setup a Directory watcher to report updated dart files. When hot reloading, grab the current set and pass them to the resident compiler.
Since we no longer need to sync source files to the device, the extra work we're doing to setup the devfs for each potential entry is wasted. We only need to sync the dill file and assets.
Additionally, including build_runner will drastically increase the number of files that will be scanned with the current approach - while there is no issue with a watcher based approach.",PERFORMANCE
175,{'login': 'danrubel'},2016-09-08T21:37:39Z,5674,2016-08-31T14:55:47Z,/flutter/flutter/issues/5674,flutter,CLOSED,"flutter tool provide hint when/how to install iOS simulator
Clean install (no iOS simulator)
flutter create test-ios
flutter run
At this point it responds with No connected devices. which is correct but unhelpful. I would like it to hint that I could install the iOS simulator and point to docs explaining how to do so. Maybe flutter doctor should indicate that it is not installed / configured as well.
Flutter Doctor
",USABILITY
176,{'login': 'Hixie'},2016-02-16T01:39:48Z,1211,2016-01-12T23:53:56Z,/flutter/flutter/issues/1211,flutter,CLOSED,"RenderObject exceptions in performLayout should also include information about what the child(ren) are
Would help debug bugs like #1210 .",NONE
177,{'login': 'Hixie'},2017-09-08T00:02:03Z,11380,2017-07-24T21:44:15Z,/flutter/flutter/issues/11380,flutter,CLOSED,"TextAlign RTL
We probably need to create an RTL-aware version of TextAlign that is used by TextPainter, then use that in RenderParagraph, RichText, and Text so that you can align start/end as well as left/right. Also ""justify"" needs to define the justification of the last line.",MAINTAINABILITY
178,{'login': 'goderbauer'},2018-03-20T07:26:11Z,15274,2018-03-08T01:08:20Z,/flutter/flutter/issues/15274,flutter,CLOSED,"a11y for password text fields
Look at what the behavior on native is (e.g. is content read out?) and ensure that flutter does the same.",NONE
179,{'login': 'mehmetf'},2017-01-11T17:28:53Z,4944,2016-07-18T17:31:42Z,/flutter/flutter/issues/4944,flutter,CLOSED,"Android overscroll indicator is causing a reload/repaint
Use Block to render a scrollable page.
The first time top or bottom is reached, observe that the content is reloaded.
Second and subsequent times the overscroll indicator is shown correctly.
Problem does not occur if we manually switch off the overscroll indicator (or switch to the iOS bounce indicator).
Flutter Doctor
[✓] Flutter (on Linux, channel alpha)
• Framework revision 9c0c022 (3 weeks ago), engine revision bb98655
[✓] Android toolchain - develop for Android devices (Android SDK 23.0.2)
• Platform android-23, build-tools 23.0.2
• OpenJDK Runtime Environment (build 1.8.0-google-v7-123992248-123972143)",NONE
180,{'login': 'bigflood'},2018-10-29T11:24:19Z,6979,2016-11-22T06:46:08Z,/flutter/flutter/issues/6979,flutter,CLOSED,"Support keyboard type on multiline text inputs (Android)
There's keyboard types for single-line inputs only.
Multiline text inputs need return key.",NONE
181,{'login': 'manujbahl'},2018-07-06T09:25:47Z,15943,2018-03-26T18:23:31Z,/flutter/flutter/issues/15943,flutter,CLOSED,"App does not work after flutter upgrade.
I just tried to upgrade flutter to 0.2.4
The engine upgrade worked but post upgrade flutter doctor reports errors and my app also does not work anymore.
",NONE
182,{'login': 'chinmaygarde'},2016-02-20T01:21:41Z,1799,2016-02-12T08:06:52Z,/flutter/flutter/issues/1799,flutter,CLOSED,"`--debug-port 0` on iOS is not ""magical""
Assuming this means the default port.",NONE
183,{'login': 'Hacker-CB'},2019-01-26T13:29:37Z,27124,2019-01-26T12:19:56Z,/flutter/flutter/issues/27124,flutter,CLOSED,"plugins/video_player: java.lang.NullPointerException in gradle
There is NullPointerException during executing gradle tasks from the android directory of the project which has dependency to video_player plugin:
Gradle version: 5.1.1
Full gradle scan: https://scans.gradle.com/s/4uiq56gz6bv7u
Update:
Seems that this happens when two dependencies are together:
Steps to reproduce
flutter create test_project
add next dependencies to pubspec.yaml:
cd android
./gradlew tasks
",NONE
184,{'login': 'ricamgar'},2019-03-08T19:34:29Z,19806,2018-07-26T07:02:38Z,/flutter/flutter/issues/19806,flutter,CLOSED,"[image_picker] Camera permission not needed
Hi,
since the plugin is not using the camera, just requesting the intent, the CAMERA permission is not needed. Here the tutorial on how to use the camera intent from the official docs: https://developer.android.com/training/camera/photobasics
Thanks!",NONE
185,{'login': 'jonahwilliams'},2020-02-04T18:54:53Z,31645,2019-04-26T05:25:28Z,/flutter/flutter/issues/31645,flutter,CLOSED,"Teach flutter test to build_runner
Experiment with build_runner and modular kernel as a pre-compilation step (swappable with the current frontend_server approach).
Open questions:
How do compilation strategies compare on the current benchmarks for small apps? For large ones? Namely: 1st run, repeated run, repeated run with changes, et cetera. See the current test benchmarks.
Does build_runner give us a better path forward for test integration into other tools?
Does this allow us to better unify web and native tests?
Can we ship the build_rules in the engine with the current build_runner integration strategy?
",NONE
186,{'login': 'vshalvaghasiya'},2019-03-15T01:27:02Z,17902,2018-05-25T11:12:38Z,/flutter/flutter/issues/17902,flutter,CLOSED,"Android dependency 'com.android.support:recyclerview-v7' has different version for the compile (23.0.1) and runtime (27.1.1) classpath. You should manually set the same version via DependencyResolution
",NONE
187,{'login': 'xster'},,17161,2018-05-01T19:18:42Z,/flutter/flutter/issues/17161,flutter,OPEN,"Attach widget creation source in build/paint profile timeline events
Would be nice in places like https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/widgets/framework.dart#L3627 and https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/rendering/object.dart#L122 to also point out where the widget was created.
cc @jacob314 I think we chatted about this but I forgot where.",NONE
188,{'login': 'mit-mit'},2017-05-17T16:02:45Z,7817,2017-02-02T15:06:13Z,/flutter/flutter/issues/7817,flutter,CLOSED,"Plugin: Battery
Tracks a simple battery plugin. We will write this as a first-party plugin to serve as a sample of how to write a plugin.
It will integrate with to BatteryManager on Android, and UIDevice on iOS",NONE
189,{'login': 'baeharam'},2019-02-15T07:24:31Z,27931,2019-02-14T12:24:39Z,/flutter/flutter/issues/27931,flutter,CLOSED,"Document references must have an even number of segments
I'm suffering horrible error of cloud firestore in whole day.... I don't know what is the problem, I asked for stackoverflow, but there was no answer, how to deal with???
What is wrong with my code??
Error
",NONE
190,{'login': 'afandria'},2015-12-03T18:21:34Z,604,2015-11-27T06:03:05Z,/flutter/flutter/issues/604,flutter,CLOSED,"setState might not update the widgets in showDrawer
Sorry if this is sparse on details. I mainly want to determine if there is a bug with showDrawer or with the way I attempted to use it.
Yesterday, I attempted to use showDrawer with the 7be58b1 alpha branch of Flutter. The drawer appeared/dismissed correctly, and the widgets I put in the drawer were correct.
Unfortunately, I noticed that no matter how I attempted to change the state of what was drawn in the drawer, it would not re-render. In my case, I tried to put a Switch inside a DrawerItem to toggle debug mode for my app. I did confirm that the state did actually change, which led me to think that Flutter has a bug.
I was following the Stocks demo app quite closely, which uses a Drawer to toggle the user's market sentiment (Optimistic vs Pessimistic). However, I was assuming that the app's behavior would match the Sky Demo app in the Google Play Store. I only just realized that this app is actually 3 months old (last update August 25), so it is very possible that in that time, the Stocks example does not function as it did before.
I am currently unable to try running these examples (though I have found the instructions), so if someone else can run the Stocks app and determine whether the Optimistic/Pessimistic radio buttons update the drawer menu or not, that would be very helpful.
Otherwise, I will look into this more after the break.",NONE
191,{'login': 'DaveShuckerow'},2018-05-08T23:50:23Z,6728,2016-11-07T05:17:16Z,/flutter/flutter/issues/6728,flutter,CLOSED,"Installation fails on Android emulators without Google APIs without a clear indication of the cause
Tried setting up a flutter app on my personal linux laptop, flutter run failed to start the app on this Android 24 emulator:
Changing the architecture to use Google APIs resolved this issue, but it was unclear from the errors I got that that was an appropriate resolution. Flutter run provided no indication of the error, and I get the following cryptic output from ADB logcat:
11-06 20:45:43.000 1246 1246 W art : Unexpected CPU variant for X86 using defaults: x86
Full ADB output:
http://pastebin.com/CEghLXgj
Note: this is unrelated to leafy, I'm just trying out the third party workflow.",USABILITY
192,{'login': 'RedBrogdon'},2018-04-10T22:56:23Z,16440,2018-04-10T20:31:59Z,/flutter/flutter/issues/16440,flutter,CLOSED,"Compiler crash when ""new"" keyword removed from default app
I accidentally left the ""new"" keyword out of a line of code while working on an app, and when I rebuilt it, the compiler crashed. I then created a brand new project in IntelliJ, and verified the issue still occurred.
Steps to reproduce:
Create a new IntelliJ Flutter project
IntelliJ spits out the ""increment counter"" app.
Remove the new keyword from line 67:
Trigger a hot reload.
The compiler crashes with the message included below.
Interestingly, the compiler does not crash if you leave line 67 alone and instead remove the ""new"" keyword from line 68. Line 67 is creating the instance that's used as the return value of the method, if that matters.
Flutter Doctor
",FAULT TOLERANCE
193,{'login': 'xster'},2019-10-17T22:57:04Z,32866,2019-05-17T03:02:23Z,/flutter/flutter/issues/32866,flutter,CLOSED,"Let add-to-app FlutterViewControllers be back swipable in a UINavigationController
",NONE
194,{'login': 'eseidelGoogle'},2017-02-08T18:33:45Z,7306,2016-12-23T06:09:07Z,/flutter/flutter/issues/7306,flutter,CLOSED,"Broken links on website
Links to FlutterViewController.h are broken for example.
A more sustainable fix is probably some sort of automated way to check these as part of publishing. :(",NONE
195,{'login': 'tvolkert'},2018-04-04T16:58:51Z,16228,2018-04-04T16:58:37Z,/flutter/flutter/issues/16228,flutter,CLOSED,"Regression in Android license status detection.
#16035 introduced a bug with Android sdkmanager version 26.1.1 because the --add-modules option isn't supported:
The manifestation was that when the user ran flutter doctor -v, they'd see the following:
",NONE
196,{'login': 'samycodes'},2019-10-13T19:43:54Z,31123,2019-04-16T10:34:45Z,/flutter/flutter/issues/31123,flutter,CLOSED,"java.lang.NoClassDefFoundError: com.google.firebase.FirebaseApp
I have an application with Flutter. It works really fine, but the app crashes on API level 18 and 19.
When I try to run test the apk in Test Lab, it show me error on API level 18 and 19
gradle.build
Can anyone help me with this?",NONE
197,{'login': 'iampawan'},2019-05-14T20:35:52Z,30185,2019-03-29T11:09:07Z,/flutter/flutter/issues/30185,flutter,CLOSED,"Cloud Firestore 0.9.7 stucks at Installing BoringSSL-GRPC (0.0.2) during pod install
Tried downgrading the version but didn't work. I am using the latest Xcode and IOS version.
Help @shihaohong @Hixie @jonahwilliams @pepegich",NONE
198,{'login': 'fredriks'},2018-01-08T16:42:19Z,13961,2018-01-06T14:20:25Z,/flutter/flutter/issues/13961,flutter,CLOSED,"Infinite loop in TextField with some inputFormatters (iOS)
Run examples/flutter_gallery on iOS (physical device or simulator)
Open ""Text fields"" under ""Material Components""
Enter one or more numbers into the ""Phone Number"" field
Hit backspace
The input field goes into a loop where it repeatedly removes and adds the last character.
Flutter Doctor
",NONE
199,{'login': 'lightyaer'},2018-12-20T17:22:29Z,25627,2018-12-20T17:11:59Z,/flutter/flutter/issues/25627,flutter,CLOSED,"Trouble parsing Json Array 
Here is my simple Todo App using Flutter and Node/Express API's and MongoDB
I am specifically having a problem with parsing json array of todo items.
Todo App
First of all when ever I run this project.
I get this error
Could not load source 'dart:core/runtime/libobject_patch.dart': .
Nothing in the debug console.
After Reloading a few times, I land on Login Page,
Then enter credentials and login,
Immediately, the app is frozen and I get the Above error.
If you look at lib/services/todoService.dart => getAll() method.
Am I doing something wrong?
Secondly,
The json returned from server is as below, and I need to parse it to a List in Dart.
I tried following this
I get the same error as above.
[ { ""completed"": true, ""completedAt"": 1545314655878, ""_id"": ""5c0244d8ac3baf291808b03a"", ""text"": ""Shop some Groceries"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 }, { ""completed"": false, ""completedAt"": null, ""_id"": ""5c1b9a14fe8f70360cd0a393"", ""text"": ""Eat before 12"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 }, { ""completed"": false, ""completedAt"": null, ""_id"": ""5c1ba02efe8f70360cd0a394"", ""text"": ""Test todo"", ""_author"": ""5c02449eac3baf291808b037"", ""__v"": 0 } ]
My Todo Class looks like this
I cant find a good way to parse it into a List and return it to a FutureBuilder.
I use VS Code, No Errors in the debug Console.
",NONE
200,{'login': 'saad-ali'},2015-09-24T23:53:49Z,13565,2015-09-03T18:33:31Z,/kubernetes/kubernetes/issues/13565,kubernetes,CLOSED,"Indirect dependency on golang test code introduced unintended CLI flags in Kubernetes binaries
From @eparis's comment in the Cinder PR: #13367 (comment)
PR #13367, Cinder Volume Plugin introduced a dependency on
github.com/rackspace/gophercloud/openstack/blockstorage/v1/volumes
github.com/rackspace/gophercloud/openstack/compute/v2/extensions/volumeattach
which in turn has a dependency on github.com/golang/go/src/testing
which declares a bunch of command line args (see https://github.com/golang/go/blob/master/src/testing/testing.go#L187).
Which means that kubernetes binaries, like kubelet now have flags for golang test code like --test.memprofilerate, --chatty, etc.",SCALABILITY
201,{'login': 'jingxu97'},2016-06-30T23:32:49Z,28318,2016-06-30T23:07:35Z,/kubernetes/kubernetes/issues/28318,kubernetes,CLOSED,"Kubelet on a node dead right before pod is created and assigned to that node cause pod disappear on apiserver
The steps to reproduce the issue, on a working gce cluster
Choose a node, edit /usr/sbin/kubelet-checker.sh, change sleep time to a large number (600s)
Create a pod and make sure it is assigned to the node you chose in step 1
kill kubelet process (kubelet process will be restarted automatically after > 5mins )
Check pod status, at the beginning, it is ContainerCreating/(or pending), after a few mins, it disappears (kubectl get pods no long shows it)
",SCALABILITY
202,{'login': 'dchen1107'},2015-03-28T03:33:53Z,4128,2015-02-04T21:19:13Z,/kubernetes/kubernetes/issues/4128,kubernetes,CLOSED,"Configure master node same as slave node
This is pre-requirement for running all master components in a pod. #3853 was filed to run etcd as a pod. To make sure we really run etcd as a pod using Kubernete network model without specifying HostPort from spec, we need config the master node:
create networking bridge called cbr0
config docker run with ""--bridge cbr0 --iptables=false""
add default route for master node (on GCE). Need to figure out other cloud providers.
",SCALABILITY
203,,2016-03-31T16:58:16Z,23062,2016-03-16T17:18:51Z,/kubernetes/kubernetes/issues/23062,kubernetes,CLOSED,"Alert someone when kubernetes-test-history stops running, and add report date
The software that generates http://storage.googleapis.com/kubernetes-test-history/static/tests-e2e.html has crashed/stopped running a few times, and nobody knew. Please add an alert.
Also, the above report does not indicate what date the report pertains to, which makes it very confusing, and difficult to tell whether the report is up to date (e.g. I looked today, and it was a week old, although this was entirely non-obvious).",AVAILABILITY
204,{'login': 'discordianfish'},2014-06-24T13:07:16Z,132,2014-06-17T13:17:56Z,/kubernetes/kubernetes/issues/132,kubernetes,CLOSED,"Add generic documentation
Hi,
I'm happy to contribute the missing documentation but I've spend now quite some time on trying to get kubernetes working on a plain docker host and had no success so far.
I've read the design doc, shell scripts and salt states and this is how I deployed it:
kubelet -config /etc/kubelet.conf -address=0.0.0.0
etcd -peer-addr 10.0.1.115:7001 -addr 10.0.1.115:4001 -discovery https://discovery.etcd.io/
apiserver -address 0.0.0.0 -etcd_servers=http://10.0.1.115:4001 -machines=10.0.1.115
controller-manager -master localhost:8080 -etcd_servers=http://10.0.1.115:4001
Kubelet log shows (I've created an empty kubelet.conf to stop it throwing errors, not sure if necessary though):
apiserver prints nothing at all.
controller-manger logs:
Now running cloudcfg fails:
If I try to create a pod I get:
",NONE
205,{'login': 'evanphx'},2015-10-21T17:47:16Z,14281,2015-09-21T16:34:26Z,/kubernetes/kubernetes/issues/14281,kubernetes,CLOSED,"`rolling-update` requires a unique name when specifying a manifest
Per a conversation in slack with @jimmidyson, rolling-update should be able to do the same rename dance when a manifest is specified as when --image is used.
Right now, it errors out with a message like: error: pods/catalog.yml cannot have the same name as the existing ReplicationController catalog",NONE
206,{'login': 'ihmccreery'},2016-02-05T17:31:19Z,15116,2015-10-05T22:10:48Z,/kubernetes/kubernetes/issues/15116,kubernetes,CLOSED,"What do we do with 1.0 tests that fail when run against 1.1?
This is a problem we're going to run into as we're running 1.0 e2e tests against 1.1 clusters, (which we do because we need to make sure that 1.1 clusters still operate the way 1.0 clusters did). If a test fails, but the failure is due to a bad test rather than a problem in 1.1, what do we do?
For example, Services/Nodeport e2es are failing on upgrade due to change in error message. They fail when running 1.0 e2es against a HEAD master (in jobs kubernetes-upgrade-gke-step3-e2e-old and kubernetes-upgrade-gke-step5-e2e-old):
Kubernetes e2e suite.Services should check NodePort out-of-range
Kubernetes e2e suite.Services should prevent NodePort collisions
@ixdy @quinton-hoole Any ideas about how to fix this? This definitely isn't a regression, but it's probably not a good idea to just disable these tests. My best thought is to cherry-pick the 1.1 test changes into 1.0, and somehow pull these tests HEAD of the 1.0 branch. That's a lot of mucking around though, and I'm not sure it's worth it.
For now, I think we should punt on these specific tests until we have a better idea of how widespread this kind of version-skew problem is going to be.",FAULT TOLERANCE
207,{'login': 'kelbongoo'},2015-09-20T07:58:48Z,14237,2015-09-19T23:38:32Z,/kubernetes/kubernetes/issues/14237,kubernetes,CLOSED,"GKE cluster running nginx seems to break nodePort
Setting up a cluster on GKE via the gcloud cli tool, and then following this tutorial https://cloud.google.com/container-engine/docs/tutorials/http-balancer, I came across two issues - the first is here #13073.
In the tutorial we set up the nginx service with a nodePort like so
Of all the nodes in the cluster (I tried 3 and 4 node clusters), the only one who responded to
was the node where the nginx container was actually hosted. The other nodes all showed TCP open at the nodePort in nmap but dropped the connection right away and nothing ever got through to nginx.
Not sure if this is expected behaviour ? From the tutorial it seems like all nodes should return nginx responses
",FAULT TOLERANCE
208,{'login': 'borg286'},2015-08-25T22:52:22Z,10774,2015-07-06T19:35:00Z,/kubernetes/kubernetes/issues/10774,kubernetes,CLOSED,"Docker k8s setup instructions to have DNS option
Please add a way to add DNS support to the setup instructions for getting a kubernetes cluster using docker.
The SkyDNS server already has a yaml file and would exist as a pod and service just as the current apiserver pods exist.",NONE
209,{'login': 'gouyang'},2015-08-17T04:09:32Z,11732,2015-07-23T03:03:54Z,/kubernetes/kubernetes/issues/11732,kubernetes,CLOSED,"Check local copy of the golang docker image is always failed.
The golang docker image is existed
But it always say ""You don't have a local copy of the golang docker image"".
#11284 fixes this.",MAINTAINABILITY
210,{'login': 'karlkfi'},2017-06-02T02:43:51Z,15412,2015-10-09T23:54:36Z,/kubernetes/kubernetes/issues/15412,kubernetes,CLOSED,"[mesos/docker] Flakey Smoke Test - TLS handshake timeout
https://teamcity.mesosphere.io/viewLog.html?buildId=56436&buildTypeId=Oss_KubernetesMesos_5SmokeTestsDockerMesos&tab=buildLog&guest=1#_focus=614
First time I've seen this. Documenting for searchability.",NONE
211,{'login': 'mikesimons'},2015-02-21T14:17:32Z,4695,2015-02-21T11:28:13Z,/kubernetes/kubernetes/issues/4695,kubernetes,CLOSED,"Services with the same name in different namespaces cause kube-proxy to bug out
Should it be possible to have services with the same name in different namespaces? Nothing prevents it and I suspect that it should be possible but there are assumptions in code that cause this not to function correctly.
Here are some failing tests: https://github.com/mikesimons/kubernetes/compare/proxy_namespace_clash
And here is what lead me to investigate:
The Accept failed: use of closed network connection is spat out indefinitely at a very high rate. All services cease to function (but timeout rather than refuse connection).
The thing of note in the logs is that in the ""Setting services"" message both redis instances are present where-as in the ""Recieved update"" message, only the project2 instance is present.
I believe that this is due to the fact that several places in pkg/proxy/config/config.go create maps keyed on service name only. Here is an example https://github.com/mikesimons/kubernetes/blob/master/pkg/proxy/config/config.go#L214
Assuming I am correct, what would be the best way to fix this? It doesn't look like the keys of the maps are used anywhere so does this need to be a map at all?",MAINTAINABILITY
212,{'login': 'erictune'},2015-06-26T22:00:00Z,7047,2015-04-20T15:26:39Z,/kubernetes/kubernetes/issues/7047,kubernetes,CLOSED,"Want create-or-update command for kubectl
If you want to build a shell script to reconcile a config file with the state on the apiserver, it is convenient to be able to do ""create or update"" in a single command.",USABILITY
213,{'login': 'marekbiskup'},2018-02-20T11:32:01Z,9937,2015-06-17T13:30:33Z,/kubernetes/kubernetes/issues/9937,kubernetes,CLOSED,"Create a command-line tool to query yaml and json files
Example usage (details to be decided):
golang templates (http://golang.org/pkg/text/template/#pkg-overview) may not be sufficient because they don't handle all characters (e.g. there have been problems with /)
The tool will be useful for:
bash scripts that have to manipulate yamls (e.g. for fixing #9849)
examples, e.g. here: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/accessing-the-cluster.md#without-kubectl-proxy
cc. @zmerlynn",NONE
214,{'login': 'fahadpolash'},2015-06-09T22:18:23Z,9459,2015-06-09T04:25:21Z,/kubernetes/kubernetes/issues/9459,kubernetes,CLOSED,"Vagrant:Fedora 21 repo failing 
Hello,
I was trying to bringing up the cluster from the github master branch which is using fedora 21. But I am getting the following errors:
==> master: Installing, enabling prerequisites
==> master:
==> master:
==> master: One of the configured repositories failed (Fedora 21 - x86_64),
==> master: and yum doesn't have enough cached data to continue. At this point the only
==> master: safe thing yum can do is fail. There are a few ways to work ""fix"" this:
==> master:
==> master: 1. Contact the upstream for the repository and get them to fix the problem.
==> master:
==> master: 2. Reconfigure the baseurl/etc. for the repository, to point to a working
==> master: upstream. This is most often useful if you are using a newer
==> master: distribution release than is supported by the repository (and the
==> master: packages for the previous distribution release still work).
==> master:
==> master: 3. Disable the repository, so yum won't use it by default. Yum will then
==> master: just ignore the repository until you permanently enable it again or use
==> master: --enablerepo for temporary usage:
==> master:
==> master: yum-config-manager --disable fedora
==> master:
==> master: 4. Configure the failing repository to be skipped, if it is unavailable.
==> master: Note that yum will try to contact the repo. when it runs most commands,
==> master: so will have to try and fail each time (and thus. yum will be be much
==> master: slower). If it is a very temporary problem though, this is often a nice
==> master: compromise:
==> master:
==> master: yum-config-manager --save --setopt=fedora.skip_if_unavailable=true
==> master:
==> master: Cannot retrieve metalink for repository: fedora/21/x86_64. Please verify its path and try again
Any idea?",NONE
215,{'login': 'erictune'},2015-10-05T23:17:31Z,14528,2015-09-24T23:17:52Z,/kubernetes/kubernetes/issues/14528,kubernetes,CLOSED,"Too many daemons created by DaemonSet
I created a DaemonSet a few days ago on my 4 node cluster. I noticed today that I have 6596 Pending pods, and 4 running pods.
This is not expected.
The running pods are on nodes that have existed for 3 or 4 days.
The pending pods all have unset nodeName. I would not have expected the DaemonSet controller to ever create a pod with this value unset. I assume they are pending due to port conflicts with the existing pods, as they use a hostPort.",PERFORMANCE
216,{'login': 'aanm'},2015-10-08T22:43:50Z,14743,2015-09-29T14:32:36Z,/kubernetes/kubernetes/issues/14743,kubernetes,CLOSED,"kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes
I'm getting an error while running the cluster/kube-up.sh with
error:
am I missing something?",NONE
217,{'login': 'DazWilkin'},2016-05-06T16:39:54Z,21635,2016-02-20T22:38:38Z,/kubernetes/kubernetes/issues/21635,kubernetes,CLOSED,"Please provide a feedback mechanism on kubernetes.io --> github.com/kubernetes/kubernetes
I may be missing something (!) but, when I wish to file bugs against kubernetes.io, instead of a ""Click here to submit feedback"", I'm resorting to Googling a section of text on the kubernetes.io to find the relevant page on github in order to reference the github content for github issues!
E.g.
From here:
http://kubernetes.io/v1.1/docs/user-guide/connecting-applications.html#securing-the-service
Must Google to help find this:
https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/connecting-applications.md#securing-the-service
From here:
http://kubernetes.io/v1.1/examples/https-nginx/README.html
Must Google to help find this:
https://github.com/kubernetes/kubernetes/blob/release-1.1/examples/https-nginx/README.md
Recommend providing a feedback feature that facilitates submitting bugs against kubenetes.io pages by either auto-referencing or facilitating finding the reference on the github page.
Alternatively, do you just accept kubernetes.io references in github issues?",USABILITY
218,{'login': 'lavalamp'},2015-11-23T22:01:53Z,17600,2015-11-20T23:35:46Z,/kubernetes/kubernetes/issues/17600,kubernetes,CLOSED,"Broken on soak cluster: Kubectl client Update Demo should do a rolling update of a replication controller [Conformance]
http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-soak-continuous-e2e-gce/3939/",NONE
219,{'login': 'caesarxuchao'},2015-05-28T22:29:11Z,8943,2015-05-28T19:54:49Z,/kubernetes/kubernetes/issues/8943,kubernetes,CLOSED,"'--api-prefix' option in 'kubectl proxy' documentation is wrong
Without --api-prefix, kubectl proxy works just fine,
With '--api-prefix=xxx-api', it stops work:
My kubectl version:
@jlowdermilk, could you take a look? Thanks.",NONE
220,{'login': 'cjcullen'},2019-11-07T02:49:01Z,21330,2016-02-16T20:00:55Z,/kubernetes/kubernetes/issues/21330,kubernetes,CLOSED,"AllowUnconditionalUpdate is very frightening
We provide an option to violate the semantics of our API, and on top of that, it is the default if you don't pass a resourceVersion. This has the ability to do very bad things.
If we want to encourage people to start writing their own controllers on top of kubernetes, we shouldn't make it so easy for them to shoot themselves in the face. Is there a reason this option exists?",NONE
221,{'login': 'yissachar'},2016-04-20T15:01:21Z,24139,2016-04-12T15:42:51Z,/kubernetes/kubernetes/issues/24139,kubernetes,CLOSED,"Bash completion fails to autocomplete files or directories
I've enabled kubectl bash completion as per the instructions:
source ./contrib/completions/bash/kubectl
Now autocompletion works for the kubectl commands. However, if I try to autocomplete a file or directory name, it fails.
kubectl create -f ./ser
Tab to autocomplete directory name
kubectl create -f ./ser-bash: _filedir: command not found
I am using OSX.",NONE
222,{'login': 'justinsb'},,30338,2016-08-10T03:17:48Z,/kubernetes/kubernetes/issues/30338,kubernetes,OPEN,"Document / rationalize CNI plugin distribution
I believe the correct place to download the CNI plugins is https://storage.googleapis.com/kubernetes-release/network-plugins/cni-c864f0e1ea73719b8f4582402b0847064f9883b0.tar.gz
A few challenges with that:
It's not clear which version is the latest of the handful in that directory (they all the same date, and the hash doesn't give any clues)
We should probably bundle it instead with the k8s version with which that k8s version is tested
I'm not entirely sure how this tar file was built
It would be nice if they were available as individual files also (i.e. expanded form), just like we distribute the key k8s binaries in expanded form and in the the kubernetes.tar.gz file. On that note they appear to actually depend on each other though, so perhaps they can't be split.",AVAILABILITY
223,{'login': 'luxas'},2016-04-20T22:15:01Z,24534,2016-04-20T16:03:06Z,/kubernetes/kubernetes/issues/24534,kubernetes,CLOSED,"v1.3.0-alpha.2 binaries not pushed
It seems like v1.3.0-alpha.2 binaries are not pushed.
@david-mcmahon @bgrant0607 @ixdy",NONE
224,{'login': 'lavalamp'},2015-12-05T00:45:06Z,3338,2015-01-08T22:15:00Z,/kubernetes/kubernetes/issues/3338,kubernetes,CLOSED,"Investigate alternative JSON parsers
e.g., I saw this on reddit: http://ugorji.net/blog/go-codecgen
The primary reason why this would be worth our time at the moment is to get better error messages when people do things like pass a string to an array, or misspell a field name. Performance will eventually become important as we scale up, but serialization performance is a very tiny issue compared with e.g. the serial health checking when you list minions.",PERFORMANCE
225,{'login': 'k8s-github-robot'},2016-09-08T18:53:37Z,32237,2016-09-07T22:18:14Z,/kubernetes/kubernetes/issues/32237,kubernetes,CLOSED,"ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}
https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke/15018/
Failed: ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}
",FAULT TOLERANCE
226,{'login': 'stephenR'},2015-05-12T23:26:02Z,7965,2015-05-08T16:58:17Z,/kubernetes/kubernetes/issues/7965,kubernetes,CLOSED,"Secure kubelet port 10250
The kubelet exposes an unauthenticated endpoint on port 10250. The issues with this:
there are the debug handlers /exec/ and /run/ that run code in any container on the host
these debug handlers are enabled by default
the code run in the container runs with full root capabilities (compared to docker's root with a capability bounding set)
",SECURITY
227,{'login': 'MrHohn'},2016-10-25T20:23:51Z,33289,2016-09-22T16:34:52Z,/kubernetes/kubernetes/issues/33289,kubernetes,CLOSED,"Rescheduler e2e should not scale kube-dns pods.
In rescheduler e2e test, for It(""should ensure that critical pod is scheduled in case there is no resources available""), kube-dns is used as the critical pod target and being scaled out and in.
However, we plan to enable the dns horizontal autoscaling feature in the near future, such as this WIP. If this feature is turned on, the corresponding autoscaler will fight with this rescheduler e2e and maintain the desired number of replicas. Hence this test will be very likely to fail.
So probably we should use another critical pod here, or either create a new critical pod that does not exist before in order to protect the ongoing functionalities.
@piosz @thockin",PERFORMANCE
228,{'login': 'jayunit100'},2016-03-17T22:54:23Z,7989,2015-05-08T21:03:33Z,/kubernetes/kubernetes/issues/7989,kubernetes,CLOSED,"E2E: Audit density.go iterator and other timeout iterators 
In my last PR @brendanburns duly noted that you can do declarative style of timeout iteration, which is an important part of the e2e's, rather than a for loop.
lets use the style of iteration in soak_k8petstore.go (pending merge now) with switch -> case statements in density.go as well, and possibly other places. There are also other examples of this online, (i.e. https://code.google.com/p/go-wiki/wiki/Timeouts)
while we're at it lets audit util.go and see if it is being used wherever possible in tests to maximize code reuse. there are now a lot of new utils in it (like RunRC and createNS so on) which weren't there when e2e's were originally created
",MAINTAINABILITY
229,{'login': 'jba'},2016-09-14T15:39:49Z,30069,2016-08-04T11:54:18Z,/kubernetes/kubernetes/issues/30069,kubernetes,CLOSED,"update Google Cloud API client import paths and more
The Google Cloud API client libraries for Go are making some breaking changes:
The import paths are changing from google.golang.org/cloud/... to
cloud.google.com/go/.... For example, if your code imports the BigQuery client
it currently reads
import ""google.golang.org/cloud/bigquery""
It should be changed to
import ""cloud.google.com/go/bigquery""
Client options are also moving, from google.golang.org/cloud to
google.golang.org/api/option. Two have also been renamed:
WithBaseGRPC is now WithGRPCConn
WithBaseHTTP is now WithHTTPClient
The cloud.WithContext and cloud.NewContext methods are gone, as are the
deprecated pubsub and container functions that required them. Use the Client
methods of these packages instead.
You should make these changes before September 12, 2016, when the packages at
google.golang.org/cloud will go away.",NONE
230,{'login': 'bgrant0607'},2016-05-27T04:16:43Z,16938,2015-11-06T19:26:31Z,/kubernetes/kubernetes/issues/16938,kubernetes,CLOSED,"Copy relevant useful docs from https://cloud.google.com/container-engine/docs/
They should have been in github in the first place.
cc @mansoorj",NONE
231,{'login': 'piosz'},2015-03-31T16:27:02Z,5184,2015-03-09T15:47:01Z,/kubernetes/kubernetes/issues/5184,kubernetes,CLOSED,"Missing service environment variables while starting pod
While debugging #5091 I noticed that when I'm creating guestbook application by running ./cluster/kubectl.sh create -f examples/guestbook there is a chance that frontend pods come up before information about environment variables of redis is propagated. In such case frontend can't connect to database during its whole life.
It's actually more general problem, since most of complex application might be affected by this. Also I think create a set of resource (especially by specifying the directory of their config files) should just work no matter of the order of creation.
I can see few solutions of such problem:
Get rid off HOST:PORT stuff and start using DNS service instead.
Add ability to wait for such information being propagated.
Injecting environment variables to container somehow(?).
",PERFORMANCE
232,{'login': 'krousey'},2016-09-12T09:39:51Z,32224,2016-09-07T20:23:17Z,/kubernetes/kubernetes/issues/32224,kubernetes,CLOSED,"registered.EnabledVersions returns all registered versions
#20846 introduced a bug where registered.EnabledVersions doesn't consult enabledVersions anymore... I think. I was trying to debug another problem and spotted this.
I think only clients use this function. APIServer has an entirely different mechanism for tracking enabled/disabled things.
cc @kubernetes/sig-api-machinery",NONE
233,{'login': 'timstclair'},2019-01-12T01:42:21Z,24614,2016-04-21T19:12:44Z,/kubernetes/kubernetes/issues/24614,kubernetes,CLOSED,"Add a `shellcheck` based pre-submit
shellcheck is a bash script linter which could help catch common problems in our (numerous) bash scripts. I know we'd like to reduce our use of bash scripts, but until then I think being stricter about script quality would be helpful.
/cc @zmerlynn",MAINTAINABILITY
234,{'login': 'caesarxuchao'},2017-03-20T23:35:31Z,21932,2016-02-24T23:35:05Z,/kubernetes/kubernetes/issues/21932,kubernetes,CLOSED,"Remove the generated client for Scale
Scale is a sub-resource, we shouldn't generate a typed client for it.
cc @lavalamp @madhusudancs",NONE
235,{'login': 'justinsb'},2016-03-16T14:56:20Z,22907,2016-03-12T17:55:43Z,/kubernetes/kubernetes/issues/22907,kubernetes,CLOSED,"AWS: Check that dynamic volumes are deleted
It was reported that dynamic volumes were not deleted on AWS. Maybe we need to enable the PersistentVolumeRecycler somehow?",NONE
236,{'login': 'derekwaynecarr'},2015-08-28T18:31:50Z,12053,2015-07-30T23:33:03Z,/kubernetes/kubernetes/issues/12053,kubernetes,CLOSED,"Merge NamespaceExists and NamespaceLifecycle admission controllers
We should have a single NamespaceLifecycle plugin that enforces the Namespace rules now that all providers are off NamespaceAutoProvision.
We may also want to make this no longer a user choice to configure and hard-wire the server to always run this check first to simplify configuration errors.
For an example
#12039",MAINTAINABILITY
237,{'login': 'lavalamp'},2015-01-23T23:21:43Z,3345,2015-01-08T23:46:41Z,/kubernetes/kubernetes/issues/3345,kubernetes,CLOSED,"Scaling clusters: 1000's of services in env vars
Clearly you ought to be able to make more services in a k8s cluster than it is reasonable to pass to pods in env vars.
Possible solutions:
Segment by namespace
Require predeclarations
@bgrant0607 I know you hate env vars; do you have a preferred solution for this?",SCALABILITY
238,{'login': 'satnam6502'},2014-11-18T05:42:43Z,2385,2014-11-14T20:33:03Z,/kubernetes/kubernetes/issues/2385,kubernetes,CLOSED,"Pod dependencies on services
One thing that is a bad experience at the moment is the bring-up behaviour of one pod that depends on another the services of another pod. For example, in my logging work the Kibana viewer (pod, service) depends on the Elasticsearch (pod, service). When I try and bring them up together from my Makefile I have an intermediate sate like this for quite a while:
i.e. the Kibana viewer fails to start up because Elasticsearch is not ready yet. Eventually things start to look better:
but even though the pods are marked as Running they are still not quite ready yet and it takes another five minutes or so before one can make queries to Elasticsearch and see log output in Kibana.
It would be nice to describe in a pod declaration its dependencies on other services so this can be taken into account during scheudling. For example:
This would delay the scheduling of this pod until the pod(s) identified by the elasticsearch service are all in the running state.",PERFORMANCE
239,{'login': 'yossi-cohen'},2016-02-04T20:37:15Z,20556,2016-02-03T15:33:54Z,/kubernetes/kubernetes/issues/20556,kubernetes,CLOSED,"service account kube-system/default was not found in ubuntu
hi
i'm trying to run with k8s version 1.1.7
i updated config-default script and removed DenyEscalatingExec key from ADMISSION_CONTROL(otherwise its not working)
after that i tried to run kube-ui (or any other pod with kube-system namespace) without any success
in kube-controller-manager.log i saw the following error
unable to create pod replica: Pod ""kube-ui-v4-"" is forbidden: service account kube-system/default was not found, retry after the service account is created
i googled it and found the following solution
Generate a signing key:
openssl genrsa -out /tmp/serviceaccount.key 2048
Update /etc/kubernetes/apiserver:
KUBE_API_ARGS=""--service_account_key_file=/tmp/serviceaccount.key""
Update /etc/kubernetes/controller-manager:
KUBE_CONTROLLER_MANAGER_ARGS=""--service_account_private_key_file=/etc/kubernetes/serviceaccount.key""
but i couldn't found the keys in the config files
i tried to update util.sh
and set the following flags (thats the only places that i found those keys)
--tls-private-key-file=/etc/kubernetes/serviceaccount.key""
--service-account-private-key-file=/etc/kubernetes/serviceaccount.key \
note: doing kubectl serviceaccount for default namespace returns 1 entry
BUT kubectl serviceaccount --namespace=kube-system returns NO ENTRIES!
i'm really desperate :) does anyone have a clue how to fix this issue
tks a lot",NONE
240,{'login': 'justinsb'},2015-05-18T16:35:37Z,4024,2015-02-02T17:48:40Z,/kubernetes/kubernetes/issues/4024,kubernetes,CLOSED,"etcd arguments are different for systemd vs non-systemd
kubernetes/cluster/saltbase/salt/etcd/default (used for systemd) and kubernetes/cluster/saltbase/salt/etcd/initd (used for initd) have diverged; in particular DAEMON_ARGS has a different bind_addr.
Also, I'm not sure whether etcd.conf is still used at all, but it has another value for the bind addresses (hard-coded to 0.0.0.0).",NONE
241,{'login': 'ronaldpetty'},2015-09-16T21:10:49Z,13060,2015-08-21T22:00:50Z,/kubernetes/kubernetes/issues/13060,kubernetes,CLOSED,"Incorrect Login For Master
Hello,
For a brand new (first time) installation using the Vagrant install fails (different reason, suspect python lib issue). I then switched to using the AWS tutorial, it succeeds after some undocumented fixes (will add new bug for that in docs). After it starts, if you visit the AWS EC2 master, the authentication is requiring the login from the previous Vagrant install. I suspect it didn't override with the new AWS settings. I will keep investigating and try to see if this is a real bug or poor docs.
Regards.
Ron",NONE
242,{'login': 'wonderfly'},2016-05-27T22:54:37Z,23389,2016-03-23T17:54:10Z,/kubernetes/kubernetes/issues/23389,kubernetes,CLOSED,"e2e flake: Kubernetes e2e suite.Kubectl client Kubectl apply should apply a new configuration to an existing RC
Recent failure on #23287 seems flaky. That PR didn't change anything but delete a few dead
Jenkins jobs.
kubernetes-pull-build-test-e2e-gce/33368/
@spxtr to delegate.",NONE
243,{'login': 'yifan-gu'},2016-02-01T19:53:36Z,19494,2016-01-11T19:24:54Z,/kubernetes/kubernetes/issues/19494,kubernetes,CLOSED,"rkt: retrieve image size from rkt api service
As rkt/rkt#1916 is merged, we should be able to return the image size information to kubelet.
cc @derekparker @sjpotter @jonboulle",NONE
244,{'login': 'msaffitz'},2016-03-18T19:46:02Z,23170,2016-03-18T01:14:36Z,/kubernetes/kubernetes/issues/23170,kubernetes,CLOSED,"Pods Killed Every 5 Minutes After Upgrading to 1.2
We've successfully been running 1.1.7 on AWS for several months. After upgrading to 1.2 today nearly all of our pods are killed every 5 minutes. In the logs we see Killing container with docker id 661a6ded11b9: Need to kill pod., but beyond that there doesn't seem to be a clear cause. I'm at a bit of loss for where to look next and how to debug this, and love any suggestions / recommendations.",SCALABILITY
245,{'login': 'apackeer'},2015-08-03T23:09:45Z,12137,2015-08-03T05:58:12Z,/kubernetes/kubernetes/issues/12137,kubernetes,CLOSED,"Named Ports not creating _name._protocol.<service> SRV entries
Hi,
I have the following service definition for use on GCE:
When i do a SRV lookup to get the port (as per instructions here) for that service using:
dig SRV _api._TCP.auth.default.cluster.local.
I get:
The service does get registered properly and is responding. I can look up the service if I use:
dig SRV default.cluster.local.
I get:
Why doesn't the port number get returned when I lookup the named port via an SRV record?",NONE
246,{'login': 'bgrant0607'},2015-09-08T17:32:48Z,12828,2015-08-17T21:29:00Z,/kubernetes/kubernetes/issues/12828,kubernetes,CLOSED,"Support setting env vars in kubectl run
Something like kubectl run --env=""VAR=value"" image. Multiple --env flags should be accepted. Comma-separate values would get into an escape rathole, so I'd like to avoid that.",USABILITY
247,{'login': 'pwittrock'},2018-02-13T17:54:12Z,26220,2016-05-24T22:19:06Z,/kubernetes/kubernetes/issues/26220,kubernetes,CLOSED,"Node e2e reporting
It would be useful to have daily or per-pr reports about the health of each distro and publish it to a dashboard somewhere",USABILITY
248,{'login': 'mingxing'},2014-12-15T22:14:14Z,2728,2014-12-03T03:09:42Z,/kubernetes/kubernetes/issues/2728,kubernetes,CLOSED,"code.google.com/p/go.tools/cmd/cover moved to godoc.org/golang.org/x/tools/cmd/cover
When I tried to install Kubernetes, I met an issue: 'package code.google.com/p/go.tools/cmd/cover: Get https://code.google.com/p/go/source/checkout?repo=tools: dial tcp 173.194.127.104:443: connection timed out'
Then I did some research and found the path of covert has been moved to https://godoc.org/golang.org/x/tools/cmd/cover
However in https://github.com/GoogleCloudPlatform/kubernetes/blob/e5e4c8a7d35a0bb981155d84d766eec3e2cd6ffa/Godeps/_workspace/src/github.com/google/gofuzz/.travis.yml, it is still use code.google.com/p/go.tools/cmd/cover, should we update it?
Best Regards
Simon",NONE
249,{'login': 'thockin'},,6132,2015-03-28T03:18:16Z,/kubernetes/kubernetes/issues/6132,kubernetes,OPEN,"kubectl should return more information on failure
After making a kubectl create call with bad JSON, kubectl would read-back the master's understanding of what I wrote and show me a diff. Something like that",USABILITY
250,{'login': 'farcaller'},2015-05-24T20:33:03Z,8753,2015-05-24T08:57:39Z,/kubernetes/kubernetes/issues/8753,kubernetes,CLOSED,"/minions not available for api v1beta3
Just got a clean deployment of 0.17.1, and there seem to be an issue with kubectl:
Indeed, requesting https://10.241.1.1:6443/api/v1beta3/minions returns:
Specifying older api version works though:
",NONE
251,{'login': 'jchambers'},2016-11-17T17:21:39Z,33202,2016-09-21T20:06:52Z,/kubernetes/kubernetes/issues/33202,kubernetes,CLOSED,"kube-up.sh fails and misreports status using AWS (KUBE_MANIFESTS_TAR_URL: unbound variable)
This is, essentially, a deliberate duplicate of #30495, which was closed by the requester before the underlying issue was resolved. In short, current versions of Kubernetes (I'm using 1.3.7) fail when using kube-up.sh to create a cluster under AWS, but incorrectly report success. This leaves some AWS resources (VPCs, security groups, etc.) in place, but with no actual nodes.
The relevant bit of output is as follows:
A viable workaround has been described in #30495 (I believe this is why the original author closed the issue), but the issue itself has not actually been fixed. Please accept my apologies for the goofy paperwork shenanigans here, but I did want to make sure the issue didn't get lost.",NONE
252,{'login': 'aaskey'},2016-09-14T17:46:25Z,31488,2016-08-26T02:29:31Z,/kubernetes/kubernetes/issues/31488,kubernetes,CLOSED,"kubectl proxy failed to run as a service or sudo in GCE
Kubernetes version (use kubectl version):
Environment:
OS (e.g. from /etc/os-release):
Kernel (e.g. uname -a):
What happened:
In a GCE instance, failed to run kubectl proxy as a service or sudo, the same command ran successfully in command line as current user.
On Mac, I can run ""kubectl proxy --port=8080"" or ""sudo kubectl proxy --port=8080"" without problem.
Run as a service, failed:
Run as sudo, failed:
Run as current user, succeeded:
Below is the systemd service file:
",NONE
253,{'login': 'fgrzadkowski'},2016-03-01T19:42:50Z,4235,2015-02-07T05:41:36Z,/kubernetes/kubernetes/issues/4235,kubernetes,CLOSED,"Kubernetes should support cross-zone cluster
Most applications should be running in multiple zones to increase availability. Kubernetes should support it. I imagine this to work in the following way:
User sets up cluster in a region in a way that minions are spread evenly across all available zones
User creates replication controller for the application with size > 1
Scheduler spreads pods within the same replication controller across available zones
That way user gets regional availability for free.
AFAIU this will mostly require changes in how we create cluster and how we schedule pods.
cc @wojtek-t",AVAILABILITY
254,{'login': 'vishh'},2016-08-31T16:52:49Z,23397,2016-03-23T21:15:56Z,/kubernetes/kubernetes/issues/23397,kubernetes,CLOSED,"Validate Docker v1.11
Docker v1.11 rcs are starting to show up. Its time to get started with the validation.
https://github.com/docker/docker/releases/tag/v1.11.0-rc1
cc @kubernetes/sig-node
EDIT(timstclair):
TASKS:
 e2e tests pass
 performance analysis
 startup tests (bootstrap, restart)
 live upgrade successful
 1 week soak tests
",NONE
255,{'login': 'bgrant0607'},2019-05-07T15:53:05Z,24343,2016-04-15T18:23:06Z,/kubernetes/kubernetes/issues/24343,kubernetes,CLOSED,"Figure out how to handle code in multiple repos
A large monorepo works for Google, but not on github.
We hit the ceiling of achievable velocity of a single github repo in early 2015:
https://github.com/kubernetes/kubernetes/graphs/contributors
There are many reasons: ACLs, notification management, issue triage, PR reviews, sequentialized submit testing, merge conflicts, etc.
We're chipping away at these issues, but we need more than incremental improvement.
We've discussed moving a number of things to other repos:
Kubelet: #444
Generic API infrastructure: #2742
Client libraries: #5660
Misc. utilities: #24156
kubectl
scheduler
examples
cloud providers + cluster code + ""getting-started guides""
auth plugins
network and storage plugins?
e2e tests
contributor documentation
We need to seriously think about how to do this.
Known issues that need to be addressed:
We need to get sprawl and package dependencies under control: #4851
We need to make most components ordinary clients of the API: #20193
We need to figure out dependency management and integration testing
An example of a Go project on github with good repo hygiene:
https://github.com/deis
I have no illusions that breaking the project into separate repos will be a silver bullet: it's necessary, but not sufficient. I also know that it will cause some pain. But that pain already exists: cadvisor, heapster, dashboard, contrib, docs, ....
Speaking of contrib, it needs to be broken up, too: kubernetes-retired/contrib#762
@thockin @smarterclayton @lavalamp @mikedanese @dchen1107 @davidopp @ixdy",MAINTAINABILITY
256,{'login': 'saad-ali'},2017-06-16T00:28:51Z,20885,2016-02-09T04:34:17Z,/kubernetes/kubernetes/issues/20885,kubernetes,CLOSED,"Modify E2E tests to use the GCE API instead of gcloud exec
PR #17747 laid the ground work to enable E2E tests to use the GCE API instead of gcloud exec. That PR only modified the PD tests to do so.
In order to make E2E more robust, we should switch over all other instances of gcloud exec in E2E tests to use the GCE API instead.",MAINTAINABILITY
257,{'login': 'RichieEscarez'},2015-07-10T19:42:26Z,10130,2015-06-19T22:21:39Z,/kubernetes/kubernetes/issues/10130,kubernetes,CLOSED,"Add the ""when"" and ""why"" to use the Downward API
We need to add more content around the Downward API to better clarify when and why to use it. We should also try to revise the ""how"" info we provide today to improve/clarify what the specific steps.
(for example: Prerequisites are xyx. To use the downward API: 1. do this. 2....3...etc..).
Some details we should make clear:
When do i want containers to consume info about the system without coupling to k8s client or REST API?
Do we say ""use the downward API when your containers need access to information about the cluster in which it resides""?
We should also tie together/link/mention the other content we have on using environment variables and the downward api example.
",NONE
258,{'login': 'euank'},2017-12-27T17:55:47Z,26816,2016-06-03T21:36:56Z,/kubernetes/kubernetes/issues/26816,kubernetes,CLOSED,"rkt: hostPath mounts to non-existent directories fail
A pod referencing a nonexistent host path, such as the one below, does not run under the rkt container runtime.. However, it runs just fine under docker (which, by default, creates nonexistent paths as empty directories when a mount references them).
The pod also does not show up in kubectl get pods in any state, though it can still be deleted.
The latter is definitely something that should be fixed; the former is an intentional behavioural difference between rkt and docker and might be up to debate.
Should the pod fail? Do we need consistent behavior with docker here? Do we think that blindly creating directories on the host masks typos and is surprising?
1b) Should rkt change to this behavior, or should we create the directory before referencing it in a bindmount? (e.g. an ExecStartPre=mkdir -p <host directories>)
Pods that fail like this should still be visible in get pods; I think this is jut that rkt failed during stage1 and there's some over-aggressive error handling in our code that masks this pod, but I haven't dived very deeply there yet.
Example failing pod
cc @yifan-gu @tmrts",FAULT TOLERANCE
259,{'login': 'pstauffer'},2017-06-02T01:51:33Z,22485,2016-03-04T00:34:58Z,/kubernetes/kubernetes/issues/22485,kubernetes,CLOSED,"fixing detecting docker version
In the getting-started-guides matches the detection of the docker version not with a version 1.1x.
https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/master.sh#L108
https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/worker.sh#L105",NONE
260,{'login': 'bgrant0607'},2016-01-26T18:32:25Z,20154,2016-01-26T15:27:48Z,/kubernetes/kubernetes/issues/20154,kubernetes,CLOSED,"test-cmd flake: first kubectl command (get nodes) fails
Encountered on #18901
cc @kargakis",NONE
261,{'login': 'brendandburns'},2017-06-01T17:57:35Z,11965,2015-07-29T03:41:53Z,/kubernetes/kubernetes/issues/11965,kubernetes,CLOSED,"Use bandwidth shaping on the master
Limit outbound and inbound bandwidth, using the tc tool, see:
https://www.iplocation.net/traffic-control",NONE
262,{'login': 'thockin'},2015-07-09T05:31:56Z,1988,2014-10-24T17:11:26Z,/kubernetes/kubernetes/issues/1988,kubernetes,CLOSED,"Add an optional ""why"" clause to ValidationError
I find myself wanting to explain WHY a validation error failed, and there's just no way to pass that down to users.",USABILITY
263,{'login': 'goltermann'},2016-04-20T22:14:46Z,22524,2016-03-04T16:30:42Z,/kubernetes/kubernetes/issues/22524,kubernetes,CLOSED,"Add go_vet to presubmit
We'd like to stay vet clean, and aren't too far away.",NONE
264,{'login': 'dashpole'},2016-11-01T18:09:21Z,33382,2016-09-23T19:00:02Z,/kubernetes/kubernetes/issues/33382,kubernetes,CLOSED,"Per-Container Inode Accounting
cc: @derekwaynecarr (how can we get devicemapper support for this?)
when a node has reached its eviction-hard threshold for imagefs.inodes or nodefs.inodes, the eviction manager does not behave optimally.
This is because we do not keep track of inodes used per-container, but rather just overall. In some cases, the pod which is not using many inodes is evicted before a pod that is using many.
Progress:
 Create an end to end test that demonstrates that a pod using a normal amount of disk capacity is evicted before a pod that uses all remaining inodes (but little disk capacity).
 Modify cadvisor api and kubernetes api to include per-container inode usage
 Modify cadvisor to publish per-container inode usage
 Modify kubelet eviction manager to take per-container inode usage into account when evicting
",NONE
265,{'login': 'roberthbailey'},2018-03-12T05:16:33Z,20820,2016-02-08T16:27:53Z,/kubernetes/kubernetes/issues/20820,kubernetes,CLOSED,"[docker 1.10] create syscall filters for k8s-supplied components
With docker 1.10, you can create a filter for syscalls that the container is allowed to execute, mainly to reduce the kernel attack surface and make it harder to use a privilege escalation vulnerability in the kernel code.
For containers that we provide (master components, add-ons) where we know the expected syscall surface we should explore locking them down by limiting the system calls that they can make.
/cc @stephenR",SECURITY
266,{'login': 'skwokmag'},2015-05-02T15:26:10Z,7493,2015-04-29T04:26:11Z,/kubernetes/kubernetes/issues/7493,kubernetes,CLOSED,"What kind of aws roles do I need to prepare for kubernetes
Hi,
iam has a few roles. Is it ""Grant API access to SAML providers""?
Thank you.
skwok",SECURITY
267,{'login': 'alex-mohr'},2015-03-04T20:20:18Z,3894,2015-01-28T22:09:52Z,/kubernetes/kubernetes/issues/3894,kubernetes,CLOSED,"kubectl: rationalize which messages to stderr/stdout
A user reports to google-containers@googlegroups.com (https://groups.google.com/forum/#!topic/google-containers/qHDR-mvh7sM) the following. I verified similar behavior in kubectl v0.9.1.
I am using Kubernetes v0.7.0, when I run ""kubectl"" to create pod, I found:
$ kubectl create -s http://192.168.122.136:8080 -f ./mariadb-pod.yaml
I0128 22:09:44.258267 30016 restclient.go:133] Waiting for completion of operation 19 --> this is to stderr
mariadb --> this is to stdout
I do not know why the message ""...Waiting for completion ..."" is considered as an error, I think it should be to stdout too.",NONE
268,{'login': 'mwielgus'},2016-03-14T18:08:10Z,22938,2016-03-14T11:54:20Z,/kubernetes/kubernetes/issues/22938,kubernetes,CLOSED,"Version is missing in Heapster configuration
cc: @piosz",NONE
269,{'login': 'derekwaynecarr'},2015-05-08T14:37:18Z,7906,2015-05-07T19:23:26Z,/kubernetes/kubernetes/issues/7906,kubernetes,CLOSED,"Kubelet is reporting Node cpu capacity as negative value
I have some reports of a Node reporting a a resource capacity for cpu as a negative value.
Any idea why cpu could report as a negative value? Is this an expected normal outcome? In its current state, it prevents any pods from being scheduled to that Node. Tips on how to debug further are appreciated.
@dchen1107 @vmarmol - any ideas?",PERFORMANCE
270,{'login': 'luxas'},2016-01-25T06:04:59Z,20070,2016-01-24T20:35:45Z,/kubernetes/kubernetes/issues/20070,kubernetes,CLOSED,"apiserver proxy: no endpoints available error if Service.spec.ports[*].name is specified
Hello everyone!
I noticed this strange behaviour today when I was trying to connect to my service via the apiserver proxy:
But when I deleted the Service.spec.ports[*].name field, it worked as it should:
Is this a known bug? Or is it made this way ""by design""?
This also makes services with two or more ports inaccessible via apiserver proxy
@ArtfulCoder @thockin",NONE
271,{'login': 'jszczepkowski'},2015-11-17T11:10:02Z,15474,2015-10-12T11:45:05Z,/kubernetes/kubernetes/issues/15474,kubernetes,CLOSED,"Implement e2e tests for horizontal pod autoscaling of deployments
We need to write e2e tests which combines horizontal pod autoscaling with deployments (scale sub-resourcer should point to a deployment object).
The tests should be similar to https://github.com/kubernetes/kubernetes/blob/master/test/e2e/horizontal_pod_autoscaling.go. The library from https://github.com/kubernetes/kubernetes/blob/master/test/e2e/autoscaling_utils.go should be extended and allow creation of a resource consumer as a deployment (currently it is always replication controller). The new tests should create a deployment resource consumer and pass a scale-sub-resource to the deployment to autoscaler.",NONE
272,{'login': 'xiaochunyn'},2017-05-31T18:23:30Z,26513,2016-05-30T08:47:09Z,/kubernetes/kubernetes/issues/26513,kubernetes,CLOSED,"How Webhook works, where can find the details? thanks
1 Is webhook plugin stable?
2 How webhook works?
3 How can I use webhook and where can find detailed information.
Thank you very much!",USABILITY
273,{'login': 'gmarek'},2016-01-11T19:00:03Z,19167,2015-12-29T11:02:32Z,/kubernetes/kubernetes/issues/19167,kubernetes,CLOSED,"Unit tests are EXTREMELY flaky lately
It's bad. kubertest-test-go suite failed 5 consecutive times because of it.
@thockin @davidopp @brendandburns @bgrant0607 @dchen1107 @fgrzadkowski @wojtek-t @nikhiljindal @aronchick
Known Issues:
 pkg/master #19141 (fixed by @wojtek-t in #19195)
 pkg/util/wait #19067 (maybe fixed by @wojtek-t in #19196)
 pkg/storage/etcd, pkg/registry/generic/etcd #18928
 pkg/apiserver #19176
 pkg/client/record/event_test.go #19151
 k8s.io/kubernetes/contrib/mesos/pkg/runtime #19186
 pkg/util/wait: #19223
 pkg/client/unversioned/portforward #19230
 plugin/pkg/scheduler/factory #19229
 pkg/storage #19254
 pkg/client/record #19268
",NONE
274,{'login': 'bprashanth'},2017-07-14T17:42:30Z,17805,2015-11-26T00:01:57Z,/kubernetes/kubernetes/issues/17805,kubernetes,CLOSED,"Fix l7 controller watch on namespaces
Observed odd flakyness when watching Ingresses from the controller scoped to a namespace. Need to get to the bottom of it, but I suspect it's a bug higher up in the stack. For now it isn't that important because the controller watches all namespaces.",NONE
275,{'login': 'proppy'},2014-07-14T23:58:27Z,424,2014-07-12T05:34:50Z,/kubernetes/kubernetes/issues/424,kubernetes,CLOSED,"support for docker links env variables format
When exposing services through environments variable to running containers, we should support the standard docker format used by links (see below). So developers don't have to modify their discovery code to connect to a service.
The code writing the env var from service definition is here:
https://github.com/GoogleCloudPlatform/kubernetes/blob/779cb84625fda1ffd0b87437c2cc002033b2d2fe/pkg/registry/service_registry.go#L46
I believe it could be easily adapted to follow the same standard defined by docker.",USABILITY
276,{'login': 'dchen1107'},2015-06-05T05:10:31Z,4783,2015-02-24T19:15:04Z,/kubernetes/kubernetes/issues/4783,kubernetes,CLOSED,"Add e2e test for etcd failures handling
We ran into a lot of issues when etcd crashes. We need tests to ensure all components / daemons are doing the right things etcd is down, and recover later.",NONE
277,{'login': 'deads2k'},2015-11-04T16:48:39Z,3800,2015-01-26T15:20:50Z,/kubernetes/kubernetes/issues/3800,kubernetes,CLOSED,"Remove --auth-path from kubectl
Since the .kubeconfig file was introduced, there is a new way to describe the information contained inside of the existing .kubernetes_auth format. .kubernetes_auth combined information that described how to recognize the api-server with information about how to authenticate the user to the api-server. .kubeconfig separates those two concepts into discretely re-useable chunks, but --auth-path was kept for backwards compatibility.
If .kubernetes_auth is eliminated, there will be one way to express that information and that will simplify the explanation of how the information is built. Right now, allowing references to .kuberentes_auth and defaulting to looking at the ~/.kubernetes_auth makes it harder to describe exactly where authentication information is coming from.
This would be a breaking change that has ripples affecting e2e tests, so I'd like to be sure there is agreement to the concept before starting a change.
/cc @jlowdermilk @smarterclayton @liggitt",SECURITY
278,{'login': 'shuoli84'},2015-10-14T15:22:21Z,13709,2015-09-09T06:27:44Z,/kubernetes/kubernetes/issues/13709,kubernetes,CLOSED,"kube-proxy's udp proxy dead
EDIT: kubernetes version 1.0.3
One of our kubernete machine not able to connect to dns server, and after some trouble shooting, it seems kube-proxy's udp socket dead.
It stops reading from socket. I don't know how to re-pro yet. All other tcp sockets are working normally. I am trying to trouble shoot this, but I has no knowledge on how to debug a running golang process.
iptables:
Nothing interesting in logs. So I just skip them.",NONE
279,{'login': 'k8s-github-robot'},2016-09-02T23:12:12Z,30962,2016-08-19T03:22:43Z,/kubernetes/kubernetes/issues/30962,kubernetes,CLOSED,"kubernetes-e2e-gke-staging: broken test run
Failed: https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke-staging/6333/
Run so broken it didn't make JUnit output!",NONE
280,{'login': 'childsb'},2016-05-25T06:46:53Z,8792,2015-05-26T00:53:41Z,/kubernetes/kubernetes/issues/8792,kubernetes,CLOSED,"make clean ; make WHAT=test/e2e/e2e.test"" fails
when building e2e_test.go using 'make WHAT=test/e2e/e2e' fails with:
(20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test
build/make-clean.sh
+++ [0525 20:52:03] Verifying Prerequisites....
+++ [0525 20:52:03] Cleaning out local _output directory
rm -rf _output
rm -rf Godeps/_workspace/pkg
hack/build-go.sh test/e2e/e2e.test
+++ [0525 20:52:04] Building go targets for darwin/amd64:
test/e2e/e2e.test
/Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh: line 367: pushd: /Users/bc/dev/git/t/kubernetes/_output/local/go/bin: No such file or directory
!!! Error in /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361
'pushd ""$(dirname ${outfile})"" > /dev/null' exited with status 1
Call stack:
1: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361 kube::golang::build_binaries_for_platform(...)
2: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:488 kube::golang::build_binaries(...)
3: hack/build-go.sh:26 main(...)
Exiting with status 1
make: *** [all] Error 1
However 'make all; make (20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test' works",NONE
281,{'login': 'lachie83'},2015-09-17T20:41:04Z,14023,2015-09-16T06:33:14Z,/kubernetes/kubernetes/issues/14023,kubernetes,CLOSED,"Pods hang in pending state indefinitely
I've been working with a 6 node cluster for the last few weeks without issue. Earlier today we ran into an open file issue (https://github.com/kubernetes/kubernetes/pull/12443/files) and I patched and restarted kube-proxy. Since then, all pods deployed to all BUT node-01 get stuck in pending state and there log messages stating the cause.
I've taking a look at both the following similar issues and they don't appear to be the cause
#4891
#3185
Cluster is running v1.0.3
Here's an example of the state
get events
scheduler log
tailing kubelet log file during pod create
kubelet process
node list
",NONE
282,{'login': 'zmerlynn'},2018-03-12T10:21:32Z,26304,2016-05-25T21:13:16Z,/kubernetes/kubernetes/issues/26304,kubernetes,CLOSED,"Fix wait.Until tests to actually verify something other than ""not hanging""
In review for #26301, it was pointed out that my copy-pasta test was verifying one thing, but then the second half of the test is more of the ""run this code and hope it doesn't hang"" variety.
Follow-up issue.",NONE
283,{'login': 'caesarxuchao'},2015-11-12T07:52:38Z,16626,2015-10-30T22:26:43Z,/kubernetes/kubernetes/issues/16626,kubernetes,CLOSED,"Document how to add a new API group
AFAIK, we don't have a documentation on how to add a new API group. Currently we have #16621 and #13146 that try to add a new API group. I will draft a guide to ease the future endeavor of adding groups. And probably by writing this guide, I will see how can we make the API group machinery easier to use.
cc @mikedanese @timstclair @lavalamp",USABILITY
284,{'login': 'nikhiljindal'},2015-10-28T22:25:51Z,16124,2015-10-22T20:27:08Z,/kubernetes/kubernetes/issues/16124,kubernetes,CLOSED,"Add a user guide readme for deployments
",USABILITY
285,{'login': 'davidopp'},2015-03-18T19:43:21Z,5585,2015-03-18T04:48:22Z,/kubernetes/kubernetes/issues/5585,kubernetes,CLOSED,"Investigate possibly broken Docker SIGTERM delivery
@rsokolowski and I found the following behavior last week when we were working with someone who was trying to run some stuff on Kubernetes. Note that this appears to be a Docker problem, not a Kubernetes problem, but I'm filing it here for now so we can verify that it's reproducible before filing a bug against Docker.
Start two containers on the same machine, each a shell script that traps SIGTERM and prints something like “I received SIGTERM” and quits. Then do “docker stop” and observe that one container exits via the SIGTERM handler path and the other just gets SIGKILL.",NONE
286,{'login': 'thockin'},2015-03-13T07:28:18Z,4628,2015-02-19T22:38:58Z,/kubernetes/kubernetes/issues/4628,kubernetes,CLOSED,"Loosen label and annotation validation and related tests
From #4486
Proposed:
label values: restrict to [A-Za-z0-9_-.]*
annotation values: no restriction
qualified names (label and annotation keys, resource names, volume plugins): loosen restrictions to ([A-Za-z0-9_-.]+/)?[A-Za-z0-9_-.]+ - this should be a strict superset of
what we allow today. We can say that convention is to use dns-compatible
domain + label, but still allow things like FOO/B_A.R. This DOES NOT allow for foo.com/bat/bat - is that an important affordance to anyone? We could allow that, but we need to be clear that ""bar/bat"" means (bar, bat) while ""foo.com/bar/bat"" means (foo.com, bar/bat).
@smarterclayton does that give you enough freedom?
@bgrant0607 does that give you enough consistency?
@quinton-hoole-google does this make the qualified name type now viable for your use case in kube-proxy?",NONE
287,{'login': 'zmerlynn'},2015-04-20T20:22:38Z,5946,2015-03-25T20:19:16Z,/kubernetes/kubernetes/issues/5946,kubernetes,CLOSED,"`validate-cluster.sh` should be using `/validate`: wants `kubectl healthy`?
We have a /validate endpoint that GKE has been using to validate the health of running clusters after the API server is available, similar to the kube-up.sh flow of validate-cluster.sh. There's no reason that validate-cluster.sh should actually do this work manually anymore - we should just go through common source. We should probably introduce a kubectl healthy or kubectl healthcheck?",MAINTAINABILITY
288,{'login': 'metral'},2016-05-15T12:45:01Z,20514,2016-02-02T21:18:52Z,/kubernetes/kubernetes/issues/20514,kubernetes,CLOSED,"Running k8s locally via Docker & 1.2.0-alpha.6 does not stand up apiserver
Following the all-in-one install via kubelet on Docker as listed in the docs is not working. I hit a ""no cloud provider specified"" and the controller and apiserver never come up and am plagued with many connection refused errors when hitting 127.0.0.1:8080 even though I set KUBERNETES_PROVIDER https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md#run-it
I'm running on an Ubuntu VM and don't need cloud provider resources like the LoadBalancer type for Services - I just want a local dev k8s environment.
Here is a snippet of the logs for the mod that enables nsenter (seeing how there were issues with Secrets not working in hyperkube - see #19069 for related info )
default
docker-compose file:
logs:
with nsenter modification
docker-compose file:
logs:
",NONE
289,{'login': 'nikhiljindal'},2018-02-15T10:33:03Z,15910,2015-10-20T00:08:54Z,/kubernetes/kubernetes/issues/15910,kubernetes,CLOSED,"Cross links dont work in api reference docs when viewed using htmlpreview
For ex: links in https://htmlpreview.github.io/?https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/api-reference/extensions/v1beta1/definitions.html dont work when they point to docs outside this one.
Links work fine when the same doc is viewed on kubernetes.io (ex: http://kubernetes.io/v1.0/docs/api-reference/definitions.html).
Looks like a bug in htmlpreview. Need to figure out a way to fix this.
cc @caesarxuchao",NONE
290,,2016-02-26T18:16:51Z,19416,2016-01-08T17:36:13Z,/kubernetes/kubernetes/issues/19416,kubernetes,CLOSED,"Add Ubernetes Lite e2e test scheduling pods with attached volumes into correct zone
Part of #19413",NONE
291,{'login': 'thockin'},2015-04-27T23:18:52Z,6265,2015-04-01T01:09:15Z,/kubernetes/kubernetes/issues/6265,kubernetes,CLOSED,"DeepHashObject should probably set DisableMethods and DisablePointerMethods for spew
For max consistency. Some String() methods might hash collide.",NONE
292,{'login': 'sacashgit'},2015-04-28T23:43:00Z,6667,2015-04-10T00:07:13Z,/kubernetes/kubernetes/issues/6667,kubernetes,CLOSED,"Service Name Resolution is not working
Hi There
I have a ubuntu multi-node cluster set up - running. I am able to create pods, controllers and services at will. But I am facing the issue where service name resolution is not working. As part of the guestbook example, I have created redis-slave, redis-master and frontend services. But when containers are trying to talk to each other through this service interface, it is not working.
E.g
When PHP application try and talk to redis-slave:6379 - it fails (This call dont get routed any where)
When redis slave try to synch with master using redis-master , it doesnt work.
When I try and ping redis-master from inside the container, it comes back with unknown host error.
Some logs to show you the problem
[8] 10 Apr 00:03:57.902 # Unable to connect to MASTER: Connection timed out
[8] 10 Apr 00:03:58.905 * Connecting to MASTER redis-master:6379
[8] 10 Apr 00:03:58.909 # Unable to connect to MASTER: Connection timed out
[8] 10 Apr 00:03:59.913 * Connecting to MASTER redis-master:6379
Temporary failure in name resolution [tcp://redis-slave:6379]' in /vendor/predis/predis/lib/Predis/Connection/AbstractConnection.php:141
I do see Kube-Proxy is making the right iptable entries.
I0409 13:04:47.690367 20353 proxier.go:556] Opened iptables from-containers portal for service ""redis-master"" on TCP 11.1.1.67:6379
I0409 13:04:47.696276 20353 proxier.go:567] Opened iptables from-host portal for service ""redis-master"" on TCP 11.1.1.67:6379
I0409 13:11:50.223702 20353 proxier.go:556] Opened iptables from-containers portal for service ""redis-slave"" on TCP 11.1.1.55:6379
I0409 13:11:50.252093 20353 proxier.go:567] Opened iptables from-host portal for service ""redis-slave"" on TCP 11.1.1.55:6379
I0409 13:14:17.024773 20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 11.1.1.58:8000
I0409 13:14:17.034296 20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 11.1.1.58:8000
I0409 13:14:17.046845 20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 10.64.80.83:8000
I0409 13:14:17.057679 20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 10.64.80.83:8000
I0409 13:14:17.067562 20353 proxier.go:556] Opened iptables from-containers portal for service ""frontend"" on TCP 10.64.80.84:8000
I0409 13:14:17.077085 20353 proxier.go:567] Opened iptables from-host portal for service ""frontend"" on TCP 10.64.80.84:8000
Do you know what is missing in the set up which is causing this problem ? I do not have any DNS (skydns) running here and think that it is desirable but not needed.
What might be going wrong here ?",NONE
293,{'login': 'alexanderguzhva'},2015-09-08T21:14:42Z,13633,2015-09-07T03:47:37Z,/kubernetes/kubernetes/issues/13633,kubernetes,CLOSED,"vagrant-based cluster is broken
I took the last version from github several days ago, installed without any problems
host machine
minion-1, everything is fine
minion-2. Completely empty
minion-3. Completely empty
ok, let us run redis from the example (replication factor is 3). Here is what I see after 10 minutes of waiting
",NONE
294,{'login': 'AaronTao1990'},2016-05-19T01:13:08Z,25792,2016-05-18T07:31:01Z,/kubernetes/kubernetes/issues/25792,kubernetes,CLOSED,"failed to set up cluster via docker
I am following this tutorial Installing a Kubernetes Master Node via Docker to set up a cluster via Dockar。
After setting up etcd and flannel. I tried to restart docker with --bip & --mtu option.
You now need to edit the docker configuration to activate new flags. Again, this is system specific.
This may be in /etc/default/docker or /etc/systemd/service/docker.service or it may be elsewhere.
Regardless, you need to add the following to the docker command line:
--bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}
And after I ran
I found my docker is already started with the right options:
But when I tried to run hello-world procedure with the following command
The error occurs
BTW, I can run hello-world find with --bip & --mtu removed.
Any kind of help will be appreciated :)",NONE
295,{'login': 'LihuaWu'},2017-07-28T02:44:02Z,32991,2016-09-18T08:53:59Z,/kubernetes/kubernetes/issues/32991,kubernetes,CLOSED,"when using client-go library to communicate with master, client list pods with watch hang forever
Problem: the following codes just hang and never returns
while codes like:
work as expected.
IMHO, the first use is kind of misleading.
Is this a request for help? (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
What keywords did you search in Kubernetes issues before filing this one? (If you have found any duplicates, you should instead reply there.):
Is this a BUG REPORT or FEATURE REQUEST? (choose one):
Kubernetes version (use kubectl version):
Environment:
OS (e.g. from /etc/os-release):
Kernel (e.g. uname -a):
What happened:
What you expected to happen:
How to reproduce it (as minimally and precisely as possible):
Anything else do we need to know:",MAINTAINABILITY
296,{'login': 'satyajitpadhyewit'},2018-02-12T15:28:11Z,10983,2015-07-09T12:08:23Z,/kubernetes/kubernetes/issues/10983,kubernetes,CLOSED,"--cpuset-cpus supported in kubernetes?
I want to know if --cpuset-cpus in supported in kubernetes so that we can launch containers with that property. Initiated at #10570 but I am not finding enough resources if it can.",NONE
297,{'login': 'feiskyer'},2016-03-13T01:33:14Z,22842,2016-03-11T06:46:45Z,/kubernetes/kubernetes/issues/22842,kubernetes,CLOSED,"Cluster failed to initialize on GCE
I tried to setup a trusty cluster on GCE, but it failed with Cluster failed to initialize within 300 seconds.
Here is my environment setting:
Am I missing something? CC @dchen1107
By the way, this problem has also been confirmed by @Random-Liu .",FAULT TOLERANCE
298,{'login': 'erictune'},2015-09-24T21:25:22Z,14385,2015-09-22T22:36:40Z,/kubernetes/kubernetes/issues/14385,kubernetes,CLOSED,"Default Job Parallelism from Completions
Currently a job's .spec.parallelism defaults to 2. @bgrant0607 suggested defaulting .spec.parallelism to .spec.completions. This will allow users to leave it unspecified in most cases, and it will do something intuitive. (Maybe this was how it was in @soltysh original PR, can't recall if it was that or 1).
The current default of 2 I liked because it encourages users to think about making their containers concurrency-safe. But so does setting it from .spec.completions. And people will ask why it was
I don't like defaulting to 1 as much because I think people will usually want to override it when they have multiple completions.",USABILITY
299,{'login': 'hhy5861'},2016-06-29T18:57:21Z,28144,2016-06-28T06:31:07Z,/kubernetes/kubernetes/issues/28144,kubernetes,CLOSED,"About redis from the main issue, I can not connect master service ip connection pod slave
I am from official instances did a redis master-slave, but I can not connect to the master service allocation in the slave containers such as ip: 10.254.31.52, contrary came in they could not connect to the master slave service assigned ip ask this. what causes the network can not communicate?
Thank you!
service ip
slave container in ping master service ip:
master container in ping slave service ip:
new start service
",AVAILABILITY
300,{'login': 'betterenvi'},2017-12-19T06:13:54Z,15383,2017-12-15T06:02:16Z,/tensorflow/tensorflow/issues/15383,tensorflow,CLOSED,"Feature request: Use placeholders to specify the inputs of TFGAN model.
",NONE
301,{'login': 'EdwardVincentMa'},2018-10-20T00:54:17Z,22826,2018-10-09T03:18:04Z,/tensorflow/tensorflow/issues/22826,tensorflow,CLOSED,"Win10 C++ TF1.9, error LNK2001, build by bazel !
I have generated the TensorFlowV1.9's .so and .lib file successfully on Win10, but when I use this in VS2017, it has errors as bellow :
MFCTestTF1.9.obj : error LNK2001: 无法解析的外部符号 ""char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)"" (?GetVarint32PtrFallback@core@tensorflow@@YAPBDPBD0PAI@Z)
1>D:\ProgramData\VS2017 Project\MFCTestTF1.9\Release\MFCTestTF1.9.exe : fatal error LNK1120: 1 个无法解析的外部命令
1>已完成生成项目“MFCTestTF1.9.vcxproj”的操作 - 失败。
And I also build TensorFlowV1.8 with CMAKE, it work OK without LNK error. But V1.9 can not build by CMAKE.",NONE
302,{'login': 'homingjui'},2018-08-08T19:43:56Z,21037,2018-07-22T17:20:01Z,/tensorflow/tensorflow/issues/21037,tensorflow,CLOSED,"tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory
can anyone help me??
Traceback (most recent call last):
File ""/Users/meow/generate_tfrecord.py"", line 99, in 
tf.app.run()
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
File ""/Users/meow/generate_tfrecord.py"", line 85, in main
writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py"", line 112, in init
compat.as_bytes(path), compat.as_bytes(compression_type), status)
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in exit
c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory",NONE
303,{'login': 'jayendra13'},2017-01-23T22:18:37Z,452,2015-12-09T07:39:14Z,/tensorflow/tensorflow/issues/452,tensorflow,CLOSED,"bazel build error for PolymerElements
I am trying to build tensorflow from source, and bazel is giving some unrelated error
FYI: I have also build bazel from source",NONE
304,{'login': 'rickragv'},,19640,2018-05-30T08:28:23Z,/tensorflow/tensorflow/issues/19640,tensorflow,OPEN,"Tensorflow Serving not using multi GPU/CUDA cores 
I'm using an AWS g3.8xlarge instance which has 2 GPUs.
TF serving is able to detect both GPUs and initialise them but while running the model it only uses 1 GPU to the maximum.
We are on version 1.7, even though the client sends upto 32 requests in parallel, the model server only uses the first GPU
06_09_21",PERFORMANCE
305,{'login': 'mrbrantofgithub'},2017-12-20T01:41:25Z,14761,2017-11-21T14:41:33Z,/tensorflow/tensorflow/issues/14761,tensorflow,CLOSED,"tensorflow lite: error when convert frozen model to lite format
I tried to convert squeezenet frozen model to lite format with the following command:
""bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3""
the output is shown below:
2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)
2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)
2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)
2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.
2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).
2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze
Then I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.
""bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3""
output:
2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)
2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)
2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)
2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.
2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).
2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze
Although I installed tensorflow with ""pip install tensorflow-gpu"", in order to convert model to lite format, I git clone the tensorflow files and configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!",PERFORMANCE
306,{'login': 'delip'},2015-11-11T22:54:36Z,111,2015-11-11T01:10:11Z,/tensorflow/tensorflow/issues/111,tensorflow,CLOSED,"configure script hardcodes location of cuda that makes it fail on OSX
Cuda installation on OSX is at $CUDA_TOOLKIT_PATH/lib (not lib64), and on OSX the shared libraries are end in .dylib (not .so).
",NONE
307,{'login': 'insectatorious'},2017-06-16T22:00:52Z,8199,2017-03-08T14:55:38Z,/tensorflow/tensorflow/issues/8199,tensorflow,CLOSED,"Tensorboard scalar summary graph distorts after resizing and toggling log scale on y-axis 
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None although a search for distorted image tensorboard doesn't help much...
Environment info
Operating System: 16.04 LTS
Firefox: 51.0.1 (64-bit)
TF: 1.0 (installed via pip)
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:
A link to the pip package you installed:
Standard TF pip url.
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".
Steps to reproduce (Firefox only)
On the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg cost or accuracy) by expanding the tab. 
Click on the expand icon 
Enable log scale of y-axis 
Disable log scale of y-axis (note the bug happens regardless of whether you do this) 
Click on expand icon to shrink the graph.
The graph is now overflowing: 
What other attempted solutions have you tried?
Tried to reproduce in Chromium 55.0.2883.87 but unable to.",FAULT TOLERANCE
308,{'login': 'leandroBorgesFerreira'},2017-12-27T08:57:05Z,15645,2017-12-26T18:48:54Z,/tensorflow/tensorflow/issues/15645,tensorflow,CLOSED,"Tensorflow lite 0.1.1 causing Build to fail
I am trying to use tensrflow-lite in Android. When I add
compile 'org.tensorflow:tensorflow-lite:0.1.1'
I get:
I am using multidex and AGP 2.3.3.
When I take tensorflow-lite off, the app builds correctly. When I put it back, the build fails. I believe this is a bug in the library.",NONE
309,{'login': 'nothasson'},2018-08-31T01:45:31Z,21851,2018-08-24T11:28:43Z,/tensorflow/tensorflow/issues/21851,tensorflow,CLOSED,"How to install tensorflow in python3.7?
Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:
It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.
Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
Exact command to reproduce:
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",NONE
310,{'login': 'yanghoonkim'},2017-05-21T11:25:41Z,10074,2017-05-21T09:09:46Z,/tensorflow/tensorflow/issues/10074,tensorflow,CLOSED,"fatal problem with saving variables
I coded a simple feedforward neural network and it works very well.
I tried to save the computation time, and created:
self.total_time = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')
in the fnn class.
and i tried to print the total training time per some training epoch.
I made it to grow with time:
# Check & Print training time
till_now = time.time() - start_time
self.total_time += till_now
print_time(self.total_time.eval())
and the result look something like this :
Epoch : 0 | Evaluation : 115 | Learning Rate : 0.50
Training Loss : 0.040919
Validation Loss : 0.0741969
Validation Accuracy : 97.77%
Total time cost : 0.38 seconds
Epoch : 1 | Evaluation : 116 | Learning Rate : 0.50
Training Loss : 0.0417941
Validation Loss : 0.073841
Validation Accuracy : 97.73%
Total time cost : 0.71 seconds
Epoch : 2 | Evaluation : 117 | Learning Rate : 0.50
Training Loss : 0.0334573
Validation Loss : 0.0745566
Validation Accuracy : 97.75%
Total time cost : 1.01 seconds
However, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of variable total_time and it initialized as 0 which is the value i first give to.
I also checked tf.global_variables() include self.total_time.
What is wrong?",NONE
311,{'login': 'vladfi1'},2016-11-18T00:12:16Z,4431,2016-09-18T02:59:41Z,/tensorflow/tensorflow/issues/4431,tensorflow,CLOSED,"Forward mode ad, directional derivatives
Say I have outputs = f(inputs) and g of the same shape as inputs. I'd like to compute the directional derivative of outputs with respect to inputs in the direction g - in other words, the derivative of f(inputs + alpha * g) with respect to alpha at the point alpha=0.
This is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?",PERFORMANCE
312,{'login': 'tampler'},2018-08-22T19:49:42Z,11937,2017-08-01T09:39:58Z,/tensorflow/tensorflow/issues/11937,tensorflow,CLOSED,"TPU support
I wanna add support for my Tensor Processing Unit chip in TensorFlow.
My TPU is implemented as an accelerator for ARM v7 32bit processor and implements multiple arithmetic kernels, similar to GPU. It implements a simple memory mapped interface, SGDMA and vector instructions over tensors. I added vector extension to GCC 7.1.1 and can run bare metal C++ nets on embedded Ubuntu 16.04
I also checked the TF port for Raspberry Pi 3, but it looks outdated and barely supported.
I'm not currently aware about the scope of work, but believe that should not be that complex, given open examples from GPU vendors and already existing port for Google TPU
Anyone interested in joining this project is highly welcome! Advise, links and code examples are much appreciated
Thank you",PERFORMANCE
313,{'login': 'colmantse'},2018-04-02T17:41:57Z,18108,2018-03-30T03:41:54Z,/tensorflow/tensorflow/issues/18108,tensorflow,CLOSED,"building from source with branch r1.7 gives tf1.5.1 after building wheel
Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:
It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.
Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
Exact command to reproduce: following this https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
I've make sure i'd pull everything from tensorflow. i remove all other branch and check out r1.7, building was successful. No errors and stuff. The wheel i got says tensorflow-1.5.1-cp36 ... etc. , i go on to install it, tf.version = 1.5.1 . I am confused how to build tf 1.7 from source.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",NONE
314,{'login': 'yakotaki'},2018-05-22T14:10:45Z,19463,2018-05-22T14:00:10Z,/tensorflow/tensorflow/issues/19463,tensorflow,CLOSED,"AttributeError: 'NoneType' object has no attribute 'rfind'
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
-TensorFlow installed from (source):
tensorflow v1.6.0-0-gd2e24b6039 1.6.0:
Python 3.5:
Reproduce
git clone tensorflow
python3 tensorflow/examples/speech_commands/train.py
python3 tensorflow/examples/speech_commands/freeze.py 
--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 
--output_file=/tmp/my_frozen_graph.pb
Error appears:
/home/lukas/.local/lib/python3.5/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
2018-05-22 21:59:00.562103: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
Converted 6 variables to const ops.
Traceback (most recent call last):
File ""/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py"", line 180, in 
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
File ""/home/lukas/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py"", line 124, in main
os.path.dirname(FLAGS.output_file),
File ""/usr/lib/python3.5/posixpath.py"", line 148, in dirname
i = p.rfind(sep) + 1
AttributeError: 'NoneType' object has no attribute 'rfind'
Thanks",NONE
315,{'login': 'pronobis'},2016-06-06T18:55:11Z,459,2015-12-09T22:48:23Z,/tensorflow/tensorflow/issues/459,tensorflow,CLOSED,"Single scalar summary point not visible in the plot
When only one event is available for a scalar summary, the plot remains empty, as shown below (here the value is 2.0):
It would be great to see one point corresponding to the value instead. The value does appear in the JSON/CSV file.",NONE
316,{'login': 'leesunfreshing'},2017-05-26T00:33:35Z,10171,2017-05-24T19:13:29Z,/tensorflow/tensorflow/issues/10171,tensorflow,CLOSED,"tf.contrib.rnn.decoder does not require explicitly build encoder?
As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.
The tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?",NONE
317,{'login': 'iNLyze'},2016-08-18T23:07:14Z,3864,2016-08-16T22:42:25Z,/tensorflow/tensorflow/issues/3864,tensorflow,CLOSED,"Tensorflow r.0.10, CUDA 8.0, cuDNN 5.1 core dumped, CUDA_ERROR_OUT_OF_MEMORY
On running a benchmark with MNIST data on a CNN (source below) tensorflow first complains about memory allocation and then appears to have trouble using cuDNN 5.1
Detailed script and output at the bottom.
Problem appears to affect cuDNN specifically as I could run CUDA examples as well as matmul on tensorflow without problems.
Environment info
Operating System: ubuntu 16.04
uname -a
Linux 4.4.0-34-generic #53-Ubuntu SMP Wed Jul 27 16:06:39 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
Installed version of CUDA and cuDNN:
ls -l $CUDA_HOME/lib64/libcud*
-rw-r--r-- 1 root root 560184 Aug 15 22:51 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root 16 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root 19 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root 394472 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root 737516 Aug 15 22:51 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root 13 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root 17 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn_static.a
Environment variables
echo $LD_LIBRARY_PATH
/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
echo $CUDA_HOME
/usr/local/cuda
echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin/:/usr/local/cuda/bin/
Tensorflow version
Compiled from source, r0.10, built into pip package and installed this pip wheel
Configured with cuDNN path and version set to system default
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".
see /// OUTPUT /// at the end
If installed from source, provide
The commit hash (git rev-parse HEAD)
n.a.
version r0.10
The output of bazel version
bazel version
Build label: 0.3.1-2016-08-15 (@936c2c2)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sun Aug 14 23:07:32 2016 (1471216052)
Build timestamp: 1471216052
Build timestamp as int: 1471216052
Steps to reproduce: run this script
import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers.convolutional import Convolution2D
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
load data
(X_train, y_train), (X_test, y_test) = mnist.load_data()
reshape to be [samples][channels][width][height]
X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')
normalize inputs from 0-255 to 0-1
X_train = X_train / 255
X_test = X_test / 255
one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
define a simple CNN model
def baseline_model():
##### create model
model = Sequential()
model.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
##### Compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
return model
build the model
model = baseline_model()
Fit the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)
Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=0)
print(""CNN Error: %.2f%%"" % (100-scores[1]*100))
///////////// OUTPUT /////////////////
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.91GiB
Free memory: 148.69MiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0: Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:840] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 148.69M (155910144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Train on 60000 samples, validate on 10000 samples
Epoch 1/10
E tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
E tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
Aborted (core dumped)",NONE
318,{'login': 'sml0820'},2018-04-22T23:22:41Z,18763,2018-04-21T21:57:29Z,/tensorflow/tensorflow/issues/18763,tensorflow,CLOSED,"Multiple Classes fails in Eager Mode (""tf.keras.Model"")
Bazel version:
N/A
Exact command to reproduce:
Describe the problem
Trying to utilize multiple classes fails in tensorflow eager mode utilizing ""tf.keras.Model"". If I change ""tf.keras.Model"" to ""tfe.Network"" it works - keep in mind I am utilizing tensorflow 1.7. The error I get running the above code results in the error below:
Source code / logs
",NONE
319,{'login': 'kinDSa'},2017-02-27T19:14:02Z,7683,2017-02-20T05:54:43Z,/tensorflow/tensorflow/issues/7683,tensorflow,CLOSED,"Package not Reslove.
Hello,
I am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:
import org.tensorflow.DataType;
import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import org.tensorflow.TensorFlow;
Can any one help to find out where some thing is missing.",NONE
320,{'login': 'harpribot'},2016-06-28T18:56:24Z,2138,2016-04-27T20:08:35Z,/tensorflow/tensorflow/issues/2138,tensorflow,CLOSED,"embedding_attention_seq2seq fails / embedding_rnn_seq2seq works
Environment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN: Cuda 7.0 and CUDNN 6.5 v4
So when I use a simple Embedding RNN Sequence to Sequence Model like this
The above implementation works perfectly, but when I just change the model from simple embedding seq2seq to Embedding Attention Seq2Seq, like this,
I get segmentation fault, with absolutely no information. My memory does not run out, neither my CPU, as I tried this with
and still got the same segmentation fault.
I get the same error, and the above configuration can certainly not eat my RAM.
This is a potential bug, if I am not getting something worng. The LSTM and GRU cell just takes the size of the hidden layer as parameter, which is a scaler.
THE BUG REPORT
The Debug result
The Backtrace is attached below
",NONE
321,{'login': 'srinathmadasu'},2017-05-01T18:44:22Z,9570,2017-05-01T14:20:43Z,/tensorflow/tensorflow/issues/9570,tensorflow,CLOSED,"Tensorflow inconsistence results every run
Please go to Stack Overflow for help and support:
http://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:
It must be a bug or a feature request.
The form below must be filled out.
Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
Exact command to reproduce:
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",NONE
322,{'login': 'leerduo'},2015-11-11T06:19:59Z,96,2015-11-10T16:10:54Z,/tensorflow/tensorflow/issues/96,tensorflow,CLOSED,"tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
Hello,I got the error when i execute:""pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl""--------[ tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.].",NONE
323,{'login': 'jxua'},2017-07-20T16:56:16Z,11633,2017-07-20T06:50:25Z,/tensorflow/tensorflow/issues/11633,tensorflow,CLOSED,"when connect mnist,download mnist data,show network connection error.
I know if the error show ,just our company can not connect to mnist, can i manual download mnist data, and use it? how can i do this?",NONE
324,{'login': 'ypxie'},2016-05-08T21:49:30Z,2269,2016-05-08T07:10:33Z,/tensorflow/tensorflow/issues/2269,tensorflow,CLOSED,"How to resize one tensor to (e.g., 1.5 * its original shape)?
The tensor shape is not fixed, and can change with different input.",NONE
325,{'login': 'lsorber'},2018-01-23T23:40:32Z,11665,2017-07-21T11:37:38Z,/tensorflow/tensorflow/issues/11665,tensorflow,CLOSED,"Feature request: add a `local_init_feed_dict` to `tf.train.Scaffold`
Exact command to reproduce: See below.
Describe the problem
Feature request: add a local_init_feed_dict to tf.train.Scaffold. It would be useful to be able to create local variables (which are not saved or restored) and have them initialized by a tf.train.MonitoredTrainingSession with a feed_dict. In the example below, the variable X_var is forced to be part of the GLOBAL_VARIABLES collection in order to be able to initialize the variable with a feed_dict. This has the undesirable consequence that the variable will be saved to disk.
Source code / logs
",NONE
326,{'login': 'zuoxingdong'},2016-07-07T16:49:01Z,3185,2016-07-04T14:29:26Z,/tensorflow/tensorflow/issues/3185,tensorflow,CLOSED,"Udacity Notebook with ""None"" kernel
I use Jupyter notebook to open the .ipynb files, but it shows a red ""None"" kernel on top right corner and all lines of code cannot run.
Method I use:
Build a new directory and extract .ipynb files from examples/udacity to the directory
In terminal, run jupyter notebook
",NONE
327,{'login': 'dbakshee'},2018-01-24T20:22:44Z,11099,2017-06-28T04:26:13Z,/tensorflow/tensorflow/issues/11099,tensorflow,CLOSED,"Typo in illustrating figure for XLA/Concatenation operation
Illustrating image for Concatenate
suggests Concat({ 2x4, 2x8 }, dimension=0) is 2x12. Should be dimension=1, and same for the other examples.",FAULT TOLERANCE
328,{'login': 'lijhong'},2017-09-02T19:31:07Z,12764,2017-09-02T12:12:36Z,/tensorflow/tensorflow/issues/12764,tensorflow,CLOSED,"the side &deep model is not good compare with deep model and wide model 
these days ,I'm learning the wide & deep model ,and run the wide_n_deep_tutorial.py, so the anwser looks like this:
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py --model_type=deep
Training data is downloaded to /tmp/tmpFB4dsd
Test data is downloaded to /tmp/tmpomj5Pi
2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpWei9wK
accuracy: 0.850071
accuracy_baseline: 0.763774
auc: 0.894038
auc_precision_recall: 0.743199
average_loss: 0.393638
global_step: 2000
label/mean: 0.236226
loss: 39.3179
prediction/mean: 0.242167
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py --model_type=wide
Training data is downloaded to /tmp/tmpFJdWft
Test data is downloaded to /tmp/tmpjB5nm7
2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpuPHsDx
accuracy: 0.835391
accuracy_baseline: 0.763774
auc: 0.882763
auc_precision_recall: 0.694257
average_loss: 0.352975
global_step: 2000
label/mean: 0.236226
loss: 35.2563
prediction/mean: 0.240918
XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py
Training data is downloaded to /tmp/tmpDdWc_T
Test data is downloaded to /tmp/tmpFF0PZJ
2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
your machine and could speed up CPU computations.
2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpq2M7SE
accuracy: 0.820834
accuracy_baseline: 0.763774
auc: 0.850518
auc_precision_recall: 0.676198
average_loss: 0.424271
global_step: 2000
label/mean: 0.236226
loss: 42.3776
prediction/mean: 0.256489
I don't know why looks likes this, so someone can help me? thank you.",PERFORMANCE
329,{'login': 'apiro'},2018-03-01T19:00:43Z,17358,2018-03-01T15:33:26Z,/tensorflow/tensorflow/issues/17358,tensorflow,CLOSED,"Distributed training: Evaluation and inference best practices
I understand tensorflow distributed training and I implemented my own script.
What I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.
Let's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.
My intuition to achieve this goal is to do the following:
Now, what about the evaluation and inference sessions?
For training, I can use tf.train.MonitoredTrainingSession, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use tf.Session.
Regarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls eval_model.eval(...) or infer_model.infer(...), but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to ""periodically"" is to sleep the thread.
What do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?
Alberto",NONE
330,{'login': 'orpillar'},2017-11-03T01:45:06Z,14170,2017-11-02T03:23:27Z,/tensorflow/tensorflow/issues/14170,tensorflow,CLOSED,"List of functions could be improved with ""const std::string&"" or ""std::string&"" instead ""std::string""
After a quick scan in the latest tensorflow ""master"" branch, here is the list of functions which could improve passing parameter by ""std::string"":
After a quick scan in the latest tensorflow ""master"" branch, here is the list of functions which could improve passing parameter by ""std::string"":
c/c_api_function.cc: static string Normalize(string name);
Suggestion: string& name;
c/c_api_function.cc:string NodeNameMapping::Normalize(string name) {
Suggestion: string& name
compiler/jit/graph_to_functiondef.cc: string NormalizeHelper(string name) const;
Suggestion: string& name
compiler/jit/graph_to_functiondef.cc: string UniquifyHelper(string name);
Suggestion: const string&
compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::NormalizeHelper(string name) const {
Suggestion: string& name
compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::UniquifyHelper(string name) {
Suggestion: const string&
compiler/tf2xla/dump_graph.cc:string MakeUniquePath(string name) {
Suggestion: string& name
compiler/xla/service/llvm_ir/llvm_util.cc:string IrName(string a) {
Suggestion: string& a
compiler/xla/service/llvm_ir/llvm_util.cc:string SanitizeFunctionName(string function_name) {
Suggestion: string& function_name
compiler/xla/service/llvm_ir/llvm_util.h:string IrName(string a);
Suggestion: string& a
compiler/xla/service/llvm_ir/llvm_util.h:string SanitizeFunctionName(string function_name);
Suggestion: string& function_name
compiler/xla/util.cc:string SanitizeFileName(string file_name) {
Suggestion: string& file_name
compiler/xla/util.h:string SanitizeFileName(string file_name);
Suggestion: string& file_name
contrib/verbs/rdma.cc:RdmaBuffer::RdmaBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaAckBuffer::RdmaAckBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaMessageBuffer::RdmaMessageBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.cc:RdmaTensorBuffer::RdmaTensorBuffer(RdmaChannel* channel, string name)
Suggestion: const string& name
contrib/verbs/rdma.h: explicit RdmaBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h: explicit RdmaAckBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h: explicit RdmaMessageBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
contrib/verbs/rdma.h: explicit RdmaTensorBuffer(RdmaChannel* channel, string name);
Suggestion: const string& name
core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithStreamAll(string device_name) {
Suggestion: string& device_name
core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithoutStream(string device_name) {
Suggestion: string& device_name
core/kernels/ops_util.cc:string SanitizeThreadSuffix(string suffix) {
Suggestion: const string& suffix
core/kernels/ops_util.h:string SanitizeThreadSuffix(string suffix);
Suggestion: const string& suffix
core/kernels/xsmm_conv2d.cc:static void chk_libxsmm_err(libxsmm_dnn_err_t status, string msg) {
Suggestion: const string& msg
stream_executor/platform.cc:PlatformKind PlatformKindFromString(string kind) {
Suggestion: const string& kind
stream_executor/platform.h:PlatformKind PlatformKindFromString(string platform_string);
Suggestion: const string& platform_string
",NONE
331,{'login': 'sibyjackgrove'},2018-05-01T18:23:31Z,18952,2018-04-29T00:38:12Z,/tensorflow/tensorflow/issues/18952,tensorflow,CLOSED,"Feature request: Option to create dataset from a subset of the columns in the CSV file using tf.contrib.data.make_csv_dataset()
Exact command to reproduce:
tf.contrib.data.make_csv_dataset()
Describe the problem
The tf.contrib.data.make_csv_dataset() is a very useful feature which allows us to convert CSV files directly into at dataset without having to use Pandas library (like shown here). However it is missing an important feature which Pandas had, that is to read a subset of the columns in the CSV file.
For example the following code:
results in following dataset:
However I don't want feature columns for 'Hour_Ending' and 'Market_Day' in my dataset (since they are not relevant training data) . This could be done in Pandas using code below:
I know the easy solution would be to create a CSV file having only the feature columns I want. But it would be a great utility feature to add before make_csv_dataset() migrates out of contrib into core TF. I can submit a PR for this if required.",NONE
332,{'login': 'bb4242'},2016-10-12T21:02:55Z,4917,2016-10-12T18:29:30Z,/tensorflow/tensorflow/issues/4917,tensorflow,CLOSED,"QueueRunner deadlock when using all CPUs
I'm building an input pipeline following the guidelines here. The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using tf.py_func), and return the processed results to an output queue. I'd like to use QueueRunner's ability to process multiple examples in parallel by launching one processing thread per CPU core. Here's a simplified example of what I'm trying to do:
The program above deadlocks waiting for sess.run to complete in python_op:
This is running on an 8-core machine; you can see that 8 python_ops are currently running but are failing to finish. If, however, we don't use all CPUs (by changing (n_cpus) to (n_cpus-1) in the line that creates the tf.train.QueueRunner, then the program runs to completion:
The program also completes successfully if we pass in fewer examples than CPUs in the input queue.
I realize it's somewhat awkward for python_op to call back into the tensorflow session. However, the threading and queues section of the manual states:
""The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.""
So, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU). Is this a bug, or is there some reason I shouldn't expect it to work?
As a side note, one option to work around my problems would be to break python_op into several smaller pieces and chain the intermediate results together in the tensorflow computation graph. However, in my real pipeline, this isn't straightforward to do, since python_op's real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.
OS: Linux
Tensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)",PERFORMANCE
333,{'login': 'ThatsDevraj'},2017-03-10T16:18:36Z,8273,2017-03-10T12:02:14Z,/tensorflow/tensorflow/issues/8273,tensorflow,CLOSED,"Error:
NOTE: Only file GitHub issues for bugs and feature requests. All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System:
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:
A link to the pip package you installed:
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".
If installed from source, provide
The commit hash (git rev-parse HEAD)
The output of bazel version
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
What other attempted solutions have you tried?
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).",NONE
334,{'login': 'lepangdan'},2018-07-27T08:04:11Z,21142,2018-07-26T03:46:24Z,/tensorflow/tensorflow/issues/21142,tensorflow,CLOSED,"Feature Request: Provide tf.pow with supporting broadcasting?
Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:
It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.
Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
Exact command to reproduce:
You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",NONE
335,,2016-02-25T07:05:30Z,1279,2016-02-24T22:13:59Z,/tensorflow/tensorflow/issues/1279,tensorflow,CLOSED,"Arch doesn't support it
After sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl or as root, I got:
tensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
It seems it doesn't work with Python 3. What should I do?",NONE
336,{'login': 'shoyer'},2018-04-18T17:35:45Z,17877,2018-03-21T01:39:58Z,/tensorflow/tensorflow/issues/17877,tensorflow,CLOSED,"tf.manip.roll silently ignores negative axes
Exact command to reproduce:
Describe the problem
axis=-1 and axis=0 should be equivalent, if tf.manip.roll() works like numpy.roll() and other TensorFlow/NumPy functions that accept negative axes. However, instead negative axes are silently ignored. At the very least, TensorFlow should have raised an informative error.",USABILITY
337,{'login': 'puneet336'},2016-12-08T07:52:18Z,5447,2016-11-07T11:39:46Z,/tensorflow/tensorflow/issues/5447,tensorflow,CLOSED,"invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive] 
Hi all,
I am trying to compile tensorflow-0.11 + bazel 0.3.2 on RHEL 6 with cuda 7.0 + cudnn 7.5.5.0 + gcc 4.9.
The compilation command is :
EXTRA_BAZEL_ARGS=""--jobs 10"" bazel build -c opt --config=cuda --jobs 10 //tensorflow/tools/pip_package:build_pip_package
Compilation of rule '//tensorflow/stream_executor:stream_executor' fails with cuda specific message.
I have latest version of compilers at non standard path , hence i had modified some variables in CROSSTOOL.tpl + third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl. I am attaching compilation error logs, CROSSTOOL.tpl and third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl for your reference.
Though i am able to compile cpu-only version of tensorflow successfully.
Please let me know if any information is needed from my side.
Eagerly awaiting your replies.
crosstool_wrapper_driver_is_not_gcc.tpl.txt
CROSSTOOL.tpl.txt
tensorflow_build2.log.txt",NONE
338,{'login': 'AnlanSun'},2017-06-16T20:35:02Z,7908,2017-02-27T00:38:02Z,/tensorflow/tensorflow/issues/7908,tensorflow,CLOSED,"Changing computer make the pretrained model fail
I switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.
using TF:1.0",NONE
339,{'login': 'ethereon'},2017-06-16T22:05:08Z,4419,2016-09-16T20:33:04Z,/tensorflow/tensorflow/issues/4419,tensorflow,CLOSED,"tf.get_variable without an explicit initializer fails for integer types
The following fails (shape and name are arbitrary):
Exception: TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.
In contrast, using tf.float32 works just fine.
The problem appears to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658
If an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that sqrt(3)==1.7320...), which of course conflicts with the requested integer type.
While this can be mitigated by doing something like:
it feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).
Tested on the current master.",FAULT TOLERANCE
340,{'login': 'bean-du'},2016-01-19T04:00:05Z,806,2016-01-19T02:39:26Z,/tensorflow/tensorflow/issues/806,tensorflow,CLOSED,"google-tensorflow
",NONE
341,{'login': 'kocabey'},2016-12-21T09:16:48Z,4021,2016-08-24T18:32:50Z,/tensorflow/tensorflow/issues/4021,tensorflow,CLOSED,"Improving Google Indexing for the Documentation
Whenever I run a Google search on a TensorFlow functionality, say, tf.reshape, it gives me the entire documentation, not the specific documentation related to that functionality.
Currently the way I use is to run a search with ctrl + f to find specific documentation related to what I search for.
Numpy has that property, i.e. when you run a Google search on np.reshape, you get the specific page.
It would be a nice improvement for the documentation if someone fixes the Google indexing for the documentation page, especially for the users who frequently use Google search for the documentation.",USABILITY
342,{'login': 'eli7310'},2018-11-18T01:05:38Z,23208,2018-10-24T08:50:07Z,/tensorflow/tensorflow/issues/23208,tensorflow,CLOSED,"TfLiteCameraDemo.apk with NNAPI
Can someone please upload a version of TfLiteCameraDemo.apk that supports NNAPI?",NONE
343,{'login': 'Franck-Dernoncourt'},2017-02-21T18:00:06Z,7638,2017-02-17T22:45:32Z,/tensorflow/tensorflow/issues/7638,tensorflow,CLOSED,"Links to Android nightly builds on README.md are broken
Links to Android nightly builds on https://github.com/tensorflow/tensorflow/blob/master/README.md are broken.
-->
",NONE
344,{'login': 'rdeepc'},2017-02-16T21:31:45Z,7558,2017-02-16T06:54:25Z,/tensorflow/tensorflow/issues/7558,tensorflow,CLOSED,"tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.
NOTE: Only file GitHub issues for bugs and feature requests. All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System:
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:
A link to the pip package you installed:
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".
If installed from source, provide
The commit hash (git rev-parse HEAD)
The output of bazel version
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
What other attempted solutions have you tried?
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).",NONE
345,{'login': 'pwaller'},2016-10-19T01:46:19Z,4397,2016-09-15T16:04:12Z,/tensorflow/tensorflow/issues/4397,tensorflow,CLOSED,"Docker build devel-gpu build fails with 8.0-cudnn5-devel (Cannot find cudnn.h under .../lib)
Summary: Why is it looking for the header file under the lib directory?
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
#3989 (and references within)
Environment info
Operating System: Ubuntu 16.04. Docker 1.12.1.
If possible, provide a minimal reproducible example
Modify devel-gpu to read ""FROM nvidia/cuda:8.0-cudnn5-devel"".
Run docker build -f Dockerfile.devel-gpu -t tf from the /tensorflow/tools/docker directory. (HEAD @ 4addf4b at time of posting)
What other attempted solutions have you tried?
None yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.
Logs or other output that would be helpful
",NONE
346,{'login': 'gunan'},2017-03-22T05:31:48Z,4848,2016-10-09T07:48:11Z,/tensorflow/tensorflow/issues/4848,tensorflow,CLOSED,"""bazel clean"" will undo ""./configure"" when source configured to build for GPU.
Environment info
Operating System:
ubuntu 14.04
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
cuda 8.0.44, cudnn 5.5
If installed from binary pip package, provide:
No
If installed from source, provide
The commit hash (git rev-parse HEAD)
c8d4896
The output of bazel version
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
./configure # configure with GPU support)
bazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one should pass
bazel clean
bazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one fails, complaining no GPU support configured.
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@ae
3629bd' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@1374ec1e', 'CONFIGURATION_FRAGMENT:com
.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@3686b55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue
$ConfigurationFragmentKey@a93d9174')
at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
David, Damien, looks like bazel clean undid some output of ""./configure"" at head.
Any idea where things went wrong?",NONE
347,{'login': 'edi-bice'},2018-02-07T23:24:41Z,2291,2016-05-09T13:34:03Z,/tensorflow/tensorflow/issues/2291,tensorflow,CLOSED,"Add -lm (Was: undefined reference to symbol 'ceil@@GLIBC_2.2.5)
Environment info
Operating System:
epel-release-6-8.noarch
redhat-release-server-6Server-6.7.0.3.el6.x86_64
Installed version of CUDA and cuDNN:
None
If installed from sources, provide the commit hash:
f8eb1d7
Steps to reproduce
bazel clean
./configure
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
Logs or other output that would be helpful
(If logs are large, please upload as attachment).
ERROR: /home/ebice/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command /opt/rh/devtoolset-2/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -no-canonical-prefixes -B/opt/rh/devtoolset-2/root/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 11 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/opt/rh/devtoolset-2/root/usr/bin/ld: /opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/libstdc++_nonshared.a(hashtable_c++0x44.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'
/opt/rh/devtoolset-2/root/usr/bin/ld: note: 'ceil@@GLIBC_2.2.5' is defined in DSO /lib64/libm.so.6 so try adding it to the linker command line
/lib64/libm.so.6: could not read symbols: Invalid operation
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build",NONE
348,{'login': 'concretevitamin'},2016-07-08T16:59:15Z,1828,2016-04-08T22:59:50Z,/tensorflow/tensorflow/issues/1828,tensorflow,CLOSED,"SparseTensor: common unary ops
Currently, we have a slew of common unary ops that work on Tensors, but not SparseTensors (ref):
and so on.
We'd like these ops to work on SparseTensor. These do not change the indices nor shape of SparseTensors, so all that's needed is transform the .values field on Python side in O(1) line.",NONE
349,{'login': 'githubgsq'},2018-07-28T00:48:36Z,19731,2018-06-04T03:11:59Z,/tensorflow/tensorflow/issues/19731,tensorflow,CLOSED,"How to release GPU memory after sess.close()?
hi, all:
I'm training models iteratively. After each model trained, I run sess.close() and recreate a new session to run a new training process. But it seems that the GPU memory was not relseased and it's increasing constantly.
I tried tf.reset_default_graph() before run session also typed gc.collect() after sess.close(), but takes no effect.
How could I release GPU memory timely to avoid OOM error please?
Thanks!",PERFORMANCE
350,{'login': 'xuancong84'},2017-06-16T23:41:48Z,6111,2016-12-06T03:56:26Z,/tensorflow/tensorflow/issues/6111,tensorflow,CLOSED,"Flawed memory management: allow_growth=True consumes more memory, causing out-of-memory
To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:
However, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:
However, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.
In principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.
Below are my system info:
",PERFORMANCE
351,{'login': 'yihong-chen'},2016-11-11T19:10:20Z,5284,2016-10-30T11:07:18Z,/tensorflow/tensorflow/issues/5284,tensorflow,CLOSED,"tf.contrib.learn output shapes : shapes (?, 1) and (?,) are incompatible
I tried to train a binary DNNClassifier similar as the example on https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart.
I got the following traceback
traceback.txt
I explored the tensorflow source code and found that it may relate to _get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py
When doing a binary classification,y_shape is something like[batch_size,1].Line 52 changes it into [1].Line 54,55 change it into [].And finally the output_shape is [batch_size,].However the correct ouput shape should be [batch_size,1].
To sum up,we do not need to skip 1st dimension if it is 1 and len(y_shape)=1.",NONE
352,{'login': 'maharjun'},2018-10-13T18:24:16Z,22249,2018-09-13T09:14:22Z,/tensorflow/tensorflow/issues/22249,tensorflow,CLOSED,"Inconsistency in supported integer types on GPU
Exact command to reproduce:
Describe the problem
It appears that the kernel of tf.reduce_sum is not registerd for GPU's if the type of the tensor to be summed is int64 (only registered for tf.int32)
Moreover the kernel of tf.tile is not registered for the case where the tensor to be tiled is of type tf.int32 (only registered for tf.int64)
Why is there this inconsistency?",NONE
353,{'login': 'wenouyang'},2017-04-02T19:32:14Z,8910,2017-04-02T19:11:53Z,/tensorflow/tensorflow/issues/8910,tensorflow,CLOSED,"regarding the ValueError: inputs must be a list of at least one Tensor with the same dtype and shape
There is a program that defines the loss function as follows:
Running the program raises the following error message
File ""/home/ decoder/kitti_multiloss.py"", line 86, in loss
name='reg_loss')
File ""/devl /tensorflow/tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py"", line 1827, in add_n
raise ValueError(""inputs must be a list of at least one Tensor with the ""
ValueError: inputs must be a list of at least one Tensor with the same dtype and shape
I am curious how to print out the tensor information of the first parameter tf.get_collection(reg_loss_col) in tf.add_n, so that I can figure out why this cause the error.",NONE
354,{'login': 'com9009'},2018-03-10T01:41:07Z,16995,2018-02-14T00:07:44Z,/tensorflow/tensorflow/issues/16995,tensorflow,CLOSED,"tf.fake_quant_with_min_max_vars returns wrong answer
Exact command to reproduce:
Describe the problem
tf.fake_quant_with_min_max_vars returns wrong answer.
Source code / logs
it prints like below
",NONE
355,{'login': 'glhfgg1024'},2018-02-19T06:25:40Z,17121,2018-02-19T06:22:19Z,/tensorflow/tensorflow/issues/17121,tensorflow,CLOSED,"Feature Request for the back-propagated errors in intermediate layers
After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:
I is the input, O is the output, W1,W2,W3 is the weights for 3 layers. C1 and C2 are the outputs for the first two layers. With O and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to C1 and C2?
I know we could get the parameter operators as follows:
My final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).
I know that we could use the tf.test.check_gradient to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op. Thus, I'm not sure if this is good or valid operator that is suitable for learning.
In the Caffe framework, it seems those errors were saved in diff memory for each layer. I want to get these back-propagated errors in each layer. Does anybody know how to get that?",NONE
356,{'login': 'jowagner'},2018-02-24T03:40:27Z,6956,2017-01-19T13:49:20Z,/tensorflow/tensorflow/issues/6956,tensorflow,CLOSED,"Error downloading nasm
As noted in issue #6950 nasm-2.12.02.tar.bz2 is currently unavailable. (www.nasm.us does not accept connections.) @trsaunders observed this for head of r0.12 and I can confirm this for v0.12.0.
Workaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.",NONE
357,{'login': 'dbikard'},2016-07-27T19:26:18Z,3439,2016-07-21T09:47:39Z,/tensorflow/tensorflow/issues/3439,tensorflow,CLOSED,"""tensorflow: Input iterator is exhausted"" when passing numpy arrays in validation monitor
I'm not sure if this is a bug or whether I just don't understand the usage of monitors (in which case I apologize for posting here). I'm trying to use a validation monitor by passing my validation set as numpy array.
(X_val and Y_val are numpy arrays)
The code runs but only the first validation step is done properly, then I get the following message in the logs:
INFO:tensorflow:Input iterator is exhausted.
Any help is welcome!",NONE
358,{'login': 'tfboyd'},2019-05-11T02:25:21Z,22357,2018-09-18T20:51:54Z,/tensorflow/tensorflow/issues/22357,tensorflow,CLOSED,"Make NVIDIA library versions to TF Version matrix more visible.
This request was a product of the TensorFlow Fall Symposium. Consider talking to the documentation team as well as maybe linking it to the top or near the top of all the release documents.",USABILITY
359,{'login': 'laket'},2018-10-24T13:08:34Z,22861,2018-10-10T08:59:19Z,/tensorflow/tensorflow/issues/22861,tensorflow,CLOSED,"Variable names created by tf.kera.Model.build() is inconsistent with that by tf.keras.Model.call()
Exact command to reproduce: Please see the below
Describe the problem
tf.keras.Model makes Variables of its weights, when its build() or call() is called first.
While I found call() makes Variables with the prefix ""MyModel/"", build() make Variables without any prefix.
That is inconvenient to manage non-object based checkpoint.
",NONE
360,{'login': 'Lldenaurois'},2017-02-13T17:44:20Z,6644,2017-01-04T21:21:42Z,/tensorflow/tensorflow/issues/6644,tensorflow,CLOSED,"Error: Data loss: file is too short to be an sstable
Hi there,
I'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm
I am getting a ""Data loss"" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.
Currently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.
",PERFORMANCE
361,{'login': 'ed-alertedh'},,12345,2017-08-17T02:33:49Z,/tensorflow/tensorflow/issues/12345,tensorflow,OPEN,"PEP 484 Type Annotations (feature request)
N/A
Describe the problem
Background
PEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.
When adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the ""untyped"" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.
Benefits of Adding Type Annotations
The expected inputs and outputs of functions become much clearer
Code completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs
Static analysis can uncover latent bugs (case study here[5])
Difficulties/Drawbacks
People may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages ""Power Features"" [4] I would argue that striving towards code that is explicit is a similar philosophy
The protobuf compiler would need to be augmented to generate type annotations.
The Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.
Tensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.
Final thoughts
I realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.
[1] https://www.python.org/dev/peps/pep-0484/
[2] http://mypy-lang.org/
[3] https://github.com/python/typeshed
[4] https://google.github.io/styleguide/pyguide.html#Power_Features
[5] http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/",MAINTAINABILITY
362,{'login': 'alanhdu'},2018-06-18T19:01:57Z,20096,2018-06-18T15:32:41Z,/tensorflow/tensorflow/issues/20096,tensorflow,CLOSED,"ModuleNotFoundError: No module named 'tensorflow.keras'
Describe the problem
Using Tensorflow 1.8.0, running:
raises an error:
Of course, from tensorflow import keras works fine.
This is a minor nit since there's an obvious workaround, but IMO this is pretty unintuitive behavior for how modules work in Python. I'm not sure what kind of sorcery is going on to end up with this result 😆.",NONE
363,{'login': 'taki0112'},2019-04-18T13:41:02Z,14070,2017-10-29T13:09:55Z,/tensorflow/tensorflow/issues/14070,tensorflow,CLOSED,"Feature request : add weight normalization
can you implement weight norm ?
I want to use it as follows.
Is it possible?",NONE
364,{'login': 'wangli0519'},2017-01-10T19:08:31Z,6769,2017-01-10T15:02:29Z,/tensorflow/tensorflow/issues/6769,tensorflow,CLOSED,"TF Learn TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16
NOTE: Only file GitHub issues for bugs and feature requests. All other topics will be closed.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
tf.cast()
Environment info
Operating System:
masOSSierra
jupyter notebook
tensforflow v0.12.1
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
No
If installed from binary pip package, provide:
A link to the pip package you installed: pip install tensorflow
The output from python -c ""import tensorflow; print(tensorflow.__version__)"".tensforflow v0.12.1
If installed from source, provide
The commit hash (git rev-parse HEAD)
The output of bazel version
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py
What other attempted solutions have you tried?
try to cast DataFrame into float32 with
X_train = X_train.astype(np.float32)
try to cast each column with
tf.cast(col, tf.float32)
but after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)
feature_columns dtype just turn to tf.float64
(tried and didn't find attribute from source code that I can change dtype here)
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
TypeError Traceback (most recent call last)
 in ()
----> 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])
/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)
189 _call_location(), decorator_utils.get_qualified_name(func),
190 func.module, arg_name, date, instructions)
--> 191 return func(*args, **kwargs)
192 new_func.doc = _add_deprecated_arg_notice_to_docstring(
193 func.doc, date, instructions)
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
353 steps=steps,
354 monitors=monitors,
--> 355 max_steps=max_steps)
356 logging.info('Loss for final step: %s.', loss)
357 return self
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)
697 # cases, but will soon be deleted after the subclasses are updated.
698 # TODO(b/32664904): Update subclasses and delete the else-statement.
--> 699 train_ops = self._get_train_ops(features, labels)
700 if isinstance(train_ops, model_fn_lib.ModelFnOps): # Default signature
701 train_op = train_ops.train_op
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)
1050 ModelFnOps object.
1051 """"""
-> 1052 return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
1053
1054 def _get_eval_ops(self, features, labels, metrics):
/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)
1019 params=self.params)
1020 else:
-> 1021 model_fn_results = self._model_fn(features, labels, mode=mode)
1022 else:
1023 model_fn_results = self._model_fn(features, labels)
 in conv_model(feature, target, mode)
10 activation_fn=tf.nn.relu)
11
---> 12 h_pool1 = max_pool_2x2(h_conv1)
13
14 with tf.variable_scope('conv_layer2'):
 in max_pool_2x2(tensor_in)
1 def max_pool_2x2(tensor_in):
----> 2 return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)
1615 padding=padding,
1616 data_format=data_format,
-> 1617 name=name)
1618
1619
/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)
1596 result = _op_def_lib.apply_op(""MaxPool"", input=input, ksize=ksize,
1597 strides=strides, padding=padding,
-> 1598 data_format=data_format, name=name)
1599 return result
1600
/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
580 for base_type in base_types:
581 _SatisfiesTypeConstraint(base_type,
--> 582 _Attr(op_def, input_arg.type_attr))
583 attrs[input_arg.type_attr] = attr_value
584 inferred_from[input_arg.type_attr] = input_name
/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)
58 ""DataType %s for attr '%s' not in list of allowed values: %s"" %
59 (dtypes.as_dtype(dtype).name, attr_def.name,
---> 60 "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
61
62
TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16
Many thanks.",NONE
365,{'login': 'nehardave'},2017-03-23T06:13:40Z,8586,2017-03-21T14:58:38Z,/tensorflow/tensorflow/issues/8586,tensorflow,CLOSED,"TensorFlow upgrade to 1.0.1 
I upgraded my server from 1.0.0 (ubuntu):
pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl
But I get the following discrepancy:
(tensorflow)$ pip list | grep tensorflow
tensorflow (1.0.0)
(tensorflow)$ python -c 'import tensorflow as tf; print(tf.version)'
1.0.1",NONE
366,{'login': 'eaplatanios'},2018-01-30T01:55:50Z,15323,2017-12-12T21:45:03Z,/tensorflow/tensorflow/issues/15323,tensorflow,CLOSED,"Beam Search Decoder API
@ebrevdo Why is it that the user needs to call tile_batch explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided initial_state in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.
Thank you!",USABILITY
367,{'login': 'sakaia'},2018-01-25T16:06:01Z,16400,2018-01-25T10:17:07Z,/tensorflow/tensorflow/issues/16400,tensorflow,CLOSED,"[doc] link to ""How to Use t-SNE Effectively"" from embeddings is broken
Have I written custom code (as opposed to using a stock example script provided in TensorFlow) NO (since Web page problem):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7:
TensorFlow installed from (source or binary) binary:
**TensorFlow version (use command below) 1.5.0rc0 **:
Python version 3.5.1:
Bazel version (if compiling from source) NOT USED:
GCC/Compiler version (if compiling from source) NOT USED:
CUDA/cuDNN version NOT USED:
**GPU model and memory NOT USED **:
Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/:
Describe the problem
Link to to ""How to Use t-SNE Effectively"" is broken.
The page link is follows (before junmping)
https://www.tensorflow.org/programmers_guide/embedding
404 page is following URL
https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/
Original page should be follows. (the URL in embedding.md should rewrite to follows)
https://distill.pub/2016/misread-tsne/
Source code / logs
The problem code is follows.
https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123
",NONE
368,{'login': 'lizaigaoge550'},2018-08-31T22:40:02Z,21985,2018-08-31T02:46:10Z,/tensorflow/tensorflow/issues/21985,tensorflow,CLOSED,"While loop no gradients provided for any variable
After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.
while_loop
def dynamic_pointing_decoder(self, U, mask):
def _HMN(ut, h, us, ue):
h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d
WD = tf.get_variable(name=""WD"", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())
r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d
ut_r = tf.concat([ut, r], axis=1) #batch,3d
W1 = tf.get_variable(name=""W1"", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())
b1 = tf.get_variable(name=""b1_Bias"", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1
mt1 = tf.reduce_max(mt1, axis=2) #batch * d
W2 = tf.get_variable(name=""W2"", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b2 = tf.get_variable(name=""b2_Bias"", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2
mt2 = tf.reduce_max(mt2, axis=2) #batch * d
mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d
W3 = tf.get_variable(name=""W3"", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b3 = tf.get_variable(name=""b3_Bias"", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())
hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3
hmn = tf.reduce_max(hmn, axis=2) #batch 1
hmn = tf.reshape(hmn, [-1]) #batch
return hmn
#no while loop
def dynamic_pointing_decoder(self, U, mask):
def _HMN(ut, h, us, ue):
h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d
WD = tf.get_variable(name=""WD"", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())
r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d
ut_r = tf.concat([ut, r], axis=1) #batch,3d
W1 = tf.get_variable(name=""W1"", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())
b1 = tf.get_variable(name=""b1_Bias"", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1
mt1 = tf.reduce_max(mt1, axis=2) #batch * d
W2 = tf.get_variable(name=""W2"", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b2 = tf.get_variable(name=""b2_Bias"", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2
mt2 = tf.reduce_max(mt2, axis=2) #batch * d
mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d
W3 = tf.get_variable(name=""W3"", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
b3 = tf.get_variable(name=""b3_Bias"", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())
hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3
hmn = tf.reduce_max(hmn, axis=2) #batch 1
hmn = tf.reshape(hmn, [-1]) #batch
return hmn
with tf.variable_scope('dynamic_pointing_decoder'):
#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)
#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])
#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)
cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)
i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
idx = tf.range(0, tf.shape(U)[0], 1)
s_idx = tf.stack([idx, i_start], axis=1)
e_idx = tf.stack([idx, i_end], axis=1)
us = tf.gather_nd(U, s_idx) #batch 2d
ue = tf.gather_nd(U, e_idx) #batch 2d
alphas, betas = [], []
state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),
tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32)) # initial hidden state of RNN
U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d
for time_step in range(4):
if time_step >= 1:
tf.get_variable_scope().reuse_variables()
us_ue = tf.concat([us,ue], axis=1) #batch 4d
h, state = cell(inputs=us_ue, state=state) #batch * d
anyone can help me? Thank you very much",NONE
369,{'login': 'martinitus'},2016-09-06T23:04:01Z,4027,2016-08-24T23:17:50Z,/tensorflow/tensorflow/issues/4027,tensorflow,CLOSED,"[cmake] Build error in dependency re2
Somehow the re2 dependency does not get build correctly.
I think its because of re2_INCLUDE_DIR in re.cmake holding two directories. Then COMMAND ${CMAKE_COMMAND} -E make_directory ${re2_INCLUDE_DIR} fails since cmake -E make_directory only takes one arg. I'm just comiping and might add a PR if it works.",NONE
370,{'login': 'carlthome'},2017-11-22T15:42:51Z,14797,2017-11-22T14:45:08Z,/tensorflow/tensorflow/issues/14797,tensorflow,CLOSED,"XLA AOT tfcompile failure due to undeclared inclusions in cc_binary rule
This happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:
graph.cc pretty much just does #include ""graph.h"" as per the tfcompile tutorial and it's weird because these headers seem to be included in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.
This is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):
I'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.",NONE
371,{'login': 'drasmuss'},2018-02-08T01:06:26Z,7251,2017-02-03T21:48:30Z,/tensorflow/tensorflow/issues/7251,tensorflow,CLOSED,"Native GPU version of `tf.dynamic_stitch`
Environment info
Operating System: Windows 10
Installed version of CUDA and cuDNN: 8.0, 5105
tensorflow release 0.12.1
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
The log_device_placement output shows that everything is assigned to the GPU, as expected. However, inspecting the trace output shows that data is being copied on and off the GPU for each call to dynamic_stitch. This is something specific to the dynamic_stitch implementation, because using tf.gather (a similar indexed read operation, and functionally equivalent in this case), doesn't show this behaviour.
Is this intended behaviour for dynamic_stitch (i.e., the copying to and from the GPU is necessary)? Or is this a bug? If it isn't a bug, is there some equivalent solution that doesn't require the data to be copied back and forth?",NONE
372,{'login': 'dhasegan'},2017-05-19T03:09:35Z,10019,2017-05-19T01:30:21Z,/tensorflow/tensorflow/issues/10019,tensorflow,CLOSED,"Java Api String tensors support
from Tensor.java:
non-scalar DataType.STRING tensors are not supported yet
Is there a plan for adding them for the Java interface?",NONE
373,{'login': 'worldforward'},2018-06-07T09:27:40Z,19814,2018-06-06T16:29:43Z,/tensorflow/tensorflow/issues/19814,tensorflow,CLOSED,"How to make statistics script using summary?
Hi, all
I want parameter distribution analysis script for pretrained models.
I do not want special script for each model, just want single program to do it.
Some person advised me to use the summary graph.
tensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc
I check the code and fell that the code does not support extracting parameters from pb file.
I wrote a draft code to analyze;
https://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py
Any suggestion is welcome, and I am beginner, please explain softly.
Best,
Syouyu",NONE
374,{'login': 'Mikun4619'},2018-02-18T11:09:25Z,16946,2018-02-12T12:21:13Z,/tensorflow/tensorflow/issues/16946,tensorflow,CLOSED,"tensorflow lite converter(toco) build error 
Describe the problem
I try to build the toco that is tensorflow lite converter.
But I can not success to build. please see below for the details.
Source code / logs
C:\tensorflow>bazel build //tensorflow/contrib/lite/toco:toco
The following error message appears.
ERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
File ""C:/tensorflow/third_party/repo.bzl"", line 88
_apply_patch(ctx, ctx.attr.patch_file)
File ""C:/tensorflow/third_party/repo.bzl"", line 59, in _apply_patch
_execute_and_check_ret_code(ctx, cmd)
File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
fail(""Non-zero return code({1}) when ...))
Non-zero return code(3) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
File ""C:/tensorflow/third_party/repo.bzl"", line 88
_apply_patch(ctx, ctx.attr.patch_file)
File ""C:/tensorflow/third_party/repo.bzl"", line 59, in _apply_patch
_execute_and_check_ret_code(ctx, cmd)
File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
fail(""Non-zero return code({1}) when ...))
Non-zero return code(3) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
INFO: Elapsed time: 27.852s
FAILED: Build did NOT complete successfully (0 packages loaded)
currently loading: tensorflow/contrib/lite/toco
plus info.
The following message appears when I input like this in command line. (for test)
patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch
patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary
patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Hunk #1 succeeded at 750 with fuzz 1 (offset 193 lines).
Hunk #2 succeeded at 825 (offset 169 lines).
Hunk #3 succeeded at 906 with fuzz 2 (offset 169 lines).
I don't know how to add --binary option to script...
ref. #10435",NONE
375,{'login': 'blueevangelion'},2017-03-10T20:03:11Z,7959,2017-03-01T04:06:42Z,/tensorflow/tensorflow/issues/7959,tensorflow,CLOSED,"ImportError: cannot import name model_fn
I tried run cnn_mnist.py and I got the following error.
Traceback (most recent call last):
File "" cnn_mnist.py"", line 13, in 
from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib
ImportError: cannot import name model_fn
cuda veriosn
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44
tensorflow version
tensorflow (0.10.0)
python version
Python 2.7.12",NONE
376,{'login': 'Jinxit'},2016-05-13T21:03:25Z,1962,2016-04-15T08:11:37Z,/tensorflow/tensorflow/issues/1962,tensorflow,CLOSED,"Killed during checkpoint save (v0.8)
Environment info
Operating System: Ubuntu 15.10
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
-rw-r--r-- 1 root root 322936 aug 15 2015 /usr/local/cuda/lib64/libcudadevrt.a lrwxrwxrwx 1 root root 16 aug 15 2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5 lrwxrwxrwx 1 root root 19 aug 15 2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18 -rwxr-xr-x 1 root root 383336 aug 15 2015 /usr/local/cuda/lib64/libcudart.so.7.5.18 -rw-r--r-- 1 root root 720192 aug 15 2015 /usr/local/cuda/lib64/libcudart_static.a -rwxr-xr-x 1 root root 61453024 mar 6 15:08 /usr/local/cuda/lib64/libcudnn.so -rwxr-xr-x 1 root root 61453024 mar 6 15:08 /usr/local/cuda/lib64/libcudnn.so.4 -rwxr-xr-x 1 root root 61453024 mar 6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48 -rw-r--r-- 1 root root 62025862 mar 6 15:08 /usr/local/cuda/lib64/libcudnn_static.a
If installed from binary pip package, provide:
Which pip package you installed.
The output from python -c ""import tensorflow; print(tensorflow.version)"".
0.8.0rc0
Relevant code:
Unfortunately I can't upload everything, but the code is based on your CIFAR-10 example. Here is the structure of my network:
http://pastebin.com/5QEJWqtm
After running for some time, I save a checkpoint:
saver.save(sess, checkpoint_path, global_step=step)
Which sometimes allocates all memory on my system and gets killed. I have 8gb RAM and 8gb swap. For the first few checkpoints it seems fine, it allocates 2gb RAM (equal to checkpoint file size), but after some time it locks up my entire system and gets killed automatically.
Didn't have any issues in 0.7.",PERFORMANCE
377,{'login': 'alex04309'},2016-04-22T20:24:45Z,2062,2016-04-22T09:09:04Z,/tensorflow/tensorflow/issues/2062,tensorflow,CLOSED,"tf.cond not working with depedencies
tf.cond seems to have a bug if one of the condition have a dependency. (Dependencies are run, whatever tf.cond arg is True or False).
To illustrate:
Output:
",NONE
378,{'login': 'jswelch'},,16226,2018-01-18T19:20:32Z,/tensorflow/tensorflow/issues/16226,tensorflow,OPEN,"Include netstat in the tensorflow docker container
Describe the problem
This is a feature request to add net-tools to the Tensorflow docker containers. Having netstat in the Tensorflow container will make it easier to find open ports in a multi-tenant environment when launching Tensorflow Distributed or Tensorboard.
Note, I have found how to add netstat (see URL below), but would prefer not having to change or maintain a modified version of the Tensorflow container.
https://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container
Exact command to reproduce:netstat
",MAINTAINABILITY
379,{'login': 'Steve7878'},2017-04-28T21:40:22Z,8633,2017-03-22T22:05:53Z,/tensorflow/tensorflow/issues/8633,tensorflow,CLOSED,"Please provide an example how to use a model trained from scratch for image classification
The following documentation of TensorFlow-Slim contains how to train a model from scratch, but it's not explained how to use the resulting checkpoint files for image classification.
https://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch
Although it's possible to load the pre-trained models (https://github.com/tensorflow/models/tree/master/slim#pre-trained-models) and use it for image classification with the example given in https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb, it seems not possible to simply use the checkpoint files generated by ""training from scratch"" in the same way.
Any example on how to use the newly generated checkpoints for image classification (for example with inception) would be appreciated.",USABILITY
380,{'login': 'volvador'},2018-02-01T12:38:21Z,16584,2018-01-30T13:00:44Z,/tensorflow/tensorflow/issues/16584,tensorflow,CLOSED,"TensorFlow op to copy weights of Keras model
I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)
after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling
does not work.
I understand I need to define this weight copy as an op and then call session(run) of that op
Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?",SCALABILITY
381,{'login': 'Davidnet'},2018-12-06T19:18:15Z,23165,2018-10-22T16:18:50Z,/tensorflow/tensorflow/issues/23165,tensorflow,CLOSED,"TensorRT engine binding error
System information*
You can collect some of this information using our environment capture script
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
:
I have frozen a graph that contains the following operations:
tf.layers.conv2d(*args, **kwargs)
tf.layers.batch_normalization(net, **batch_norm)
I will try to provide a minimal graph that have this problem.
:
I create a trt_graph by using the function trt.create_inference_graph and it creates a graph successfully but whenever I try to make an inference I encounter:
Any guideline on how to provide a better log issue?
Thanks",NONE
382,{'login': 'Cogitans'},2017-03-07T22:16:28Z,8138,2017-03-06T20:10:51Z,/tensorflow/tensorflow/issues/8138,tensorflow,CLOSED,"gradient_override_map should raise an error if passed invalid gradient names.
Currently, gradient_override_map does not complain if passed in nonsensical information (apart from the simple check to make sure the map is a map from strings to strings).
For instance, the following lines of code run without issue:
 with graph.gradient_override_map({""nonsense"": ""more_nonsense""}): input = tf.sign(input)
A more subtle point (that happened with me), when attempting to override sign's gradient, the following typo ran without problem:
 with graph.gradient_override_map({""sign"": ""Identity""}): input = tf.sign(input)
(""sign"" should be ""Sign"").
Seems like a fairly simple issue, but I am not quite versed enough in the Tensorflow backend to suggest a fix to this problem.",NONE
383,{'login': 'ZhuFengdaaa'},2016-04-26T14:58:49Z,2106,2016-04-26T07:22:53Z,/tensorflow/tensorflow/issues/2106,tensorflow,CLOSED,"[ distribution ] How to use multiple GPU on each replica ?
The Code Here shows how to set each replica which has a single tower that uses one GPU. I'm wondering if there is a way changing this code a little bit to make use of multiple GPU on one machine like that example.
The way I currently used for using all GPU on a worker machine is starting the number of workers that equal to the number of GPUs. then the workers can communicate to each other as if they are not on one machine. That is slower than if I can start a woker that control more than one GPU.",PERFORMANCE
384,{'login': 'AndreasMadsen'},2016-10-23T19:07:22Z,5122,2016-10-21T20:48:58Z,/tensorflow/tensorflow/issues/5122,tensorflow,CLOSED,"custom CUDA op example returns random values
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Nothing
Environment info
Operating System:
Installed version of CUDA and cuDNN: CUDA: 8, cuDNN 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from source, provide
The commit hash (git rev-parse HEAD) 5a5a25e
The output of bazel version
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Using the CUDA example from: https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op
compile example
edit tensorflow.g3doc.how_tos.adding_an_op import cuda_op to import cuda_op in cuda_op_test.py.
What other attempted solutions have you tried?
I tried a non CUDA example, worked fine.
I tried a diffrent cuda kernel (square operator) also failed.
I added printf to the kernel launcher and made sure it was executed.
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
It looks like the output just contains random memory. Perhaps the GPU memory isn't copied back to the host memory.",PERFORMANCE
385,{'login': 'manuelreis'},2017-08-09T15:20:30Z,12078,2017-08-07T13:11:05Z,/tensorflow/tensorflow/issues/12078,tensorflow,CLOSED,"Assert randomly fails when training with multiple threads
Exact command to reproduce: for ((n=0;n<100;n++)); do python mnist_softmax_parallel_issue.py; done
Describe the problem
The following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads.
Source code / logs
mnist_softmax_device_issue.py
Traceback
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::T ensorEvaluator(const XprType&, const Device&) [with Broadcast = const Eigen::IndexList<Eigen::type2index<1l>, int>; ArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::XprType = Eigen::TensorBroadcastingOp<const Eigen::IndexList<Eigen::type2index<1l>, int>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer> >]: Assertion input_dims[i] > $' failed.",NONE
386,{'login': 'ztwe'},2018-08-05T11:21:43Z,21200,2018-07-28T04:14:14Z,/tensorflow/tensorflow/issues/21200,tensorflow,CLOSED,"the weights of tf.contrib.rnn.BasicLSTMCell can't be updated
Environment
OS: Ubuntu 16.04
Tensorflow-gpu: 1.8
M and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module？
",NONE
387,{'login': 'luchensk'},2017-09-26T03:55:40Z,13207,2017-09-21T09:40:50Z,/tensorflow/tensorflow/issues/13207,tensorflow,CLOSED,"Failed to compile tensorflow offline with '--fetch=false' after all external dependencies fetched by bazel
Exact command to reproduce:
Fetch all external dependencies by docker image with internet access.
Complie tensorflow offline without internet access with --fetch=false.
Describe the problem
I fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub.
After that, I tried to compile tensorflow offline by using the image with bazel build --fetch=false since there is no internet access on my server, but it failed with the error ""no such package '@xxx'"", as follows:
In fact, be sure that the package eigen_archive was existing in the container, as below:
And the ps stdout of bazel process was as follows:
So, please help for that and let me know if something wrong with my operation, thanks.",NONE
388,{'login': 'ivopivo'},2018-10-14T13:42:12Z,22954,2018-10-12T21:25:16Z,/tensorflow/tensorflow/issues/22954,tensorflow,CLOSED,"Error Building from source on Windows / my CPU doesn't have AVX 
Exact command to reproduce: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Describe the problem
I can't build from source as it gives me the error
ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command
Perhaps because my CPU doesnt have AVX instructions set
on my CPU Supported Instructions sets	MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T
Lack of AVX don't allow me to pip install tf>1.5
My question is how to install from source without AVX instructions set?
Source code / logs
C:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan🚋 target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.
WARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).
INFO: Found 1 target...
INFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command
cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow
SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Users\ivo\AppData\Local\Microsoft\WindowsApps;C:\Users\ivo\AppData\Local\Programs\Python\Python36;C:\Users\ivo\AppData\Local\Programs\Python\Python36\Scripts;C:\bazel;C:\msys64\usr\bin
SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe
SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages
SET TF_DOWNLOAD_CLANG=0
SET TF_NEED_CUDA=0
SET TF_NEED_OPENCL_SYCL=0
SET TF_NEED_ROCM=0
C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command
cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow
SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Users\ivo\AppData\Local\Microsoft\WindowsApps;C:\Users\ivo\AppData\Local\Programs\Python\Python36;C:\Users\ivo\AppData\Local\Programs\Python\Python36\Scripts;C:\bazel;C:\msys64\usr\bin
SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe
SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages
SET TF_DOWNLOAD_CLANG=0
SET TF_NEED_CUDA=0
SET TF_NEED_OPENCL_SYCL=0
SET TF_NEED_ROCM=0
C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc
/usr/bin/bash: line 1: 7128 Illegal instruction bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 74.199s, Critical Path: 2.64s
INFO: 42 processes: 42 local.
FAILED: Build did NOT complete successfully
",NONE
389,{'login': 'ophiry'},2018-09-11T18:40:34Z,21429,2018-08-07T06:08:42Z,/tensorflow/tensorflow/issues/21429,tensorflow,CLOSED,"TocoConvertor: converting keras models to tflite doesn't support custom objects
Have I written custom code: No
OS Platform and Distribution: Mac
TensorFlow installed from: pip
TensorFlow version: 1.10-rc1
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)
Mobile device: N/A
This is required to convert models that use custom layers or loss functions
this is the fix (will submit a PR soon):
",NONE
390,{'login': 'hemalshahX'},2016-11-11T17:45:36Z,5179,2016-10-25T00:25:45Z,/tensorflow/tensorflow/issues/5179,tensorflow,CLOSED,"Undefined reference to CheckOpMessageBuilder::NewString() when linking libtensorflow_cc.so
I am trying to use the TensorFlow Session C++ API (Python-free) to load a pre-trained model for inference. For build-time considerations, I am trying to deploy TensorFlow as a ""system"" package by linking against libtensorflow_cc.so and including headers into my Bazel-based workspace which has its own copies of protobuf and Eigen. I am almost there except that I have run into linker errors for missing implementations of tensorflow::internal::CheckOpMessageBuilder::NewString(). The symbols appear to be exported by libtensorflow_cc.so and it does seem to all be linking correctly, just not this symbol.
Any help fixing this issue or suggestions for a better way of doing this would be greatly appreciated.
Thanks,
Hemal
My setup is the following:
Docker image from ubuntu:16.04 using gcc5.
Bazel 0.3.1 (needed to upgrade from 0.3.0 because of other Tensorflow build issues)
I matched the Eigen version but the protobuf used to build the Tensorflow wheel below is installed via apt-get and there is another copy (3.0.0) within my workspace's third_party directory.
The following is in my Dockerfile to build and ""deploy"" Tensorflow:
RUN git clone https://github.com/tensorflow/tensorflow.git /tmp/tensorflow \
&& cd /tmp/tensorflow && git checkout r0.11 \
&& yes '' | ./configure \
&& bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \
&& /tmp/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \
&& pip2 install --quiet --upgrade /tmp/tensorflow_pkg/*.whl \
&& bazel build -c opt //tensorflow:libtensorflow_cc.so \
&& cp /tmp/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so /usr/lib/libtensorflow_cc.so \
&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow /usr/include/tensorflow \
&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/third_party /usr/include/third_party
164:1: Linking of rule '//estimation/detection:playback_ground_truth' failed: clang-3.6 failed: error executing command
(cd /code/.cache/bazel/_bazel_hemalshah/6fa7a91faa1abdfbb41bc875fa66f0f6/execroot/robotics && 
exec env - 
/usr/bin/clang-3.6 -o bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth -L/usr/lib/python2.7/config-x86_64-linux-gnu -L/usr/lib -Wl,-O1 -Wl,-Bsymbolic-functions -pthread -B/usr/bin/ -Wl,@bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
bazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'",FAULT TOLERANCE
391,{'login': 'smartcat2010'},2016-06-03T23:45:55Z,2233,2016-05-05T12:09:26Z,/tensorflow/tensorflow/issues/2233,tensorflow,CLOSED,"protobuf message overflow on trying distributed 
I'm trying to build an RNN on multi-machines following the Distributed Tensorflow.
when I use ""with sv.managed_session(server.target) as sess:"", it shows error:
AttributeError: 'Supervisor' object has no attribute 'managed_session'
So I follow the code of ""Inception"":
with sv.prepare_or_wait_for_session(server.target, config = sess_config) as sess :
Then it starts to run, but hangs immediately after reporting the following error:
[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:569] Reading dangerously large protocol message. If the message turns out to be larger than 67108864 bytes, parsing will be halted for security reasons. To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes). To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 67108864
E tensorflow/core/framework/tensor.cc:105] Input size was 67108839 and expected 72000800
Would you please help me on this?
Thanks a lot in advance!",NONE
392,{'login': 'cgorman'},2017-06-16T19:38:15Z,2510,2016-05-26T00:38:21Z,/tensorflow/tensorflow/issues/2510,tensorflow,CLOSED,"Tensorboard feature request - Text summary
Would it be reasonable to add a basic text summary feature to Tensorboard? Personally I've run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where I could put some arbitrary text where I just wrote the key differences in my runs.
For example, on the Events page (or somewhere else) there would be a dropdown, similar to the summaries on the Events and Histograms page, with text I added (either hardcoded or as a script argument) that says what I did differently this run. Maybe I would print out the argument values for each run as well, that would be pretty useful, but basically something where I can say ""What did I do with this run again? Why was it different than the one before? Oh yeah I changed the batch size"" or ""Oh yeah I used my other dataset instead.""
Obviously if it's arbitrary text you could maybe use it to write up a description of the network or whatever you want.",MAINTAINABILITY
393,{'login': 'olesalscheider'},2018-03-03T18:23:13Z,16669,2018-02-01T17:53:48Z,/tensorflow/tensorflow/issues/16669,tensorflow,CLOSED,"grappler: memory optimizer fails with: No output shape in Conv2DBackpropInput op / Conv2DBackpropFilter op.
Exact command to reproduce: -
Describe the problem
When I enable the memory optimizer in grappler, it fails with the following errors:
My network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.
Is this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?",PERFORMANCE
394,{'login': 'chihyuwang'},2017-10-03T06:18:15Z,13458,2017-10-03T05:20:50Z,/tensorflow/tensorflow/issues/13458,tensorflow,CLOSED,"How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)
I want to limit the total memory of each GPU in mnist,
https://www.tensorflow.org/tutorials/using_gpu
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
session = tf.Session(config=config, ...)
and I added the above code to the mnis.py
https://github.com/tensorflow/models/tree/master/official/mnist
here is the modified code in mnis.py :
def main(unused_argv):
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
mnist_classifier = tf.estimator.Estimator(
model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)
but I get the below error:
Traceback (most recent call last):
File ""mnist.py"", line 231, in 
tf.app.run()
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""mnist.py"", line 206, in main
model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 142, in init
config)
ValueError: config must be an instance of RunConfig, but provided gpu_options {
per_process_gpu_memory_fraction: 0.4
}
.
My question is :
How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)",NONE
395,{'login': 'Azorlogh'},2016-12-14T18:05:05Z,6314,2016-12-14T15:05:31Z,/tensorflow/tensorflow/issues/6314,tensorflow,CLOSED,"Error 404 when downloading Tensorflow on Windows
The links provided on the website to the .whl used to install Tensorflow on Windows seem to be broken : https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows
I'm getting an HTTP Error 404.
I found the CPU Build elsewhere but I can't find the GPU build which I would like tu use.",AVAILABILITY
396,{'login': 'ShownX'},2017-04-07T20:47:38Z,4474,2016-09-19T21:01:47Z,/tensorflow/tensorflow/issues/4474,tensorflow,CLOSED,"Feature request: Patch extracting given location implementation needed.
Hello there,
Feature
I consider a lot but finally decide put this request here.
I know there are some matrix operation and image operations like image_patch_extract and image.extract_glimpse().
But what I want is extract the patches given several locations.
Input Tensor:
Output Tensor:
Reference:
Georgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.
But when I use it in v0.10.0, it requires me to define a shape function.
I really want to use it in the future, so I am glad it can be added as a new feature.
Other solution I tried
I have tried use extract_glimpse() instead.
",MAINTAINABILITY
397,{'login': 'benvand'},2017-06-16T23:35:01Z,4675,2016-09-30T09:51:54Z,/tensorflow/tensorflow/issues/4675,tensorflow,CLOSED,"LinearClassifier feature_columns overwritten in LinearClassifier.fit
tensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit
effectively returns
tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit
Estimator.fit calls:
tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model
which has these lines:
tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs
So we get to a point where features, as derived from the input_fn, is treated as our feature columns set.
In pseudo code:
The issue is:
Although this line appears to indicate that we will be making an estimation based on the feature columns supplied:
lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be
What happens is that the passed in feature_columns is unused and instead the return from the input_function supplied to fit are used.
Am I correct in thinking that if the feature_columns arg is supplied that only those columns should be used by the classifiers estimator?
That when we instantiate the classifier we are setting the feature_columns we expect to be used?
The work around for this is simply to only return the columns you need from your input function however I found this misleading.
Point in the tutorial:
Either the code or the tutorial need to be changed.
https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model
Error raised:
WARNING:tensorflow:Setting feature info to
as per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613",USABILITY
398,{'login': 'FBerendsen'},2018-11-21T18:08:25Z,23344,2018-10-29T09:37:04Z,/tensorflow/tensorflow/issues/23344,tensorflow,CLOSED,"LayoutOptimizer optimizes to unsupported data_format for max_pool on CPU
TensorFlow automatically replaces my MaxPoolingOp by one using another data format which is not supported, subsequently.
This throws the InvalidArgumentError as seen below.
The problem arose when upgrading from TensorFlow 1.8 to 1.11 and from the error it seems to be caused by a TransposeNHWCToNCHW-LayoutOptimizer. When isolating the issue to reproduce it, it seems that max_pool, dataset and squeeze are involved to raise the error. The only related (closed) issue I could find: #19497 ""NHWC convolution sometimes incorrectly considered NCHW""
 InvalidArgumentError (see above for traceback): Default MaxPoolingOp only supports NHWC on device type CPU [[{{node label_image_dilated}} = MaxPool[T=DT_INT32, data_format=""NCHW"", ksize=[1, 1, 3, 3], padding=""SAME"", strides=[1, 1, 1, 1]](label_image_dilated-0-TransposeNHWCToNCHW-LayoutOptimizer)]] [[{{node OneShotIterator_2}} = OneShotIterator[container="""", dataset_factory=_make_dataset_UaZD9hBkHvg[], output_shapes=[[?,?]], output_types=[DT_INT32], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
To not throw the error as was the case for TensorFlow 1.8.
Code to reproduce the issue
",NONE
399,{'login': 'VinceMarron'},2017-12-15T19:54:38Z,15103,2017-12-04T19:43:46Z,/tensorflow/tensorflow/issues/15103,tensorflow,CLOSED,"No GPU OpKernel for tf.exp() operation for Complex64
I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea' on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:
It seems that there is no OpKernel on device='GPU' for the tf.exp() operation applied to complex numbers in eager mode. This can be reproduced with the below code:
which results in
a more practical example that would lead to this same error:
Keeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks",NONE
400,{'login': 'borgdylan'},2019-10-10T00:48:29Z,53812,2018-07-08T17:09:55Z,/microsoft/vscode/issues/53812,vscode,CLOSED,"Request: Left Close/Max/Min buttons variant for custom title bar
I really like the new title bar style that can be activated by ""editor.titleBarStyle"": ""custom"". However, as a long time Ubuntu user I'm accustomed to my window close/max/min controls being on the left hand side of the title bar. Is is possbible to have another member in the titleBarStyle enum ""custom-left"" or something named similarly to have the controls render on the far left rather than the far right in the order close, minimize, maximize(restore)?
Below, the native menu for VS Code (I do not mind the square buttons of the ciustom style as they fit well with the VS brand):
",USABILITY
401,{'login': 'Olddude'},2017-10-30T15:10:51Z,37058,2017-10-27T20:35:47Z,/microsoft/vscode/issues/37058,vscode,CLOSED,"When relaunch vscode after aborted vscode update then Uncaught Exception thrown
Update vscode
While updating launch a second vscode instance
Abort the Installation on Dialog
Start vscode again
Reproduces without extensions: Yes
",NONE
402,{'login': 'eparizzi'},2016-07-08T19:11:38Z,8966,2016-07-08T18:32:58Z,/microsoft/vscode/issues/8966,vscode,CLOSED,"Integratel Terminal doesn't find commands from PATH
I have installed Git for Windows with both the git and Unix commands in the PATH.
VSCode doesn't seem to find the Unix commands.
",NONE
403,{'login': 'electronik54'},,40634,2017-12-21T10:04:35Z,/microsoft/vscode/issues/40634,vscode,OPEN,"Color picker wont open when hovering over color box or moving between box and value
Extensions:
Extension
Author (truncated)
Version
staticserver
gao
0.0.5
Steps to Reproduce:
just mention a color in hex format and hover arrow of the hex code, this should normally open a in-biult color picker. This is not working only in the insider version, color picker works perfectly in Visual Code.
however, whenever the color picker would appear, it would quickly close itself and very rarely it would work properly.
This issue is since 2 previous versions of VSinsider
Reproduces without extensions: Yes",NONE
404,{'login': 'wmeints'},2017-06-14T16:39:18Z,28693,2017-06-14T07:03:33Z,/microsoft/vscode/issues/28693,vscode,CLOSED,"Icons are fuzzy in the UI
Since the update to 1.13 I am seeing fuzzy icons in the UI of VSCode. I am using the light theme. This is unrelated to the theme I select. Switching between light and dark themes doesn't fix the problem.
Open VS Code
Open a code file
",SCALABILITY
405,{'login': 'michelkaporin'},2017-06-01T09:16:36Z,27456,2017-05-29T14:47:39Z,/microsoft/vscode/issues/27456,vscode,CLOSED,"Test: Automated Smoke Test
Ref: #25291
Complexity: 2
OS:
 Windows @alexandrudima
 OS X @weinand
 Linux @Tyriar
As part of engineering work we have automated major part of our smoke test. To ensure its stability, it is important to run it on different machines.
Running automated test
Prerequisites
Update stable to the latest version.
Close all VS Code instances that will be used in a test (e.g. latest insiders, stable) not to have any interference with the running test that will spawn them itself.
Once the test is running, please do not interfere with it by doing selection or focusing anywhere within spawned test VS Code instance.
Approximate running time
10 minutes minimum (the more failures it has, the more it runs due to adaptive retry strategy of the waiting time).
Procedure
Run git checkout michelkaporin/smoketest in vscode repository. Currently the code lives on my branch only.
cd test/smoke
npm install
npm test -- --latest ""path/to/binary"" --stable ""path/to/binary"", where latest argument is the path to VS Code executable to conduct testing, and stable is previous stable version (in order to run 'Data Migration' tests). If the latter argument is ommited, 'Data Migration' tests won't run.
Example commands:
npm test -- --latest ""C:\Program Files (x86)\Microsoft VS Code Insiders\Code - Insiders.exe"" --stable ""C:\Program Files (x86)\Microsoft VS Code\Code.exe""
or
npm test -- --latest ""/Applications/Visual Studio Code - Insiders.app/Contents/MacOS/Electron"" --stable ""/Applications/Visual Studio Code.app/Contents/MacOS/Electron""
Known problems
**#27452 bug results in very many failing tests on OS X, once that is fixed, all should pass.
Spectron spawns multiple cmd windows on Windows OS. This leads to VS Code window to be overlapped by another one. To look at what the test does you need to click on the VS Code title bar to focus on it in every test. (electron-userland/spectron#60)
If you get failing tests, please dump the log with failed error messages here.
Any feedback is appreciated 👍",NONE
406,{'login': 'CleyFaye'},2020-02-12T06:27:11Z,90455,2020-02-11T15:05:12Z,/microsoft/vscode/issues/90455,vscode,CLOSED,"Inconsistency between unsaved file dialog and unsaved workspace dialog
Issue Type: Bug
When closing files with unsaved changes, a dialog open with the three possible actions: ""Don't save"", ""Cancel"", ""Save"" in that order.
When closing an unsaved workspace, a similar dialog appear with actions ""Save"", ""Cancel"", ""Don't save"" in that order.
That feel inconsistent and could cause confusion. The order should probably be the same in every dialog.
VS Code version: Code 1.42.0 (ae08d54, 2020-02-06T10:51:23.649Z)
OS version: Linux x64 5.0.0-38-generic
",USABILITY
407,{'login': 'Yjq14114'},2018-10-05T08:59:30Z,60013,2018-10-05T08:57:31Z,/microsoft/vscode/issues/60013,vscode,CLOSED,"terminal 
Issue Type: Bug
i open terminal, but its look like is not work !
VS Code version: Code 1.27.2 (f46c4c4, 2018-09-12T16:17:45.060Z)
OS version: Windows_NT x64 10.0.17763
",NONE
408,{'login': 'warpdesign'},2017-06-15T16:11:44Z,28823,2017-06-15T16:09:18Z,/microsoft/vscode/issues/28823,vscode,CLOSED,"You can set themes that are disabled in extensions panel
Themes appear in the extensions panel and can be disabled just like normal extensions.
I think disabled themes shouldn't appear when running the preferences: Color Theme command. Right now you may set a theme that's disabled wihch is a bit weird.",USABILITY
409,{'login': 'imeoer'},2017-11-30T19:41:12Z,32275,2017-08-11T02:41:35Z,/microsoft/vscode/issues/32275,vscode,CLOSED,"The incorrect IME position when the first character is entered
Extensions:
Extension
Author (truncated)
Version
material-icon-theme
PKi
2.1.0
vscode-docker
Pet
0.0.16
code-settings-sync
Sha
2.8.2
go
Twe
0.0.1
html-css-class-completion
Zig
1.8.0
vscode-color
ans
0.4.5
vscode-eslint
dba
1.2.11
python
don
0.7.0
vscode-babel-coloring
dza
0.0.4
gitlens
eam
4.3.3
vscode-html-css
ecm
0.1.7
tslint
eg2
0.17.0
nim
kos
0.5.26
Go
luk
0.6.63
markdown-shortcuts
mdi
0.8.1
prettify-json
moh
0.0.3
debugger-for-chrome
msj
3.1.7
color-highlight
nau
2.3.0
vetur
oct
0.9.3
proto
pet
0.0.2
Ruby
reb
0.13.0
PostCSS
ric
1.0.1
stylelint
shi
0.28.0
babelrc
wad
1.0.0
vscode-import-cost
wix
2.0.0
vscode-proto3
zxh
0.1.2
(5 theme extensions excluded)
Switch to Chinese IME (e.g. Sougou / Baidu)
See the screenshot:
I reproduced this problem in the monaco editor, I think it should be the text input implementation issue using textarea, did not find this problem in ace editor.
Reproduces without extensions: Yes",NONE
410,{'login': 'sagg155'},2018-09-25T15:08:05Z,58686,2018-09-14T10:48:07Z,/microsoft/vscode/issues/58686,vscode,CLOSED,"Debugger attached. Waiting for the debugger to disconnect...
Start Node Debugger
Error
Debugger attached.
Waiting for the debugger to disconnect...
Does this issue occur when all extensions are disabled?: Yes
I have tried everything as follows but nothing works->
restarted vs code
restarted my machine
uninstall node,npm,vs code
please suggest some solution for this bug.Basically i'm debugging typescript file.",NONE
411,{'login': 'geuis'},2018-06-11T08:42:20Z,51539,2018-06-10T03:22:16Z,/microsoft/vscode/issues/51539,vscode,CLOSED,"Local editor settings not being overridden by ESLint settings
ESLint Extension: 1.4.12
The default behavior for VSCode is to add spaces to objects:
I have a .eslintrc config setup with the rule ""object-curly-spacing"": [""error"", ""never""]. Linting occurs and reports the default spacing behavior as an error as expected.
This behavior can be overridden in the VSCode settings via ""javascript.format.insertSpaceAfterOpeningAndBeforeClosingNonemptyBraces"": false
I was expecting that the default behavior of VSCode would be overridden by the ESLint rules when formatting code.
Does this issue occur when all extensions are disabled?: No. The ESLint extension must be installed for VSCode to utilize the .eslintrc rules.
This may be an issue correctable in the ESLint extension. If so I will move this issue to that project.",MAINTAINABILITY
412,{'login': 'mohsen1'},2015-12-08T11:41:41Z,1052,2015-12-06T18:52:56Z,/microsoft/vscode/issues/1052,vscode,CLOSED,"Incorrect error in TypeScript code
I'm using version df35236
For this code I get an incorrect error.
TypeScript compiler does not complain for this code but VSCode throws this error:
",NONE
413,{'login': 'Apostata'},2018-10-04T15:12:13Z,59547,2018-09-27T12:38:37Z,/microsoft/vscode/issues/59547,vscode,CLOSED,"After update throwing : Couldn't find declaration for many modules 
It was working before.
now at import the module:
import { Validation } from 'bunnyjs/src/Validation';
[ts]
Could not find a declaration file for module 'bunnyjs/src/Validation'. 'c:/Dev/Notanet/frontend-env/cockpit/node_modules/bunnyjs/src/Validation.js' implicitly has an 'any' type.
Try npm install @types/bunnyjs if it exists or add a new declaration (.d.ts) file containing declare module 'bunnyjs';",NONE
414,{'login': 'vscodeerrors'},2018-09-24T17:16:27Z,59227,2018-09-24T12:41:15Z,/microsoft/vscode/issues/59227,vscode,CLOSED," Cannot read property 'forEach' of null
Issue Id: 35acb833-bda7-37c3-14da-3ab8f360751bVersions - 1.28.0-insider (9/20/2018 2:13:44 PM)Stack TypeError: Cannot read property 'forEach' of null/vs/base/browser/ui/menu/menu.ts#158:13 (style)/vs/platform/theme/common/styler.ts#53:20 (call)/vs/base/common/event.ts#140:15 (fire)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#350:26 (applyTheme)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#295:17 (onComplete)/vs/base/common/winjs.base.js#1191:0 (_notify)/vs/base/common/winjs.base.js#867:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1057:0 (onComplete)/vs/base/common/winjs.base.js#1587:0 (then)/vs/base/common/winjs.base.js#762:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1073:0 at Object.g [as _notify] (out/vs/workbench/workbench.m",NONE
415,{'login': 'bpasero'},2017-08-25T14:23:02Z,32946,2017-08-22T10:52:31Z,/microsoft/vscode/issues/32946,vscode,CLOSED,"Multi root: revisit workspace file format
/cc @sandy081",NONE
416,{'login': 'xiaogwu'},2017-07-21T08:40:50Z,31170,2017-07-21T00:42:35Z,/microsoft/vscode/issues/31170,vscode,CLOSED,"Show Preview of file while in Go to File... 
Feature Request
It would be awesome if when using Go to File... (Command-P) that a preview of the current highlighted file is show and as you up/down arrow key a different file, you see a preview of the current selected file.",USABILITY
417,{'login': 'Peteous'},2017-05-04T08:10:38Z,25704,2017-04-30T17:59:46Z,/microsoft/vscode/issues/25704,vscode,CLOSED,"Git version control can't interact with repos that are already local on the machine.
Extensions:
Extension
Author
Version
python
donjayamanne
0.6.3
cpptools
ms-vscode
0.11.0
vscode-arduino
vsciot-vscode
0.1.2
open VS Code
open file from git repo already local on machine
try to commit from git version control panel
fails
sad day
",NONE
418,{'login': 'rolandbrake'},2017-12-03T13:15:01Z,39462,2017-12-02T11:17:10Z,/microsoft/vscode/issues/39462,vscode,CLOSED,"can we go back please the new logo is so ugly!
",NONE
419,{'login': 'alexdima'},2018-09-11T08:32:59Z,27712,2017-05-31T09:04:07Z,/microsoft/vscode/issues/27712,vscode,CLOSED,"Cannot click on nodejs built-in modules
Testing #26203
Have a js file with the following:
Start debugging (F5)
The exception widget shows up at the correct location
The module.js is clickable but it tries to open a file called module.js from the workspace instead of opening the nodejs built-in module
Clicking on module.js from the call stack works correctly, so perhaps the debug adapter needs to be asked about how to resolve links in the error stackframe, and only if it doesn't know how to do it, the default should be to fallback to trying to open files?
Other use-cases would be debugging a remote target where the error stack trace would contain the file paths as installed on the remote machine and these file paths would need to be translated according to the debug configuration to the local filesystem, or otherwise opened in readonly mode by fetching their sources from nodejs
fyi @weinand @isidorn",NONE
420,{'login': 'BenjaVR'},2017-11-10T17:16:45Z,38009,2017-11-10T01:15:17Z,/microsoft/vscode/issues/38009,vscode,CLOSED,"Terminal sidebar close button not visible
Extensions:
Extension
Author (truncated)
Version
html-snippets
abu
0.1.0
django-snippets
bib
1.1.0
vscode-styled-jsx
bla
0.1.1
npm-intellisense
chr
1.3.0
path-intellisense
chr
1.4.2
gitignore
cod
0.5.0
vscode-eslint
dba
1.4.3
gitlens
eam
6.0.0
tslint
eg2
1.0.16
vscode-npm-script
eg2
0.3.3
vsc-material-theme
Equ
1.1.1
prettier-vscode
esb
0.24.0
php-debug
fel
1.11.1
php-intellisense
fel
1.5.4
beautify
Hoo
1.1.1
intellij-idea-keybindings
k--
0.2.16
vscode-github
Kni
0.23.0
MagicPython
mag
1.0.12
Kotlin
mat
1.3.0
HTMLHint
mka
0.4.0
vscode-apache
mrm
1.1.1
vscode-attrs-sorter
mrm
2.1.0
vscode-jade-snippets
mrm
1.0.1
vscode-pugbeautify
mrm
1.0.2
vscode-puglint
mrm
2.3.0
vscode-scss
mrm
0.6.2
vscode-stylefmt
mrm
2.5.0
python
ms-
0.8.0
cpptools
ms-
0.14.2
csharp
ms-
1.13.0
PowerShell
ms-
1.5.0
typescript-javascript-grammar
ms-
0.0.24
material-icon-theme
PKi
2.2.4
vscode-template-literal-editor
pli
0.8.4
java
red
0.14.0
vscode-icons
rob
7.17.0
prettier-eslint-vscode
Rob
0.7.1
code-settings-sync
Sha
2.8.5
shader
sle
1.1.2
twig
wha
1.0.2
jinja
who
0.0.8
ReactSnippets
xab
1.4.0
Put the terminal panel to the sidebar.
Make that window smaller.
Close button is not visible, and not reachable in any way.
As you see here, the 'X' button should be in the upper right.
Reproduces without extensions: Yes",USABILITY
421,{'login': 'bpasero'},2017-02-02T16:17:37Z,19662,2017-02-01T06:29:42Z,/microsoft/vscode/issues/19662,vscode,CLOSED,"Do not expect app.getPath('userData') to be there before app.once('ready')
There is a discussion going on in 08a1700 where @chrmarti saw an issue on startup for new users that never started Code before. I never saw this issue but the fix seems odd to me: we now suddenly create the directory (userData) via mkdirp that is owned by Electron/Chrome, which seems wrong. It should be created by the framework, not us.
Can we move the getNodeCachedDataDir() into the app.once('ready') callback? That is the place where all code should go that assumes a certain directory structure to be present, not before.",NONE
422,{'login': 'bhoreanna'},2019-02-24T06:33:13Z,69295,2019-02-24T06:32:52Z,/microsoft/vscode/issues/69295,vscode,CLOSED,"run code
Issue Type: Bug
codes are not able to run the program
VS Code version: Code 1.30.1 (dea8705, 2018-12-18T18:12:07.165Z)
OS version: Windows_NT x64 6.1.7600
",NONE
423,{'login': 'DominicTobias'},2018-03-09T20:44:21Z,45409,2018-03-09T14:40:59Z,/microsoft/vscode/issues/45409,vscode,CLOSED,"Go to definition (F12) doesn't work in node_modules with TS
Press F12 on an import in a node_module file
Behaviour:
Goes to the Typescript definition same as clicking ""Go to Type Definition"".
Expected:
Should go to the implementation.",NONE
424,{'login': 'mhinton'},2019-10-14T15:11:57Z,81867,2019-10-02T20:35:35Z,/microsoft/vscode/issues/81867,vscode,CLOSED,"Horizontal split hides the line the cursor is on if it's below the midpoint of the view
Issue Type: Bug
Have the current line positioned below the middle of the viewport. Create a horizontal split. The line with the cursor is now hidden in both views. This is super annoying. The line with the cursor should be visible in both views.
https://cl.ly/14374e89477a
VS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:31:32.854Z)
OS version: Darwin x64 18.7.0
",USABILITY
425,{'login': 'yufw'},2019-10-10T06:05:06Z,75202,2019-06-10T14:40:17Z,/microsoft/vscode/issues/75202,vscode,CLOSED,"Extra pixels under status bar
There are some extra pixels under the status bar. Increase the font size to see it more clearly. Also, resizing the window vertically will change the width of the extra pixels.
Launch VS Code
Does this issue occur when all extensions are disabled?: Yes",USABILITY
426,{'login': 'joaomoreno'},2018-04-26T08:13:04Z,48692,2018-04-25T17:24:57Z,/microsoft/vscode/issues/48692,vscode,CLOSED,"Git clone should use the new progress API
",NONE
427,{'login': 'spmeesseman'},2019-05-06T09:25:15Z,72808,2019-04-24T12:20:36Z,/microsoft/vscode/issues/72808,vscode,CLOSED,"GIt: Remote Changes Shelf
For all the GREAT stuff in VSCode, I can't believe there isn't a ""Remote Changes"" shelf in the SCM view for Git repos (i.e. together with the ""Changes"" and ""Staged Changes"" shelves). The 3rd party SVN extension has this feature, even the old dinosaur Eclipse has it.
I was even more confused when I didn't find a duplicate in the issues. Am I missing something? If I am, could someone please let me know what do Git users do in VSCode (admittedly I am coming from CVS/SVN and am new to Git) to see remote changes? (without using the 3rd party plugin GitLens which is not easy to navigate with all the clutter). I assume people just don't 'sync' or 'pull' blindly without knowing what changes are coming in?
",NONE
428,{'login': 'luabud'},2019-10-09T18:49:41Z,82123,2019-10-08T21:36:57Z,/microsoft/vscode/issues/82123,vscode,CLOSED,"Update test explorer beaker icon to match new UI 
I believe right now this is the icon that is being used by VS Code: https://github.com/microsoft/vscode/blob/master/src/vs/workbench/api/browser/media/test.svg
It'd be great if that could be updated to match the new UI.",USABILITY
429,{'login': 'bschlenk'},2019-10-23T17:51:43Z,43294,2018-02-09T08:46:05Z,/microsoft/vscode/issues/43294,vscode,CLOSED,"Make ""editor.scrollBeyondLastLine"" configurable beyond true or false
I'm the kind of person who always presses enter 5 times after running a shell command. I like having some space. But too much space leaves me feeling vulnerable, as if I was stranded in the middle of the ocean. This all or nothing approach with the ""editor.scrollBeyondLastLine"" option isn't working for me.
Can this option be configurable by the line number? If I want to be able to scroll past the bottom by 5 lines, I should be able to set the value to 5. If I want to have the current behavior, I could set the value to true, or ""all"".",MAINTAINABILITY
430,{'login': 'mherodev'},2017-08-31T02:34:14Z,33444,2017-08-29T19:01:30Z,/microsoft/vscode/issues/33444,vscode,CLOSED,"""Go to File..."" shows files in search.exclude
""Go to File..."" is essentially a search operation and should be be managed by search.exclude, or optionally an additional config, e.g. goto.exclude. Typical use-case is having Explorer reflect true contents of project (e.g. not hiding generated files), while other operations are for navigating files one will edit.
Include node_modules in search.exclude, but not in files.exclude in settings
cmd + p
See excluded files appearing in results
",USABILITY
431,{'login': 'martanko'},2017-11-25T07:23:50Z,29764,2017-06-28T14:39:00Z,/microsoft/vscode/issues/29764,vscode,CLOSED,"Unable to open file with leading space in file name when passed as a parameter from command line
Create a file with leading space, e.g. "" my file.txt""
Whilst in the text file folder, pass the file name to VS Code from command line, e.g. ""c:\Program Files (x86)\Microsoft VS Code\Code.exe"" "" my file.txt""; VSCode will try to open file ""my file.txt"" (space removed).
When doing the same with full file path, e.g. ""c:\Program Files (x86)\Microsoft VS Code\Code.exe"" ""c:\ my file.txt""; VSCode will not open any file
Note: When opening from within Code by File->Open, everything works fine
Reproduces without extensions: Yes",NONE
432,{'login': 'smlombardi'},2017-05-08T14:25:44Z,25790,2017-05-02T17:03:29Z,/microsoft/vscode/issues/25790,vscode,CLOSED,"Closing script tags not colored not using ligatures
Extensions:
Extension
Author
Version
ng-template
Angular
0.1.3
code-settings-sync
Shan
2.6.2
sort-lines
Tyriar
1.2.0
Bookmarks
alefragnani
0.14.1
project-manager
alefragnani
0.15.1
path-intellisense
christian-kohler
1.2.0
vscode-eslint
dbaeumer
1.2.8
vscode-html-css
ecmel
0.1.2
tslint
eg2
0.12.0
Angular2
johnpapa
2.2.3
vscode-icon-theme
jtlowe
1.5.0
theme-karyfoundation-themes
karyfoundation
11.1.0
HTMLHint
mkaufman
0.3.3
vscode-autoprefixer
mrmlnc
2.0.0
vscode-stylefmt
mrmlnc
2.3.0
angular2-inline
natewallace
0.0.17
typescript-hero
rbbit
0.12.0
project-snippets
rebornix
0.5.0
stylelint
shinnn
0.24.0
darcula-extended
smlombardi
3.3.2
slime
smlombardi
1.15.0
theme-tesla
smlombardi
6.0.0
change-case
wmaurer
1.0.0
in script tags, the opening angle bracket in closing </script> tags and no longer colored like other tag brackets:
for comparison, some non-script tags:
Furthermore, you can see that this font, Fira Code, uses ligatures for the </ yet the script tags no longer are using them.
Both things happened in the current build.",NONE
433,{'login': 'bobbrow'},2017-06-09T10:30:37Z,28203,2017-06-07T20:51:07Z,/microsoft/vscode/issues/28203,vscode,CLOSED,"Update protocol document for CompletionItemKind and SymbolKind?
The C/C++ extension has been following the protocol here to define our protocol messages (the bulk of our extension is written in C++) https://github.com/Microsoft/language-server-protocol/blob/master/protocol.md
I stumbled on this thread #2628 and saw commits that looked like they changed the enum values of CompletionItemKind and SymbolKind. So it appears that we are sending incorrect values back to VS Code now.
I just wanted to double-check... are the enum values in VS Code now different than the ones in the protocol?",NONE
434,{'login': 'atishay'},2018-05-30T12:56:30Z,50004,2018-05-16T22:21:01Z,/microsoft/vscode/issues/50004,vscode,CLOSED,"[PHP] provideCompletionItems is not called for items inside of -> methods and also for variables starting with $
Issue Type: Bug
If you register a registerCompletionItemProvider in an exptension for PHP, it is not always called and completion items are not provided in the following cases:
Variable name starts with $
Arrow functions
NOTE: The completion item provider is not registered with any trigger characters.
VS Code version: Code 1.23.0 (7c7da59, 2018-05-03T15:23:14.634Z)
OS version: Darwin x64 17.5.0
",NONE
435,{'login': 'zendorx'},2018-09-12T07:55:27Z,58429,2018-09-11T09:50:35Z,/microsoft/vscode/issues/58429,vscode,CLOSED,"UNITY PROJECT STOPS LOADED AFTER UPDATE
Issue Type: Bug
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-firstpass.csproj
[info]: OmniSharp.MSBuild.ProjectManager
Update project: NavMeshComponents
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-Editor.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\Assembly-CSharp-Editor-firstpass.csproj
fail: OmniSharp.MSBuild.ProjectManager
Attemped to update project that is not loaded: c:\work\IDLE-CITY\Project\NavMeshComponentsEditor.csproj
VS Code version: Code 1.27.1 (5944e81, 2018-09-06T09:21:18.328Z)
OS version: Windows_NT x64 10.0.17134
",NONE
436,{'login': 'sipi41'},2019-01-31T09:47:16Z,67571,2019-01-30T18:55:02Z,/microsoft/vscode/issues/67571,vscode,CLOSED,"Encoding Files Problem
I have noticed a weird behavior, every time I create a new file, it is encoded in ANSI format, instead of the one I expect to have, which is ""files.encoding"": ""utf8""
Anybody has an idea on why vs is creating files on ANSI instead of UTF-8??
I noticed this problem when I was trying to load a js file, it said the first character was invalid, but the code was fine... then I checked into the encoding and it was ANSI... saved the file as UTF-8 using notepad and the problem went away... PLEASE HELP",NONE
437,{'login': 'AccessibilityTestingTeam-TCS'},2018-07-17T15:36:33Z,52000,2018-06-15T09:27:15Z,/microsoft/vscode/issues/52000,vscode,CLOSED,"Viewlets should focus the first item, not the whole tree
Environment Details:
VSCode Version : 1.24.0
OS Version : Win10
Additional Details:
MAS Violated : MAS 2.1.1
Tools Used : Keyboard
Repro Steps:
Launch VS Code.
Navigate to Activity Bar and select ""Explorer""(Cntrl+Shift+E) button.
Navigate to ""File Explorer"" treeview items using ""Tab"".
Actual:
Keyboard focus moves to overall treeview items which is non-interactive. Then using downward arrow keys focus goes to items in treeview.
Expected:
Keyboard focus should not move to the items as a whole which is non-interactive. The focus should move to first item in the treeview.
Recommendations:
Remove tab-index from the outer that contains all the treeview items.
or,
Refer below link which is repository of bug fixes code snippets:
https://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx
User Impact:
The keyboard only users will move to non-interactive elements on the screen which will be time consuming to reach only interactive elements on the page.
MAS Reference:
https://microsoft.sharepoint.com/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}
Attachment for Reference:
A11y_VSCode_ViewExplorer_Keyboard_TreeViewItems.pptx
Does this issue occur when all extensions are disabled?: Yes",NONE
438,{'login': 'Ben07'},2017-01-04T00:39:02Z,17849,2016-12-27T10:25:06Z,/microsoft/vscode/issues/17849,vscode,CLOSED,"[html] format not work correctly in html file when which has style property in tag
In html file,when a tag has style property.It's will influence format
for example
when I press shift+alt+f to format code
will become
",USABILITY
439,{'login': 'WToorenburghEasyMarkit'},2017-11-24T14:58:07Z,34176,2017-09-12T00:13:05Z,/microsoft/vscode/issues/34176,vscode,CLOSED,"Editor not rendering soft hyphen
I ran into a case where a soft hyphen had snuck into a regex expression, yet the character was not visible in the editor. It doesn't seem to render in most places, including Stack Exchange; it has only shown up for me when pasted into a terminal window like cmd.exe or terminal.app. Full disclosure: I don't know if this is behaviour as intended or not. This could also be an Electron issue.
Copy this regex expression using Ctrl+A: http://rubular.com/r/M58qyBfuxF (Link provided as it seems GitHub scrubs the soft hyphen before posting)
Paste it into Code, and verify that there is no hyphen visible.
Paste it into a terminal window and verify that there is a hyphen directly after the $
Reproduces without extensions: Yes
Also reproduces with both the default font and Roboto Mono",NONE
440,{'login': 'roblourens'},2019-01-22T17:55:03Z,66866,2019-01-21T20:58:13Z,/microsoft/vscode/issues/66866,vscode,CLOSED,"Rename file in explorer styling issue
Click a file in the explorer to select
Focus the editor
Focus the explorer again with the keyboard
Press enter to rename file
The row is highlighted but the input box doesn't appear
Try renaming other rows, sometimes the input box appears, sometimes not
Sometimes the wrong text appears after pressing enter, not sure if that's a separate issue. That happens a couple times in the gif
",NONE
441,{'login': 'bpasero'},2016-06-28T01:15:43Z,8167,2016-06-27T04:55:47Z,/microsoft/vscode/issues/8167,vscode,CLOSED,"OutputWorker: Cannot read property 'getWorkspace' of undefined
When opening the output panel:
",NONE
442,{'login': 'joaomoreno'},2017-04-12T07:47:32Z,16489,2016-12-05T09:31:02Z,/microsoft/vscode/issues/16489,vscode,CLOSED,"Empty line between settings sections
We could remove that extra line between sections.",MAINTAINABILITY
443,{'login': 'CharlieIGG'},2019-02-22T01:25:27Z,67126,2019-01-25T16:21:32Z,/microsoft/vscode/issues/67126,vscode,CLOSED,"AutoImport not working for NPM packages, only in JS/JSX
Issue Type: Bug
I have this weird issue where AutoImport won't work for NPM packages when using JS or JSX. However it DOES work for:
Local files
when using Typescript instead of JS
I haven't found any issue quite like this, so I finally decided to ask for clues. Any hints at how to fix this would be greatly appreciated.
VS Code version: Code 1.30.2 (61122f8, 2019-01-07T22:48:31.260Z)
OS version: Darwin x64 18.2.0
",NONE
444,{'login': 'SuEric'},2017-03-28T20:40:34Z,23459,2017-03-28T16:00:21Z,/microsoft/vscode/issues/23459,vscode,CLOSED,"File Tree Automatically Focus when closing a file
This is not a bug
It is more an enhancement
Thing is when I close a tab (file), file tree changes it focus automatically to the now active file; this is really annoying because many times I just open and close many files (as I'm sure other devs do) and I have to scroll all ways down/up again! Hope you improve this.
Note: Sublime Text doesn't automatically focus the file tree after closing a file.",USABILITY
445,{'login': 'mattiamerlini'},2017-09-13T21:56:48Z,34204,2017-09-12T07:58:11Z,/microsoft/vscode/issues/34204,vscode,CLOSED,"Problem with syntax highlighting (HTML/PHP)
Problem with syntax highlighting as reported below:
Write HTML code
Insert multiline pieces of PHP code
See that the syntax doesn't recognise the open/close php tag
",NONE
446,{'login': 'dougmartin'},2017-04-07T23:51:30Z,24249,2017-04-07T17:42:52Z,/microsoft/vscode/issues/24249,vscode,CLOSED,"Feature request: Add context menu to branch name in lower left
Version 1.11 changed the default action when you clicked on the branch name on the lower left from opening an input with git checkout already filled to an input for the git checkout command itself. The previous pre-filled input was nice in that you could replace ""checkout"" with ""branch"" and create a new branch.
Instead of cluttering the new popup ui with a branch option you could add a context menu to the branch name in the lower left corner that would contain branch and optionally other common commands which you can access now in the ellipsis menu in the git pane (note: branch is not available right now in the git pane ellipsis menu).",MAINTAINABILITY
447,{'login': 'mrichman'},2017-09-01T14:44:20Z,32716,2017-08-17T12:21:21Z,/microsoft/vscode/issues/32716,vscode,CLOSED,"Duplicate setting ""npm.runSilent"" in vscode://defaultsettings/settings.json
Extensions:
Extension
Author (truncated)
Version
markdown-toc
Ala
1.5.6
xml
Dot
1.9.2
EditorConfig
Edi
0.9.4
material-icon-theme
PKi
2.1.0
vscode-docker
Pet
0.0.16
code-settings-sync
Sha
2.8.2
html-css-class-completion
Zig
1.8.0
hugofy
akm
0.1.0
vscode-color
ans
0.4.5
cform
aws
0.0.10
better-toml
bun
0.2.0
path-intellisense
chr
1.4.2
gitignore
cod
0.5.0
typewriter
dan
1.0.1
vscode-eslint
dba
1.2.11
githistory
don
0.2.3
gitlens
eam
4.3.3
vscode-npm-script
eg2
0.2.0
lambda-snippets
log
0.2.0
Go
luk
0.6.63
prettify-json
moh
0.0.3
debugger-for-chrome
msj
3.1.8
vscode-icons
rob
7.12.0
gitblame
wad
2.1.0
(3 theme extensions excluded)
Line 1975 and 2061 are duplicates.
Does NOT repro with extensions disabled.
Reproduces without extensions: No",NONE
448,{'login': 'dontsave'},,58987,2018-09-19T21:42:54Z,/microsoft/vscode/issues/58987,vscode,OPEN,"moving TypeScript files in explorer fails to update/prompt imports when containing folder is moved
Typescript: 3.0.3
Set update imports on move typescript setting to prompt
Create an empty project with 2 typescript files, one importing from the other.
In the explorer manually move the depended upon file to a new directory. You will be prompted to update the import. This works fine.
Now move that directory into ANOTHER new directory.
This time there is no prompt and the import will be broken
",NONE
449,{'login': 'aeschli'},2016-09-20T07:26:20Z,9764,2016-07-26T09:50:18Z,/microsoft/vscode/issues/9764,vscode,CLOSED,"[tab completion] multi cursor behaviour different to emmet
Testing #9698:
enable tab completion: ""editor.tabCompletion"": true
create a html snippet
in a html file have a multiple lines with content 'div'. Set multiple cursors after each div. Press tab.
👉 primary div is expanded
do the same with multiple lines of span, press tab
👉 tab character is added at each of the cursors
Not sure if the emmet behaviour is intentional.
It would be cool if the expansion happens on every cursor.",NONE
450,{'login': 'rwmartinez'},2018-09-01T05:32:39Z,57704,2018-08-31T17:34:30Z,/microsoft/vscode/issues/57704,vscode,CLOSED,"Scrolling
When scrolling through a file, the view is not fluid. It jumps, skips, pauses and is not a constant view as I move up or down the file.
Version: July 2018 (version 1.26)",USABILITY
451,{'login': 'ckehoe'},2016-07-03T05:44:34Z,8633,2016-07-01T16:54:39Z,/microsoft/vscode/issues/8633,vscode,CLOSED,"Mac OS - Highlighting text in the editor requires two clicks when the integrated terminal is open
Open a file in the editor
Open the integrated terminal
Type some commands on the terminal
Try to copy some text on the editor by highlighting and typing Mac + c
You will have to click on the section twice before the text successfully copies
",USABILITY
452,{'login': 'antkn33'},2018-08-22T20:01:49Z,56153,2018-08-10T14:11:55Z,/microsoft/vscode/issues/56153,vscode,CLOSED,"open in broswer
Please natively support opening a file in the browser. LIke a right click menu option. The extensions don't work well especially in chrome.
thanks
",MAINTAINABILITY
453,{'login': 'fabiospampinato'},,56234,2018-08-12T23:34:14Z,/microsoft/vscode/issues/56234,vscode,OPEN,"TreeItem is too slow
I've added the following view to Todo+:
I've been benchmarking it against magento2, which contains about 30k files and 2M lines of code. If the user has ag installed in his system the process of finding those todos is quite fast, its takes about 4s to do that on my laptop.
Quite surprisingly though the bulk of the time is spent on these lines, which are executed about 1k times, since the extension creates about 1k TreeItems under these circumstances:
Adding the tooltips requires about 500 extra milliseconds, I would expect that number to be near 0ms, I see basically nothing going on here that can justify that number.
Adding the commands requires about 4 extra seconds. 4 seconds for doing what? Creating those objects should be almost free, and until those TreeItems get clicked they don't even change anything as far as the user is concerned, I think.
Adding the icons requires about 14 extra seconds. There are less than 10 different images loaded, but for some reason loading them a few hundred times is that slow.
",PERFORMANCE
454,{'login': 'rojepp'},2016-09-28T09:28:24Z,5876,2016-04-27T06:28:16Z,/microsoft/vscode/issues/5876,vscode,CLOSED,"Additional instance starts in wrong virtual desktop
I'm using desktops.exe from SysInternals to add virtual desktops to Windows 7.
When having an open instance of code in a not active desktop, starting an additional instance will not start on the currently active desktop.
Open VSCode in desktop 1
Switch to desktop 2
Start another VSCode instance: code .. The new instance starts in desktop 1
",NONE
455,{'login': 'roblourens'},2016-10-31T11:17:43Z,14679,2016-10-28T21:51:32Z,/microsoft/vscode/issues/14679,vscode,CLOSED,"Intellisense in package.json shows packages inconsistently
From #13835 (comment),
When typing quickly, I'm selecting packages by initials (or something), but when I type slowly, I'm selecting packages by prefix:
Open package.json
Put the cursor in ""dependencies""
ctrl+space, then esc (It doesn't repro unless the intellisense window has been shown on this line)
type blu quickly
The intellisense window appears again and I expect to see suggestions for all packages that match. Instead I only see ""blackbaud-npi-datamart-ux"", with the 'blu' highlighted. It's always this one package.
If you type blu more slowly, then it shows 'blu', 'blu-css', 'blu-generator' ...
Another example -
Typing ""blah"" quickly
Slowly:
The first type of fuzzy matching would be useful but it should be more consistent.",NONE
456,{'login': 'vscodeerrors'},2016-02-05T11:27:41Z,2470,2016-01-27T20:55:58Z,/microsoft/vscode/issues/2470,vscode,CLOSED," Uncaught SyntaxError: Unexpected end of input
Issue Id: 3c0f493a-37c5-196b-ce04-755343aba4dbVersions - 0.10.6-release- dfc08dcStack SyntaxError: Unexpected end of input at Object.parse (native)[/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 (V8Protocol.dispatch)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 %28V8Protocol.dispatch%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 (V8Protocol.handleData)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 %28V8Protocol.handleData%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 (V8Protocol.connect)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 %28V8Protocol.connect%29) at emitOne (events.js:77:13) at Socket.emit (events.js:169:7) at readableAddChunk (_stream_readable.js:146:16) at Socket.Readable.push (_stream_readable.js:110:10) at Pipe.onread (net.js:523:20)",FAULT TOLERANCE
457,{'login': 'fiendish'},2018-04-24T04:57:58Z,48410,2018-04-23T13:59:55Z,/microsoft/vscode/issues/48410,vscode,CLOSED,"""workbench.editor.labelFormat"": ""default"" should show paths with split window
Make two identically named files in different subfolders of a project.
Set ""workbench.editor.labelFormat"": ""default"" in settings.
Open both files. See path info that differentiates them.
Split the window and move one over. No longer see path info that differentiates them.
Desired behavior:
I should still see path info that differentiates them, since there are two different files open with the same title and the goal of ""default"" is to be able to differentiate them by path.
Does this issue occur when all extensions are disabled?: Yes/No
Yes.
",NONE
458,{'login': 'kabforks'},2018-08-15T16:36:19Z,56364,2018-08-14T13:56:50Z,/microsoft/vscode/issues/56364,vscode,CLOSED,"Unresponsive VSCode after upgrade to 1.26.
Hello!
After installing VSCode 1.26, the GUI is almost completely unresponsive when I open my project. After a few minutes, it can be used. However, the CPU usage will remain at 24% (i have 4 cores). Even after the project or folder is closed.
If I open VSCode without a project or folder, it behaves.
VSCode is responsive if launched with --disable-extensions.
Could it be an extentions that misbehaves?
Here is a --status dump:
Version: Code 1.26.0 (4e93618, 2018-08-13T16:29:31.933Z)
OS Version: Windows_NT x64 10.0.16299
CPUs: Intel(R) Core(TM) i7-6820HQ CPU @ 2.70GHz (8 x 2712)
Memory (System): 15.85GB (3.24GB free)
VM: 0%
Screen Reader: no
Process Argv: C:\Users\forsbdan\AppData\Local\Programs\Microsoft VS Code\Code.exe
GPU Status: 2d_canvas: enabled
checker_imaging: disabled_off
flash_3d: enabled
flash_stage3d: enabled
flash_stage3d_baseline: enabled
gpu_compositing: enabled
multiple_raster_threads: enabled_on
native_gpu_memory_buffers: disabled_software
rasterization: enabled
video_decode: enabled
video_encode: enabled
webgl: enabled
webgl2: enabled
CPU % Mem MB PID Process
0 100 34680 code main
0 84 22972 shared-process
0 146 23460 gpu-process
23 1271 28764 window (Program.cs - demo2 - Visual Studio Code)
0 6 26188 winpty-process
0 65 16204 C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe
0 10 19472 console-window-host (Windows internal process)
0 51 30296 searchService
0 14 31020 electron-crash-reporter
0 11 31384 watcherService
0 10 21716 console-window-host (Windows internal process)
0 191 33784 extensionHost
0 38 22568 searchService
0 3 37816 cmd /s /c ""C:\Users\forsbdan.vscode\extensions\ms-vscode.csharp-1.15.2.omnisharp\1.30.1\OmniSharp.exe -s c:\src\demo2\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4""
0 10 7140 console-window-host (Windows internal process)
0 115 19888 C:\Users\forsbdan.vscode\extensions\ms-vscode.csharp-1.15.2.omnisharp\1.30.1\OmniSharp.exe -s c:\src\demo2\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4
Workspace Stats:
| Window (Program.cs - demo2 - Visual Studio Code)
| Folder (demo2): 83 files
| File types: json(15) cshtml(12) cs(8) js(5) ts(5) scss(5) cache(5)
| map(3) vue(3) txt(2)
| Conf files: launch.json(2) tasks.json(2) sln(1) csproj(1)
| package.json(1) tsconfig.json(1) webpack.config.js(1)
| settings.json(1)
| Launch Configs: coreclr(2)
",PERFORMANCE
459,{'login': 'GerbenJdeBoer'},2018-09-18T20:28:46Z,46332,2018-03-22T11:47:13Z,/microsoft/vscode/issues/46332,vscode,CLOSED,"ctrl+z/clrl+y after block mode edit does not follow the (multi-line) cursor to follow the undo/redo steps
Issue Type: Bug
create dummy text1
do block mode edit inside text1
create more dummy text, preferably so much you dont see text1 any more
ctrl + z
VS Code version: Code 1.21.1 (79b44aa, 2018-03-14T14:46:47.128Z)
OS version: Windows_NT x64 10.0.15063
Reproduces only with extensions
",USABILITY
460,{'login': 'bpasero'},2016-07-15T10:37:10Z,4886,2016-04-03T15:43:06Z,/microsoft/vscode/issues/4886,vscode,CLOSED,"HTML tests are failing
https://travis-ci.org/Microsoft/vscode/jobs/120206982",NONE
461,{'login': 'roblourens'},2019-04-24T18:14:01Z,72813,2019-04-24T14:59:34Z,/microsoft/vscode/issues/72813,vscode,CLOSED,"Settings editor crashes when restoring
Open settings editor
Tab away
Tab back to it
",FAULT TOLERANCE
462,{'login': 'TheJCAB'},2019-03-16T03:19:26Z,70591,2019-03-15T20:03:22Z,/microsoft/vscode/issues/70591,vscode,CLOSED,"VSCode should do like any decent editor, and provide a drop-down from all search text boxes
Issue Type: Feature Request
Decent editors provide a drop-down list of older search phrases in their search (and replace!) text entry UI elements.
But VSCode doesn't seem to do this, for some reason.
It's only polite to remember such things for the comfy user, you know :)
VS Code version: Code 1.32.3 (a3db5be, 2019-03-14T23:43:35.476Z)
OS version: Windows_NT x64 10.0.18348
",USABILITY
463,{'login': 'sean-mcmanus'},2018-12-20T20:11:58Z,65327,2018-12-18T21:23:04Z,/microsoft/vscode/issues/65327,vscode,CLOSED,"Reopen issue 21518 or 18782 
Please reopen either #21518 or #18782 . The bug still repros with C/C++.
void foo(int, const char*, const char *) { // also repros when the params are subsets, e.g. a, aa, aaa
}
int main()
{
foo(a, ) // signature Help selects the the 3rd param instead of the 2nd.
}",NONE
464,{'login': 'babramczyk'},2019-10-10T08:18:05Z,81237,2019-09-20T15:03:33Z,/microsoft/vscode/issues/81237,vscode,CLOSED,"Cannot specify global problemMatcher in tasks.json
Specify ""problemMatcher"": [] at top level of tasks.json object
Run a task (i.e. an npm task) that has no task config
VS Code still prompts you for how to scan the task output
Does this issue occur when all extensions are disabled?: No
Other comments
Essentially, I want to be able to disable the problem matcher by default. Personally, I never use it, and it's just noise, because every time I run a task I haven't ran before, I have to select Never scan the task output, which opens up tasks.json and adds a new config for that task, just so it can specifiy ""problemMatcher"": [].
Related issues
#43003
This issue was closed, but I still have the problem
",NONE
465,{'login': 'gregvanl'},2016-07-07T15:22:38Z,4391,2016-03-17T22:28:42Z,/microsoft/vscode/issues/4391,vscode,CLOSED,"Can't do column selection with keyboard over Windows remote desktop
Try Ctrl+Shift+Alt+arrow keys to ""draw"" a rectangle.
Nothing happens
Shift+Alt and mouse works correctly.",NONE
466,{'login': 'jabbera'},2017-03-21T18:52:52Z,22970,2017-03-21T18:50:09Z,/microsoft/vscode/issues/22970,vscode,CLOSED,"ARM template type not accepted even though valid.
This issue appears to have come back (or some variation of it) #881
",NONE
467,{'login': 'qcz'},2018-04-18T11:28:46Z,47566,2018-04-10T09:18:44Z,/microsoft/vscode/issues/47566,vscode,CLOSED,"Nonsensical message in extensionManagementService.ts
Found while translating to Hungarian. The message is ""Unknown error while"" in
src/vs/platform/extensionManagement/node/extensionManagementService.ts.
It does not make any sense, the end is missing. Added by @sandy081 in db7ddfd.",NONE
468,{'login': 'CODER-2204'},2019-11-01T22:23:59Z,83854,2019-11-01T20:52:40Z,/microsoft/vscode/issues/83854,vscode,CLOSED,"I am not able to run my code.
Issue Type: Bug
I am not able to run my code in Visual Studio. It is repeatedly showing that include errors are detected and to update my includePath for which I have already installed vcpkg from GitHub , still the issue is not yet solved.
VS Code version: Code 1.39.2 (6ab5985, 2019-10-15T15:35:18.241Z)
OS version: Windows_NT x64 10.0.18362
",NONE
469,{'login': 'garfieldbanks'},2017-11-17T11:38:55Z,17371,2016-12-16T04:33:37Z,/microsoft/vscode/issues/17371,vscode,CLOSED,"Add option to view default settings in single plain text file
Pretty self explanatory. I don't have anything against the new UI or anything but I don't like that I can no longer see the default settings in a single plain text file.",USABILITY
470,{'login': 'aaronfranke'},,66308,2019-01-10T00:22:10Z,/microsoft/vscode/issues/66308,vscode,OPEN,"Allow extensions to contribute build task types
Currently, when I press Ctrl+Shift+B and click ""Configure Build Task..."" then ""Create tasks.json from template"", the only options are MSBuild, maven, .NET Core, and Others.
I would like to be able to easily create a build task template for my Mono projects. .NET Core does not have the full feature-set of Mono or .NET Framework yet, and even if it did, I still have existing projects using Mono that I'd like to edit with VS Code. Please add this feature!",MAINTAINABILITY
471,{'login': 'vscodeerrors'},2016-04-06T10:11:16Z,5019,2016-04-06T08:31:15Z,/microsoft/vscode/issues/5019,vscode,CLOSED," spawn osascript ENOENT
Issue Id: def759c3-62fe-b6f1-ff32-27679721a3ffVersions - 0.10.8- f291f4a- 43ff6af- 5b5f4db- 17fa1cbStack Error: spawn osascript ENOENT at exports._errnoException (util.js:837:11) at Process.ChildProcess._handle.onexit (internal/child_process.js:178:32) at onErrorNT (internal/child_process.js:344:16) at doNTCallback2 (node.js:442:9) at process._tickCallback (node.js:356:17)",NONE
472,{'login': 'quicksnap'},2017-11-17T11:38:27Z,35637,2017-10-05T19:08:36Z,/microsoft/vscode/issues/35637,vscode,CLOSED,"Preference to disable Recently Opened Files
I personally have no need for recent files to be promoted to the top of the Goto File results:
These results actually slow me down, since I a used to Sublime Text fuzzy match behavior. If the ""Recently Opened"" results were not there, I would be able to jump to files in a predictable way.
Any preference option I'm missing?",USABILITY
473,{'login': 'dbaeumer'},2016-08-25T07:51:34Z,5650,2016-04-22T10:15:24Z,/microsoft/vscode/issues/5650,vscode,CLOSED,"Workbench should provide language agnostic actions to apply all quick fixes to a file
Instead of extensions and language servers providing their own language specific actions the workbench should provide actions to apply all available quick fixes to a file. I think that this is even possible with the extension API today since we can ask for all code actions for a given range.
See microsoft/vscode-eslint#70.",NONE
474,{'login': 'ramya-rao-a'},2016-10-03T18:48:02Z,13103,2016-09-30T21:44:03Z,/microsoft/vscode/issues/13103,vscode,CLOSED,"Missing localizations in 1.6
In Menu Items:
Help -> Search (Mac OS X)
View -> Toggle Render Whitespace
Edit -> Start Dictation (Mac OS X)
In Settings:
The below is not localized in User Settings, but is localized in Workspace Settings
// Place your settings in this file to overwrite the default settings
The below is not localized in Keyboard Shortcut settings
// Place your key bindings in this file to overwrite the defaults
In Launch.json
Tooltips for all properties except the below are not localized in launch.json
name
type
request
preLaunch task
while debugging
The title for the Debug Console in German appears as ""DebugKonsole"". There should be a space between the ""Debug"" and the ""Console""
Add watch for a symbol. When it is not available, the value says ""not available"" in English
while closing unsaved files
The dialog box that appears has text that is not localized",NONE
475,{'login': 'NilsEnevoldsen'},2019-06-03T14:39:14Z,74759,2019-06-02T19:21:40Z,/microsoft/vscode/issues/74759,vscode,CLOSED,"Other NSDocument apps don't recognize when VS Code changes a document
Open a document with an NSDocument-aware app such as TextEdit
(Optionally, open that document with a second NSDocument-aware window)
Open that document with VS Code
(Optionally, open that document with a second VS Code window)
(Optionally, edit and save the document in the first app, observe that all VS Code windows update)
Edit and save the document in VS Code, observe that the first app doesn't update (but all other VS Code windows update)
(Optionally, re-save the document in the first app. Observe that, prior to interacting with the warning dialog, all other NSDocument-aware windows update.)
This bug impairs working with the same file in multiple applications on macOS.
This may be related to NSFilePresenter and presentedItemDidChange of NSDocument.
Does this issue occur when all extensions are disabled?: Yes
",NONE
476,{'login': 'varadero'},2017-03-27T04:42:52Z,23062,2017-03-23T00:40:33Z,/microsoft/vscode/issues/23062,vscode,CLOSED,"Scrolling with touchpad behaves like pressing up and down arrow keys
I have a laptop with touchpad and scrolling (by touching the touchpad with two fingers and sliding them vertically) file content, files list, terminal window or anything inside VSCode behaves like I am pressing an holding up/down arrow keys. This behavior is not only annoying but it doesn't allow me to scroll say terminal window, because scrolling simply shows the list of last run commands - the same as if you press up arrow key multiple times. Horizontal scrolling behaves as left/right arrow keys - instead of scrolling horizontally, I move the caret. I have a wireless Microsoft mouse and when I use it for scrolling, everything works a expected. When I use my touchpad to scroll in the same way any other application like Chrome, Notepad, whatever, it behaves normally (the caret stays on its location and only the view is scrolled - actually I can normally scroll this GitHub's text area I am writing the issue text in). Does anyone have such problem with VSCode ? Could it because of touchpad drivers (I think mine is ""ELAN Pointing Device"") ?
List of extensions:
1 Debugger for Chrome 2.7.0
2. Git History (git log) 0.2.0
3. TSLint 0.8.1
4. vscode-icons 7.4.0
I tried to disable all extensions and reload VSCode but nothing changed.
The video below is made without using arrows keys - only scrolling functionality of my touchpad.
",USABILITY
477,{'login': 'kieferrm'},2018-02-07T23:46:31Z,41061,2018-01-02T22:51:58Z,/microsoft/vscode/issues/41061,vscode,CLOSED,"Iteration Plan for January 2018
Happy 2018 everyone! This plan captures our work in January. This is a 5 week iteration. We will ship early February.
Endgame
January 29th: Code freeze for the endgame
February 2nd: Endgame done
The endgame details for this iteration are tracked in #42374
Plan Items
Below is a summary of the top level plan items. Given the large number of explorations, we'll diverge from our usual practice of having plan items for all bullets upfront. This time we'll add them as we go.
Legend of annotations:
Mark
Description
🏃
work in progress
✋
blocked task
💪
stretch goal for this iteration
🔴
missing issue reference
🔵
more investigation required to remove uncertainty
⚫
under discussion within the team
Install/Update
 Investigate in improving the update experience on Windows #41676 @joaomoreno
Workbench
 Switch to async dialog API #39536 @bpasero
 Support saving a file in admin mode #1614 @bpasero
 UX for notification improvements #22388 @bpasero @stevencl
 Multi-select in the Explorer, Open Editor #1023 @isidorn
 Reimplement drop downs for Linux/Windows (themable, fixes initial empty contents) #25965 (PR @cleidigh) @bpasero
 Enable Error decorations in explorer #782 @jrieken
 Support natural language search in Settings editor #40957 @roblourens
 Explore improving how a user changes a setting #41040 @roblourens @sandy081
Editor
 Text model and storage reimplementation to improve performance #41042 @alexandrudima @rebornix
 Allow to save large files > 256 MB #32503 @bpasero @alexandrudima
 Support language-type independent snippets #13182 @jrieken
 More customization for the caret #41052 @ramya-rao-a
Debug
 Launch configs for multi root workspaces #38134 @isidorn
 Support auto attach for node.js subprocess (aka cluster support) #40123 @weinand
 Support to use nvm configuratons in node launch configs #25386 @weinand
 Explore how to run DebugAdapter inside extension #40906 @weinand
Terminal
 Improve accessibility of built-in terminal #8339 @Tyriar
SCM
 Submodule support @joaomoreno
 Git commit message length counter @joaomoreno
Output Panel
 Show product logs in the output panel #39638 @sandy081
 Make log viewing in output panel more memory efficient #40196 @sandy081
Languages
Language Server Support
 Create a website for LSP @dbaeumer @auchenberg
 Protocol extension for goto implementation microsoft/language-server-protocol#156 @dbaeumer
Emmet
 Explore how to improve emmet activation in html and css files #29113 @ramya-rao-a
JavaScript/TypeScript
 Adoption of TS 2.7 #41046 @mjbvz
CSS/HTML
 Catchup with latest CSS/Less syntax microsoft/vscode-css-languageservice#56 microsoft/vscode-css-languageservice#57 microsoft/vscode-css-languageservice#58 microsoft/vscode-css-languageservice#47 @octref @aeschli
Extensions
 vsce - Warn when package.json misses repository entry #41677 @joaomoreno
 Improve quality of recommended extensions #41054 @ramya-rao-a
 Tastefully extend recommendations to a wider range of file types #38543 @ramya-rao-a
Extension Contributions
 Refresh JS Hint support microsoft/vscode-jshint#48 @RMacfarlane
API
 Migrate proposed Code action API to stable #34664 @jrieken @mjbvz
 Propose API to resolve rename/definition scope #7340 @jrieken @mjbvz
 Explore improving the HTMLPreview support #41047 @mjbvz @jrieken
 Propose refactoring provider API #41048 @jrieken @mjbvz
 Expose logging API to extensions #40053 @roblourens
 Propose Search Provider API @jrieken, @roblourens
 Propose API to create/delete/rename resources for refactorings @jrieken @mjbvz
 Enhance custom tree view API, primary actions, use icons from resource URI, preserve expansion state, improve managing contributions #27823 @sandy081
 Debug API to create/remove breakpoints @weinand
 Support to modify the root folder of a workspace #35407 @bpasero
Performance
 🏃 Use ASAR for bundled node modules #41350, #41353 @alexandrudima
 Explore using plain nodejs for helper processes #41685 @alexandrudima
 Explore local storage replacement #18439 @bpasero
Serviceability
 Issue reporter in separate renderer windows #41041 @RMacfarlane @octref
Logs
 Propagate log level to all processes dynamically #39754 @sandy081
 Support to upload logs #40056 @mjbvz @roblourens
Engineering
 Self-host on @ts-check for our JS code #41678 @joaomoreno @egamma
 🏃 Support extensions that contribute translations aka ""language packs"" #39178 @dbaeumer @sandy081 @aeschli
 🏃 Tool to generate/update a language pack from transifex #41682 @aeschli
 Speed up gulp-build @alexandrudima
Improve issue tracking support bot @chrmarti
 Delay action of auto-assignment bot by 15s #33999
 Explore support for detecting duplicate issues #41292
Documentations
 Make 5min Node.js debugging video to be embedded in docs and uploaded to YouTube channel @auchenberg
 Add Debugging Recipe for VueJS. microsoft/vscode-recipes#55 @auchenberg
Deferred
Improve documentation of our electron upgrade process #41036 @Tyriar
Add intelisense support for src attributes and href in html #2037 @octref @aeschli
Adopt logging service @joaomoreno the adoption by the #41680 team
💪 Render white space for selection option #1477 @ramya-rao-a
Provide API for creating a file based output channel #41672 @sandy081
✋ Electron update to 2.0 @Tyriar @bpasero
💪 Docathon team
✋ Support 32-bit apt repositories #20790 @Tyriar
Better support for webpack TBD
Support web-site for issue reporting @octref @RMacfarlane
Improve stability of smoke test, run it as part of the builds #41679 # @joaomoreno
💪 Process explorer as a separate renderer window #41045 @RMacfarlane
",NONE
478,{'login': 'alexdima'},2017-05-31T05:42:00Z,27480,2017-05-30T05:26:54Z,/microsoft/vscode/issues/27480,vscode,CLOSED,"Test editor.multicursorModifier
Testing #27193
 OSX @bpasero
 windows @chrisdias
 linux @isidorn
Complexity: 2
There is a new setting editor.multicursorModifier. On Linux and Windows it can have the value ""ctrl"" or ""alt"". On OSX it can have the value ""cmd"" or ""alt"". Please check:
the default is ""alt"" (just as before). So without any changes, out of the box, multiple cursors are added via ""alt"" + click. It is a known issue that under some Linux distributions, multiple cursors cannot be added via ""alt"" + click, which is used to move windows, this new option aims to also overcome this limitation.
when using alt as the multicursor modifier: ctrl+click (cmd+click) is used for going to definition and opening links. ctrl+alt+click (cmd+alt+click) is used for going to definition and opening in a side editor and for opening a link in a side editor (you can craft a file:/// link to test this)
when using ctrl or cmd as the multicursor modifier, the 3 features swap modifiers and going to definition / opening a link is alt+click. ctrl+alt+click (cmd+alt+click) will still be used for going to definition / opening a link to the side. The hover message on links correctly shows the modifier to be used to open a link.
",NONE
479,{'login': 'szmcdull'},2019-01-22T17:45:01Z,66311,2019-01-10T03:54:17Z,/microsoft/vscode/issues/66311,vscode,CLOSED,"IDE slows to freeze as more and more output written to internal debug console
Debug a GO program
The program writes to the standard output
IDE starts to slow down and finally freeze.
",PERFORMANCE
480,{'login': 'octref'},2016-11-15T23:56:12Z,15542,2016-11-15T22:52:28Z,/microsoft/vscode/issues/15542,vscode,CLOSED,"Markdown preview doesn't work correctly when zoomed in
While smoke testing 1.7.2 eb1f17e
",FAULT TOLERANCE
481,{'login': 'javadbat'},2019-01-11T06:37:09Z,65163,2018-12-16T15:26:39Z,/microsoft/vscode/issues/65163,vscode,CLOSED,"nodejs debugger nodemon crush 
i use nodemon in project and it work smooth and good but when i try to run exact gulp command 'gulp serve' in vs code debugger it cuase following error:
'[nodemon] app crashed - waiting for file changes before starting...'
it work really when i type gulp command but in debug mode i get error.
the code is:
",FAULT TOLERANCE
482,{'login': 'jordangarside'},2017-09-19T17:12:02Z,34586,2017-09-18T19:34:27Z,/microsoft/vscode/issues/34586,vscode,CLOSED,"C++ intelliSense with different file system types (Windows 10)
Extensions:
Extension
Author (truncated)
Version
cpptools
ms-
0.12.4
IntelliSense doesn't seem to be working with SSHFS filesystem type in Windows 10 (no import errors detected and no squiggly lines). I know this is a fringe case, but I'm wondering if there is an easy fix.
IntelliSense works fine for this example in the local filesystem as well as on an exfat formatted flash drive.
Install winsshfs and mount a remote file system
Open the a project locally from that mounted file system
Reproduces without extensions: No (Need the C++ extension for intellisense)",NONE
483,{'login': 'weinand'},2017-12-05T13:04:11Z,39553,2017-12-04T14:33:44Z,/microsoft/vscode/issues/39553,vscode,CLOSED,"Test extension API for breakpoints 
Test for #23188:
Complexity: 4
 Any OS - @jrieken
The November milestone of VS Code proposes extension API for reading the breakpoints of a workspace and tracking added, removed, and changed breakpoints:
https://github.com/Microsoft/vscode/blob/a42cd0efc5b4baa17075fcd8da1c5e2097419c6f/src/vs/vscode.proposed.d.ts#L251-L329
Verify:
API makes sense (especially the Breakpoint, SourceBreakpoint, FunctionBreakpoint hierarchy and its use of the type discriminator). Will this work if we extend the API to create those types?
write a simple extension that accesses breakpoints and registers for BreakpointsChangeEvents. Please note that accessing breakpoints initially returns an empty array but triggers a subsequent event that has the full set of breakpoints in its added property.
",MAINTAINABILITY
484,{'login': 'makeway4pK'},2019-07-30T13:08:48Z,77406,2019-07-15T15:18:46Z,/microsoft/vscode/issues/77406,vscode,CLOSED,"new behavior of File duplication not smart(file<space>copy.ext)
Issue Type: Performance Issue
The new behavior of file copy pasting (ctrl+c, ctrl+v) in the proj explorer adds whitespace to the new file name, often causing command line scripts to not work. including double quotes to the script is not always easy, and many devs(probably you as well) have a habit of eliminating whites just to avoid this clumsiness.
Older behavior was much better, if not best, file.ext -> file.1.ext
plz restore it
VS Code version: Code 1.36.1 (2213894, 2019-07-08T22:59:35.033Z)
OS version: Windows_NT x64 10.0.17134
",NONE
485,{'login': 'BrunnerLivio'},2017-04-05T04:49:01Z,21868,2017-03-03T09:29:54Z,/microsoft/vscode/issues/21868,vscode,CLOSED,"FTP file gets deleted when drag and drop from file explorer
Open file explorer
Open any ftp folder
Drag an drop any file from FIle Explorer into VSCode
The dragged file gets removed from FTP folder, but it is not openend in VSCode
",NONE
486,{'login': 'alexdima'},2017-06-26T10:34:31Z,27537,2017-05-30T10:46:18Z,/microsoft/vscode/issues/27537,vscode,CLOSED,"Cannot run integration test while having the vim extension installed 
Testing #27456
I have the vim extension installed and disabled (always), but it looks like the integration test picks it up and enables it. This causes the integration test to fail in the Data Migration -> checks if the Untitled file is restored migrating from stable to latest test
I will continue by uninstalling the vim extension.",FAULT TOLERANCE
487,{'login': 'P-mir'},2019-09-20T00:44:23Z,81149,2019-09-19T09:20:40Z,/microsoft/vscode/issues/81149,vscode,CLOSED,"Bug when running python code in console after running it using the ""run the selection"" feature
Issue Type: Bug
1- Write print('hello') in a new python file
2- Select the chunk of code and run it with run selection (shift+enter)
3- Run the code again, but using run python file in terminal.
expected: hello
actual:
File """", line 1
& C:/Users/me/AppData/Local/Programs/Python/Python37-32/python.exe ""mypath/test.py""
^
SyntaxError: invalid syntax
VS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:35:15.005Z)
OS version: Windows_NT x64 10.0.18362
",NONE
488,{'login': 'MashaMSFT'},2019-02-19T18:51:11Z,68659,2019-02-13T22:51:14Z,/microsoft/vscode/issues/68659,vscode,CLOSED,"CLI code snippets do not change font color
When I use a code block for something like powershell or t-sql, the colors of the font change, making it easier to parse the text.
For example
Font color changes in accordance to powershell standards.
However, when I do so for CLI, the color doesn't change, and it should be changing, right?
No font color change in accordance to CLI standards.",USABILITY
489,{'login': 'usernamehw'},2019-10-14T09:43:54Z,82420,2019-10-12T09:55:02Z,/microsoft/vscode/issues/82420,vscode,CLOSED,"[SCM tree]: Keep first level indent the same as list
Pretty similar to #66863. SCM tree should exclude first level items and keep indent at 8px
Indent for first level items is too big, because of
",NONE
490,{'login': 'octref'},2017-11-01T15:10:37Z,37335,2017-10-31T19:04:26Z,/microsoft/vscode/issues/37335,vscode,CLOSED,"When launching attach config unsuccessfully, debug status bar item does not show
While testing #35904
I don't know if this is as-designed, but this seems to be one of the original motivation for #31745.
When I'm pressing F5 while not focusing on debug viewlet, I got no visual feedback as to what's the debug target.
Would it make sense to enable the status bar item on launching debug instead of on launching debug successfully?
",USABILITY
491,{'login': 'shizengzhou'},2018-04-29T00:38:45Z,48889,2018-04-28T14:15:45Z,/microsoft/vscode/issues/48889,vscode,CLOSED,"Why the dark+ theme's syntax is different from the light+ theme's syntax?
Why the dark+ theme's syntax is different from the light+ theme's syntax?
VS Code version: Code 1.22.2 (3aeede7, 2018-04-12T16:38:45.278Z)
OS version: Windows_NT x64 10.0.15063
",NONE
492,{'login': 'egamma'},2017-06-29T06:21:08Z,29736,2017-06-28T09:45:41Z,/microsoft/vscode/issues/29736,vscode,CLOSED,"No completion proposal for the new presentation property.
I've upgraded a task.json to 2.0.0 and get a deprecated warning
Trying to fix by inserting a presentation property, but there is no Intellisense proposal for presentation.",NONE
493,{'login': 'kishandonepudi'},2018-09-03T03:01:49Z,57059,2018-08-23T05:47:07Z,/microsoft/vscode/issues/57059,vscode,CLOSED,"Replace text
Hi Team,
I have replaced a word from multiple files through find tab and saved all.
But again when i open the ts file to edit some other line , its is showing a working directory.
It was fixing by VSC restart.
Please fix this with an alternative",MAINTAINABILITY
494,{'login': 'Eldaw'},2018-09-19T14:57:16Z,32853,2017-08-20T18:28:39Z,/microsoft/vscode/issues/32853,vscode,CLOSED,"Text gets selected, to the next line, when the menu bar auto hides
Extensions:
Extension
Author (truncated)
Version
spellright
ban
1.1.16
python
don
0.7.0
cpptools
ms-
0.12.3
csharp
ms-
1.12.1
PowerShell
ms-
1.4.1
blank-line-organizer
rin
0.1.2
sort-lines
Tyr
1.3.0
change-case
wma
1.0.0
Set your menu bar to auto hide (i.e. set it to toggle).
Press Alt to show the menu bar.
Click somewhere near the beginning of a line of text in your currently visible text document.
As the menu bar disappears, notice that the text gets selected, started from the position where you clicked and ending on the line below where you clicked.
Expected:
Nothing should get selected when the menu bar auto hides. The cursor should simply be wherever you clicked.
Reproduces without extensions: Yes/No",USABILITY
495,{'login': 'genzoman'},2017-05-12T08:41:53Z,26483,2017-05-11T19:33:47Z,/microsoft/vscode/issues/26483,vscode,CLOSED,"Go to current file from viewing diff. 
edit a file in a Git repo with VS Code
click the Git icon in the left well to see the diff.
I often want to check the differences between two files, and then immediately go to the current working index of that file.
expect: (something like) cmd+ right click on the current working index of a Git diff takes you to that file.",NONE
496,{'login': 'pankajsaini123'},2018-06-08T10:00:47Z,51415,2018-06-08T04:17:07Z,/microsoft/vscode/issues/51415,vscode,CLOSED,"unable to update 
Issue Type: Bug
I am using vs code, and i am facing issue regarding updates .
why it always require admin permission to start, and
It able to fetch updates but does not able to update.
VS Code version: Code 1.24.0 (6a6e02c, 2018-06-06T17:35:40.560Z)
OS version: Windows_NT x64 6.0.6002
",NONE
497,{'login': 'AccessibilityTestingTeam-TCS'},2018-07-05T14:46:44Z,53485,2018-07-03T11:17:41Z,/microsoft/vscode/issues/53485,vscode,CLOSED,"Move editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not working
Environment Details:
VSCode Version : 1.24.1
Additional Details:
MAS Violated: MAS2.1.1
Repro Steps:
1)Launch VS Code.
2)Open Keyboard Shortcuts.
Actual:
Move editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not working. After using this command, screen starts rotating.
Expected:
Both the commands should work.
Recommendations:
Refer below link which is repository of bug fixes code snippets:
https://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx
MAS Reference
https://microsoft.sharepoint.com/:w:/r/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}
Attachment for Reference:
Does this issue occur when all extensions are disabled?: Yes",NONE
498,{'login': 'chrisdias'},2017-01-19T14:14:37Z,9638,2016-07-22T20:22:21Z,/microsoft/vscode/issues/9638,vscode,CLOSED,"close icon on terminal/console/etc. pane should be ""x""
The terminal, debug output, console, etc. panes at the bottom have a small down arrow for the action to close or hide the pane. clicking on this makes the pane slide down and disappear.
The down arrow to me suggests that the window will be collapsed down rather than closed. Whenever I click on this I expect there to be an action at the bottom of the editor to restore the pane. Instead I have to use the keyboard or menu to bring these back.
As a result, a better icon for this pane would be the close ""x"".
Alternatively, there should be a visualization at the bottom of the editor that there is a pane that can be restored.",USABILITY
499,{'login': 'jrieken'},2018-07-04T13:15:44Z,53533,2018-07-04T13:03:39Z,/microsoft/vscode/issues/53533,vscode,CLOSED,"ERR Model is disposed!: Error: Model is disposed!
",NONE