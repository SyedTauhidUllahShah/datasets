[
  {
    "id": 1976,
    "text": "Magic variable role_names to also list roles belonged to via dependencies?\n\nISSUE TYPE\n\nFeature Idea\n\nANSIBLE VERSION\nansible 2.0.2.0 (not that relevant here though)\nCOMPONENT NAME\nmagic variables\nSUMMARY\nThe magic variable role_names, containing the list of all role names that the current target of the playbook belongs to, is quite useful in certain scenarios. For example, I use the same playbook to set up development vagrant VMs as well as test and production servers. A few tasks in certain roles are conditional on role membership, e.g. having when: \"'dev' in role_names\".\nHowever, it looks like roles that are included via role dependencies in meta/main.yml files are not included in the role_names list. You could make an argument either way, but it seems to me that they should be there for completeness' sake, since role membership via dependency is still a type of role membership.\nSeeing as role_names isn't really documented (\"role_names\" site:docs.ansible.com on Google returns nothing, at least), it probably isn't very widely used right now, and changing the behaviour probably won't break much. Is this something you could consider?",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1977,
    "text": "SmartOS IP address fact\n\nOn SmartOS, in contrast with the other OS's, it appears that the IP address object is an actual list of IPs.  Is that to be expected?\nfor example, in a smartos, I have to address it like this:\n{{ ansible_net1['ipv4'][0]['address'] }}\n\nwhere on linux\n{{ ansible_net1['ipv4']['address'] }}\n\nwill suffice.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1978,
    "text": "maven_artifact module should allow preserving the name of the downloaded artifact\n\nISSUE TYPE\n\nFeature Idea\n\nCOMPONENT NAME\nmaven_artifact\nANSIBLE VERSION\nansible 2.3.0 (devel 3e4be156d7) last updated 2017/03/06 21:46:14 (GMT +200)\n\nCONFIGURATION\nN/A\nOS / ENVIRONMENT\nN/A\nSUMMARY\nWhen downloading an artifact you can ask the module to determine the latest version automatically (it gets extracted from the repo's metadata). The artifact's name in the repository comes with a version string which currently gets overwritten with 'latest' when downloading.\nThe problem with this approach is that you sometimes need to know what version the downloaded artifact has for further processing, My use case is transferring the version to the RPM that I build from it.\nSTEPS TO REPRODUCE\nThe implementation should introduce an additional parameter which causes what I described above. This is only relevant for the constellation version: latest and dest being a directory, because else the filename chosen by dest or the version explicitly selected win.\n\n- hosts: localhost\n  gather_facts: no\n  tasks:\n    - maven_artifact:\n        version: latest\n        artifact_id: spring-core\n        group_id: org.springframework\n        dest: /tmp/\n        keep_name: yes\nEXPECTED RESULTS\nThe downloaded artifact contains the dynamically determined version string. In this case, the last version found in this list. At the moment of creating this issue this is /tmp/spring-core-4.3.7.RELEASE.jar\nACTUAL RESULTS\nThe downloaded artifact is renamed to /tmp/spring-core-latest.jar",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1979,
    "text": "SSH connection is established for every command\n\nHere is the log of the run\n$ ansible-playbook deploy.yml -i hosts -vvvv -c ssh\n\nPLAY [app] ********************************************************************\n\nGATHERING FACTS ***************************************************************\n<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx\n<hostname> REMOTE_MODULE setup\n<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/me/.ansible/cp/ansible-ssh-%h-%p-%r\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p \"[sudo via ansible, key=jzpkjtgucxrncjxcvemfmfxcxsgigepl] password: \" -u root /bin/sh -c '\"'\"'echo BECOME-SUCCESS-jzpkjtgucxrncjxcvemfmfxcxsgigepl; LANG=C LC_CTYPE=C /usr/bin/python'\"'\"''\nok: [hostname]\n\nTASK: [install app-pkg] ****************************************************\n<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx\n<hostname> REMOTE_MODULE apt pkg=app-pkg state=latest\n<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/me/.ansible/cp/ansible-ssh-%h-%p-%r\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p \"[sudo via ansible, key=yyqqlkoftogxutbcuhdaczmvxpbeurqf] password: \" -u root /bin/sh -c '\"'\"'echo BECOME-SUCCESS-yyqqlkoftogxutbcuhdaczmvxpbeurqf; LANG=C LC_CTYPE=C /usr/bin/python'\"'\"''\nok: [hostname] => {\"changed\": false}\n\nTASK: [install nginx] *********************************************************\n<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx\n<hostname> REMOTE_MODULE apt pkg=nginx state=latest\n<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/me/.ansible/cp/ansible-ssh-%h-%p-%r\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p \"[sudo via ansible, key=hbtqwkkfqtfqkfdfcamgahpoycrmilqw] password: \" -u root /bin/sh -c '\"'\"'echo BECOME-SUCCESS-hbtqwkkfqtfqkfdfcamgahpoycrmilqw; LANG=C LC_CTYPE=C /usr/bin/python'\"'\"''\nok: [hostname] => {\"changed\": false}\n\nPLAY RECAP ********************************************************************\nhostname      : ok=3    changed=0    unreachable=0    failed=0\n\nThe deply.yml is as simple as\n\n---\n- hosts: app\n  vars: ~\n  remote_user: xxxxxxxxx\n  sudo: yes\n\nfollowed by 2 tasks to install packages via apt-get.\nThe OS on both machines is ubuntu trusty, the requiretty does not exist in sudoers (on the target machine) and setting Defaults !requiretty does not change anything.\nIn the ansible config I have made only these 2 changes:\n\n\nuncommented\nssh_args = -o ControlMaster=auto -o ControlPersist=60s\n\n\nenabled\npipelining = True\n\n\nIt's for ansible 1.9.1 installed from the PPA.\nWhat am I doing wrong?",
    "annotations": [{ "label": 140, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1980,
    "text": "rax_identity stacktraces with pyrax >= 1.8.0\n\nStarting with pyrax 1.8.0 and greater installed, the rax_identity module now dies with a stacktrace trying to serialize the output:\nfailed: [localhost] => {\"failed\": true, \"parsed\": false}\ninvalid output was: Traceback (most recent call last):\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 1438, in <module>\n    main()\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 110, in main\n    cloud_identity(module, state, pyrax.identity)\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 82, in cloud_identity\n    module.exit_json(changed=changed, identity=instance)\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 1051, in exit_json\n    print self.jsonify(kwargs)\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 1029, in jsonify\n    return json.dumps(data)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py\", line 243, in dumps\n    return _default_encoder.encode(obj)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 207, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 270, in iterencode\n    return _iterencode(o, 0)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 184, in default\n    raise TypeError(repr(o) + \" is not JSON serializable\")\nTypeError: <'load_balancer' Service object at 0x102bc23d0> is not JSON serializable\n\nAs a workaround, rolling back to < 1.8.0 fixes the problem.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1981,
    "text": "Force_handlers with_items does not execute handlers\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nForce handlers with_items\nANSIBLE VERSION\nansible 2.3.0.0\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = configured module search path = Default w/o overrides\npython version = 2.7.10 (default, May  1 2017, 19:24:18) [GCC 4.4.7 20120313 (Red Hat 4.4.7-16)]\nCONFIGURATION\nOS / ENVIRONMENT\nSUMMARY\nWe expect handlers to run when at least one item is changed when using with_items and force_handlers.\nSTEPS TO REPRODUCE\n---\n- hosts: localhost\n  gather_facts: no\n  force_handlers: True\n  handlers:\n    - name: hello_world\n      debug:\n  vars:\n    commands:\n      - ls\n      - pwd\n      - asdf\n  tasks:\n    - shell: \"{{ item }}\"\n      with_items: \"{{ commands }}\"\n      notify: hello_world\n\nEXPECTED RESULTS\nPLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************\nTASK [command] ****************************************************************************************************************************************************************************************************************************************************************\nchanged: [localhost] => (item=ls)\nchanged: [localhost] => (item=pwd)\nfailed: [localhost] (item=asdf) => {\"changed\": true, \"cmd\": \"asdf\", \"delta\": \"0:00:00.001902\", \"end\": \"2017-05-10 13:59:28.650743\", \"failed\": true, \"item\": \"asdf\", \"rc\": 127, \"start\": \"2017-05-10 13:59:28.648841\", \"stderr\": \"/bin/sh: asdf: command not found\", \"stderr_lines\": [\"/bin/sh: asdf: command not found\"], \"stdout\": \"\", \"stdout_lines\": []}\nRUNNING HANDLER [hello_world] *************************************************************************************************************************************************************************************************************************************************\nok: [localhost] => {\n\"changed\": false,\n\"msg\": \"Hello world!\"\n}\nPLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=1\nACTUAL RESULTS\nPLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************\nTASK [command] ****************************************************************************************************************************************************************************************************************************************************************\nchanged: [localhost] => (item=ls)\nchanged: [localhost] => (item=pwd)\nfailed: [localhost] (item=asdf) => {\"changed\": true, \"cmd\": \"asdf\", \"delta\": \"0:00:00.001916\", \"end\": \"2017-05-10 14:00:00.284031\", \"failed\": true, \"item\": \"asdf\", \"rc\": 127, \"start\": \"2017-05-10 14:00:00.282115\", \"stderr\": \"/bin/sh: asdf: command not found\", \"stderr_lines\": [\"/bin/sh: asdf: command not found\"], \"stdout\": \"\", \"stdout_lines\": []}\nPLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************\nlocalhost                  : ok=0    changed=0    unreachable=0    failed=1",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": null
  },
  {
    "id": 1982,
    "text": "Stack trace in YAML Validation Code\n\nFrom the ansible-project list, the stack trace is as follows:\nTraceback (most recent call last):\n  File \"/usr/local/share/python/ansible-playbook\", line 268, in <module>\n    sys.exit(main(sys.argv[1:]))\n  File \"/usr/local/share/python/ansible-playbook\", line 208, in main\n    pb.run()\n  File \"/usr/local/lib/python2.7/site-packages/ansible/playbook/__init__.py\", line 228, in run\n    play = Play(self, play_ds, play_basedir)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py\", line 80, in __init__\n    ds = self._load_roles(self.roles, ds)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py\", line 288, in _load_roles\n    roles = self._build_role_dependencies(roles, [], self.vars)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py\", line 178, in _build_role_dependencies\n    defaults_data = utils.parse_yaml_from_file(defaults)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py\", line 419, in parse_yaml_from_file\n    process_yaml_error(exc, data, path)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py\", line 397, in process_yaml_error\n    msg = process_common_errors(msg, probline, mark.column)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py\", line 358, in process_common_errors\n    elif len(probline) and probline[column] == \":\" and probline.find(\"=\") != -1:\nIndexError: string index out of range\nThe issue ended up being that I missed a ':' between a variable key and value. I assume that this can be reproduced by leaving off a colon anywhere in a variable definition (or maybe even tasks). I specifically created this  it by creating a vars section and putting:\n---\nvariable value\nI fixed it by instead doing:\n---\nvariable: value\n@mpdehaan mentioned that this would be a pretty quick fix.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1983,
    "text": "password_hash/get_encrypted_password uses passlib default of rounds=656000 which is 131 times glibc default\n\nIn the password_hash filter function the underlying passlib call uses the default rounds parameter.\nThe default for glibc is 5000, the passlib default for sha512 is 656000. This means on a login in a linux account the hash calculation will take significantly longer.\nActually you get basically no rounds parameter when setting it to 5000\n$ python -c \"from passlib.hash import sha512_crypt; print sha512_crypt.encrypt('foo', rounds=5000)\"\n$6$0zDM3P3/MbIoF0G0$pVmR8hc0ZNYdfGLnFhVKeCGhd32kDXo6ky83JmUWTZCfVQm2IrhnIbrrg5Vyl9XxE5aWzdmuYpvTA6gyOFc5Z.\n$ python -c \"from passlib.hash import sha512_crypt; print sha512_crypt.encrypt('foo', rounds=5001)\"\n$6$rounds=5001$VFyJ36YLb/RLhx0e$CAFCB/W7ebYEHIFsZtlJSdvuzEtsYOAvtOwCF7ahqpWqLj62TJp4LlzZ/FiW1B2U5kSKh4xibiJgZyd2ceVoI1\nCould a simple parameter be added to get_encrypted_password to set the value which has a preferable default what glibc does?",
    "annotations": [{ "label": 143, "user": 1 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1984,
    "text": "Broken 'Edit on GitHub' link at http://docs.ansible.com/ansible/intro_adhoc.html\n\nISSUE TYPE\n\nDocumentation Report\n\nSUMMARY\n'Edit on GitHub' http://joxi.ru/l2Z6VxFwlbVL2J?d=1 link leads to 404 error page http://joxi.ru/eAO14Wfxv1Gdmo?d=1\nSTEPS TO REPRODUCE\n1 Go to http://docs.ansible.com/ansible/intro_adhoc.html\n2 Press 'Edit on GitHub' at top right corner.\n3 See 404 error page.\nEXPECTED RESULTS\nSome kind of valid git hub page for editing related page.\nACTUAL RESULTS\n404 error page",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1985,
    "text": "ansible fails with an exception in python\n\nI sinply run command: \"forever stopall\"\nbut I get this output from stdout when it fails:\nfatal: [node1] => Traceback (most recent call last):\nFile \"/Library/Python/2.7/site-packages/ansible/runner/init.py\", line 586, in _executor\nexec_rc = self._executor_internal(host, new_stdin)\nFile \"/Library/Python/2.7/site-packages/ansible/runner/init.py\", line 789, in _executor_internal\nreturn self._executor_internal_inner(host, self.module_name, self.module_args, inject, port, complex_args=complex_args)\nFile \"/Library/Python/2.7/site-packages/ansible/runner/init.py\", line 1005, in _executor_internal_inner\nnum_args_post = self._count_module_args(module_args)\nFile \"/Library/Python/2.7/site-packages/ansible/runner/init.py\", line 433, in _count_module_args\nvargs = split_args(args)\nFile \"/Library/Python/2.7/site-packages/ansible/module_utils/splitter.py\", line 73, in split_args\nargs = args.strip()\nAttributeError: 'dict' object has no attribute 'strip'",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1986,
    "text": "--diff Flag crashes on win_copy\n\nIssue Type:\n\nBug Report\n\nAnsible Version:\nansible 2.1.0 (devel cd51ba7965) last updated 2016/02/24 21:58:23 (GMT +000)\n  lib/ansible/modules/core: (detached HEAD e9454fa44f) last updated 2016/02/24 21:58:47 (GMT +000)\n  lib/ansible/modules/extras: (detached HEAD fade5b7936) last updated 2016/02/24 21:58:47 (GMT +000)\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = /opt/maxpoint/ansible/release/library\n\nAnsible Configuration:\nStock\nEnvironment:\nRunning from: Ubuntu 14.04\nTargeting: Windows 2k12\nSummary:\nThe --diff flag fails when using the win_copy module.\nSteps To Reproduce:\nRun any playbook that contains the win_copy module while using the --diff flag against a Windows box.\n- name: Copy Over Pool Config Scripts\n  win_copy:\n    src: \"iis_pool_settings.ps1\"\n    dest: D:\\\\Temp\\\\iis_pool_settings.ps1\n\nExpected Results:\nI expect to see a diff between the file that was already on disk, and the one I just copied over.\nActual Results:\n\nHere's the entire output\nhttps://gist.github.com/blakfeld/542b191c85f4385e50bf\nHere's the relevant output\nTASK [company_bidder_deploy : Copy Over company Pool Config Scripts] *********\ntask path: /opt/company/ansible/release/roles/company_bidder_deploy/tasks/pre_install.yml:29\n<computername.companyinteractive.com> ESTABLISH WINRM CONNECTION FOR USER: Administrator on PORT 5986 TO computername.companyinteractive.com\n<computername.companyinteractive.com> WINRM CONNECT: transport=ssl endpoint=https://computername.companyinteractive.com:5986/wsman\n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\n(New-Item -Type Directory -Path $env:temp -Name \"ansible-tmp-1456414325.16-195672407265922\").FullName | Write-Host -Separator '';\n<computername.companyinteractive.com> WINRM OPEN SHELL: ABD20C31-4581-4A04-9E03-527A04A2F27F\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADUALgAxADYALQAxADkANQA2ADcAMgA0ADAANwAyADYANQA5ADIAMgAiACkALgBGAHUAbABsAE4AYQBtAGUAIAB8ACAAVwByAGkAdABlAC0ASABvAHMAdAAgAC0AUwBlAHAAYQByAGEAdABvAHIAIAAnACcAOwA=']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"C:\\\\Users\\\\Administrat\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> PUT \"/tmp/tmpfOwmcO\" TO \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\\stat.ps1\"\n<computername.companyinteractive.com> WINRM EXEC 'PowerShell' ['-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted', '-EncodedCommand', 'YgBlAGcAaQBuACAAewAKACQAcABhAHQAaAAgAD0AIAAiAEMAOgBcAFUAcwBlAHIAcwBcAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAXABBAHAAcABEAGEAdABhAFwATABvAGMAYQBsAFwAVABlAG0AcABcAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADUALgAxADYALQAxADkANQA2ADcAMgA0ADAANwAyADYANQA5ADIAMgBcAHMAdABhAHQALgBwAHMAMQAiAAoAJABEAGUAYgB1AGcAUAByAGUAZgBlAHIAZQBuAGMAZQAgAD0AIAAiAEMAbwBuAHQAaQBuAHUAZQAiAAoAJABFAHIAcgBvAHIAQQBjAHQAaQBvAG4AUAByAGUAZgBlAHIAZQBuAGMAZQAgAD0AIAAiAFMAdABvAHAAIgAKAFMAZQB0AC0AUwB0AHIAaQBjAHQATQBvAGQAZQAgAC0AVgBlAHIAcwBpAG8AbgAgADIACgAkAGYAZAAgAD0AIABbAFMAeQBzAHQAZQBtAC4ASQBPAC4ARgBpAGwAZQBdADoAOgBDAHIAZQBhAHQAZQAoACQAcABhAHQAaAApAAoAJABzAGgAYQAxACAAPQAgAFsAUwB5AHMAdABlAG0ALgBTAGUAYwB1AHIAaQB0AHkALgBDAHIAeQBwAHQAbwBnAHIAYQBwAGgAeQAuAFMASABBADEAQwByAHkAcAB0AG8AUwBlAHIAdgBpAGMAZQBQAHIAbwB2AGkAZABlAHIAXQA6ADoAQwByAGUAYQB0AGUAKAApAAoAJABiAHkAdABlAHMAIAA9ACAAQAAoACkAIAAjAGkAbgBpAHQAaQBhAGwAaQB6AGUAIABmAG8AcgAgAGUAbQBwAHQAeQAgAGYAaQBsAGUAIABjAGEAcwBlAAoAfQAKAHAAcgBvAGMAZQBzAHMAIAB7AAoAJABiAHkAdABlAHMAIAA9ACAAWwBTAHkAcwB0AGUAbQAuAEMAbwBuAHYAZQByAHQAXQA6ADoARgByAG8AbQBCAGEAcwBlADYANABTAHQAcgBpAG4AZwAoACQAaQBuAHAAdQB0ACkACgAkAHMAaABhADEALgBUAHIAYQBuAHMAZgBvAHIAbQBCAGwAbwBjAGsAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAJABiAHkAdABlAHMALgBMAGUAbgBnAHQAaAAsACAAJABiAHkAdABlAHMALAAgADAAKQAgAHwAIABPAHUAdAAtAE4AdQBsAGwACgAkAGYAZAAuAFcAcgBpAHQAZQAoACQAYgB5AHQAZQBzACwAIAAwACwAIAAkAGIAeQB0AGUAcwAuAEwAZQBuAGcAdABoACkACgB9AAoAZQBuAGQAIAB7AAoAJABzAGgAYQAxAC4AVAByAGEAbgBzAGYAbwByAG0ARgBpAG4AYQBsAEIAbABvAGMAawAoACQAYgB5AHQAZQBzACwAIAAwACwAIAAwACkAIAB8ACAATwB1AHQALQBOAHUAbABsAAoAJABoAGEAcwBoACAAPQAgAFsAUwB5AHMAdABlAG0ALgBCAGkAdABDAG8AbgB2AGUAcgB0AGUAcgBdADoAOgBUAG8AUwB0AHIAaQBuAGcAKAAkAHMAaABhADEALgBIAGEAcwBoACkALgBSAGUAcABsAGEAYwBlACgAIgAtACIALAAgACIAIgApAC4AVABvAEwAbwB3AGUAcgBJAG4AdgBhAHIAaQBhAG4AdAAoACkACgAkAGYAZAAuAEMAbABvAHMAZQAoACkACgBXAHIAaQB0AGUALQBPAHUAdABwAHUAdAAgACIAewAiACIAcwBoAGEAMQAiACIAOgAiACIAJABoAGEAcwBoACIAIgB9ACIACgB9AA==']\n<computername.companyinteractive.com> WINRM PUT \"/tmp/tmpfOwmcO\" to \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\\stat.ps1\" (offset=10177 size=10177)\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"{\"sha1\":\"4b5fc9661a0\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT {\"sha1\":\"4b5fc9661a08b2372dc5efa608f719480e61502b\"}\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\nTry\n{\n& \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\\stat.ps1\"\n}\nCatch\n{\n$_obj = @{ failed = $true }\nIf ($_.Exception.GetType)\n{\n$_obj.Add('msg', $_.Exception.Message)\n}\nElse\n{\n$_obj.Add('msg', $_.ToString())\n}\nIf ($_.InvocationInfo.PositionMessage)\n{\n$_obj.Add('exception', $_.InvocationInfo.PositionMessage)\n}\nElseIf ($_.ScriptStackTrace)\n{\n$_obj.Add('exception', $_.ScriptStackTrace)\n}\nTry\n{\n$_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))\n}\nCatch\n{\n}\nEcho $_obj | ConvertTo-Json -Compress -Depth 99\nExit 1\n}\nFinally { Remove-Item \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\" -Force -Recurse -ErrorAction SilentlyContinue }\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'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']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"{\"stat\":{\"exists\":fa\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT {\"stat\":{\"exists\":false},\"changed\":false}\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\n(New-Item -Type Directory -Path $env:temp -Name \"ansible-tmp-1456414326.29-74854495923127\").FullName | Write-Host -Separator '';\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgAyADkALQA3ADQAOAA1ADQANAA5ADUAOQAyADMAMQAyADcAIgApAC4ARgB1AGwAbABOAGEAbQBlACAAfAAgAFcAcgBpAHQAZQAtAEgAbwBzAHQAIAAtAFMAZQBwAGEAcgBhAHQAbwByACAAJwAnADsA']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"C:\\\\Users\\\\Administrat\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.29-74854495923127\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\n(New-Item -Type Directory -Path $env:temp -Name \"ansible-tmp-1456414326.6-190477404467970\").FullName | Write-Host -Separator '';\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgA2AC0AMQA5ADAANAA3ADcANAAwADQANAA2ADcAOQA3ADAAIgApAC4ARgB1AGwAbABOAGEAbQBlACAAfAAgAFcAcgBpAHQAZQAtAEgAbwBzAHQAIAAtAFMAZQBwAGEAcgBhAHQAbwByACAAJwAnADsA']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"C:\\\\Users\\\\Administrat\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.6-190477404467970\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> PUT \"/tmp/tmpcEFZin\" TO \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.6-190477404467970\\file.ps1\"\n<computername.companyinteractive.com> WINRM EXEC 'PowerShell' ['-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted', '-EncodedCommand', 'YgBlAGcAaQBuACAAewAKACQAcABhAHQAaAAgAD0AIAAiAEMAOgBcAFUAcwBlAHIAcwBcAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAXABBAHAAcABEAGEAdABhAFwATABvAGMAYQBsAFwAVABlAG0AcABcAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgA2AC0AMQA5ADAANAA3ADcANAAwADQANAA2ADcAOQA3ADAAXABmAGkAbABlAC4AcABzADEAIgAKACQARABlAGIAdQBnAFAAcgBlAGYAZQByAGUAbgBjAGUAIAA9ACAAIgBDAG8AbgB0AGkAbgB1AGUAIgAKACQARQByAHIAbwByAEEAYwB0AGkAbwBuAFAAcgBlAGYAZQByAGUAbgBjAGUAIAA9ACAAIgBTAHQAbwBwACIACgBTAGUAdAAtAFMAdAByAGkAYwB0AE0AbwBkAGUAIAAtAFYAZQByAHMAaQBvAG4AIAAyAAoAJABmAGQAIAA9ACAAWwBTAHkAcwB0AGUAbQAuAEkATwAuAEYAaQBsAGUAXQA6ADoAQwByAGUAYQB0AGUAKAAkAHAAYQB0AGgAKQAKACQAcwBoAGEAMQAgAD0AIABbAFMAeQBzAHQAZQBtAC4AUwBlAGMAdQByAGkAdAB5AC4AQwByAHkAcAB0AG8AZwByAGEAcABoAHkALgBTAEgAQQAxAEMAcgB5AHAAdABvAFMAZQByAHYAaQBjAGUAUAByAG8AdgBpAGQAZQByAF0AOgA6AEMAcgBlAGEAdABlACgAKQAKACQAYgB5AHQAZQBzACAAPQAgAEAAKAApACAAIwBpAG4AaQB0AGkAYQBsAGkAegBlACAAZgBvAHIAIABlAG0AcAB0AHkAIABmAGkAbABlACAAYwBhAHMAZQAKAH0ACgBwAHIAbwBjAGUAcwBzACAAewAKACQAYgB5AHQAZQBzACAAPQAgAFsAUwB5AHMAdABlAG0ALgBDAG8AbgB2AGUAcgB0AF0AOgA6AEYAcgBvAG0AQgBhAHMAZQA2ADQAUwB0AHIAaQBuAGcAKAAkAGkAbgBwAHUAdAApAAoAJABzAGgAYQAxAC4AVAByAGEAbgBzAGYAbwByAG0AQgBsAG8AYwBrACgAJABiAHkAdABlAHMALAAgADAALAAgACQAYgB5AHQAZQBzAC4ATABlAG4AZwB0AGgALAAgACQAYgB5AHQAZQBzACwAIAAwACkAIAB8ACAATwB1AHQALQBOAHUAbABsAAoAJABmAGQALgBXAHIAaQB0AGUAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAJABiAHkAdABlAHMALgBMAGUAbgBnAHQAaAApAAoAfQAKAGUAbgBkACAAewAKACQAcwBoAGEAMQAuAFQAcgBhAG4AcwBmAG8AcgBtAEYAaQBuAGEAbABCAGwAbwBjAGsAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAMAApACAAfAAgAE8AdQB0AC0ATgB1AGwAbAAKACQAaABhAHMAaAAgAD0AIABbAFMAeQBzAHQAZQBtAC4AQgBpAHQAQwBvAG4AdgBlAHIAdABlAHIAXQA6ADoAVABvAFMAdAByAGkAbgBnACgAJABzAGgAYQAxAC4ASABhAHMAaAApAC4AUgBlAHAAbABhAGMAZQAoACIALQAiACwAIAAiACIAKQAuAFQAbwBMAG8AdwBlAHIASQBuAHYAYQByAGkAYQBuAHQAKAApAAoAJABmAGQALgBDAGwAbwBzAGUAKAApAAoAVwByAGkAdABlAC0ATwB1AHQAcAB1AHQAIAAiAHsAIgAiAHMAaABhADEAIgAiADoAIgAiACQAaABhAHMAaAAiACIAfQAiAAoAfQA=']\n<computername.companyinteractive.com> WINRM PUT \"/tmp/tmpcEFZin\" to \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.6-190477404467970\\file.ps1\" (offset=10656 size=10656)\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"{\"sha1\":\"7f5d2fe5184\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT {\"sha1\":\"7f5d2fe5184e28c170ff1de2a7b611897f896275\"}\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\nTry\n{\n& \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.6-190477404467970\\file.ps1\"\n}\nCatch\n{\n$_obj = @{ failed = $true }\nIf ($_.Exception.GetType)\n{\n$_obj.Add('msg', $_.Exception.Message)\n}\nElse\n{\n$_obj.Add('msg', $_.ToString())\n}\nIf ($_.InvocationInfo.PositionMessage)\n{\n$_obj.Add('exception', $_.InvocationInfo.PositionMessage)\n}\nElseIf ($_.ScriptStackTrace)\n{\n$_obj.Add('exception', $_.ScriptStackTrace)\n}\nTry\n{\n$_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))\n}\nCatch\n{\n}\nEcho $_obj | ConvertTo-Json -Compress -Depth 99\nExit 1\n}\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgBUAHIAeQAKAHsACgAmACAAIgBDADoAXABVAHMAZQByAHMAXABBAGQAbQBpAG4AaQBzAHQAcgBhAHQAbwByAFwAQQBwAHAARABhAHQAYQBcAEwAbwBjAGEAbABcAFQAZQBtAHAAXABhAG4AcwBpAGIAbABlAC0AdABtAHAALQAxADQANQA2ADQAMQA0ADMAMgA2AC4ANgAtADEAOQAwADQANwA3ADQAMAA0ADQANgA3ADkANwAwAFwAZgBpAGwAZQAuAHAAcwAxACIACgB9AAoAQwBhAHQAYwBoAAoAewAKACQAXwBvAGIAagAgAD0AIABAAHsAIABmAGEAaQBsAGUAZAAgAD0AIAAkAHQAcgB1AGUAIAB9AAoASQBmACAAKAAkAF8ALgBFAHgAYwBlAHAAdABpAG8AbgAuAEcAZQB0AFQAeQBwAGUAKQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAbQBzAGcAJwAsACAAJABfAC4ARQB4AGMAZQBwAHQAaQBvAG4ALgBNAGUAcwBzAGEAZwBlACkACgB9AAoARQBsAHMAZQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAbQBzAGcAJwAsACAAJABfAC4AVABvAFMAdAByAGkAbgBnACgAKQApAAoAfQAKAEkAZgAgACgAJABfAC4ASQBuAHYAbwBjAGEAdABpAG8AbgBJAG4AZgBvAC4AUABvAHMAaQB0AGkAbwBuAE0AZQBzAHMAYQBnAGUAKQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAZQB4AGMAZQBwAHQAaQBvAG4AJwAsACAAJABfAC4ASQBuAHYAbwBjAGEAdABpAG8AbgBJAG4AZgBvAC4AUABvAHMAaQB0AGkAbwBuAE0AZQBzAHMAYQBnAGUAKQAKAH0ACgBFAGwAcwBlAEkAZgAgACgAJABfAC4AUwBjAHIAaQBwAHQAUwB0AGEAYwBrAFQAcgBhAGMAZQApAAoAewAKACQAXwBvAGIAagAuAEEAZABkACgAJwBlAHgAYwBlAHAAdABpAG8AbgAnACwAIAAkAF8ALgBTAGMAcgBpAHAAdABTAHQAYQBjAGsAVAByAGEAYwBlACkACgB9AAoAVAByAHkACgB7AAoAJABfAG8AYgBqAC4AQQBkAGQAKAAnAGUAcgByAG8AcgBfAHIAZQBjAG8AcgBkACcALAAgACgAJABfACAAfAAgAEMAbwBuAHYAZQByAHQAVABvAC0ASgBzAG8AbgAgAHwAIABDAG8AbgB2AGUAcgB0AEYAcgBvAG0ALQBKAHMAbwBuACkAKQAKAH0ACgBDAGEAdABjAGgACgB7AAoAfQAKAEUAYwBoAG8AIAAkAF8AbwBiAGoAIAB8ACAAQwBvAG4AdgBlAHIAdABUAG8ALQBKAHMAbwBuACAALQBDAG8AbQBwAHIAZQBzAHMAIAAtAEQAZQBwAHQAaAAgADkAOQAKAEUAeABpAHQAIAAxAAoAfQA=']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 1, out \"{\"msg\":\"path will no\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT {\"msg\":\"path will not be created\",\"failed\":true}\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> WINRM CLOSE SHELL: ABD20C31-4581-4A04-9E03-527A04A2F27F\nAn exception occurred during task execution. The full traceback is:\nTraceback (most recent call last):\n  File \"/opt/ansible-src/lib/ansible/executor/task_executor.py\", line 122, in run\n    res = self._execute()\n  File \"/opt/ansible-src/lib/ansible/executor/task_executor.py\", line 418, in _execute\n    result = self._handler.run(task_vars=variables)\n  File \"/opt/ansible-src/lib/ansible/plugins/action/copy.py\", line 202, in run\n    diffs.append(self._get_diff_data(dest_file, source_full, task_vars))\n  File \"/opt/ansible-src/lib/ansible/plugins/action/__init__.py\", line 614, in _get_diff_data\n    if peek_result['state'] == 'absent':\nKeyError: 'state'\n\nThe problem appears to be because the --diff command is relying upon the existence of a 'state' key in the returned JSON. This appears to be a key added in the module_utils.basic Python module, specifically the add_path_info method, which seems to be called by load_common_file_arguments. I was going to just hack this into win_copy, but it seems to me that the most reasonable solution would be to add load_common_file_arguments to powershell_common.ps1. I'm more than happy to start work on that if the community agrees that is the best answer.",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1987,
    "text": "Variable overding during nested includes issue\n\nIt seems variables are not overridden when using nested includes (this works ok in v1 ) .This might be related to   #11353 . To reproduce the issue you can follow the steps from #11353 ...\nIn build.yml I have a variable docker_tags what is overriden in  tasks/docker/base_build.yml... instead of printing the overridden value it prints the value from build.yml. See debug statements after running  'ansible-playbook -v build_admin_ui.yml -i inventories/local/hosts --extra-vars \"admin_ui_version=1234\"'",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1988,
    "text": "[v2] \"win_iis_webbinding\" module require additional parameters to run?\n\nI was run\nansible iis01 -m win_iis_webbinding -a \"name=tiger\"\nAn exception occurred during task execution. To see the full traceback, use -vvv. The error was: +     ~~~~~~~~~~~~~~~~~~~\niis01 | FAILED! => {\n    \"changed\": false, \n    \"failed\": true, \n    \"msg\": \"Property 'host_header' cannot be found on this object. Make sure that it exists.\"\n}\nI need run this,it's work\nansible iis01 -m win_iis_webbinding -a \"name=tiger host_header=www.tiger.com protocol=http port=8083 ip=127.0.0.1\"\niis01 | SUCCESS => {\n    \"added\": [], \n    \"changed\": false, \n    \"matched\": [], \n    \"parameters\": {\n        \"HostHeader\": \"www.tiger.com\", \n        \"IPAddress\": \"127.0.0.1\", \n        \"Name\": \"tiger\", \n        \"Port\": \"8083\", \n        \"Protocol\": \"http\"\n    }, \n    \"removed\": []\n}",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1989,
    "text": "ec2 dynamic_inventory tag issue\n\nThe special characters escape to underscores in ec2.py does not appear to work poperly.\nI have the following tag for an ec2 instance:\naws:cloudformation:stack-name => imdev-flask-3-2-298-l1FiTlA\nPer the docs, this should translate to an underscore between stack and name: 'ec2_tag_aws_cloudformation_stack_name'\nThis does not work, and produced the following var undefined error:\nOne or more undefined variables: 'ec2_tag_aws_cloudformation_stack_name' is undefined\nrunning ec2.py, the actual output is \"ec2_tag_aws_cloudformation_stack-name\": \"imdev-flask-3-2-298-l1FiTlA\"\nIf I try to use in the ec2.py form, ansible will throw an error for the invalid character:\nFailed to template msg=\"{{ ec2_tag_aws_cloudformation_stack-name == removed_version }}\": Unable to look up a name or access an attribute in template string. Make sure your variable name does not contain invalid characters like '-'.\nHowever, accessing through the hostvars allows reference to the item with the \"-\". Ex:\n\"{{  hostvars[inventory_hostname]['ec2_tag_aws_cloudformation_stack-name'] }}\"",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1990,
    "text": "documented used of 'lookup' now generates variable undefined error \n\nThe following documented use of the lookup plugin\n\n---\n- hosts: all\n  tasks:\n     - debug: msg=\"{{ lookup('env','HOME') }} is an environment variable\"\n\nnow generates the following error:\nTASK: [debug msg=\"{{lookup('env','HOME')}} is an environment variable\"] ******* \nfatal: [localhost] => One or more undefined variables: 'lookup' is undefined\n\nI've traced this back to commit 5031104.",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1991,
    "text": "Wrong diagnostics in ansible-galaxy on certificate issues\n\nIssue Type:\n\nBug Report\n\nComponent Name\n\nansible-galaxy\n\nAnsible Version:\nevg@thinkpad ~ $ansible --version\nansible 2.2.2.0\n  config file = /home/evg/.ansible.cfg\n  configured module search path = Default w/o overrides\n\nAnsible Configuration:\nClean out-of-box configuration (on SOME OS).\nEnvironment:\nN/A. Generic, non-mainstream OS. In my case, ALT Linux Sisypus (unstable branch).\nSummary:\nOn non-mainstream OS with non-standard SSL/TLS CA certificate paths (not in '/etc/ssl/certs', '/etc/pki/ca-trust/extracted/pem', '/etc/pki/tls/certs', '/usr/share/ca-certificates/cacert.org', '/etc/ansible' in my case) diagnostics on any problem with certificate check in ansible-galaxy is just plain wrong (ERROR! Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED).\nSteps To Reproduce:\nTry to install any galaxy role just after clean ansible install (ansible-galaxy will fail to check correct SSL/TLS certificate of galaxy.ansible.com with wrong error message).\nExpected Results:\nevg@thinkpad ~ $ansible-galaxy install dj-wasabi.zabbix-agent \n- downloading role 'zabbix-agent', owned by dj-wasabi\n- downloading role from https://github.com/dj-wasabi/ansible-zabbix-agent/archive/0.3.0.tar.gz\n- extracting dj-wasabi.zabbix-agent to /home/evg/.ansible/roles/dj-wasabi.zabbix-agent\n- dj-wasabi.zabbix-agent was installed successfully\n\nActual Results:\nevg@thinkpad ~ $ansible-galaxy install dj-wasabi.zabbix-agent   \n [WARNING]: - dj-wasabi.zabbix-agent was NOT installed successfully: Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED\n\nERROR! - you can use --ignore-errors to skip failed roles and finish processing the list.\n\nSanity check:\nevg@thinkpad ~ $curl https://galaxy.ansible.com/api/\n{\"available_versions\":{\"v1\":\"/api/v1/\"},\"description\":\"GALAXY REST API\",\"current_version\":\"v1\"}\n\nWorkarounds:\nNone. Old workaround for ansible-1.9.x NOT working anymore:\nroot@thinkpad /etc/ansible #ln -s /usr/share/ca-certificates/ca-bundle.crt .",
    "annotations": [{ "label": 140, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1992,
    "text": "Trailing new lines aren't kept by default by template module\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nJinja\nANSIBLE VERSION\nansible 2.2.0.0\n\nCONFIGURATION\n[defaults]\nask_pass = False\nremote_user = root\ninventory = inventories\ntimeout = 5\nroles_path = roles\n\nansible_managed = This file is managed by DOCK42 Configuration Management, all manual changes will be lost.\n\n[ssh_connection]\ncontrol_path = %(directory)s/%%h-%%p-%%r\n\nOS / ENVIRONMENT\n\nSUMMARY\n(referring to commit 1998edd)\nWhen trying to wrap a conditional inside a template in an one liner, the line break will be skipped if there's no trailing character (e.g. like a whitespace to workaround this issue)\nSTEPS TO REPRODUCE\nCreate a template like this:\n# GSSAPI options\nGSSAPIAuthentication {% if openssh.gssapi_authentication %}yes{% else %}no{% endif %}\nGSSAPICleanupCredentials no\n#GSSAPIStrictAcceptorCheck yes\n\n openssh.gssapi_authentication: no\n\n- name: Configure sshd\n  template: \n    src: sshd_config.j2\n    dest: /etc/ssh/sshd_config\n    backup: yes\n    owner: root \n    group: root \n    mode: 0600\n    validate: '/usr/sbin/sshd -T -f %s'\n  notify:\n    - restart sshd\n  become: yes\n  tags:\n    - sshd_config\nEXPECTED RESULTS\n# GSSAPI options\nGSSAPIAuthentication no \nGSSAPICleanupCredentials no\n\nACTUAL RESULTS\n # GSSAPI options\nGSSAPIAuthentication noGSSAPICleanupCredentials no",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1993,
    "text": "pre_tasks not working while gathering=smart in ansible.cfg\n\nIssue Type:\nBug report\nAnsible Version:\nansible 2.1.0 (devel 45e05ec072) last updated 2016/02/01 14:17:33 (GMT +200)\n  lib/ansible/modules/core: (detached HEAD 88e0bfd75d) last updated 2015/11/20 12:34:36 (GMT +200)\n  lib/ansible/modules/extras: (detached HEAD 7da1f8d4ca) last updated 2015/11/20 12:34:44 (GMT +200)\n  config file = ansible.cfg\n  configured module search path = Default w/o overrides\n\nAnsible Configuration:\n[defaults]\ngathering = smart\n\nEnvironment:\nMac OS X\nSummary:\nWhen gathering=smart used in ansible.cfg pre_tasks are not running.\nSteps To Reproduce:\nansible.cfg:\n[defaults]\ngathering = smart\n\nplay.yml:\n\n---\n- hosts: localhost\n\n  pre_tasks:\n    - name: pre_task\n      shell: echo 'hello'\n\n  tasks:\n    - name: task\n      shell: echo 'still busy'\n\n  post_tasks:\n    - name: post_task\n      shell: echo 'goodbye'\n\nExpected Results:\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nok: [localhost]\n\nTASK [pre_task] ****************************************************************\nchanged: [localhost]\n\nTASK [task] ********************************************************************\nchanged: [localhost]\n\nTASK [post_task] ***************************************************************\nchanged: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=4    changed=3    unreachable=0    failed=0\n\nActual Results:\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nok: [localhost]\n\nTASK [task] ********************************************************************\nchanged: [localhost]\n\nTASK [post_task] ***************************************************************\nchanged: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=3    changed=2    unreachable=0    failed=0",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1994,
    "text": "Issue with 'include' statements and role path\n\nMy ansible's 'include' statement was working fine, but recently after including ymls in subfolder, it somehow brokes the role's path.\nHere's the tree of my project:\n.\n├── site.yml\n├── inventory.ini\n└── roles\n    └── webservers\n        ├── files\n        │   └── crt.crt\n        ├── tasks\n        │   ├── main.yml\n        │   ├── httpd.yml\n        │   ├── dev\n        │   │   ├── httpd.yml\n        │   │   └── main.yml\n        │   └── prod\n        │       ├── httpd.yml\n        │       └── main.yml\n        ├── templates\n        │   └── httpd_conf.j2\n        └── vars\n            └── main.yml\n\nsite.yml:\n - name: Install base software\n   sudo: yes\n   vars:\n      profile:    \"dev\"\n   roles:\n      - webservers\n\nroles/webservers/tasks/main.yml:\n - name: Install httpd\n   include: httpd.yml\n\n - name: Including specific tasks\n   include: \"{{ profile }}/main.yml\" \n\nPrior to this point, it works just fine. But the next step, after including the \"profile\"/main.yml, brokes the role's path.\nroles/webservers/tasks/dev/main.yml:\n - name:  Create httpd.conf from template\n   template: src=httpd_conf.j2 dest=/etc/httpd/conf/httpd.conf\n   with_items: webservers_httpd_vhosts\n\nresults an error:\nTASK: [webservers | Create httpd.conf from template] *********************\nfatal: [192.168.0.2] => input file not found at /home/me/git/repo/projects/ans/roles/webservers/tasks/templates/httpd_conf.j2 or /home/me/git/repo/projects/ans/httpd_conf.j2\n\nFATAL: all hosts have already failed -- aborting\n\nSo, after this 'include', it somehow thinks that role's root is located in 'webservers/tasks/', instead of 'webservers/'. But in the same time, it sees the variables in 'webservers/vars/' path.\nMoreover, it was working fine and recently just broke. I haven't updated ansible, nor edited it's parameters, just updated the playbook.\nIf I specify the 'src=../../templates/httpd_conf.j2' it works fine, but since this behaviour just appeared like from nowhere, I can't rely on this path.\nI've managed to use this workaround: template: src=\"{{ role_path }}/templates/httpd_conf.j2\" dest=/etc/httpd/conf/httpd.conf But that's a workaround. The reason of auto-pathing is broke is still undiscovered.\nAnsilbe's version: 1.9.0.1",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1995,
    "text": "Ansible 2.0.0.2 : \"ERROR! file or module does not exist\" while running a playbook with script module\n\nAfter installing ansible 2.0.0.2 from rpm:\n[root@ansibletest setup]# ansible-playbook --version\nansible-playbook 2.0.0.2\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\nI get the below error on running the playbook which uses the script module:\n[root@ansibletest setup]# ansible-playbook test.yml\n\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nEnter passphrase for key '/root/.ssh/id_rsa':\nok: [192.168.2.101]\n\nTASK [Run test.sh script] ****************************************************\nfatal: [192.168.2.101]: FAILED! => {\"failed\": true, \"msg\": \"ERROR! file or module does not exist: /path/to/script/test.sh\"}\n\nPLAY RECAP *********************************************************************\n192.168.2.101              : ok=1    changed=0    unreachable=0    failed=1\n\nThe Playbook in question: -\n\n---\n- hosts: 192.168.2.101\n  remote_user: root\n  vars:\n    var1: data\n    param1: 1\n  tasks:\n    - name: Run test.sh script\n      become: yes\n      script: /{{ var1 }}/path/to/test.sh {{ param1 }} creates=/{{ var1 }}/{{ param1 }}/executed.txt",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1996,
    "text": "Variable interpolation in hostvars\n\nIssue Type: Feature Idea\nAnsible Version: ansible 1.6.6\nEnvironment: N/A\nSummary:\nWhen variables are accessed through hostvars, jinja2 expressions inside those variables should be interpolated.\nThere was discussion about this issue on ansible-project mailing list: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/ansible-project/pdWFR2Q8U8E/m6sgB1C5K1YJ\nAnd here is a filter that shows simple implementation (but might be pretty slow, and I think it should be in hostvars not a filter): https://groups.google.com/group/ansible-project/attach/16318558e74073ae/hostvars.py?part=0.1&view=1\nSteps To Reproduce:\nGiven vars like:\nfoo: y\nbar: \"x is {{ foo }}\"\n\nThe values for both {{ bar }} and {{ hostvars[some_host].bar }} should be \"x is y\".\nCurrently the value of {{ hostvars[some_host].bar }} would be (literally) \"x is {{ foo }}\".",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1997,
    "text": "win_chocolatey installing powershell when powershell4 is installed and state: latest\n\nISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\nwin_chocolatey\nANSIBLE VERSION\n\n$ ansible --version\nansible 2.4.0 (devel cc50b803df) last updated 2017/03/22 14:15:18 (GMT -500)\n  config file =\n  configured module search path = Default w/o overrides\n  python version = 2.7.10 (default, Jul 30 2016, 19:40:32) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)]\n\nCONFIGURATION\nAnsible configuration from git clone\nOS / ENVIRONMENT\nControl host macOS 10.12.3\nManaged host Windows 7\nSUMMARY\nAttempting to upgrade powershell4 to powershell5 using the win_chocolatey module with state: latest\nMight be related to #21873\nRelated to #22892\nSTEPS TO REPRODUCE\n\nHave chocolatey installed\nInstall powershell4\n\n\n- name: install powershell \n  win_chocolatey:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n    - \"powershell\"\n  register: check_powershell5\nEXPECTED RESULTS\nPowershell4 would be upgraded to Powershell5\nACTUAL RESULTS\nhttps://gist.github.com/basictheprogram/74b48320e386d308a69f82f7c90019b5\nchoco_summary.log\nhttps://gist.github.com/basictheprogram/de146516fe292a43630c10e7df205f38\nchocolatey.log\nhttps://gist.github.com/basictheprogram/5cc240c1f89fba6ef7ac769ff5e41a01",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 1998,
    "text": "Docker module: argument memory_limit is of type <type 'str'> and we were unable to convert to int\" on  Ansible 2.0.2.0-1.el7\n\nISSUE TYPE\n\n\nBug Report\n\nANSIBLE VERSION\n\nansible 2.0.2.0\n\nOS / ENVIRONMENT\nCentos 7\nSUMMARY\nAfter upgrade to ansible 2.0.2.0 it's not possible to enter memory_limit as human readable string (ie. 265MB) only bytes are accepted.\nSTEPS TO REPRODUCE\ntry set memory_limit:  256MB\n\n- name: sphinx container\n  docker:\n        name: sphinx\n        image: michalzubkowicz/docker-sphinxsearch\n        state: started\n        restart_policy: always\n        memory_limit: 256MB\n\n\nEXPECTED RESULTS\n\nShould accept string as early versions\nACTUAL RESULTS\n\nIs showing error\nargument memory_limit is of type <type 'str'> and we were unable to convert to int",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 1999,
    "text": "openbsd_pkg: add support for pkgname%branch syntax\n\nISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\nopenbsd_pkg\nANSIBLE VERSION\nansible 2.3.1.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, Jun 17 2017, 15:16:02) [GCC 4.2.1 20070719 ]\n\n\nCONFIGURATION\nOS / ENVIRONMENT\nOpenBSD -current or 6.1\nSUMMARY\nTrying to use the pkgname%branch syntax fails\nSTEPS TO REPRODUCE\n\n\nansible -C localhost -m openbsd_pkg -a 'name=openldap-server--%openldap state=installed'\n\n\nEXPECTED RESULTS\ninstalling openldap-server 2.44 from the databases/openldap branch. works fine in a shell:\n#pkg_add -n openldap-server--%openldap\nquirks-2.304 signed on 2017-04-02T15:01:33Z\nopenldap-server-2.4.44p3:icu4c-58.2p0: ok\nopenldap-server-2.4.44p3:openldap-client-2.4.44p3: ok\nopenldap-server-2.4.44p3:db-4.6.21p3v0: ok\nopenldap-server-2.4.44p3:e2fsprogs-1.42.12p4: ok\nopenldap-server-2.4.44p3: ok\n\n\nACTUAL RESULTS\n\nThe openbsd_pkg module errors out.\n\n$ansible -C localhost -m openbsd_pkg -a 'name=openldap-server--%openldap state=installed'\n\nlocalhost | FAILED! => {\n    \"changed\": false, \n    \"failed\": true, \n    \"msg\": \"unable to parse package name at versionless_match: openldap-server--%openldap\"\n}",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2000,
    "text": "Error with paramiko 2.1.0 <--> 2.2.1: Unicode-objects must be encoded before hashing\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nios_command\nbut I suspect all commands are impacted\nANSIBLE VERSION\nBoth latest stable & unstable versions:\nansible 2.3.1.0 (detached HEAD ecb38fdf73) last updated 2017/06/27 16:57:40 (GMT +200)\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = [u'/home/actionmystique/Ansible/git-yang-networkop/ansible-101/library']\n  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]\n\nansible 2.4.0 (devel 9f7fcf15be) last updated 2017/06/27 16:41:44 (GMT +200)\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = [u'/home/actionmystique/Ansible/git-yang-networkop/ansible-101/library']\n  ansible python module location = /home/actionmystique/src/Ansible/git-ansible/lib/ansible\n  executable location = /home/actionmystique/src/Ansible/git-ansible/bin/ansible\n  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]\n\nCONFIGURATION\ninventory   = ./hosts\nlibrary        = /home/actionmystique/Ansible/git-yang-networkop/ansible-101/library\nforks = 1000\ngathering = explicit\ngather_timeout = 30\nroles_path = /home/actionmystique/Ansible/Roles/roles\nprivate_role_vars = yes\nhash_behaviour = merge\nlog_path = /var/log/ansible.log\nretry_files_enabled = False\nshow_custom_stats = True\ntimeout = 60\npipelining = True\nconnect_timeout = 60\nconnect_retries = 30\nconnect_interval = 1\nOS / ENVIRONMENT\n\nhost: Ubuntu 17.04 4.10\ntarget:\n\nIOS-XEv 16.4.1\n\n\nparamiko: 2.2.1\n\nSUMMARY\ncf. title, for instance with a show running-config command.\nI am able to manually ssh into the remote device.\nIs the paramiko version too recent?\nSTEPS TO REPRODUCE\nStructure passed as \"provider\": connections.ssh\nconnections\n...\n        ssh:\n          transport: cli \n          host: \"{{ ansible_host }}\"\n          # ansible_port\n          port: 22\n          # ansible_user\n          username: admin\n          # ansible_ssh_pass\n          password: xxxxxxxxxxx\n          authorize: yes\n          # enable_secret_password\n          auth_pass: xxxxxxxxxxx\n          # private_key_file\n          ssh_keyfile: \"~/.ssh/id_rsa\"\n          version: 2\n          timeout: 10\n\nRole: ios_pull_config:\n- include_vars: \"../defaults/{{ os_family }}/connections.yml\"\n  when: (connections is undefined)\n\n- name: Fetching config from the remote node\n  ios_command:\n        provider: \"{{ connections.ssh }}\"\n        commands:\n          - \"show {{ config }}\"\n  register: configuration\n\nPlaybook:\n- name: Pulling IOS/IOSv/IOSv-L2/IOS-XE/IOS-XEv (CSR-1000v) startup and running configs\n  hosts:\n    - iosv\n    - iosv_l2\n    - ios_xev\n  gather_facts: no\n  roles:\n    - { role: ios_pull_config, config: startup-config, with_date_time: 'no' }\n    - { role: ios_pull_config, config: running-config, with_date_time: 'no' }\n\nEXPECTED RESULTS\nStartup & running configurations from the target IOSv node.\nACTUAL RESULTS: CLI\n...\n<172.21.100.111> using connection plugin network_cli\n<172.21.100.111> socket_path: \nfatal: [XEv_Spine_11]: FAILED! => {\n    \"changed\": false, \n    \"failed\": true, \n    \"msg\": \"unable to open shell. Please see: https://docs.ansible.com/ansible/network_debug_troubleshooting.html#unable-to-open-shell\"\n}\n...\n\nACTUAL RESULTS: log\n...\n2017-06-27 16:48:19,004 p=27279 u=root |  task path: /home/actionmystique/Ansible/Roles/roles/ios_pull_config/tasks/main.yml:77\n2017-06-27 16:48:21,476 p=27374 u=root |  creating new control socket for host 172.21.100.111:22 as user admin\n2017-06-27 16:48:21,476 p=27374 u=root |  control socket path is /root/.ansible/pc/4746f9877e\n2017-06-27 16:48:21,477 p=27374 u=root |  current working directory is /media/actionmystique/SAMSUNG-850-Ext4/Labs/GNS3/git-Public-Labs-Collection/CCNP/ROUTE/VRF/Jean-Christophe_Manciot/1+/AD between VRFs in Leaf & Spine DC-linux/IOS-XE 16.5.1/Ansible\n2017-06-27 16:48:21,477 p=27374 u=root |  using connection plugin network_cli\n2017-06-27 16:48:21,495 p=27375 u=root |  creating new control socket for host 172.21.100.112:22 as user admin\n2017-06-27 16:48:21,496 p=27375 u=root |  control socket path is /root/.ansible/pc/96255242f1\n2017-06-27 16:48:21,496 p=27375 u=root |  current working directory is /media/actionmystique/SAMSUNG-850-Ext4/Labs/GNS3/git-Public-Labs-Collection/CCNP/ROUTE/VRF/Jean-Christophe_Manciot/1+/AD between VRFs in Leaf & Spine DC-linux/IOS-XE 16.5.1/Ansible\n2017-06-27 16:48:21,496 p=27375 u=root |  using connection plugin network_cli\n2017-06-27 16:48:21,585 paramiko.transport starting thread (client mode): 0x9f70b2d0L\n2017-06-27 16:48:21,586 paramiko.transport Local version/idstring: SSH-2.0-paramiko_2.2.1\n2017-06-27 16:48:21,586 paramiko.transport Remote version/idstring: SSH-2.0-Cisco-1.25\n2017-06-27 16:48:21,586 paramiko.transport Connected (version 2.0, client Cisco-1.25)\n2017-06-27 16:48:21,587 paramiko.transport starting thread (client mode): 0xb9ea82d0L\n2017-06-27 16:48:21,587 paramiko.transport kex algos:[u'diffie-hellman-group-exchange-sha1', u'diffie-hellman-group14-sha1'] server key:[u'ssh-rsa'] client encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] server encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] client mac:[u'hmac-sha1', u'hmac-sha1-96'] server mac:[u'hmac-sha1', u'hmac-sha1-96'] client compress:[u'none'] server compress:[u'none'] client lang:[u''] server lang:[u''] kex follows?False\n2017-06-27 16:48:21,587 paramiko.transport Local version/idstring: SSH-2.0-paramiko_2.2.1\n2017-06-27 16:48:21,587 paramiko.transport Kex agreed: diffie-hellman-group-exchange-sha1\n2017-06-27 16:48:21,587 paramiko.transport HostKey agreed: ssh-rsa\n2017-06-27 16:48:21,587 paramiko.transport Remote version/idstring: SSH-2.0-Cisco-1.25\n2017-06-27 16:48:21,587 paramiko.transport Cipher agreed: aes128-ctr\n2017-06-27 16:48:21,587 paramiko.transport MAC agreed: hmac-sha1\n2017-06-27 16:48:21,587 paramiko.transport Connected (version 2.0, client Cisco-1.25)\n2017-06-27 16:48:21,587 paramiko.transport Compression agreed: none\n2017-06-27 16:48:21,588 paramiko.transport kex algos:[u'diffie-hellman-group-exchange-sha1', u'diffie-hellman-group14-sha1'] server key:[u'ssh-rsa'] client encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] server encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] client mac:[u'hmac-sha1', u'hmac-sha1-96'] server mac:[u'hmac-sha1', u'hmac-sha1-96'] client compress:[u'none'] server compress:[u'none'] client lang:[u''] server lang:[u''] kex follows?False\n2017-06-27 16:48:21,588 paramiko.transport Kex agreed: diffie-hellman-group-exchange-sha1\n2017-06-27 16:48:21,589 paramiko.transport HostKey agreed: ssh-rsa\n2017-06-27 16:48:21,589 paramiko.transport Cipher agreed: aes128-ctr\n2017-06-27 16:48:21,589 paramiko.transport MAC agreed: hmac-sha1\n2017-06-27 16:48:21,589 paramiko.transport Compression agreed: none\n2017-06-27 16:48:21,789 paramiko.transport Got server p (2048 bits)\n2017-06-27 16:48:21,790 paramiko.transport Got server p (2048 bits)\n2017-06-27 16:48:21,884 paramiko.transport kex engine KexGex specified hash_algo <built-in function openssl_sha1>\n2017-06-27 16:48:21,885 paramiko.transport Switch to new keys ...\n2017-06-27 16:48:21,890 p=27374 u=root |  connecting to host 172.21.100.111 returned an error\n2017-06-27 16:48:21,890 p=27374 u=root |  Unicode-objects must be encoded before hashing\n2017-06-27 16:48:21,911 paramiko.transport kex engine KexGex specified hash_algo <built-in function openssl_sha1>\n2017-06-27 16:48:21,912 paramiko.transport Switch to new keys ...\n2017-06-27 16:48:21,915 p=27375 u=root |  connecting to host 172.21.100.112 returned an error\n2017-06-27 16:48:21,915 p=27375 u=root |  Unicode-objects must be encoded before hashing\n2017-06-27 16:48:21,986 paramiko.transport EOF in transport thread\n2017-06-27 16:48:22,012 paramiko.transport EOF in transport thread\n2017-06-27 16:48:31,524 p=27279 u=root |  fatal: [XEv_Spine_11]: FAILED! => {\n    \"changed\": false, \n    \"failed\": true, \n    \"msg\": \"unable to open shell. Please see: https://docs.ansible.com/ansible/network_debug_troubleshooting.html#unable-to-open-shell\"\n}\n...\n\nACTUAL RESULTS: Manual SSH\n# ssh admin@172.21.100.111\nCC\n**************************************************************************\n* IOSv is strictly limited to use for evaluation, demonstration and IOS  *\n* education. IOSv is provided as-is and is not supported by Cisco's      *\n* Technical Advisory Center. Any use or disclosure, in whole or in part, *\n* of the IOSv Software or Documentation to any third party for any       *\n* purposes is expressly prohibited except as otherwise authorized by     *\n* Cisco in writing.                                                      *\n**************************************************************************CC\nXEv_Spine_11# #sh run\nBuilding configuration...\nCurrent configuration : 12818 bytes\n!\n! Last configuration change at 14:35:32 UTC Tue Jun 27 2017\n!\nversion 16.4\n...\n\nWORKAROUND: older paramiko\nThe latest paramiko version is 2.2.1.\nWhen downgrading to paramiko 2.0.6, this issue is ... gone.\nI recommend to put a more stringent requirement on paramiko in requirements.txt until this issue is solved with more recent paramiko, for instance:\nparamiko <= 2.0.6\n\nThe issue begins with paramiko 2.1.0+",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2001,
    "text": "[modules-core: file] Allow for \"directory\" or \"link\" in place of \"file\" with state \n\nFeature Idea\n\nANSIBLE VERSION\nansible 2.0.2.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\nSUMMARY\nusing \"file\" to check for directories (or links) is not very intuitive.\nIt would be great if\ndirectory: path=x/y/z\n\ncould be used as an alias for\nfile: path=x/y/z state=directory",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2002,
    "text": "GATHERING FACTS fails\n\n$ ansible --version\nansible 1.9.4\n\n$ uname -a\nLinux psi-nb 4.3.3-2-ARCH #1 SMP PREEMPT Wed Dec 23 20:09:18 CET 2015 x86_64 GNU/Linux\n\nGuest is CentOS 7 Vagrant BOX\n$ uname -a\nLinux jenkins 4.4.0-1.el7.elrepo.x86_64 #1 SMP Sun Jan 10 21:17:16 EST 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nWhen i run vagrant provision or ansible-playbook ... gathering facts fails MOST time:\nPLAY [all] ******************************************************************** \n\nGATHERING FACTS *************************************************************** \n<127.0.0.1> ESTABLISH CONNECTION FOR USER: vagrant\n<127.0.0.1> REMOTE_MODULE setup\n<127.0.0.1> EXEC ssh -C -tt -v -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/psi/.ansible/cp/ansible-ssh-%h-%p-%r\" -o StrictHostKeyChecking=no -o Port=2222 -o IdentityFile=\"/home/psi/o2/ansible/vagrant/jenkins/.vagrant/machines/jenkins/virtualbox/private_key\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=vagrant -o ConnectTimeout=10 127.0.0.1 /bin/sh -c 'mkdir -p $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191 && chmod a+rx $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191 && echo $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191'                                                                                      \n<127.0.0.1> PUT /tmp/tmpOf1gPr TO /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/setup\n<127.0.0.1> EXEC ssh -C -tt -v -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/psi/.ansible/cp/ansible-ssh-%h-%p-%r\" -o StrictHostKeyChecking=no -o Port=2222 -o IdentityFile=\"/home/psi/o2/ansible/vagrant/jenkins/.vagrant/machines/jenkins/virtualbox/private_key\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=vagrant -o ConnectTimeout=10 127.0.0.1 /bin/sh -c 'LANG=C LC_CTYPE=C /usr/bin/python /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/setup; rm -rf /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/ >/dev/null 2>&1'\nfailed: [jenkins] => {\"failed\": true, \"parsed\": false}\n{\"verbose_override\": true, \"changed\": false, \"ansible_facts\": {\"ansible_product_serial\": \"NA\", \"ansible_form_factor\": \"Other\", \"ansible_product_version\": \"1.2\", \"ansible_fips\": false, \"ansible_swaptotal_mb\": 1535, \"ansible_user_id\": \"vagrant\", \"module_setup\": true, \"ansible_userspace_bits\": \"64\", \"ansible_architecture\": \"x86_64\", \"ansible_distribution_version\": \"7.1.1503\", \"ansible_domain\": \"localdomain\", \"ansible_date_time\": {\"tz\": \"EST\", \"hour\": \"08\", \"time\": \"08:08:47\", \"epoch\": \"1452776927\", \"month\": \"01\", \"tz_offset\": \"-0500\", \"second\": \"47\", \"iso8601_micro\": \"2016-01-14T13:08:47.283819Z\", \"weekday\": \"Thursday\", \"year\": \"2016\", \"date\": \"2016-01-14\", \"iso8601\": \"2016-01-14T13:08:47Z\", \"day\": \"14\", \"minute\": \"08\"}, \"ansible_python_version\": \"2.7.5\", \"ansible_processor_cores\": 1, \"ansible_virtualization_role\": \"guest\", \"ansible_env\": {\"LANG\": \"C\", \"TERM\": \"xterm-256color\", \"SHELL\": \"/bin/bash\", \"XDG_RUNTIME_DIR\": \"/run/user/1000\", \"SHLVL\": \"2\", \"SSH_TTY\": \"/dev/pts/1\", \"LC_CTYPE\": \"C\", \"LESSOPEN\": \"||/usr/bin/lesspipe.sh %s\", \"PATH\": \"/usr/local/bin:/usr/bin\", \"PWD\": \"/home/vagrant\", \"LOGNAME\": \"vagrant\", \"USER\": \"vagrant\", \"MAIL\": \"/var/mail/vagrant\", \"HOME\": \"/home/vagrant\", \"SSH_CONNECTION\": \"10.0.2.2 41752 10.0.2.15 22\", \"XDG_SESSION_ID\": \"9\", \"SSH_CLIENT\": \"10.0.2.2 41752 22\", \"_\": \"/usr/bin/python\"}, \"ansible_processor_vcpus\": 1, \"ansible_docker0\": {\"macaddress\": \"02:42:33:3c:c5:e0\", \"interfaces\": [], \"mtu\": 1500, \"active\": false, \"promisc\": false, \"stp\": false, \"ipv4\": {\"netmask\": \"255.255.0.0\", \"network\": \"172.17.0.0\", \"address\": \"172.17.0.1\"}, \"device\": \"docker0\", \"type\": \"bridge\", \"id\": \"8000.0242333cc5e0\"}, \"ansible_bios_version\": \"VirtualBox\", \"ansible_processor\": [\"GenuineIntel\", \"Intel(R) Core(TM) i5-5200U CPU @ 2.20GHz\"], \"ansible_virtualization_type\": \"virtualbox\", \"ansible_lo\": {\"mtu\": 65536, \"active\": true, \"promisc\": false, \"ipv4\": {\"netmask\": \"255.0.0.0\", \"network\": \"127.0.0.0\", \"address\": \"127.0.0.1\"}, \"ipv6\": [{\"scope\": \"host\", \"prefix\": \"128\", \"address\": \"::1\"}], \"device\": \"lo\", \"type\": \"loopback\"}, \"ansible_memtotal_mb\": 2000, \"ansible_ssh_host_key_ecdsa_public\": \"AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBK5Jp5g/+qSXGzxlC6ZlEQfUpiAuZRcXv1gFPe9h1f9T/yVCBqZs2fM37/6cIljYzSwxiJVSveGiomGLxyba9G0=\", \"ansible_default_ipv4\": {\"macaddress\": \"52:54:00:d3:eb:a4\", \"network\": \"10.0.2.0\", \"mtu\": 1500, \"alias\": \"eth0\", \"netmask\": \"255.255.255.0\", \"address\": \"10.0.2.15\", \"interface\": \"eth0\", \"type\": \"ether\", \"gateway\": \"10.0.2.2\"}, \"ansible_swapfree_mb\": 1535, \"ansible_default_ipv6\": {}, \"ansible_distribution_release\": \"Core\", \"ansible_system_vendor\": \"innotek GmbH\", \"ansible_os_family\": \"RedHat\", \"ansible_cmdline\": {\"LANG\": \"en_US.UTF-8\", \"systemd.debug\": true, \"BOOT_IMAGE\": \"/vmlinuz-4.4.0-1.el7.elrepo.x86_64\", \"biosdevname\": \"0\", \"quiet\": true, \"net.ifnames\": \"0\", \"rhgb\": true, \"rd.lvm.lv\": \"VolGroup00/LogVol00\", \"crashkernel\": \"auto\", \"console\": \"ttyS0,115200\", \"ro\": true, \"root\": \"/dev/mapper/VolGroup00-LogVol00\"}, \"ansible_user_gid\": 1000, \"ansible_selinux\": {\"status\": \"disabled\"}, \"ansible_userspace_architecture\": \"x86_64\", \"ansible_product_uuid\": \"NA\", \"ansible_system\": \"Linux\", \"ansible_pkg_mgr\": \"yum\", \"ansible_memfree_mb\": 1333, \"ansible_devices\": {\"sda\": {\"scheduler_mode\": \"deadline\", \"rotational\": \"1\", \"vendor\": \"ATA\", \"sectors\": \"83886080\", \"host\": \"\", \"sectorsize\": \"512\", \"removable\": \"0\", \"support_discard\": \"0\", \"model\": \"VBOX HARDDISK\", \"size\": \"40.00 GB\", \"holders\": [], \"partitions\": {\"sda2\": {\"start\": \"4096\", \"sectorsize\": 512, \"sectors\": \"409600\", \"size\": \"200.00 MB\"}, \"sda3\": {\"start\": \"413696\", \"sectorsize\": 512, \"sectors\": \"83472384\", \"size\": \"39.80 GB\"}, \"sda1\": {\"start\": \"2048\", \"sectorsize\": 512, \"sectors\": \"2048\", \"size\": \"1.00 MB\"}}}}, \"ansible_user_uid\": 1000, \"ansible_memory_mb\": {\"real\": {\"total\": 2000, \"free\": 1333, \"used\": 667}, \"swap\": {\"cached\": 0, \"total\": 1535, \"used\": 0, \"free\": 1535}, \"nocache\": {\"used\": 300, \"free\": 1700}}, \"ansible_distribution\": \"CentOS\", \"ansible_distribution_major_version\": \"7\", \"ansible_user_dir\": \"/home/vagrant\", \"ansible_processor_countOpenSSH_7.1p1, OpenSSL 1.0.2e 3 Dec 2015\ndebug1: Reading configuration data /home/psi/.ssh/config\ndebug1: Reading configuration data /etc/ssh/ssh_config\ndebug1: /etc/ssh/ssh_config line 20: Applying options for *\ndebug1: auto-mux: Trying existing master\ndebug1: mux_client_request_session: master session id: 2\nShared connection to 127.0.0.1 closed.\n\nanyone konws why?\nFor me the output looks like incomplete JSON",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2003,
    "text": "Shell with_items Prompts for ssh key password on second command\n\nI have seen something when running the shell module along with a with_items where it now prompts for a key pass phrase. It runs the first command with no password prompt but then on the second one it prompts for a key password and fails the task:\n-name: Shell commands\n shell: \"{{item}}\"\n with_items:\n  - echo \"test 1\"\n  - echo \"test 2\"\n\nWhen running the playbook you get prompted for a key passphrase:\nEnter passphrase for key '/home/vagrant/.ssh/id_rsa':\n\nThis previously worked on the v2.0.0-0.3.beta1 tag but no longer works on later tags and on the dev branch.",
    "annotations": [{ "label": 140, "user": 1 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2004,
    "text": "Add a way to make boto not verify SSL certs\n\nStarting a year ago with version 2.6.0, boto began verifying servers' SSL certificates by default.  Since most Eucalyptus and Nova installs have self-signed certs, this breaks Ansible's AWS-related modules when they're pointed at one of those clouds using a relatively current version of boto.\nThey added a validate_certs=False arg to calls like boto.connect_ec2_endpoint one can use to disable this behavior, but right now Ansible doesn't have a way to trigger that.  Modules like ec2, ec2_elb, and so on would benefit from a parameter that lets one turn cert verification off when they need to talk to services with self-signed certs.\nEPEL bug that triggered this report:  https://bugzilla.redhat.com/show_bug.cgi?id=1003105",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2005,
    "text": "ansible-galaxy file parameter full path\n\nHi,\nIt would be nice when you setup a galaxy dependencies file like this:\njdauphant.nginx,v1.1.1,nginx\n\nto be able to specify the name of the final module path. The goal is when i launch it like this:\nansible-galaxy install -r galaxy.txt -p roles\n\nI get a folder 'roles/nginx' which is better than having 'roles/jdauphant.nginx'\nThanks",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2006,
    "text": "JSON output gets truncated on spaces\n\nIssue Type:\nBug report\nAnsible Version:\nλ ansible --version\nansible 2.0.0\n  configured module search path = ../library\n\nand\nλ ansible --version\nansible 1.9.0.1\n  configured module search path = ../library\n\nEnvironment:\nN/A\nSummary:\ncopy: content=\"{{ some_data|to_json }}\\n\" dest=/tmp/wtf.yaml results in truncated json.\nSteps To Reproduce:\nExample playbook:\n- hosts: web414\n  vars:\n    some_data:\n      something:\n        wow: \"hey ho\"\n  tasks:\n    - name: write some json\n      copy: content=\"{{ some_data|to_json }}\\n\" dest=/tmp/wtf.yaml\nResulting file:\nweb414 ~ # cat /tmp/wtf.yaml\n\"{\"something\": {\"wow\": \"heyweb414 ~ #\n\nExpected Results:\nFile should not be truncated.\nActual Results:\nFile gets truncated.",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2007,
    "text": "Misleading error when a file isn't found in a role\n\nIssue Type:\nBug Report\nAnsible Version:\n\nansible 2.0.0.2\nconfig file = /home/user/ansible/ansible.cfg\nconfigured module search path = Default w/o overrides\n\nAnsible Configuration:\nClean.\nEnvironment:\nUbuntu 14.04.\nSummary:\nWhen a copy or assemble operation can't access the src file in a role a misleading error suggests to look at the playbooks directory.\nSteps To Reproduce:\nuser@server:~/ansible$ touch roles/test/files/snari\nuser@server:~/ansible$ cat playbooks/test.yml\n\n---\n- hosts: all\n  roles:\n    - test\n\nuser@server:~/ansible$ cat roles/test/tasks/main.yml\n\n---\n- copy:\n    src: snari\n    dest: /tmp/snari\n\nuser@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml\n\nPLAY ***************************************************************************\n\nTASK [test : copy] *************************************************************\nok: [server]\n\nPLAY RECAP *********************************************************************\nserver                     : ok=1    changed=0    unreachable=0    failed=0\n\nuser@server:~/ansible$ mv roles/test/files/snari{,_}\n\nExpected Results:\nuser@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml\n\nPLAY ***************************************************************************\n\nTASK [test : copy] *************************************************************\nfatal: [server]: FAILED! => {\"changed\": false, \"failed\": true, \"msg\": \"could not find snari in the search path: /home/user/ansible/roles/test/files, /home/user/ansible/playbooks\"}\n\nPLAY RECAP *********************************************************************\nserver                     : ok=0    changed=0    unreachable=0    failed=1\n\nActual Results:\nuser@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml\n\nPLAY ***************************************************************************\n\nTASK [test : copy] *************************************************************\nfatal: [server]: FAILED! => {\"changed\": false, \"failed\": true, \"msg\": \"could not find src=/home/user/ansible/playbooks/snari\"}\n\nPLAY RECAP *********************************************************************\nserver                     : ok=0    changed=0    unreachable=0    failed=1",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2008,
    "text": "Copy module shows 'changed' in check mode\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\ncopy\nANSIBLE VERSION\n2.3.0.0\n\nCONFIGURATION\nNA\nOS / ENVIRONMENT\nNA\nSUMMARY\nWhen using the copy module to copy a file into a directory on the target system, the check mode yields changed though there is no change on the target system. The problem only occurs if the path\nis given without trailing slash.\nSTEPS TO REPRODUCE\nCall in check mode after file exists:\n- name: copy file\n  copy:\n    src=myfile\n    dest=/path/to/folder\nEXPECTED RESULTS\nchanged: false\nACTUAL RESULTS\nchanged: true\nThis effect does not occur if dest is given with a trailing slash:\n- name: copy file\n  copy:\n    src=myfile\n    dest=/path/to/folder/",
    "annotations": [{ "label": 142, "user": 1 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2009,
    "text": "Ansible fails to load Yaml file with equal-sign as value\n\nAnsible fails when loading a Yaml file like this:\n--- \n  foo: \n    - =\nThe Yaml file is valid (see http://www.yamllint.com/) but ansible fails with ERROR: Syntax Error while loading YAML script ...\nPutting quotes around the equal-sign works but i can't do that in my case as this file will be generated by a Yaml parser. So using a workaround is not an option.",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2010,
    "text": "fedora 25 dnf installed packages not marked as user installed\n\nISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\n\ndnf\nANSIBLE VERSION\n\nansible 2.2.1.0\n  config file = /home/edo/Sviluppo/fedora-vmware/ansible.cfg\n  configured module search path = Default w/o overrides\n\nCONFIGURATION\nansible.cfg\n[defaults]\ninventory=inventory\n\n[privilege_escalation]\nbecome_method=sudo\nbecome_user=root\n\nOS / ENVIRONMENT\nFedora 25\nSUMMARY\n\nWhen running the playbook below not every package is marked as user installed, this means for example that running \"dnf autoremove\" afterwards will remove packages installed by the playbook.\nSTEPS TO REPRODUCE\nRun the following playbook, then run \"dnf history userinstalled\":\n\n---\n- name: Fedora on VMware post-install\n  hosts: localhost\n  become: yes\n  tasks:\n    - name: Update everything\n      dnf: name=\"*\" state=latest\n    - name: Install packages\n      dnf: name={{ item }} state=latest\n      with_items:\n        - vim-enhanced\n        - emacs\n        - tmux\n        - powertop\n        - gnome-tweak-tool\n        - libselinux-python\n        - dconf-editor\n        - open-vm-tools-desktop\n        - git\n...\ndnf history userinstalled output:\nPackages installed by user\nanaconda-25.20.9-1.fc25.x86_64\nansible-2.2.1.0-1.fc25.noarch\ndocker-2:1.12.6-6.gitae7d637.fc25.x86_64\ndracut-live-044-78.fc25.x86_64\ngnome-tweak-tool-3.22.0-1.fc25.noarch\ngoogle-chrome-stable-56.0.2924.87-1.x86_64\nkernel-modules-extra-4.9.12-200.fc25.x86_64\nlangpacks-en-1.0-8.fc25.noarch\nmemtest86+-5.01-15.fc25.x86_64\npython2-dnf-1.1.10-5.fc25.noarch\nsyslinux-6.04-0.1.fc25.x86_64\ntmux-2.2-3.fc25.x86_64\n\nEXPECTED RESULTS\nAll the packages in the playbook should be listed as user installed, so that 'dnf autoremove' will not remove them. A possible workaround is running 'dnf mark '.\nPlus, if you support also dnf autoremove #20333, then interesting things will happen.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2011,
    "text": "`defaults/main.yml` overriding variables `include_vars:` in roles\n\nIn defaults/main.yml:\n---\nfoo: bar\nIn vars/Darwin.yml:\n---\nfoo: baz\nIn tasks/main.yml:\n- name: include env specific vars\n  include_vars: \"{{ item }}\"\n  with_first_found:\n    - \"{{ ansible_distribution }}.yml\"\n    - \"{{ ansible_os_family }}.yml\"\n- debug: var=foo\nAssume this is run on a Darwin machine we get\nfoo: bar\nwhere I would expect to get baz.\nThe work around is to ditch the defaults folder and instead load it with a defaults file with   with_first_found:\nMaybe I am doing something backwards here, but the defaults folder seem to take higher precedence over including variables selectively included in the task.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2012,
    "text": "ansible os_server module doesnot work with async_status\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\n\nmodule : os_server\nANSIBLE VERSION\n\nansible 2.1.0.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\n\nCONFIGURATION\n\nOS / ENVIRONMENT\n\nN/A\nSUMMARY\n\nAnsible is unable to parse the os_server module output using async_status .\nSTEPS TO REPRODUCE\n\nThe following tasks are to be written\n\nname: \"Async:: provision/deprovision os_server resources by looping on count\"\nos_server:\nstate: \"{{ instance.4 }}\"\nauth:\nauth_url: \"{{ instance.0 }}\"\nusername: \"{{ instance.1 }}\"\npassword: \"{{ instance.2 }}\"\nproject_name: \"{{ instance.3 }}\"\nname: \"{{ instance.9 }}{{ instance.10 }}{{ instance.11 }}\"\nimage: \"{{ instance.5 }}\"\nkey_name: \"{{ instance.6  }}\"\napi_timeout: 99999\nflavor: \"{{ instance.7 }}\"\nnetwork: \"{{ instance.8 }}\"\nwith_nested:\n\n[\"{{ endpoint }}\"]\n[\"{{ username }}\"]\n[\"{{ password }}\"]\n[\"{{ project }}\"]\n[\"{{ state }}\"]\n[\"{{ res_def['image'] }}\"]\n[\"{{ res_def['keypair']  }}\"]\n[\"{{ res_def['flavor']  }}\"]\n[\"{{ res_def['networks'][0] }}\"]\n[\"{{ res_grp_name }}\"]\n[\"{{ res_def['res_name'] }}\"]\n\"{{ res_count.stdout }}\"\nloop_control:\nloop_var: instance\nasync: 1000\npoll: 0\nregister: res_def_output\nwhen: async == true\n\n\nname: 'check on fire and forget task'\nasync_status: jid={{ item.ansible_job_id }}\nregister: job_result\nuntil: job_result.finished\nretries: 30\nwith_items: \"{{ res_def_output['results'] }}\"\n\nGives following error :\nEXPECTED RESULTS\n\nexpected results are the output of the booted instance\nACTUAL RESULTS\n\nAnsible is unable to parse the output of the job\n\n<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: root\n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041 `\" && echo ansible-tmp-1470849705.2-237917709656041=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041 `\" ) && sleep 0'                                                                                              \n<127.0.0.1> PUT /tmp/tmpXkXV6P TO /root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/\" > /dev/null 2>&1 && sleep 0'                                                                         \nFAILED - RETRYING: TASK: openstack : check on fire and forget task (29 retries left).Result was: {\"ansible_job_id\": \"309653937203.28231\", \"attempts\": 1, \"changed\": false, \"finished\": 0, \"invocation\": {\"module_args\": {\"jid\": \"309653937203.28231\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"retries\": 30, \"started\": 1}\n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974 `\" && echo ansible-tmp-1470849710.29-212087273581974=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974 `\" ) && sleep 0'                                                                                           \n<127.0.0.1> PUT /tmp/tmpvCVj1J TO /root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/\" > /dev/null 2>&1 && sleep 0'                                                                       \nFAILED - RETRYING: TASK: openstack : check on fire and forget task (28 retries left).Result was: {\"ansible_job_id\": \"309653937203.28231\", \"attempts\": 2, \"changed\": false, \"finished\": 0, \"invocation\": {\"module_args\": {\"jid\": \"309653937203.28231\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"retries\": 30, \"started\": 1}\n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926 `\" && echo ansible-tmp-1470849715.39-35966468008926=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926 `\" ) && sleep 0'                                                                                              \n<127.0.0.1> PUT /tmp/tmpj1oc9X TO /root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/\" > /dev/null 2>&1 && sleep 0'                                                                         \nFAILED - RETRYING: TASK: openstack : check on fire and forget task (27 retries left).Result was: {\"ansible_job_id\": \"309653937203.28231\", \"attempts\": 3, \"changed\": false, \"finished\": 0, \"invocation\": {\"module_args\": {\"jid\": \"309653937203.28231\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"retries\": 30, \"started\": 1}\n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550 `\" && echo ansible-tmp-1470849720.47-221980927570550=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550 `\" ) && sleep 0'                                                                                           \n<127.0.0.1> PUT /tmp/tmpUBrX3j TO /root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/\" > /dev/null 2>&1 && sleep 0'                                                                       \nfailed: [localhost] (item={'_ansible_no_log': False, u'ansible_job_id': u'309653937203.28231', u'started': 1, '_ansible_item_result': True, u'instance': [u'http://localhost:5000/v2.0', u'e2e-openstack', u'rgHdUfMqshXWSBYdlfIp', u'e2e-openstack', u'present', u'rhel-6.5_jeos', u'ci-factory', u'm1.small', u'e2e-openstack', u'testgroup1', u'ano_inst', 0], u'results_file': u'/root/.ansible_async/309653937203.28231'}) => {\"ansible_job_id\": \"309653937203.28231\", \"failed\": true, \"finished\": 1, \"invocation\": {\"module_args\": {\"jid\": \"309653937203.28231\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"item\": {\"ansible_job_id\": \"309653937203.28231\", \"instance\": [\"http://localhost:5000/v2.0\", \"e2e-openstack\", \"rgHdUfMqshXWSBYdlfIp\", \"e2e-openstack\", \"present\", \"rhel-6.5_jeos\", \"ci-factory\", \"m1.small\", \"e2e-openstack\", \"testgroup1\", \"ano_inst\", 0], \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"started\": 1}, \"msg\": \"Could not parse job output: No handlers could be found for logger \\\"keystoneauth.identity.base\\\"\\n\\n{\\\"invocation\\\": {\\\"module_args\\\": {\\\"auth_type\\\": null, \\\"availability_zone\\\": null, \\\"image\\\": \\\"rhel-6.5_jeos\\\", \\\"image_exclude\\\": \\\"(deprecated)\\\", \\\"flavor_include\\\": null, \\\"meta\\\": null, \\\"flavor\\\": \\\"m1.small\\\", \\\"cloud\\\": null, \\\"scheduler_hints\\\": null, \\\"boot_from_volume\\\": false, \\\"userdata\\\": null, \\\"network\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"nics\\\": [], \\\"floating_ips\\\": null, \\\"flavor_ram\\\": null, \\\"volume_size\\\": false, \\\"state\\\": \\\"present\\\", \\\"auto_ip\\\": true, \\\"security_groups\\\": [\\\"default\\\"], \\\"config_drive\\\": false, \\\"volumes\\\": [], \\\"key_name\\\": \\\"ci-factory\\\", \\\"api_timeout\\\": 99999, \\\"auth\\\": {\\\"username\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"project_name\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"password\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"auth_url\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"}, \\\"endpoint_type\\\": \\\"public\\\", \\\"boot_volume\\\": null, \\\"key\\\": null, \\\"cacert\\\": null, \\\"wait\\\": true, \\\"name\\\": \\\"testgroup1_ano_inst_0\\\", \\\"region_name\\\": null, \\\"timeout\\\": 180, \\\"cert\\\": null, \\\"terminate_volume\\\": false, \\\"verify\\\": true, \\\"floating_ip_pools\\\": null}}, \\\"openstack\\\": {\\\"OS-EXT-STS:task_state\\\": null, \\\"addresses\\\": {\\\"e2e-openstack\\\": [{\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:da:36:f5\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"172.16.100.91\\\", \\\"OS-EXT-IPS:type\\\": \\\"fixed\\\"}, {\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:da:36:f5\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"10.8.183.233\\\", \\\"OS-EXT-IPS:type\\\": \\\"floating\\\"}]}, \\\"image\\\": {\\\"id\\\": \\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\", \\\"name\\\": \\\"rhel-6.5_jeos\\\"}, \\\"OS-EXT-STS:vm_state\\\": \\\"active\\\", \\\"OS-SRV-USG:launched_at\\\": \\\"2016-08-10T17:21:52.000000\\\", \\\"NAME_ATTR\\\": \\\"name\\\", \\\"flavor\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"m1.small\\\"}, \\\"az\\\": \\\"nova\\\", \\\"id\\\": \\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\", \\\"cloud\\\": \\\"defaults\\\", \\\"user_id\\\": \\\"9c770dbddda444799e627004fee26e0a\\\", \\\"OS-DCF:diskConfig\\\": \\\"MANUAL\\\", \\\"networks\\\": {\\\"e2e-openstack\\\": [\\\"172.16.100.91\\\", \\\"10.8.183.233\\\"]}, \\\"accessIPv4\\\": \\\"10.8.183.233\\\", \\\"accessIPv6\\\": \\\"\\\", \\\"security_groups\\\": [{\\\"id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"name\\\": \\\"default\\\", \\\"security_group_rules\\\": [{\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\"}, {\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\"}], \\\"description\\\": \\\"Default security group\\\"}], \\\"key_name\\\": \\\"ci-factory\\\", \\\"progress\\\": 0, \\\"OS-EXT-STS:power_state\\\": 1, \\\"OS-EXT-AZ:availability_zone\\\": \\\"nova\\\", \\\"metadata\\\": {}, \\\"status\\\": \\\"ACTIVE\\\", \\\"updated\\\": \\\"2016-08-10T17:21:52Z\\\", \\\"hostId\\\": \\\"45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\", \\\"HUMAN_ID\\\": true, \\\"OS-SRV-USG:terminated_at\\\": null, \\\"public_v4\\\": \\\"10.8.183.233\\\", \\\"public_v6\\\": \\\"\\\", \\\"private_v4\\\": \\\"172.16.100.91\\\", \\\"interface_ip\\\": \\\"10.8.183.233\\\", \\\"name\\\": \\\"testgroup1_ano_inst_0\\\", \\\"created\\\": \\\"2016-08-10T17:21:46Z\\\", \\\"tenant_id\\\": \\\"f1dda47890754241a3e111f9b7394707\\\", \\\"region\\\": \\\"\\\", \\\"adminPass\\\": \\\"B2zNdwbcP8ha\\\", \\\"os-extended-volumes:volumes_attached\\\": [], \\\"volumes\\\": [], \\\"config_drive\\\": \\\"\\\", \\\"human_id\\\": \\\"testgroup1_ano_inst_0\\\"}, \\\"changed\\\": true, \\\"id\\\": \\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\", \\\"server\\\": {\\\"OS-EXT-STS:task_state\\\": null, \\\"addresses\\\": {\\\"e2e-openstack\\\": [{\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:da:36:f5\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"172.16.100.91\\\", \\\"OS-EXT-IPS:type\\\": \\\"fixed\\\"}, {\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:da:36:f5\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"10.8.183.233\\\", \\\"OS-EXT-IPS:type\\\": \\\"floating\\\"}]}, \\\"image\\\": {\\\"id\\\": \\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\", \\\"name\\\": \\\"rhel-6.5_jeos\\\"}, \\\"OS-EXT-STS:vm_state\\\": \\\"active\\\", \\\"OS-SRV-USG:launched_at\\\": \\\"2016-08-10T17:21:52.000000\\\", \\\"NAME_ATTR\\\": \\\"name\\\", \\\"flavor\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"m1.small\\\"}, \\\"az\\\": \\\"nova\\\", \\\"id\\\": \\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\", \\\"cloud\\\": \\\"defaults\\\", \\\"user_id\\\": \\\"9c770dbddda444799e627004fee26e0a\\\", \\\"OS-DCF:diskConfig\\\": \\\"MANUAL\\\", \\\"networks\\\": {\\\"e2e-openstack\\\": [\\\"172.16.100.91\\\", \\\"10.8.183.233\\\"]}, \\\"accessIPv4\\\": \\\"10.8.183.233\\\", \\\"accessIPv6\\\": \\\"\\\", \\\"security_groups\\\": [{\\\"id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"name\\\": \\\"default\\\", \\\"security_group_rules\\\": [{\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\"}, {\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\"}], \\\"description\\\": \\\"Default security group\\\"}], \\\"key_name\\\": \\\"ci-factory\\\", \\\"progress\\\": 0, \\\"OS-EXT-STS:power_state\\\": 1, \\\"OS-EXT-AZ:availability_zone\\\": \\\"nova\\\", \\\"metadata\\\": {}, \\\"status\\\": \\\"ACTIVE\\\", \\\"updated\\\": \\\"2016-08-10T17:21:52Z\\\", \\\"hostId\\\": \\\"45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\", \\\"HUMAN_ID\\\": true, \\\"OS-SRV-USG:terminated_at\\\": null, \\\"public_v4\\\": \\\"10.8.183.233\\\", \\\"public_v6\\\": \\\"\\\", \\\"private_v4\\\": \\\"172.16.100.91\\\", \\\"interface_ip\\\": \\\"10.8.183.233\\\", \\\"name\\\": \\\"testgroup1_ano_inst_0\\\", \\\"created\\\": \\\"2016-08-10T17:21:46Z\\\", \\\"tenant_id\\\": \\\"f1dda47890754241a3e111f9b7394707\\\", \\\"region\\\": \\\"\\\", \\\"adminPass\\\": \\\"B2zNdwbcP8ha\\\", \\\"os-extended-volumes:volumes_attached\\\": [], \\\"volumes\\\": [], \\\"config_drive\\\": \\\"\\\", \\\"human_id\\\": \\\"testgroup1_ano_inst_0\\\"}}\\n{\\\"msg\\\": \\\"Traceback (most recent call last):\\\\n  File \\\\\\\"/root/.ansible/tmp/ansible-tmp-1470849702.44-113930776716519/async_wrapper\\\\\\\", line 89, in _run_module\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/__init__.py\\\\\\\", line 339, in loads\\\\n    return _default_decoder.decode(s)\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/decoder.py\\\\\\\", line 364, in decode\\\\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/decoder.py\\\\\\\", line 382, in raw_decode\\\\n    raise ValueError(\\\\\\\"No JSON object could be decoded\\\\\\\")\\\\nValueError: No JSON object could be decoded\\\\n\\\", \\\"failed\\\": 1, \\\"cmd\\\": \\\"/root/.ansible/tmp/ansible-tmp-1470849702.44-113930776716519/os_server\\\", \\\"data\\\": \\\"No handlers could be found for logger \\\\\\\"keystoneauth.identity.base\\\\\\\"\\\\n\\\\n{\\\\\\\"invocation\\\\\\\": {\\\\\\\"module_args\\\\\\\": {\\\\\\\"auth_type\\\\\\\": null, \\\\\\\"availability_zone\\\\\\\": null, \\\\\\\"image\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\", \\\\\\\"image_exclude\\\\\\\": \\\\\\\"(deprecated)\\\\\\\", \\\\\\\"flavor_include\\\\\\\": null, \\\\\\\"meta\\\\\\\": null, \\\\\\\"flavor\\\\\\\": \\\\\\\"m1.small\\\\\\\", \\\\\\\"cloud\\\\\\\": null, \\\\\\\"scheduler_hints\\\\\\\": null, \\\\\\\"boot_from_volume\\\\\\\": false, \\\\\\\"userdata\\\\\\\": null, \\\\\\\"network\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"nics\\\\\\\": [], \\\\\\\"floating_ips\\\\\\\": null, \\\\\\\"flavor_ram\\\\\\\": null, \\\\\\\"volume_size\\\\\\\": false, \\\\\\\"state\\\\\\\": \\\\\\\"present\\\\\\\", \\\\\\\"auto_ip\\\\\\\": true, \\\\\\\"security_groups\\\\\\\": [\\\\\\\"default\\\\\\\"], \\\\\\\"config_drive\\\\\\\": false, \\\\\\\"volumes\\\\\\\": [], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"api_timeout\\\\\\\": 99999, \\\\\\\"auth\\\\\\\": {\\\\\\\"username\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"project_name\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"password\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"auth_url\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\"}, \\\\\\\"endpoint_type\\\\\\\": \\\\\\\"public\\\\\\\", \\\\\\\"boot_volume\\\\\\\": null, \\\\\\\"key\\\\\\\": null, \\\\\\\"cacert\\\\\\\": null, \\\\\\\"wait\\\\\\\": true, \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\", \\\\\\\"region_name\\\\\\\": null, \\\\\\\"timeout\\\\\\\": 180, \\\\\\\"cert\\\\\\\": null, \\\\\\\"terminate_volume\\\\\\\": false, \\\\\\\"verify\\\\\\\": true, \\\\\\\"floating_ip_pools\\\\\\\": null}}, \\\\\\\"openstack\\\\\\\": {\\\\\\\"OS-EXT-STS:task_state\\\\\\\": null, \\\\\\\"addresses\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [{\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:da:36:f5\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"fixed\\\\\\\"}, {\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:da:36:f5\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"floating\\\\\\\"}]}, \\\\\\\"image\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\"}, \\\\\\\"OS-EXT-STS:vm_state\\\\\\\": \\\\\\\"active\\\\\\\", \\\\\\\"OS-SRV-USG:launched_at\\\\\\\": \\\\\\\"2016-08-10T17:21:52.000000\\\\\\\", \\\\\\\"NAME_ATTR\\\\\\\": \\\\\\\"name\\\\\\\", \\\\\\\"flavor\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"2\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"m1.small\\\\\\\"}, \\\\\\\"az\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\\\\\", \\\\\\\"cloud\\\\\\\": \\\\\\\"defaults\\\\\\\", \\\\\\\"user_id\\\\\\\": \\\\\\\"9c770dbddda444799e627004fee26e0a\\\\\\\", \\\\\\\"OS-DCF:diskConfig\\\\\\\": \\\\\\\"MANUAL\\\\\\\", \\\\\\\"networks\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [\\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"10.8.183.233\\\\\\\"]}, \\\\\\\"accessIPv4\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"accessIPv6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"security_groups\\\\\\\": [{\\\\\\\"id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"default\\\\\\\", \\\\\\\"security_group_rules\\\\\\\": [{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\\\\\"}, {\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\\\\\"}], \\\\\\\"description\\\\\\\": \\\\\\\"Default security group\\\\\\\"}], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"progress\\\\\\\": 0, \\\\\\\"OS-EXT-STS:power_state\\\\\\\": 1, \\\\\\\"OS-EXT-AZ:availability_zone\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"metadata\\\\\\\": {}, \\\\\\\"status\\\\\\\": \\\\\\\"ACTIVE\\\\\\\", \\\\\\\"updated\\\\\\\": \\\\\\\"2016-08-10T17:21:52Z\\\\\\\", \\\\\\\"hostId\\\\\\\": \\\\\\\"45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\\\\\", \\\\\\\"HUMAN_ID\\\\\\\": true, \\\\\\\"OS-SRV-USG:terminated_at\\\\\\\": null, \\\\\\\"public_v4\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"public_v6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"private_v4\\\\\\\": \\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"interface_ip\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\", \\\\\\\"created\\\\\\\": \\\\\\\"2016-08-10T17:21:46Z\\\\\\\", \\\\\\\"tenant_id\\\\\\\": \\\\\\\"f1dda47890754241a3e111f9b7394707\\\\\\\", \\\\\\\"region\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"adminPass\\\\\\\": \\\\\\\"B2zNdwbcP8ha\\\\\\\", \\\\\\\"os-extended-volumes:volumes_attached\\\\\\\": [], \\\\\\\"volumes\\\\\\\": [], \\\\\\\"config_drive\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"human_id\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\"}, \\\\\\\"changed\\\\\\\": true, \\\\\\\"id\\\\\\\": \\\\\\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\\\\\", \\\\\\\"server\\\\\\\": {\\\\\\\"OS-EXT-STS:task_state\\\\\\\": null, \\\\\\\"addresses\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [{\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:da:36:f5\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"fixed\\\\\\\"}, {\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:da:36:f5\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"floating\\\\\\\"}]}, \\\\\\\"image\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\"}, \\\\\\\"OS-EXT-STS:vm_state\\\\\\\": \\\\\\\"active\\\\\\\", \\\\\\\"OS-SRV-USG:launched_at\\\\\\\": \\\\\\\"2016-08-10T17:21:52.000000\\\\\\\", \\\\\\\"NAME_ATTR\\\\\\\": \\\\\\\"name\\\\\\\", \\\\\\\"flavor\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"2\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"m1.small\\\\\\\"}, \\\\\\\"az\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\\\\\", \\\\\\\"cloud\\\\\\\": \\\\\\\"defaults\\\\\\\", \\\\\\\"user_id\\\\\\\": \\\\\\\"9c770dbddda444799e627004fee26e0a\\\\\\\", \\\\\\\"OS-DCF:diskConfig\\\\\\\": \\\\\\\"MANUAL\\\\\\\", \\\\\\\"networks\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [\\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"10.8.183.233\\\\\\\"]}, \\\\\\\"accessIPv4\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"accessIPv6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"security_groups\\\\\\\": [{\\\\\\\"id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"default\\\\\\\", \\\\\\\"security_group_rules\\\\\\\": [{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\\\\\"}, {\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\\\\\"}], \\\\\\\"description\\\\\\\": \\\\\\\"Default security group\\\\\\\"}], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"progress\\\\\\\": 0, \\\\\\\"OS-EXT-STS:power_state\\\\\\\": 1, \\\\\\\"OS-EXT-AZ:availability_zone\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"metadata\\\\\\\": {}, \\\\\\\"status\\\\\\\": \\\\\\\"ACTIVE\\\\\\\", \\\\\\\"updated\\\\\\\": \\\\\\\"2016-08-10T17:21:52Z\\\\\\\", \\\\\\\"hostId\\\\\\\": \\\\\\\"45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\\\\\", \\\\\\\"HUMAN_ID\\\\\\\": true, \\\\\\\"OS-SRV-USG:terminated_at\\\\\\\": null, \\\\\\\"public_v4\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"public_v6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"private_v4\\\\\\\": \\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"interface_ip\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\", \\\\\\\"created\\\\\\\": \\\\\\\"2016-08-10T17:21:46Z\\\\\\\", \\\\\\\"tenant_id\\\\\\\": \\\\\\\"f1dda47890754241a3e111f9b7394707\\\\\\\", \\\\\\\"region\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"adminPass\\\\\\\": \\\\\\\"B2zNdwbcP8ha\\\\\\\", \\\\\\\"os-extended-volumes:volumes_attached\\\\\\\": [], \\\\\\\"volumes\\\\\\\": [], \\\\\\\"config_drive\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"human_id\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\"}}\\\\n\\\", \\\"ansible_job_id\\\": \\\"309653937203.28231\\\"}\", \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"started\": 1}                                                                                                                                               \n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.76-738324392915 `\" && echo ansible-tmp-1470849720.76-738324392915=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.76-738324392915 `\" ) && sleep 0'                                                                                                    \n<127.0.0.1> PUT /tmp/tmpvHYKYF TO /root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/\" > /dev/null 2>&1 && sleep 0'                                                                             \nfailed: [localhost] (item={'_ansible_no_log': False, u'ansible_job_id': u'557360551178.28260', u'started': 1, '_ansible_item_result': True, u'instance': [u'http://localhost:5000/v2.0', u'e2e-openstack', u'rgHdUfMqshXWSBYdlfIp', u'e2e-openstack', u'present', u'rhel-6.5_jeos', u'ci-factory', u'm1.small', u'e2e-openstack', u'testgroup1', u'ano_inst', 1], u'results_file': u'/root/.ansible_async/557360551178.28260'}) => {\"ansible_job_id\": \"557360551178.28260\", \"failed\": true, \"finished\": 1, \"invocation\": {\"module_args\": {\"jid\": \"557360551178.28260\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"item\": {\"ansible_job_id\": \"557360551178.28260\", \"instance\": [\"http://localhost:5000/v2.0\", \"e2e-openstack\", \"rgHdUfMqshXWSBYdlfIp\", \"e2e-openstack\", \"present\", \"rhel-6.5_jeos\", \"ci-factory\", \"m1.small\", \"e2e-openstack\", \"testgroup1\", \"ano_inst\", 1], \"results_file\": \"/root/.ansible_async/557360551178.28260\", \"started\": 1}, \"msg\": \"Could not parse job output: No handlers could be found for logger \\\"keystoneauth.identity.base\\\"\\n\\n{\\\"invocation\\\": {\\\"module_args\\\": {\\\"auth_type\\\": null, \\\"availability_zone\\\": null, \\\"image\\\": \\\"rhel-6.5_jeos\\\", \\\"image_exclude\\\": \\\"(deprecated)\\\", \\\"flavor_include\\\": null, \\\"meta\\\": null, \\\"flavor\\\": \\\"m1.small\\\", \\\"cloud\\\": null, \\\"scheduler_hints\\\": null, \\\"boot_from_volume\\\": false, \\\"userdata\\\": null, \\\"network\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"nics\\\": [], \\\"floating_ips\\\": null, \\\"flavor_ram\\\": null, \\\"volume_size\\\": false, \\\"state\\\": \\\"present\\\", \\\"auto_ip\\\": true, \\\"security_groups\\\": [\\\"default\\\"], \\\"config_drive\\\": false, \\\"volumes\\\": [], \\\"key_name\\\": \\\"ci-factory\\\", \\\"api_timeout\\\": 99999, \\\"auth\\\": {\\\"username\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"project_name\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"password\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"auth_url\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"}, \\\"endpoint_type\\\": \\\"public\\\", \\\"boot_volume\\\": null, \\\"key\\\": null, \\\"cacert\\\": null, \\\"wait\\\": true, \\\"name\\\": \\\"testgroup1_ano_inst_1\\\", \\\"region_name\\\": null, \\\"timeout\\\": 180, \\\"cert\\\": null, \\\"terminate_volume\\\": false, \\\"verify\\\": true, \\\"floating_ip_pools\\\": null}}, \\\"openstack\\\": {\\\"OS-EXT-STS:task_state\\\": null, \\\"addresses\\\": {\\\"e2e-openstack\\\": [{\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:89:04:7c\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"172.16.100.92\\\", \\\"OS-EXT-IPS:type\\\": \\\"fixed\\\"}, {\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:89:04:7c\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"10.8.182.45\\\", \\\"OS-EXT-IPS:type\\\": \\\"floating\\\"}]}, \\\"image\\\": {\\\"id\\\": \\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\", \\\"name\\\": \\\"rhel-6.5_jeos\\\"}, \\\"OS-EXT-STS:vm_state\\\": \\\"active\\\", \\\"OS-SRV-USG:launched_at\\\": \\\"2016-08-10T17:21:52.000000\\\", \\\"NAME_ATTR\\\": \\\"name\\\", \\\"flavor\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"m1.small\\\"}, \\\"az\\\": \\\"nova\\\", \\\"id\\\": \\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\", \\\"cloud\\\": \\\"defaults\\\", \\\"user_id\\\": \\\"9c770dbddda444799e627004fee26e0a\\\", \\\"OS-DCF:diskConfig\\\": \\\"MANUAL\\\", \\\"networks\\\": {\\\"e2e-openstack\\\": [\\\"172.16.100.92\\\", \\\"10.8.182.45\\\"]}, \\\"accessIPv4\\\": \\\"10.8.182.45\\\", \\\"accessIPv6\\\": \\\"\\\", \\\"security_groups\\\": [{\\\"id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"name\\\": \\\"default\\\", \\\"security_group_rules\\\": [{\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\"}, {\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\"}], \\\"description\\\": \\\"Default security group\\\"}], \\\"key_name\\\": \\\"ci-factory\\\", \\\"progress\\\": 0, \\\"OS-EXT-STS:power_state\\\": 1, \\\"OS-EXT-AZ:availability_zone\\\": \\\"nova\\\", \\\"metadata\\\": {}, \\\"status\\\": \\\"ACTIVE\\\", \\\"updated\\\": \\\"2016-08-10T17:21:52Z\\\", \\\"hostId\\\": \\\"bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\\\", \\\"HUMAN_ID\\\": true, \\\"OS-SRV-USG:terminated_at\\\": null, \\\"public_v4\\\": \\\"10.8.182.45\\\", \\\"public_v6\\\": \\\"\\\", \\\"private_v4\\\": \\\"172.16.100.92\\\", \\\"interface_ip\\\": \\\"10.8.182.45\\\", \\\"name\\\": \\\"testgroup1_ano_inst_1\\\", \\\"created\\\": \\\"2016-08-10T17:21:48Z\\\", \\\"tenant_id\\\": \\\"f1dda47890754241a3e111f9b7394707\\\", \\\"region\\\": \\\"\\\", \\\"adminPass\\\": \\\"iRvzMzj2Q33e\\\", \\\"os-extended-volumes:volumes_attached\\\": [], \\\"volumes\\\": [], \\\"config_drive\\\": \\\"\\\", \\\"human_id\\\": \\\"testgroup1_ano_inst_1\\\"}, \\\"changed\\\": true, \\\"id\\\": \\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\", \\\"server\\\": {\\\"OS-EXT-STS:task_state\\\": null, \\\"addresses\\\": {\\\"e2e-openstack\\\": [{\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:89:04:7c\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"172.16.100.92\\\", \\\"OS-EXT-IPS:type\\\": \\\"fixed\\\"}, {\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:89:04:7c\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"10.8.182.45\\\", \\\"OS-EXT-IPS:type\\\": \\\"floating\\\"}]}, \\\"image\\\": {\\\"id\\\": \\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\", \\\"name\\\": \\\"rhel-6.5_jeos\\\"}, \\\"OS-EXT-STS:vm_state\\\": \\\"active\\\", \\\"OS-SRV-USG:launched_at\\\": \\\"2016-08-10T17:21:52.000000\\\", \\\"NAME_ATTR\\\": \\\"name\\\", \\\"flavor\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"m1.small\\\"}, \\\"az\\\": \\\"nova\\\", \\\"id\\\": \\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\", \\\"cloud\\\": \\\"defaults\\\", \\\"user_id\\\": \\\"9c770dbddda444799e627004fee26e0a\\\", \\\"OS-DCF:diskConfig\\\": \\\"MANUAL\\\", \\\"networks\\\": {\\\"e2e-openstack\\\": [\\\"172.16.100.92\\\", \\\"10.8.182.45\\\"]}, \\\"accessIPv4\\\": \\\"10.8.182.45\\\", \\\"accessIPv6\\\": \\\"\\\", \\\"security_groups\\\": [{\\\"id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"name\\\": \\\"default\\\", \\\"security_group_rules\\\": [{\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\"}, {\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\"}], \\\"description\\\": \\\"Default security group\\\"}], \\\"key_name\\\": \\\"ci-factory\\\", \\\"progress\\\": 0, \\\"OS-EXT-STS:power_state\\\": 1, \\\"OS-EXT-AZ:availability_zone\\\": \\\"nova\\\", \\\"metadata\\\": {}, \\\"status\\\": \\\"ACTIVE\\\", \\\"updated\\\": \\\"2016-08-10T17:21:52Z\\\", \\\"hostId\\\": \\\"bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\\\", \\\"HUMAN_ID\\\": true, \\\"OS-SRV-USG:terminated_at\\\": null, \\\"public_v4\\\": \\\"10.8.182.45\\\", \\\"public_v6\\\": \\\"\\\", \\\"private_v4\\\": \\\"172.16.100.92\\\", \\\"interface_ip\\\": \\\"10.8.182.45\\\", \\\"name\\\": \\\"testgroup1_ano_inst_1\\\", \\\"created\\\": \\\"2016-08-10T17:21:48Z\\\", \\\"tenant_id\\\": \\\"f1dda47890754241a3e111f9b7394707\\\", \\\"region\\\": \\\"\\\", \\\"adminPass\\\": \\\"iRvzMzj2Q33e\\\", \\\"os-extended-volumes:volumes_attached\\\": [], \\\"volumes\\\": [], \\\"config_drive\\\": \\\"\\\", \\\"human_id\\\": \\\"testgroup1_ano_inst_1\\\"}}\\n{\\\"msg\\\": \\\"Traceback (most recent call last):\\\\n  File \\\\\\\"/root/.ansible/tmp/ansible-tmp-1470849703.74-111819930279072/async_wrapper\\\\\\\", line 89, in _run_module\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/__init__.py\\\\\\\", line 339, in loads\\\\n    return _default_decoder.decode(s)\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/decoder.py\\\\\\\", line 364, in decode\\\\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/decoder.py\\\\\\\", line 382, in raw_decode\\\\n    raise ValueError(\\\\\\\"No JSON object could be decoded\\\\\\\")\\\\nValueError: No JSON object could be decoded\\\\n\\\", \\\"failed\\\": 1, \\\"cmd\\\": \\\"/root/.ansible/tmp/ansible-tmp-1470849703.74-111819930279072/os_server\\\", \\\"data\\\": \\\"No handlers could be found for logger \\\\\\\"keystoneauth.identity.base\\\\\\\"\\\\n\\\\n{\\\\\\\"invocation\\\\\\\": {\\\\\\\"module_args\\\\\\\": {\\\\\\\"auth_type\\\\\\\": null, \\\\\\\"availability_zone\\\\\\\": null, \\\\\\\"image\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\", \\\\\\\"image_exclude\\\\\\\": \\\\\\\"(deprecated)\\\\\\\", \\\\\\\"flavor_include\\\\\\\": null, \\\\\\\"meta\\\\\\\": null, \\\\\\\"flavor\\\\\\\": \\\\\\\"m1.small\\\\\\\", \\\\\\\"cloud\\\\\\\": null, \\\\\\\"scheduler_hints\\\\\\\": null, \\\\\\\"boot_from_volume\\\\\\\": false, \\\\\\\"userdata\\\\\\\": null, \\\\\\\"network\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"nics\\\\\\\": [], \\\\\\\"floating_ips\\\\\\\": null, \\\\\\\"flavor_ram\\\\\\\": null, \\\\\\\"volume_size\\\\\\\": false, \\\\\\\"state\\\\\\\": \\\\\\\"present\\\\\\\", \\\\\\\"auto_ip\\\\\\\": true, \\\\\\\"security_groups\\\\\\\": [\\\\\\\"default\\\\\\\"], \\\\\\\"config_drive\\\\\\\": false, \\\\\\\"volumes\\\\\\\": [], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"api_timeout\\\\\\\": 99999, \\\\\\\"auth\\\\\\\": {\\\\\\\"username\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"project_name\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"password\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"auth_url\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\"}, \\\\\\\"endpoint_type\\\\\\\": \\\\\\\"public\\\\\\\", \\\\\\\"boot_volume\\\\\\\": null, \\\\\\\"key\\\\\\\": null, \\\\\\\"cacert\\\\\\\": null, \\\\\\\"wait\\\\\\\": true, \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\", \\\\\\\"region_name\\\\\\\": null, \\\\\\\"timeout\\\\\\\": 180, \\\\\\\"cert\\\\\\\": null, \\\\\\\"terminate_volume\\\\\\\": false, \\\\\\\"verify\\\\\\\": true, \\\\\\\"floating_ip_pools\\\\\\\": null}}, \\\\\\\"openstack\\\\\\\": {\\\\\\\"OS-EXT-STS:task_state\\\\\\\": null, \\\\\\\"addresses\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [{\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:89:04:7c\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"172.16.100.92\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"fixed\\\\\\\"}, {\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:89:04:7c\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"floating\\\\\\\"}]}, \\\\\\\"image\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\"}, \\\\\\\"OS-EXT-STS:vm_state\\\\\\\": \\\\\\\"active\\\\\\\", \\\\\\\"OS-SRV-USG:launched_at\\\\\\\": \\\\\\\"2016-08-10T17:21:52.000000\\\\\\\", \\\\\\\"NAME_ATTR\\\\\\\": \\\\\\\"name\\\\\\\", \\\\\\\"flavor\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"2\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"m1.small\\\\\\\"}, \\\\\\\"az\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\\\\\", \\\\\\\"cloud\\\\\\\": \\\\\\\"defaults\\\\\\\", \\\\\\\"user_id\\\\\\\": \\\\\\\"9c770dbddda444799e627004fee26e0a\\\\\\\", \\\\\\\"OS-DCF:diskConfig\\\\\\\": \\\\\\\"MANUAL\\\\\\\", \\\\\\\"networks\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [\\\\\\\"172.16.100.92\\\\\\\", \\\\\\\"10.8.182.45\\\\\\\"]}, \\\\\\\"accessIPv4\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"accessIPv6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"security_groups\\\\\\\": [{\\\\\\\"id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"defaul \\\\\\\"security_group_rules\\\\\\\": [{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_gd\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\\\\\"}], \\\\\\\"description\\\\\\\": \\\\\\\"t security group\\\\\\\"}], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"progress\\\\\\\": 0, \\\\\\\"OS-EXT-STS:power_state\\\\\\\": 1, \\\\\\\"OS-EXT-AZ:availability_zone\\\\\\\": \\\\\\\"nova\\\\\\\", tadata\\\\\\\": {}, \\\\\\\"status\\\\\\\": \\\\\\\"ACTIVE\\\\\\\", \\\\\\\"updated\\\\\\\": \\\\\\\"2016-08-10T17:21:52Z\\\\\\\", \\\\\\\"hostId\\\\\\\": \\\\\\\"bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67, \\\\\\\"HUMAN_ID\\\\\\\": true, \\\\\\\"OS-SRV-USG:terminated_at\\\\\\\": null, \\\\\\\"public_v4\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"public_v6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"private_v4\\\\\\\": \\\\\\\"172.16.100\", \\\\\\\"interface_ip\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\", \\\\\\\"created\\\\\\\": \\\\\\\"2016-08-10T17:21:48Z\\\\\\\", \\\\\\\"tenant_id\\\\\\\": \\\\\\\"f1dda47841a3e111f9b7394707\\\\\\\", \\\\\\\"region\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"adminPass\\\\\\\": \\\\\\\"iRvzMzj2Q33e\\\\\\\", \\\\\\\"os-extended-volumes:volumes_attached\\\\\\\": [], \\\\\\\"volumes\\\\\\\": [], \\\\\\\"conive\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"human_id\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\"}, \\\\\\\"changed\\\\\\\": true, \\\\\\\"id\\\\\\\": \\\\\\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\\\\\", \\\\\\\"server\\\\\\\": {\\\\XT-STS:task_state\\\\\\\": null, \\\\\\\"addresses\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [{\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:89:04:7c\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\.16.100.92\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"fixed\\\\\\\"}, {\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:89:04:7c\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"floating\\\\\\\"}]}, \\\\\\\"image\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\"}, \\\\\\\"OS-EXT-STtate\\\\\\\": \\\\\\\"active\\\\\\\", \\\\\\\"OS-SRV-USG:launched_at\\\\\\\": \\\\\\\"2016-08-10T17:21:52.000000\\\\\\\", \\\\\\\"NAME_ATTR\\\\\\\": \\\\\\\"name\\\\\\\", \\\\\\\"flavor\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"2\\\\\\\", \\\\\\\\\\\": \\\\\\\"m1.small\\\\\\\"}, \\\\\\\"az\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\\\\\", \\\\\\\"cloud\\\\\\\": \\\\\\\"defaults\\\\\\\", \\\\\\\"user_id\\\\\\\": \\\\\\\"9c770d44799e627004fee26e0a\\\\\\\", \\\\\\\"OS-DCF:diskConfig\\\\\\\": \\\\\\\"MANUAL\\\\\\\", \\\\\\\"networks\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [\\\\\\\"172.16.100.92\\\\\\\", \\\\\\\"10.8.182.45\\\\\\\"]}, \\\\\\\"accessI\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"accessIPv6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"security_groups\\\\\\\": [{\\\\\\\"id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"default\\\\\\\",ecurity_group_rules\\\\\\\": [{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_i \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\\\\\"}, irection\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a763c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\\\\\"}], \\\\\\\"description\\\\\\\": \\\\\\\"Defaulrity group\\\\\\\"}], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"progress\\\\\\\": 0, \\\\\\\"OS-EXT-STS:power_state\\\\\\\": 1, \\\\\\\"OS-EXT-AZ:availability_zone\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"me\\\\\\\": {}, \\\\\\\"status\\\\\\\": \\\\\\\"ACTIVE\\\\\\\", \\\\\\\"updated\\\\\\\": \\\\\\\"2016-08-10T17:21:52Z\\\\\\\", \\\\\\\"hostId\\\\\\\": \\\\\\\"bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\\\\\\\"HUMAN_ID\\\\\\\": true, \\\\\\\"OS-SRV-USG:terminated_at\\\\\\\": null, \\\\\\\"public_v4\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"public_v6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"private_v4\\\\\\\": \\\\\\\"172.16.100.92\\\\\\\"interface_ip\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\", \\\\\\\"created\\\\\\\": \\\\\\\"2016-08-10T17:21:48Z\\\\\\\", \\\\\\\"tenant_id\\\\\\\": \\\\\\\"f1dda47890754211f9b7394707\\\\\\\", \\\\\\\"region\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"adminPass\\\\\\\": \\\\\\\"iRvzMzj2Q33e\\\\\\\", \\\\\\\"os-extended-volumes:volumes_attached\\\\\\\": [], \\\\\\\"volumes\\\\\\\": [], \\\\\\\"config_dr\": \\\\\\\"\\\\\\\", \\\\\\\"human_id\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\"}}\\\\n\\\", \\\"ansible_job_id\\\": \\\"557360551178.28260\\\"}\", \"results_file\": \"/root/.ansible_async/557360551178.282started\": 1}",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2013,
    "text": "Please enhance when: to accept a list\n\nThe semantics should be an AND of the items in the list\nwhen:\n\nexpression-1\nexpression-2\n\nshould be equivalent to:\nwhen: expression-1 and expression-2\nJustification:\nLong/complex expressions are made eminently more readable by breaking them down into components.\nThis is quite general, any boolean expression can be transformed into an AND-form with negation: a or b = ^(^a and ^b)",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2014,
    "text": "include fails to expand host variables\n\nThe playbook:\n\n---\n- hosts: localhost\n  vars:\n    - othertasks:\n       - othertask1\n       - othertask2\n  tasks:\n    - name: debug\n      debug: msg=\"tasks/$item.yml\"\n      with_items: $othertasks\n\n    - include: tasks/$item.yml\n      with_items: $othertasks\n\nthis works as long as othertasks is defined inline.\nWhen othertasks is removed from playbook and defined in the inventory file or in host_vars, debug continues working but include fails.\nError is ERROR: file not found: ./tasks/$othertasks.yml.",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2015,
    "text": "Copying files with mode 400 uploads with a mode of 620\n\nHi,\nI did post this in the Google forum, but my post never seemed to appear so asking here now instead.\nThis is probably something that isn't recommended but I'm trying to copy some PEM keys to a destination server so that they can be used by that server at a later time.\nI need to make sure that they are set to a mode of 400 as well, so using the following code:\n- name: add default ssh keys\n  become: true\n  copy:\n    src: ./.ssh/\n    dest: ~/.ssh\n    mode: 400\n    force: true\n\nI would hope and expect to see the following when listing the files on the destination server:\n-r--------\n\nBut instead I'm seeing the following:\n-rw--w----\n\nAny particular reason why the files being uploaded are actually getting the incorrect permissions?\nOr is there a different approach to making sure these files have the correct permissions?\nThanks,\nGary",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2016,
    "text": "tags are not inherented by multiple include levels\n\nIssue Type:\n\nBug Report (or feature request, depends on how you see it)\n\nAnsible Version:\n1.9.4\nAnsible Configuration:\nDefault\nEnvironment:\nDebian 8.2 (jessie)\nSummary:\nSee the project structure below\nSteps To Reproduce:\nI have the following project structure:\nroles/\n  local/   \n    tasks/\n      main.yml [1]\n      media/\n        main.yml [2]\n        hipchat.yml\n        slack.yml\n       ...\n\n[1] main.yml has content:\n- include: media/main.yml tags=media\n[2] main.yml has content:\n- include: hipchat.yml tags=hipchat\n- include: slack.yml tags=slack\n...\n\nExpected Results:\nI expect all tasks from media/main.yml will inherit all tags specified in tasks/main.yml. So, if I run ansible-playbook -i hosts site.yml --tags media, it will run all tasks specified in media/main.yml, and $ ansible-playbook --list-tags -i hosts site.yml reports this tag as well\nplaybook: site.yml\n\n  play #1 (localhost):  TAGS: []\n    TASK TAGS: [alternatives, apt, atom, chrome, clojure, docker, dot-files, dropbox, git, gnome, go, hipchat, idea, jdk, maven, packages, permissions, productivity, sdkman, skype, slack, spotify, system, tools, viber, vim]\n\nActual Results:\nIt fails with the following error:\nERROR: tag(s) not found in playbook: media.  possible values: alternatives,apt,atom,chrome,clojure,docker,dot-files,dropbox,git,gnome,go,hipchat,idea,jdk,maven,packages,permissions,productivity,sdkman,skype,slack,spotify,system,tools,viber,vim\n\nP.S.1: The whole source code of the project is available here https://github.com/zshamrock/ididitagain\nP.S.2: As a workaround (or the right way to do?), move specific subdirectories, like media, dev, dot-files, etc, in its own roles, and assign tags with role instead, as mentioned here http://docs.ansible.com/ansible/playbooks_tags.html in my site.yml?\nroles:\n  - { role: webserver, port: 5000, tags: [ 'web', 'foo' ] }",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2017,
    "text": "Seems like default() filter does not work with ansible 2.0.0@devel\n\nIssue Type: Bug Report\nAnsible Version: 2.0.0 devel@8d16638\nAnsible Configuration:\n[defaults]\nhost_key_checking = False\nforks=20\n\nEnvironment: Mac OS X El Capitan 10.11.1\nSummary\nI have a task like this:\n    - name: Spinning up EC2 instances\n      ec2:\n        key_name: \"{{ instances_keypair }}\"\n        instance_type: \"{{ item.0.instance_type }}\"\n        image: \"{{ item.0.image }}\"\n        instance_tags: \"{{ item.0.instance_tags }}\"\n        exact_count: \"{{ item.1.exact_count }}\"\n        count_tag: \"{{ item.0.count_tag }}\"\n        vpc_subnet_id: \"{{ item.1.vpc_subnet_id }}\"\n        group: \"{{ item.0.group }}\"\n        volumes: \"{{ item.0.volumes }}\"\n        zone: \"{{ ec2_region + item.1.az }}\"\n        region: \"{{ ec2_region }}\"\n        wait: \"{{ item.0.wait|default(yes) }}\"\n        wait_timeout: \"{{ item.0.wait_timeout|default(300) }}\"\n      register: ec2\n      with_subelements:\n        - ec2_instances\n        - azs\n\n...where ec2_instances is:\nec2_instances:\n  # Memcache Servers\n  - type: memcache\n    instance_type: m3.medium\n    image: \"{{ image_id }}\"\n    group: ['private']\n    instance_tags:\n      Name: \"memcache-{{ env }}\"\n      role: memcache\n      environment: \"{{ env }}\"\n      memcache_environment: \"{{ env }}\"\n      deployment: ansible\n    volumes:\n      - device_name: /dev/sdp\n        # For any volume, a volume size less than 1 will be interpreted as a request not to create the volume.\n        volume_size: 0\n        delete_on_termination: true\n    count_tag:\n      role: memcache\n      environment: \"{{ env }}\"\n      memcache_environment: \"{{ env }}\"\n      deployment: ansible\n    azs:\n      - az: a\n        # Private Subnet ID in \"a\"\n        vpc_subnet_id: \"{{ subnets['a']['private'] }}\"\n        exact_count: 0\n      - az: b\n        # Private Subnet ID in \"b\"\n        vpc_subnet_id: \"{{ subnets['b']['private'] }}\"\n        exact_count: 1\n      - az: d\n        # Private Subnet ID in \"d\"\n        vpc_subnet_id: \"{{ subnets['d']['private'] }}\"\n        exact_count: 0\n      - az: e\n        # Private Subnet ID in \"e\"\n        vpc_subnet_id: \"{{ subnets['e']['private'] }}\"\n        exact_count: 0\n    wait: yes\n    wait_timeout: 360\n\n  #1 redis cluster (2 nodes)\n  - type: redis\n    instance_type: m3.medium\n    image: \"{{ image_id }}\"\n    group: ['private']\n    instance_tags:\n      Name: \"redis-{{ env }}\"\n      role: redis\n      environment: \"{{ env }}\"\n      redis_environment: \"{{ env }}\"\n      deployment: ansible\n    count_tag:\n      role: redis\n      environment: \"{{ env }}\"\n      redis_environment: \"{{ env }}\"\n      deployment: ansible\n    volumes:\n      - device_name: /dev/sdp\n        volume_size: 10\n        delete_on_termination: true\n    azs:\n      - az: a\n        # Private Subnet ID in \"a\"\n        vpc_subnet_id: \"{{ subnets['a']['private'] }}\"\n        exact_count: 1\n      - az: b\n        # Private Subnet ID in \"b\"\n        vpc_subnet_id: \"{{ subnets['b']['private'] }}\"\n        exact_count: 1\n      - az: d\n        # Private Subnet ID in \"d\"\n        vpc_subnet_id: \"{{ subnets['d']['private'] }}\"\n        exact_count: 0\n      - az: e\n        # Private Subnet ID in \"e\"\n        vpc_subnet_id: \"{{ subnets['e']['private'] }}\"\n        exact_count: 0\n\nWhen I execute the task - I face an error when we reach the block without defined (that redis cluster in my case):\n    wait: yes\n    wait_timeout: 360\n\nThe error is:\nfatal: [localhost]: FAILED! => {\"failed\": true, \"msg\": \"ERROR! 'yes' is undefined\"}\n\nExpected Results: workeable default() filter",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2018,
    "text": "Playbooks are runned over other group of hosts than listed by list-hosts\n\nIssue Type:\nBug report\nAnsible Version:\nstarting from commit ae9843f\nansible 1.5 is affected\nEnvironment:\nat last Debian 6\nSummary:\nPlease summarize your request in this space.  You will earn bonus points for being succinct, but please add enough detail so we can understand the request.\nSteps To Reproduce:\nrun\n\nansible-playbook -C site.yml -i hosts --list-hosts\nansible-playbook -C site.yml -i hosts\n\nusing files\nhosts:\n[physical]\nphost1\n\n[phost1]\nvm1.phost1\n\nsite.yml:\n\n---\n- hosts: physical\n  tasks:\n    - ping:\n\nExpected Results:\nGroups not declared as children of group 'physical' should not be involved.\nplaybook: site.yml\n\n  play #1 (physical): host count=1\n    phost1\n\n\nPLAY [physical] *************************************************************** \n\nTASK: [ping ] ***************************************************************** \nok: [phost1]\n\nPLAY RECAP ******************************************************************** \nphost1                     : ok=1    changed=0    unreachable=0    failed=0   \n\nActual Results:\nHosts from groups named after hosts included in group 'physical' (not declared as group's children) are involved by play.\nplaybook: site.yml\n\n  play #1 (physical): host count=1\n    phost1\n\n\nPLAY [physical] *************************************************************** \n\nTASK: [ping ] ***************************************************************** \nok: [vm1.phost1]\nok: [phost1]\n\nPLAY RECAP ******************************************************************** \nphost1                     : ok=1    changed=0    unreachable=0    failed=0   \nvm1.phost1                 : ok=1    changed=0    unreachable=0    failed=0",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2019,
    "text": "Variables from group_vars/all are not being picked up when using inventory script\n\nIn our case we have seen the above, I still need a minimal test-case to demonstrate this.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2020,
    "text": "Duplicated newline in Jinja2 variable when copy module is used\n\nIssue Type: Bug Report\nAnsible Version: ansible 1.7.2\nEnvironment: Arch Linux\nSummary: Variable content gets newlines duplicated when pasted into a file via the copy module\nSteps To Reproduce:\n\nAdd a step copy: content=\"{{ ssl_private_key }}\" dest=/etc/ssl/private/foo.pem in a playbook\nWrite the content of ssl_private_key in an ansible-vault vars file\nRun the playbook\nCheck if /etc/ssl/private/foo.pem will have an extra newline for each line in the ssl_private_key variable\n\nExpected Results: if the variable ssl_private_key is defined like this:\nssl_private_key: |\n   -----BEGIN PRIVATE KEY-----\n   MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0\n   MQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv\n   ZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ\n   -----END PRIVATE KEY-----\n\nI expect to have this in /etc/ssl/private/foo.pem:\n-----BEGIN PRIVATE KEY-----\nMIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0\nMQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv\nZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ\n-----END PRIVATE KEY-----\n\nAnd this is what I actually get with Ansible 1.7.1 and some previous versions.\nActual Results: But instead, with Ansible 1.7.2 the result will have duplicated \\n:\n-----BEGIN PRIVATE KEY-----\n\nMIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0\n\nMQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv\n\nZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ\n\n-----END PRIVATE KEY-----",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": null
  },
  {
    "id": 2021,
    "text": "synchronize action should maybe force the connection for hosts named localhost (edited title)\n\nIssue Type:\nBug Report\nAnsible Version:\nansible 1.6 (devel 317c2f4) last updated 2014/04/03 12:41:29 (GMT +200)\nEnvironment:\nopenSUSE 12.2 (x86_64)\nSummary:\nUsing the synchronize action while deploying to a VM listening on localhost confuses ansible as it tries to unnecessarily delegate to localhost, and also forgets to clear the port.\nSteps To Reproduce:\nI'm running a simple playbook to deploy some software to a VM running on localhost:2222.\nInventory file:\n[localvm]\nlocalhost ansible_ssh_user=root ansible_ssh_port=2222\n\ntasks/main.yml:\n- name: Install via rsync\n  synchronize: src=../pkg dest=/srv/pkg/\n\nThis fails because it tries to connect to 127.0.0.1:2222 with my current username. What is happening, apparently, is that it assumes I'm doing a delegate action and tries to connect to localhost with my current username, but retains the faulty port. There is no need to delegate at all.\nExpected Results:\nI'd assume it would simply run rsync locally, without delegation, to deploy to the VM.\nActual Results:\nIt fails because it tries to run an ssh connection to <username>@127.0.0.1 port 2222, which does not work because <username> does not exist on the VM, and port 2222 is the VM. It would succeed on port 22, but that's not necessary.\nThe following change also makes the above work for me, so it is indeed some bogus delegation business. (The second hunk is only necessary because the first change removes the conn.delegate attribute.)\n--- a/lib/ansible/runner/action_plugins/synchronize.py\n+++ b/lib/ansible/runner/action_plugins/synchronize.py\n@@ -81,7 +81,7 @@ class ActionModule(object):\n         self.transport_overridden = False\n\n         if inject.get('delegate_to') is None:\n-            inject['delegate_to'] = '127.0.0.1'\n+            # inject['delegate_to'] = '127.0.0.1'\n             # IF original transport is not local, override transport and disable sudo.\n             if self.original_transport != 'local':\n                 inject['ansible_connection'] = 'local'\n@@ -136,11 +136,11 @@ class ActionModule(object):\n\n         # CHECK DELEGATE HOST INFO\n         use_delegate = False\n-        if conn.delegate != conn.host:\n-            if 'hostvars' in inject:\n-                if conn.delegate in inject['hostvars'] and self.original_transport != 'local':\n-                    # use a delegate host instead of localhost\n-                    use_delegate = True\n+        # if conn.delegate != conn.host:\n+        #     if 'hostvars' in inject:\n+        #         if conn.delegate in inject['hostvars'] and self.original_transport != 'local':\n+        #             # use a delegate host instead of localhost\n+        #             use_delegate = True\n\n         # COMPARE DELEGATE, HOST AND TRANSPORT\n         process_args = False",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2022,
    "text": "Docker - dns argument not working\n\nIssue Type:\nBug Report\nAnsible Version:\nAnsible 1.7 (but had the same issue with 1.6.1 and 1.6)\nEnvironment:\nRunning ansible from Mac OS\nManaging Debian\nSummary:\nWith the docker module, using the dns argument does not add the dns to the container. (nothing's added to the container's /etc/resolv.conf)\nSteps To Reproduce:\n    - name: Create docker proxy\n      docker: image=ubuntu name=test hostname=test count=1 privileged=yes dns=127.0.0.1 command=/bin/bash\n\n\nExpected Results:\nssh containers_ip\ncat /etc/resolv.conf\n\nnameserver 127.0.0.1 should appear at the top of the file\nActual Results:\nnameserver 127.0.0.1 does not appear",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2023,
    "text": "--limit does not work with intersecting hostgroups\n\nIssue Type:\nBug Report, Feature Idea\nAnsible Version:\nansible 1.5.5\nEnvironment:\nN/A\nSummary:\nI'll be nice, if the --limit switch, will limit task execution to the given hostgroup, also when both hostgroups consists of the same hosts\nSteps To Reproduce:\nInventory:\n[webservers]\nwebAA\n\n[webservers-with-cache]\nwebAA\nwebBB\n\nPlaybook:\n---\n- hosts: webservers\n  remote_user: a\n  tasks:\n  - shell: id\n- hosts: webservers-with-cache\n  remote_user: b\n  tasks:\n  - shell: uname -a\n$ ansible-playbook  -v -i ... --limit webservers playbook.yml\nExpected Results:\nPLAY [webservers] *************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [webAA]\n\nTASK: [shell id] **************************************************************\nchanged: [webAA] => {\"changed\": true, \"cmd\": \"id \", \n\"delta\": \"0:00:00.004137\", \"end\": \"2014-05-05 18:31:23.507347\", \"rc\": 0,\n\"start\": \"2014-05-05 18:31:23.503210\", \"stderr\": \"\", \n\"stdout\": \"uid=1000(a) gid=1000(a) groups=1000(a)\"}\n\nPLAY RECAP ********************************************************************\nwebAA              : ok=2    changed=1    unreachable=0    failed=0\n\nActual Results:\nPLAY [webservers] *************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [webAA]\n\nTASK: [shell id] **************************************************************\nchanged: [webAA] => {\"changed\": true, \"cmd\": \"id \", \n\"delta\": \"0:00:00.004137\", \"end\": \"2014-05-05 18:31:23.507347\", \"rc\": 0, \n\"start\": \"2014-05-05 18:31:23.503210\", \"stderr\": \"\", \n\"stdout\": \"uid=1000(a) gid=1000(a) groups=1000(a)\"}\n\nPLAY [webservers-with-cache] **************************************************\n\nGATHERING FACTS ***************************************************************\nok: [webAA]\n\nTASK: [shell uname -a] ********************************************************\nchanged: [webAA] => {\"changed\": true, \"cmd\": \"uname -a \", \n\"delta\": \"0:00:00.002848\", \"end\": \"2014-05-05 18:31:25.982012\", \"rc\": 0, \n\"start\": \"2014-05-05 18:31:25.979164\", \"stderr\": \"\", \n\"stdout\": \"Linux ... 2.6.18-371.3.1.el5 #1 SMP Mon Nov 11 03:23:58 ES..\"}\n\nPLAY RECAP ********************************************************************\nwebAA              : ok=4    changed=2    unreachable=0    failed=0",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2024,
    "text": "sudo options in connection.py (CentOS 5.4, -u and -k incompatible?)\n\n「sudo: the `-u' and '-k' options may not be used together」\nI modified below, and fine.\nconnection.py\n        sudocmd = 'sudo -k -p \"%s\" -u %s -- \"$SHELL\" -c %s' % (prompt,\n\n        sudocmd = 'sudo -k;sudo -p \"%s\" -u %s -- \"$SHELL\" -c %s' % (prompt,",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2025,
    "text": "Template variable stuff broken\n\nYesterday everything was working fine, but after a git pull; make install my run borked big time.\nI got:\nAt revision 233.\nTraceback (most recent call last):\n  File \"/usr/bin/ansible-playbook\", line 172, in <module>\n    sys.exit(main(sys.argv[1:]))\n  File \"/usr/bin/ansible-playbook\", line 118, in main\n    subset=options.subset,\n  File \"/usr/lib/python2.6/site-packages/ansible/playbook/__init__.py\", line 119, in __init__\n    (self.playbook, self.play_basedirs) = self._load_playbook_from_file(playbook)\n  File \"/usr/lib/python2.6/site-packages/ansible/playbook/__init__.py\", line 169, in     _load_playbook_from_file\n    p['vars'].update(incvars)\nAttributeError: 'list' object has no attribute 'update'\n\nInvestigating this pointed me to my issue playbook, containing\n# vim:ff=unix ts=4 sw=4 ai expandtab\n# $Id: init.yml 209 2012-11-11 13:26:06Z tonk $\n\n---\n- hosts: all\n  tasks:\n      - name: deploy issue file\n        template: src=issue.in dest=/etc/issue owner=root mode=0444\n\nAnd the template containing\n  ------------------------------------------------------------------------------\n                               -- W A R N I N G --\n                  UNAUTHORIZED ACCESS STRICTLY PROHIBITED!!\n  ------------------------------------------------------------------------------\n           System Name : {{ \"%-25s\"|format(ansible_hostname) }} Location : {{ location }}\n           Managed by  : {{ \"%-25s\"|format(name)             }} Room     : {{ room }}\n  ------------------------------------------------------------------------------\n{% if issueremarks is defined %}\n{{ issueremarks.center(80) }}\n  ------------------------------------------------------------------------------\n{% endif %}\n\nWhen I roll back to the code of yesterday, things work again.\nIt does look as if an undefined variable is used things break. But that's what the is defined is for, so there seems to be a bug in the template stuff of last night.\nCould you look into that, please?",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2026,
    "text": "Hipchat Callback not working\n\nISSUE TYPE\n\nBug Report\n\nANSIBLE VERSION\nansible 2.1.0.0\n  config file = /root/ansible/ansible.cfg\n  configured module search path = ['modules']\n\nCONFIGURATION\n[defaults]\nlibrary = modules\nlog_path = /tmp/ansible.log\nroles_path = roles\ncallback_plugins = callbacks/\ndeprecation_warnings=False\ncallback_whitelist = hipchat\n\nOS / ENVIRONMENT\nCentOS7\nSUMMARY\nHipchat Callback: https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/callback/hipchat.py\nis not working.\nVars can not be set.\nSTEPS TO REPRODUCE\nEnable hipchat callback via ansible.cfg whitelisting.\nConfigure the required Hipchat ENV-Vars.\nRun any playbook, following error occurs:\nPLAY [Staging Packages] ********************************************************\n [WARNING]: Failure using method (v2_playbook_on_play_start) in callback plugin (</usr/lib/python2.7/site-packages/ansible/plugins/callback/hipchat.CallbackModule object at 0x31c4750>):\n'Play' object has no attribute 'playbook'\n [WARNING]: Failure using method (v2_playbook_on_stats) in callback plugin (</usr/lib/python2.7/site-packages/ansible/plugins/callback/hipchat.CallbackModule object at 0x2c4c750>):\n'CallbackModule' object has no attribute 'display'\n\nEXPECTED RESULTS\nMessage send to hipchat room.\nACTUAL RESULTS\nHipchat message not working\nMISC\nThe display error can be solved by changing the callback from:\nself.display.warning('\nto\nself._display.warning('",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2027,
    "text": "Add documentation for privilege_escalation/become to ansible.cfg\n\nBy looking at the example file here it looks as though the new group would look similar to this:\n[privilege_escalation]\nbecome=True\nbecome_method='sudo'\nbecome_user='root'\nbecome_ask_pass=False\n\nI can create a pull request to add that section to the docs, just wanted to confirm that will be the correct setup moving forward.\nDocs:\n\nBecome docs\nAnsible configuration file docs",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2028,
    "text": "Delegated Hosts Don't Use Inventory Details or Confirm Existence In Inventory\n\nI wasn't sure if this was a bug or intended behavior. When I think about the issue, it seems more intuitive that you'd use the same name that you would for a host in your inventory and expect Ansible to use the same connection details when running the task.\nIssue Type: Bug Report\nAnsible Version: 1.7\nEnvironment: Arch to drive, managing Ubuntu 12.04\nSummary:\nWhen running a task with delegate_to it doesn't look the provided host up in the inventory. You can pass it anything and it will try to SSH to it. Since that's the case it also doesn't properly look up the ansible_ssh_port for the delegated host.\nSteps To Reproduce:\nHosts File:\n(Using 127.0.0.1 as an example, but assume that's a WAN IP with a bunch of NAT one-to-many forwards.)\n host1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=22\n host2 ansible_ssh_host=127.0.0.1 ansible_ssh_port=22222\n\nPlaybook:\n- name: A Most Noble Reproduction\n  hosts: host1\n  remote_user: root\n\n  tasks:\n    - name: Do The Delegationating\n      shell: hostname\n      delegate_to: host2\n\n    - name: Do The Delegationating, Part Deux\n      shell: echo 'If you see this in your syslog maybe someone should disable passwordless root logins at the googles.'\n      delegate_to: google.com\n\nExpected Results:\nchanged: [host1 -> host2] => {\"changed\": true, \"cmd\": \"hostname\", \"stderr\": \"\", \"stdout\": \"host2\"}\nfatal: [host1 -> google.com] => Host google.com ain't all up in your inventory.\n\nActual Results:\nchanged: [host1 -> host2] => {\"changed\": true, \"cmd\": \"hostname\", \"stderr\": \"\", \"stdout\": \"host1\"}\nfatal: [host1 -> google.com] => SSH encountered an unknown error during the connection. We recommend you re-run the command using -vvvv, which will enable SSH debugging output to help diagnose the issue\n\nEdit: Seems likely related to Issue 8224.\nEdit2: Made the title better. Seemed ambiguous/not as explanatory as it could be.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2029,
    "text": "SSH connection got stuck when IP and host have different keys in known_hosts\n\nIn following situatation, I'd like to have some kind of error report, but now the connection just got stuck.\nI reinstalled one machine, and tried to use same IP and host after installation. The initial ansible connection reported nicely\n\nfatal: [node2-jenkins-slave.dev.sysart.fi] => Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this.  Please add this host's fingerprint to your known_hosts file to manage this host.\n\nAfter adding doing this and trying to connect, the connection got stuck\n\nansible -i hosts node2-jenkins-slave.dev.sysart.fi -m ping -u root -k -vvvv\nSSH password:\n<node2-jenkins-slave.dev.sysart.fi> ESTABLISH CONNECTION FOR USER: root\n<node2-jenkins-slave.dev.sysart.fi> REMOTE_MODULE ping\n<node2-jenkins-slave.dev.sysart.fi> EXEC sshpass -d6 ssh -C -tt -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/jyrki/.ansible/cp/ansible-ssh-%h-%p-%r\" -o GSSAPIAuthentication=no -o PubkeyAuthentication=no -o User=root -o ConnectTimeout=10 node2-jenkins-slave.dev.sysart.fi /bin/sh -c 'mkdir -p $HOME/.ansible/tmp/ansible-tmp-1431337315.21-244074993784678 && echo $HOME/.ansible/tmp/ansible-tmp-1431337315.21-244074993784678'\n\nI think that the reason for this had something to do with the warning I got with ssh\n\nssh node2-jenkins-slave.dev.sysart.fi\nWarning: the ECDSA host key for 'node2-jenkins-slave.dev.sysart.fi' differs from the key for the IP >address '192.168.179.43'\nOffending key for IP in /home/jyrki/.ssh/known_hosts:107\nMatching host key in /home/jyrki/.ssh/known_hosts:133\n\nAfter removing both keys and adding the host to known_hosts, everything worked.",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2030,
    "text": "Single * pattern not working\n\nIssue Type:\nBug Report\nAnsible Version:\n$ ansible --version\nansible 1.9.1\nconfigured module search path = None\nAnsible Configuration:\nI think I changed nothing to the base configuration.\nEnvironment:\nCentos 7\nSummary:\nIn the docs it says it is possible to use a single * pattern to target every host, but it doesn't seem to work for me.\nSteps To Reproduce:\n$ ansible * -m ping\nExpected Results:\nSame result as $ ansible all -m ping, which is working fine for me.\nActual Results:\nUsage: ansible <host-pattern> [options]",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2031,
    "text": "ansible 1.7 passing an argument with newlines to the shell module eats newlines (edited title)\n\nI'm  using quite a lot of multiline strings, and according to http://stackoverflow.com/questions/3790454/in-yaml-how-do-i-break-a-string-over-multiple-lines the way this works in YAML is to use | isntead of > to preserve newlines.\nThis worked in ansible 1.6.x without problems.\nNow, newlines are stripped away..\nExample:\n\n---\n- hosts: all\n  remote_user: root\n  tasks:\n  - name: bla\n    args:\n      executable: /bin/bash\n    shell: |\n      echo hi\n      echo hi2\n\nThis should print hi and hi2 in two lines but it doesn't anymore.\nIt breaks a lot of ansible code for me which, unfortunately, relies on shell scripts.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2032,
    "text": "C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.\n\nISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\n\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.\nANSIBLE VERSION\n\n\n\nCONFIGURATION\n\nOS / ENVIRONMENT\n\nSUMMARY\n\nSTEPS TO REPRODUCE\n\n\n\n\nEXPECTED RESULTS\n\nACTUAL RESULTS",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2033,
    "text": "Docs ansible-1.1: \"register\" example broken?\n\n[root@util01 ansible]# ansible-playbook --version\nansible-playbook 1.1\n[root@util01 ansible]# rpm -q ansible\nansible-1.1-1.el6.noarch\n[root@util01 ansible]# rpm -q --qf \"%{NAME} - %{VENDOR}\\n\" ansible\nansible - Fedora Project\n[root@util01 ansible]# which ansible-playbook\nalias ansible-playbook='ansible-playbook -v -i /root/ansible/hosts'\n        /usr/bin/ansible-playbook\n\nDescription:\nThe example [1] for register appears to be broken.\nSteps to reproduce:\n\nCopy example from [1] into a yaml file\nRun the file with ansible-playbook\n\n[root@util01 tmp]# grep \"hi\" /etc/motd\nThis motd has the word 'hi' in it\n[root@util01 tmp]# cat test_when.yaml\n- name: test play\n  hosts: util\n\n  tasks:\n\n      - action: shell cat /etc/motd\n        register: motd_contents\n\n      - action: shell echo \"motd contains the word hi\"\n        when: motd_contents.stdout.find('hi') != -1\n\nExpected Result:\nAnsible prints the message \"motd contains the word hi\"\nObserved result:\n[root@util01 tmp]# ansible-playbook test_when.yaml\nERROR: invalid usage of when_ operator: motd_contents.stdout.find('hi') != -1\n\nThis is failing when it gets to the final 'else' clause in compile_when_to_only_if. I added some debugging statements:\n    else:\n        print \"Tokens: \" + str(tokens)\n        print map(lambda t: type(t), tokens)\n        raise errors.AnsibleError(\"invalid usage of when_ operator: %s\" % expression)\n\nwhich outputs:\n[root@util01 tmp]# ansible-playbook test_when.yaml\nTokens: [\"motd_contents.stdout.find('hi')\", '!=', '-1']\n[<type 'str'>, <type 'str'>, <type 'str'>]\nERROR: invalid usage of when_ operator: motd_contents.stdout.find('hi') != -1\n\n[1] http://ansible.cc/docs/playbooks2.html#register-variables",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2034,
    "text": "New keyword for 'with_sequence' in order to skip task if count < 0 or start > end\n\nIssue Type: Feature request\nAnsible Version: 1.8.4\nEnvironment: N/A\nSummary:\nWhen using with_sequence \"programmatically\" (i.e., in combination with variables), it would be useful to allow a negative value for the count argument, or a start value that is larger than end. Currently this results in an error (\"can't count backwards\"), but it could be useful to simply skip the task if the range is empty. This could be achieved by adding an extra keyword, e.g. skip_with_emtpy_range (although it should probably be less verbose), which would be False by default for backwards compatibility.\nExample:\n---\n- hosts: all\n  gather_facts: False\n  vars:\n    - M: 2\n    - N: 1\n  tasks:\n    - name: This taks should be skipped because 'skip_with_empty_range' is True.\n      command: echo \"Hello world\"\n      with_sequence: start={{ M }} end={{ N }} skip_with_empty_range=True\n      #with_sequence: count={{ M - N }} skip_with_empty_range=True\nExpected Results:\nThe task should be skipped because skip_with_empty_range is True:\nTASK: [This taks should be skipped because 'skip_with_empty_range' is True.] *** \nskipping: [localhost]\n\nActual Results:\nCurrently the task fails with the error message \"can't count backwards\" (of course, skip_with_emtpy_range needs to be omitted when running the example above because it is not supported yet).\nTASK: [This taks should be skipped because 'skip_with_empty_range' is True.] *** \nfatal: [localhost] => can't count backwards\n\nFATAL: all hosts have already failed -- aborting",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2035,
    "text": "Could not create retry file '*.retry'. [Errno 2] No such file or directory: ''\n\nISSUE TYPE\n\nBug Report\n\nANSIBLE VERSION\nansible 2.1.1.0 (stable-2.1 4d4bbcbb33) last updated 2016/06/20 08:55:21 (GMT +200)\n\nCONFIGURATION\nN/A\nOS / ENVIRONMENT\nUbuntu\nSUMMARY\nIf retry_files_save_path isn't set in ansible.cfg, when a playbook fails, a retry files tries to be created in an empty directory.\nSTEPS TO REPRODUCE\nMake any playbook fail.\nEXPECTED RESULTS\nA try file should be created in the user home directory.\nACTUAL RESULTS\nA warning message is printed and a retry file is not created:\n [WARNING]: Could not create retry file 'test_fail.retry'.         [Errno 2] No such file or directory: ''\n\nIt looks like the default value for the retry files isn't set to be ~/ like it's mentioned in the documentation.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2036,
    "text": "Unable to use some environment variables\n\nISSUE TYPE: Bug report\nCOMPONENT NAME: ansible_env\nANSIBLE VERSION:\nansible 2.2.1.0\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = ['/usr/share/my_modules/', '/etc/ansible/roles/glassfish/library']\nCONFIGURATION:\ndefined values under [default]\nOS: Debian Jessie\nSUMMARY: Ansible is unable to interpolate an environment variable when it is called from a hosts file or a group_vars/all file.\nSTEPS TO REPRODUCE:\nIn /etc/profile.d/required_env_vars.sh --\nDEPLOY_DIR=/myhome/deployments\nIn testPlaybook.yml\ndeploy_dir = \"{{ ansible_env.DEPLOY_DIR }}/RT{{rt_number}}\"\nwhere rt_number is defined via vars_prompt\nThis fails because DEPLOY_DIR is not found in the output of the setup command for the host (looked in the ansible_env), but HOME is. So it looks like Ansible is unable to use environment variables defined in a profile.d script.\nAs a work-around, I added an [all:vars] section the inventories/poc/hosts file --\n[all:vars]\ndeploy_dir = $HOME/deployments --> did not work\ndeploy_dir = /myhome/deployments --> worked\ndeploy_dir = $DEPLOY_DIR --> did not work\ndeploy_dir = \"{{ ansible_env.DEPLOY_DIR }}\" --> did not work\nSince the second one worked (hardcoded value), I removed the [all:vars] section and used an inventories/poc/group_vars/all file --\ndeploy_dir = /myhome/deployments\nThis resulted in the error below --\nERROR! Unexpected Exception: dictionary update sequence element #0 has length 1; 2 is required\nthe full traceback was:\nTraceback (most recent call last):\nFile \"/usr/bin/ansible-playbook\", line 103, in \nexit_code = cli.run()\nFile \"/usr/lib/python2.7/dist-packages/ansible/cli/playbook.py\", line 132, in run\ninventory = Inventory(loader=loader, variable_manager=variable_manager, host_list=self.options.inventory)\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 98, in init\nself.parse_inventory(host_list)\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 165, in parse_inventory\ngroup.vars = combine_vars(group.vars, self.get_group_variables(group.name))\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 555, in get_group_variables\nself._vars_per_group[groupname] = self._get_group_variables(groupname, vault_password=vault_password)\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 573, in _get_group_variables\nvars = combine_vars(vars, self.get_group_vars(group))\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 775, in get_group_vars\nreturn self._get_hostgroup_vars(host=None, group=group, new_pb_basedir=new_pb_basedir, return_results=return_results)\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 839, in _get_hostgroup_vars\nhost_results = self._variable_manager.add_group_vars_file(base_path, self._loader)\nFile \"/usr/lib/python2.7/dist-packages/ansible/vars/init.py\", line 619, in add_group_vars_file\ndata = self._load_inventory_file(path, loader)\nFile \"/usr/lib/python2.7/dist-packages/ansible/vars/init.py\", line 577, in _load_inventory_file\nrval.update(data)\nValueError: dictionary update sequence element #0 has length 1; 2 is required\nOn my target server, I also tried to set up DEPLOY_DIR as an environment variable in various places --\n\n/etc/environment -- which is not preferred because it can't do variable interpolation (only key=val pairs, no variables on the right hand side). The output of the env command showed the variables I defined as expected.\n/etc/profile -- The output of the env command showed the variables defined also.\n/etc/profile.d/required_env_vars.sh -- this is preferred because I'd like to keep /etc/profile the same across all servers, and just add custom environment variables via profile.d script. The output of the env command DID NOT show the variables defined, but doing \"echo $variable\" showed the expected value\n\nNone of these affected the content of ansible_env (I thought ansible_env would get the variables if the were returned by the \"env\" command?).\nSo it looks like Ansible does not include all the environment variables defined in its \"environment\" (ansible_env).\nUPDATE 1\nOkay. My bad. I changed the content of group_vars/all to use a colon instead of an equal and it worked.\ndeploy_dir : /home/b013000915/deployments\nGot confused because in some cases (e.g. facts.d content, some modules), I had to use an equal sign.\nBut the other issue (not getting some of the environment variables defined) still persists.\nUPDATE 2\nI also tried using lookup to get the value of the following variables on the target node but it did not work for custom variables (defined via /etc/profile)\nIn /etc/profile\nDEPLOY_DIR=/myhome/deployments\nIn testPlaybook.yml\n\nname: check using lookup env\ndebug:\nmsg: \"deploy_dir is {{ lookup('env', 'DEPLOY_DIR') }}, home is {{ lookup('env', 'HOME') }}\"\n\nOutput was\nok: [IP address] => {\n\"msg\": \"deploy_dir is , home is /myhome\"\n}",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2037,
    "text": "--limit isn't honored at all\n\nIssue Type:\nBug Report\nAnsible Version:\n$ ansible --version\nansible 2.0.0 (devel c30e464388) last updated 2015/09/22 16:21:32 (GMT +200)\n  lib/ansible/modules/core: (detached HEAD 59afecace4) last updated 2015/09/22 16:21:46 (GMT +200)\n  lib/ansible/modules/extras: (detached HEAD dee690d7f4) last updated 2015/09/22 16:21:59 (GMT +200)\n  config file = \n  configured module search path = None\n\nAnsible Configuration:\nn/a\nEnvironment:\nn/a\nSummary:\nI have a simple inventory with four hosts (master1, master2, slave1, slave2) in two different groups (app-server, app-slave). When I use a host-pattern to select one group (e.g. app-server) and specify a host with --limit, ansible just selects all hosts. It does not matter if the host is contained in the selected group.\nSteps To Reproduce:\nhosts:\n[app-server]\nmaster1\nmaster2\n\n[app-slave]\nslave1\nslave2\n\nRun some module with this inventory, use a host-pattern and limit it with a host. E.g:\nansible -i hosts -m debug -a \"msg=foo\" app-slave -l master1\nansible -i hosts -m debug -a \"msg=foo\" app-slave -l slave1\nExpected Results:\nNo hosts matched\n\nActual Results:\n$ ansible -i hosts -m debug -a \"msg=foo\" app-slave -l master1\nslave1 | SUCCESS => {\n    \"changed\": false, \n    \"msg\": \"foo\"\n}\nslave2 | SUCCESS => {\n    \"changed\": false, \n    \"msg\": \"foo\"\n}\n\n$ ansible -i hosts -m debug -a \"msg=foo\" app-slave -l slave1\nslave1 | SUCCESS => {\n    \"changed\": false, \n    \"msg\": \"foo\"\n}\nslave2 | SUCCESS => {\n    \"changed\": false, \n    \"msg\": \"foo\"\n}",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2038,
    "text": "Update, remove or migrate CODING_GUIDELINES.md\n\nISSUE TYPE\n\nDocumentation Report\n\nCOMPONENT NAME\nCODING_GUIDELINES.md\nANSIBLE VERSION\nN/A\n\nCONFIGURATION\nN/A\nOS / ENVIRONMENT\nN/A\nSUMMARY\nThe CODING_GUIDELINES.md file is extremely out of date.  It contains information about coding style, tests, etc.\nI bean looking into this document, but found the amount of information needing updating a bit daunting for me to undertake at the moment.\nThis file should either be updated, removed and potentially migrated into the docsite.\nSTEPS TO REPRODUCE\nN/A\nEXPECTED RESULTS\nN/A\nACTUAL RESULTS\nN/A",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2039,
    "text": "search filter on an undefined variable returns a non-descriptive error\n\nISSUE TYPE\nBug Report\nCOMPONENT NAME\ncore\nANSIBLE VERSION\n2.1\nCONFIGURATION\nOS / ENVIRONMENT\nSUMMARY\nIn templates\nTrying to use this template fails with a non-descriptive error when STRINGG is undefined:\n{% if STRINGG | search('abc') %}\nworks!\n{% endif %}\nThe error is:\nTypeError: expected string or buffer\n\nThe error should be:\nAnsibleUndefinedVariable: One or more undefined variables: 'STRINGG' is undefined\n\nIn plays\nwhen: \"undefined | search('abc')\"\nThe error is:\nFailed to template {% if undefined | search('abc') %} True {% else %} False {% endif %}: an unexpected type error occurred. Error was expected string or buffer\n\nSTEPS TO REPRODUCE\nEXPECTED RESULTS\nACTUAL RESULTS",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2040,
    "text": "apt_repository: Failed to validate the SSL certificate for launchpad.net:443\n\nIssue Type:\nBug Report\nAnsible Version:\nansible 1.8.2\nconfigured module search path = /usr/share/ansible\nEnvironment:\nRunning from: Linux Mint 17.1 (based on Ubuntu 14.04)\nManaging: Ubuntu 10.04, 12.04\nSummary:\nAdding a PPA with the apt_repository module fails with certificate validation problems.\nManually running add-apt-repository on the host works.\nSteps To Reproduce:\n- name: \"PHP: Add PHP 5.4 PPA\"\n  apt_repository:\n    repo: ppa:ondrej/php5-oldstable\nExpected Results:\nPPA added under /etc/apt/sources.list.d/\nActual Results:\nTASK: [php | PHP: Add PHP 5.4 PPA] ******************************************** \nfailed: [host.example.com] => {\"failed\": true}\nmsg: Failed to validate the SSL certificate for launchpad.net:443. Use validate_certs=no or make sure your managed systems have a valid CA certificate installed. Paths checked for this platform: /etc/ssl/certs, /etc/pki/ca-trust/extracted/pem, /etc/pki/tls/certs, /usr/share/ca-certificates/cacert.org, /etc/ansible",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2041,
    "text": "scp_if_ssh parameter ignored in ansible 2.0.1.0\n\nIssue Type:\n\nBug Report\n\nAnsible Version:\nansible 2.0.1.0\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = /home/share/library\nAnsible Configuration:\n$ cat /etc/ansible/ansible.cfg | grep scp\nif True, make ansible use scp if the connection type is ssh\nscp_if_ssh = True\nEnvironment:\nN/A\nSummary:\nParameter scp_if_ssh is set to True in ansible.cfg. It fails to connect to host with error message \"unable to open an sftp connection\". It shouldn't be attempting an sftp connection.\nSteps To Reproduce:\nRun the following on a system that rejects sftp connections:\nansible  -m setup\n\n$ ansible-playbook service-pack-6.2.x.yml \n\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nfatal: [GoldenBoy]: FAILED! => {\"failed\": true, \"msg\": \"failed to open a SFTP connection (Channel closed.)\"}\n\nNO MORE HOSTS LEFT *************************************************************\n    to retry, use: --limit @service-pack-6.2.x.retry\n\nPLAY RECAP *********************************************************************\nGoldenBoy                  : ok=0    changed=0    unreachable=0    failed=1\n$ ansible-playbook service-pack-6.2.x.yml \n\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nfatal: [GoldenBoy]: FAILED! => {\"failed\": true, \"msg\": \"failed to open a SFTP connection (Channel closed.)\"}\n\nNO MORE HOSTS LEFT *************************************************************\n    to retry, use: --limit @service-pack-6.2.x.retry\n\nPLAY RECAP *********************************************************************\nGoldenBoy                  : ok=0    changed=0    unreachable=0    failed=1\n\n\nWhen I uninstall ansible and reinstall 1.9.4 with no configuration changes setup works.\n**$ pip uninstall ansible\n<< ommitted >>\n$ pip install ansible==1.9.4\n<<ommitted>>\n$ ansible --version\nansible 1.9.4\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = /home/share/library\n$ ansible tigh -m setup\ntigh | success >> {\n    \"ansible_facts\": {\n        \"ansible_all_ipv4_addresses\": [\n...\n}, \n    \"changed\": false\n}\n\nPlaybook works as well\n\n$ ansible-playbook service-pack-6.2.x.yml \n\nPLAY [GoldenBoy] ************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [GoldenBoy]\n\nTASK: [debug var=inventory_hostname] ****************************************** \nok: [GoldenBoy] => {\n    \"var\": {\n        \"inventory_hostname\": \"GoldenBoy\"\n    }\n}\n\nTASK: [debug var=ansible_host] ************************************************ \nok: [GoldenBoy] => {\n    \"var\": {\n        \"ansible_host\": \"ansible_host\"\n    }\n}\n\nTASK: [revert to snapshot] **************************************************** \nchanged: [GoldenBoy -> 127.0.0.1]\n...\n\n\nExpected Results:\nIt uses scp to succeed.\nActual Results:\nIt attempts to connect with SFTP.\n$ ansible GoldenBoy -m setup\nGoldenBoy | FAILED! => {\n    \"failed\": true, \n    \"msg\": \"failed to open a SFTP connection (Channel closed.)\"\n}",
    "annotations": [{ "label": 140, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2042,
    "text": "Allow multiple vault passwords/files\n\nvault password could keep prompting until empty password is supplied, vault file could take a list of files\nThis allows for having multiple vault files with different keys, good for ops team having access to all vaults but qa or dev having access only to specific ones",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2043,
    "text": "Let the inventory file location be a directory\n\nIf the file is a directory, run all items within it, whether script or INI file, and blend the results.\nThis will allow the inventory directory to be used in conf.d form, or even one group per file.\nIt will also allow for hybrid EC2/local inventory, etc.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2044,
    "text": "Apt module does not work well when specifying version along with with_items loop\n\nWhen trying to install several packages with this task:\n- name: Install deb packages\n  apt: name={{ item }}=1.0*\n       update_cache=yes\n       state=present\n       force=yes\n  with_items:\n    - package1\n    - package2\n\nOnly package2 (last one in the loop) is forced to install with 1.0 version. Package1 is installed with the latest version found on the repository. Running the playbook with \"-vvvv\" I can se how the command is eventually executed:\nREMOTE_MODULE apt name=package1,package2=1.0* update_cache=yes state=present force=yes\nI got it working by rewriting the task as following:\n- name: Install deb packages\n  apt: name={{ item }}\n       update_cache=yes\n       state=present\n       force=yes\n  with_items:\n    - package1=1.0*\n    - package2=1.0*",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2045,
    "text": "Debug task is run even though dependent task is skipped\n\nI found a case in ansible 1.9.4 (also present in 1.9.2) where a debug task with when: whatever|success runs even though the task that registers whatever was skipped.\nI was able to reproduce with this playbook:\n- hosts: all\n  user: cloud-user\n\n  tasks:\n    - name: command that always fails\n      shell: /bin/false\n      ignore_errors: yes\n      register: false_command\n\n    - name: dummy git task\n      git: accept_hostkey=yes\n      when: false_command|success\n      register: dummy_git\n\n    - name: dummy debug\n      debug:\n        msg: \"This should never happen\"\n      when: dummy_git|success\n\nwhich results in the following output:\n$ ansible-playbook -i 10.60.3.31, ansible_problem.yml \n\nPLAY [all] ******************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [10.60.3.31]\n\nTASK: [command that always fails] ********************************************* \nfailed: [10.60.3.31] => {\"changed\": true, \"cmd\": \"/bin/false\", \"delta\": \"0:00:00.002888\", \"end\": \"2015-11-13 11:26:25.460150\", \"rc\": 1, \"start\": \"2015-11-13 11:26:25.457262\", \"warnings\": []}\n...ignoring\n\nTASK: [dummy git task] ******************************************************** \nskipping: [10.60.3.31]\n\nTASK: [dummy debug] *********************************************************** \nok: [10.60.3.31] => {\n    \"msg\": \"This should never happen\"\n}\n\nPLAY RECAP ******************************************************************** \n10.60.3.31                 : ok=3    changed=1    unreachable=0    failed=0",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2046,
    "text": "yum: ansible not able to find package\n\nansible-playbook/yum module isn't able to find a package that I can find manually. This issue 'goes away' if I install the package manually.\n[wes@mgmt001 ~]$ ssh root@host.domain yum clean all\nLoaded plugins: fastestmirror\nCleaning up Everything\nCleaning up list of fastest mirrors\n[wes@mgmt001 ~]$ ssh root@host.domain yum list epel-release\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\nAvailable Packages\nepel-release.noarch                        5-4                        private-repository\n[wes@mgmt001 ~]$ ansible-playbook -i /tmp/all_hosts_badtz -f 30 ~/ansible/playbooks/company/company.yml\n\nTASK: [yum: install epel-release] *********************\n\nfailed: [host.domain] => {\"changed\": false, \"failed\": true, \"msg\": \"No Package matching 'epel-release' found available, installed or updated\"}\nFrom playbook:\n\nname: \"yum: install epel-release\"\naction: yum pkg=epel-release state=latest",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2047,
    "text": "Wrong temp path OSX\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\ncore\nANSIBLE VERSION\nansible 2.3.0.0\n  config file = /Users/mateus/ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, Apr  4 2017, 08:47:57) [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)]\n\nCONFIGURATION\nask_become_pass=True\nask_sudo_pass=True\nlocal_tmp = /Users/mateus/.ansible/tmp\nOS / ENVIRONMENT\nmacOS 101.12.5\nSUMMARY\nFor some reason Ansible is trying to create a temp file on the wrong directory, since macOS uses /Users/ instead of /home/.\nMy $HOME variable is fine (/Users/mateus/)\nAlready tried setting local_tmp and remote_tmp.\nAs you can see below, the ControlPath is right, only the mkdir is wrong\nACTUAL RESULTS\n<[HIDDEN_IP]> ESTABLISH SSH CONNECTION FOR USER: mateus\n<[HIDDEN_IP]> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o 'IdentityFile=\"/Users/mateus/.ssh/id_rsa\"' -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=mateus -o ConnectTimeout=10 -o ControlPath=/Users/mateus/.ansible/cp/6cf90d6e66 [HIDDEN_IP] '/bin/sh -c '\"'\"'echo ~ && sleep 0'\"'\"''\n<[HIDDEN_IP]> (0, '/home/mateus\\n', '')\n<[HIDDEN_IP]> ESTABLISH SSH CONNECTION FOR USER: mateus\n<[HIDDEN_IP]> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o 'IdentityFile=\"/Users/mateus/.ssh/id_rsa\"' -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=mateus -o ConnectTimeout=10 -o ControlPath=/Users/mateus/.ansible/cp/6cf90d6e66 [HIDDEN_IP] '/bin/sh -c '\"'\"'( umask 77 && mkdir -p \"` echo /home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629 `\" && echo ansible-tmp-1495220924.36-187159279265629=\"` echo /home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629 `\" ) && sleep 0'\"'\"''\n<[HIDDEN_IP]> (1, '', 'mkdir: cannot create directory \\xe2\\x80\\x98/home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629\\xe2\\x80\\x99: No space left on device\\n')",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2048,
    "text": "async with command not working\n\nIssue Type:\n\nbug\n\nAnsible Version:\nansible 2.0.0.2\n\nAnsible Configuration:\ndefault\nEnvironment:\nOS X Yosemite 10.10.4\n\nSummary:\nWhen doing async reboot using ansible, if I call /sbin/shutdown directly, it works.\nHowever if I ran /bin/sleep first then, nothing happens.\nSteps To Reproduce:\n- name: Reboots machine to new kernel (async)\n  command: /bin/sleep 5 && /sbin/shutdown -r now \"Reboot triggered by Ansible\"\n  async: 1\n  poll:  0\n  ignore_errors: true\n\nsyslog:\nJan 22 19:21:44 example ansible-async_wrapper: Starting module and watcher\nJan 22 19:21:44 example ansible-async_wrapper: Start watching 1282 (1)\nJan 22 19:21:44 example ansible-async_wrapper: Start module (1282)\nJan 22 19:21:44 example ansible-command: Invoked with warn=True executable=None chdir=None _raw_params=/bin/sleep 5 && /sbin/shutdown -r now \"Reboot triggered by Ansible\" removes=None creates=None _uses_shell=False\nJan 22 19:21:44 example ansible-async_wrapper: Module complete (1282)",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2049,
    "text": "Allow group_vars and host_vars to override role variables\n\nISSUE TYPE\n\n\nFeature Idea\n\nCOMPONENT NAME\n\nVariable Precedence\nANSIBLE VERSION\n\nansible 2.1.1.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\nCONFIGURATION\n\nN/A\nOS / ENVIRONMENT\n\nN/A\nSUMMARY\n\nChange Ansible's variable precedence so that groups_vars and host_vars can override all role variables. Roles should be considered standalone or isolated modules and maintain there own variable precedence. Roles can use conditions to set OS specific variables in their vars folder. All variables defined in a role should be able to be overridden outside of the role by group_vars and host_vars. This would take care of the use case for being able to set OS specific variable defaults in a role and being able to override them.\nSTEPS TO REPRODUCE\n\n\n\n\n\nEXPECTED RESULTS\n\nACTUAL RESULTS",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2050,
    "text": "Different behavior for variables used in an included task in a dependent role since 2.0.2.0\n\nISSUE TYPE\nBug Report\nCOMPONENT NAME\nroles\nANSIBLE VERSION\nansible 2.2.0 (devel d8a3feb976) last updated 2016/07/19 150214 (GMT -700)\nlib/ansible/modules/core (detached HEAD 7de287237f) last updated 2016/07/15 125903 (GMT -700)\nlib/ansible/modules/extras (detached HEAD 68ca157f3b) last updated 2016/07/15 125903 (GMT -700)\nconfig file =\nconfigured module search path = Default w/o overrides\n\nCONFIGURATION\nOS / ENVIRONMENT\nN/A\nSUMMARY\nThis one is really similar to #16729\nWhen a dependency role (configured with allow_duplicates ) uses a variable set by a dependent role inside an included task and multiple role are using the same dependency role, the dependency role always use the variable defined in the last called role\nSTEPS TO REPRODUCE\nCreate a playbook\nAdd two role (role1 and role2) with a dependency on another role configured with allow_duplicates: yes (common-role)\nUse a variable ( app_name) in an included task in the common role which is defined in role1/vars/main.yml and role2/vars/main.yml\nDisplay the var value in a debug task inside the common role\nDirectory structure\n roles\n     common-role\n        meta\n           included.yml\n           main.yml\n        tasks\n            main.yml\n     role1\n        meta\n           main.yml\n        vars\n              main.yml\n     role2\n         meta\n            main.yml\n         vars\n             main.yml\n\nplaybook.yml\n  - hosts: localhost\n    roles:\n      - { role: role1}\n      - { role: role2 }\n\nrole/common-role/meta/main.yml\nallow_duplicates: yes\n\nroles/common-role/tasks/included.yml\n- name: \"debug {{app_name}}\"\n  debug: msg=\"apps_name --- {{app_name}}\"\n\nroles/common-role/tasks/main.yml\n- include: tasks/included.yml\n\nroles/role1/meta/main.yml\ndependencies:\n  - { role: common-role }\n\nroles/role1/vars/main.yml\napp_name: role1AppName\n\nroles/role2/meta/main.yml\ndependencies:\n  - { role: common-role }\n\nroles/role2/vars/main.yml\napp_name: role2AppName\n\nEXPECTED RESULTS\nansible 2.0.1.0 ( and the previous versions) was producing this :\nPLAY ***************************************************************************\nTASK [setup] *******************************************************************\nok: [localhost]\nTASK [common-role : include] ***************************************************\nincluded: /Users/maximederavet/Development/temp/inheritancebug-ansible/roles/common-role/tasks/included.yml for localhost\nTASK [common-role : debug included role1AppName] *******************************\nok: [localhost] => {\n    \"msg\": \"apps_name included --- role1AppName\"\n}\nTASK [common-role : include] ***************************************************\nincluded: /Users/maximederavet/Development/temp/inheritancebug-ansible/roles/common-role/tasks/included.yml for localhost\nTASK [common-role : debug included role2AppName] *******************************\nok: [localhost] => {\n    \"msg\": \"apps_name included --- role2AppName\"\n}\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=5    changed=0    unreachable=0    failed=0\n\nACTUAL RESULTS\nansible 2.0.2+ produces this :\nPLAY [localhost] ***************************************************************\nTASK [setup] *******************************************************************\nok: [localhost]\nTASK [common-role : debug included role2AppName] *******************************\nok: [localhost] => {\n    \"msg\": \"apps_name included --- role2AppName\"\n}\nTASK [common-role : debug included role2AppName] *******************************\nok: [localhost] => {\n    \"msg\": \"apps_name included --- role2AppName\"\n}\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=3    changed=0    unreachable=0    failed=0\n\ntested with ansible 2.0.2.0 , the stable-2.1 branch and the devel branch, same results.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2051,
    "text": "Wait_for should return matches to groups in its search_regex\n\nISSUE TYPE\n\nFeature Idea\n\nCOMPONENT NAME\nwait_for\nANSIBLE VERSION\n2.3\n\nCONFIGURATION\nn/a\nOS / ENVIRONMENT\nn/a\nSUMMARY\nWhen using the wait_for module to monitor, say, a file for a string matching a particular search_regex, it would be helpful if the matches to that regex were available in the module's output.\nIn my current use case, I'm monitoring a log file from a remote process which was started asynchronously and want to display certain pertinent information from it in the Ansible output.\nSTEPS TO REPRODUCE\n\n - name: Wait for message in log file\n    wait_for:\n        path: \"somefile.log'\n        search_regex: \"SOMETHING_HAPPENED start_delimiter(.*)end_delimiter\"\n    register: foo\n\n - name: show text between delimiters\n    debug: msg=\"The text between delimiters was {{foo.result.matches[0]}}\"\nEXPECTED RESULTS\nThe wait_for task will pause until text matching the search_regex is found in somefile.log (or timeout occurs).  The text between the parens in the regex will be available somewhere in the variable registered in wait_for task.\nACTUAL RESULTS\nThe matches are not available in the registered variable.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2052,
    "text": "broken plugins cause UnboundLocalError: local variable 'obj' referenced before assignment\n\nISSUE TYPE\n\nBug Report\n\nANSIBLE VERSION\n2.2 devel\n\nCONFIGURATION\nN/A\nOS / ENVIRONMENT\nN/A\nSUMMARY\nPluginLoader.all() creates a variable named \"obj\" in try/except, raises a warning if failed and then attempts to use the variable later even though it may not exist.\nSTEPS TO REPRODUCE\nCreate a bad plugin file in \"filter_plugins\" and run a playbook with a debug: var=\nmkdir filter_plugins\ntouch filter_plugins/foobar.py\n\nEXPECTED RESULTS\nNo traceback, just a warning.\nACTUAL RESULTS\ntask path: .../tasks/main.yml:57\n[WARNING]: Skipping plugin (....library/<BROKEMODULE>.py) as it seems to be invalid: 'module' object has no attribute 'FilterModule'\n\nAn exception occurred during task execution. The full traceback is:\nTraceback (most recent call last):\n File \"/usr/local/lib/python2.7/dist-packages/ansible/executor/task_executor.py\", line 124, in run\n   res = self._execute()\n File \"/usr/local/lib/python2.7/dist-packages/ansible/executor/task_executor.py\", line 401, in _execute\n   self._task.post_validate(templar=templar)\n File \"/usr/local/lib/python2.7/dist-packages/ansible/playbook/task.py\", line 246, in post_validate\n   super(Task, self).post_validate(templar)\n File \"/usr/local/lib/python2.7/dist-packages/ansible/playbook/base.py\", line 317, in post_validate\n   value = templar.template(getattr(self, name))\n File \"/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py\", line 358, in template\n   d[k] = self.template(variable[k], preserve_trailing_newlines=preserve_trailing_newlines, fail_on_undefined=fail_on_undefined, overrides=overrides)\n File \"/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py\", line 330, in template\n   result = self._do_template(variable, preserve_trailing_newlines=preserve_trailing_newlines, escape_backslashes=escape_backslashes, fail_on_undefined=fail_on_undefined, overrides=overrides)\n File \"/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py\", line 467, in _do_template\n   myenv.filters.update(self._get_filters())\n File \"/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py\", line 186, in _get_filters\n   plugins = [x for x in self._filter_loader.all()]\n File \"/usr/local/lib/python2.7/dist-packages/ansible/plugins/__init__.py\", line 388, in all\n   obj = obj(*args, **kwargs)\nUnboundLocalError: local variable 'obj' referenced before assignment",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2053,
    "text": "vmware_guest doesn't create a properly formatted /etc/resolv.conf file\n\nI'm trying Ansible 2.3 RC2\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nvmware_guest: customization: dns_servers\nANSIBLE VERSION\n/usr/lib64/python2.6/site-packages/cryptography/__init__.py:26: DeprecationWarning: Python 2.6 is no longer supported by the Python core team, please upgrade your Python. A future version of cryptography will drop support for Python 2.6\n  DeprecationWarning\nansible 2.3.0.0\n  config file = /opt/ansible/.ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.6.6 (r266:84292, Aug 18 2016, 15:13:37) [GCC 4.4.7 20120313 (Red Hat 4.4.7-17)]\n\nCONFIGURATION\nforks=10\n\nOS / ENVIRONMENT\nCentos 6.8\n\nSUMMARY\nI want to provision a VMWare VM and  populate /etc/resolv.conf, so I've puth this in my playbook:\n- vmware_guests:\n      customization:\n        domain: \"{{ guest_domain }}\"\n        dns_servers: \"{{ guest_dns_servers }}\"\n        dns_suffix: \"{{ guest_dns_suffix }}\"\n\nAnd defined this variable in a group_vars file\nguest_dns_servers:\n- 10.75.228.65\n  10.75.228.66\n\nBut I get a wrongly formatted /etc/resolv.conf file:\n[ansible@ansclient111 etc]$ cat resolv.conf\nsearch  mydomain.com\nnameserver      10.75.228.65 10.75.228.66\n\nInstead of getting one nameserver line per server, I get all of them in a single line.\nSTEPS TO REPRODUCE\nProvision the VM using the aforementioned playbook/group_vars file\nEXPECTED RESULTS\nGet a resolv.conf file like this:\nsearch  mydomain.com\nnameserver 10.75.228.65\nnameserver 10.75.228.66\n\nACTUAL RESULTS\nGot a resolv.conf file like this:\nsearch  mydomain.com\nnameserver      10.75.228.65 10.75.228.66",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2054,
    "text": "Unable to manage windows server after binding an IIS https site with option \"Require Server Name Identification\"\n\nISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\n\nANSIBLE VERSION\n\nansible 2.2.1.0\n  config file = /home/rui/test/ansible.cfg\n  configured module search path = Default w/o overrides\n\nCONFIGURATION\n\ninventory      = ./hosts\nOS / ENVIRONMENT\nrunning Ansible from: Linux ubuntu16_Ansible 4.4.0-59-generic\nmanaging: Windows 2016 with IIS\nSUMMARY\nUnable to manage windows server after binding an IIS https site with option \"Require Server Name Identification\"\nSTEPS TO REPRODUCE\n\nOn the Windows 2016 machine there is a IIS web site with binding for https on port 443 using a certificate.\nThe ansible commands and playbooks are able to manage this machine.\nAfter I change the binding and select the option \"Require Server Name Identification\" (required to use a single IP address to service multiple sites with certificates using \"host name\") ansible stops communication with server.\n\nI can only restore communication after I remove the binding and restart the windows server.\n\nansible win2016gui2 -m win_ping -vvvv\n\nEXPECTED RESULTS\nwin2016gui2 | SUCCESS => {\n\"changed\": false,\n\"ping\": \"pong\"\n}\nACTUAL RESULTS\n\n\nUsing /home/rui/test/ansible.cfg as config file\nLoading callback plugin minimal of type stdout, v2.0 from /usr/lib/python2.7/dist-packages/ansible/plugins/callback/__init__.pyc\nUsing module file /usr/lib/python2.7/dist-packages/ansible/modules/core/windows/win_ping.ps1\n<192.168.233.141> ESTABLISH WINRM CONNECTION FOR USER: administrator on PORT 5986 TO 192.168.233.141\nwin2016gui2 | UNREACHABLE! => {\n    \"changed\": false,\n    \"msg\": \"ssl: (\\\"bad handshake: SysCallError(104, 'ECONNRESET')\\\",)\",\n    \"unreachable\": true\n}",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2055,
    "text": "ansible_memfree_mb fact combines disk-cache use of memory\n\nIssue Type: Bug Report\nAnsible Version: ansible 1.6.3\nEnvironment: N/A\nSummary:\nI setup a play to alert whenever memory utilization is dangerously high. I immediately received a handful of alerts and was a bit concerned until I realized it takes into account memory being used by the disk cache and not 'real' memory utilization.\nSteps To Reproduce:\nTo reproduce just compare the ansible_memfree_mb fact, which in my example is\n        \"ansible_memfree_mb\": 10550,\n        \"ansible_memtotal_mb\": 15042,\n\nwith the return value of running\nfree -m\n\nExpected Results: I expected 'real' memory available\nActual Results:\nIn my case returns\n             total       used       free     shared    buffers     cached\nMem:         15042       4483      10559        139        231       2859\n-/+ buffers/cache:       1393      13649\nSwap:            0          0          0\n\nAs you can see, the ansible_facts are correct but misleading, in this case saying only 10gb are free when in fact I have 13gb that are useable.\nPerhaps this could be added as a ansible_nocache_memfree_mb fact?",
    "annotations": [{ "label": 143, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2056,
    "text": "Include inventory modules in distribution\n\nCurrently inventory modules are only available on github.\nIt would be cool if they could be bundled in with the release so they can be used without needing to be separately downloaded",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2057,
    "text": "Missing `xsltproc` Debian packaging README\n\npackaging/debian/README.md does not include installation of xsltproc which provides missing local eponym command.\nmake deb\n[...]\na2x -D docs/man/man1/ -d manpage -f manpage docs/man/man1/ansible.1.asciidoc\na2x: WARNING: --destination-dir option is only applicable to HTML based outputs\na2x: ERROR: \"xsltproc\"  --stringparam callout.graphics 0 --stringparam navig.graphics 0 --stringparam admon.textlabel 1 --stringparam admon.graphics 0  \"/etc/asciidoc/docbook-xsl/manpage.xsl\" \"<snip>/docs/man/man1/ansible.1.xml\" returned non-zero exit status 127\nMakefile:117: recipe for target 'docs/man/man1/ansible.1' failed\nmake: *** [docs/man/man1/ansible.1] Error 1\nrm docs/man/man1/ansible.1.asciidoc",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2058,
    "text": "Ansible inventory allows groups to have same name as hosts\n\nIssue Type\nBug Report\nAnsible Version:\nansible 1.6 (devel 9da26da) last updated 2014/03/18 11:15:04 (GMT +1000)\nEnvironment:\nN/A\nSummary:\nIf you define a group containing another group without the :children, then the intended group is a host, but will also become a group when defining its contents.\nThe net result is that the host that should inherit characteristics of several layers of parents will not inherit those characteristics as the definitions will be:\ngrandparent -> parent(host)\nparent(group) -> host\nrather than\ngrandparent -> parent -> host\nI believe that it should not be possible to define a group with the same name as a host (or at least a warning suggesting it might not be what you want should happen)\nSteps To Reproduce:\nSee https://gist.github.com/willthames/9614054\nExpected Results:\nSee https://gist.github.com/willthames/9614054\nActual Results:\nSee https://gist.github.com/willthames/9614054",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {},
    "annotation_approver": null
  },
  {
    "id": 2059,
    "text": "Inconsistent expansion of the variables in the lookup 'with_items'\n\nIssue Type:\nBug Report\nAnsible Version:\n\nProduction:\n\n/usr/lib/python2.6/site-packages/ansible # ansible --version\nansible 1.4.1\n\n\nFresh clone (13.02.2014)\n\n /local/home/mike/dev/ansible-latest/ansible # ansible --version\nansible 1.5 (devel 6f405c8970) last updated 2014/02/13 13:14:25 (GMT +200)\n\nEnvironment:\n# uname -a\nLinux xxx.cern.ch 2.6.32-431.3.1.el6.x86_64 #1 SMP Mon Jan 6 11:34:51 CET 2014 x86_64 x86_64 x86_64 GNU/Linux\n\n # lsb_release -a\nLSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\nDistributor ID: ScientificCERNSLC\nDescription:    Scientific Linux CERN SLC release 6.5 (Carbon)\nRelease:    6.5\nCodename:   Carbon\n\nSummary:\nDear Ansible Dev Team,\nWhile migrating code from old ${...} variable expansion to new Jinja2 style {{...}}, I've noticed some incompatibilities with the previous syntax, while I would expect {{...}} be a full equivalent of ${...}, i.e the following case should work just fine:\n- name: Java JDK installation\n  action: yum2 name=\"{{ item }}\" enablerepo=acc-external-do state=installed\n  with_items:\n     -   - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} {{ jdk_next }}\n\n\nwhere jdk_next is a list of values:\njdk_pro: 1.7.0_45\njdk_6: 1.6.0_43\njdk_7: 1.7.0_45\njdk_next:\n  - jdk1.7.0_45\n\nUnfortunately, it does not work.\nSteps To Reproduce:\n\nCreate the playbook, which uses roles, for example server.yml:\n\n-\n  hosts: server\n  gather_facts: yes\n  user: root\n  vars:\n    local_to_opt: yes\n  roles:\n    - base\n\nAnd put the mentioned vars in role's \"base\" vars/ and mentioned task in role's \"base\" tasks/, as it should be.\n\nExecute ansible playbook.\n\n ansible-playbook --connection=local server.yml\n\nExpected Results:\nWith previous syntax:\n- name: Java JDK installation\n  action: yum2 name=\"{{ item }}\" enablerepo=acc-external-do state=installed\n  with_items:\n    - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} ${jdk_next}\n\n- the result is just fine:\nPLAY [server] ***************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [127.0.0.1]\n\nTASK: [base | Java JDK installation] ****************************************** \nok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)\n\nPLAY RECAP ******************************************************************** \n127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0   \n\nActual Results:\nWith the code written in Jinja2 style, I've received with the bad result:\nPLAY [server] ***************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [127.0.0.1]\n\nTASK: [base | Java JDK installation] ****************************************** \nfailed: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']) => {\"failed\": true, \"item\": \"jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']\"}\nmsg: ['jdk1.7.0_45'] packages has been failed for installation;\n\nFATAL: all hosts have already failed -- aborting\n\nPLAY RECAP ******************************************************************** \n           to retry, use: --limit @/root/server.retry\n\n127.0.0.1                  : ok=1    changed=0    unreachable=0    failed=1   \n\nThen, even more interesting: I found, that it's not possible anymore specify variable enclosed in {{..}} as a first member of iteration sequence like this:\n- name: Java JDK installation\n  action: yum2 name=\"{{ item }}\" enablerepo=acc-external-do state=installed\n  with_items:\n    - {{ jdk_next }} jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }}\n\nThe result is a syntax error:\nERROR: Syntax Error while loading YAML script, /var/lib/ansible/roles/base/tasks/main.yml\nNote: The error may actually appear before this position: line 596, column 22\n\n  with_items:\n    - {{ jdk_next }} jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} ${jdk_next}\n\nWhile the old syntax was ok:\nPLAY [server] ***************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [127.0.0.1]\n\nTASK: [base | Java JDK installation] ****************************************** \nok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)\n\nPLAY RECAP ******************************************************************** \n127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0   \n\nQuick debug & hacking showed the following things:\n\nif you use original anisble syntax, the file expansion string, which arrives in the variable varname of the template() function of utils/template.py, is [u'jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45'], which is correct\nif you use jinja2 syntax, the final expansion string, which arrives in the variable varname of the template() function of utils/template.py is [u\"jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']\"] which is incorrect.\n\nI do not have comments about SyntaxError I've provided earlier as I have no time to debug it :(\nWorkaround\nThis hack is acceptable but still ugly - the idea is to use the join() Jinja2 filter, i.e. the following code:\n- name: Java JDK installation\n  action: yum2 name=\"{{ item }}\" enablerepo=acc-external-do state=installed\n  with_items:\n    - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} {{ jdk_next | join(' ') }}\n\nworks:\nPLAY [server] ***************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [127.0.0.1]\n\nTASK: [base | Java JDK installation] ****************************************** \nok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)\n\nPLAY RECAP ******************************************************************** \n127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2060,
    "text": "Implement RHEL5 python-dmidecode support\n\nFor older systems lacking sysfs I plan to implement python-dmidecode support in setup as a backup.",
    "annotations": [{ "label": 145, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2061,
    "text": "include_once is skipped when required roles are partially skipped\n\nI use an include_role task in a role, but it is skipped when I use a conditional depency role.\nI tried to find more infos in the documentation, but there is nothing I can find to explain it if I do anything wrong...\nISSUE TYPE\nBug Report\nCOMPONENT NAME\ninclude_role or meta/dependencies\nANSIBLE VERSION\nansible 2.3.0.0\n  config file =\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, Dec 18 2016, 07:03:39) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]\n\nCONFIGURATION\nDefault configuration\nOS / ENVIRONMENT\nMac OS X / Debian\nSUMMARY\nI use a include_role task which is skipped because I use conditional required roles.\nSTEPS TO REPRODUCE\nroles/shell/meta/main.yml\n# Role dependencies\ndependencies:\n  - { role: homebrew, when: ansible_os_family == 'Darwin' }\n  - { role: apt, when: ansible_distribution == 'Debian' }\nroles/shell/tasks/main.yml\n---\n# Tasks for roles\n\n- name: Ensure Oh My Zsh required packages are installed\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  become: \"{{ ansible_os_family != 'Darwin' }}\"\n  with_items:\n   - curl\n   - git\n   - zsh\n\n- name: Ensure Zsh is the default shell\n  shell: \"chsh -s /bin/zsh {{ansible_user_id}}\"\n  become: yes\n  register: chsh_result\n  changed_when: \"chsh_result.stderr.find('no changes made') == -1\"\n\n- name: Ensure Oh My Zsh is installed\n  shell: \"sh -c \\\"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\\\"\"\n  args:\n    creates: \"{{ ansible_env.HOME}}/.oh-my-zsh\"\n    executable: /bin/zsh\n\n- name: Ensure auto-suggestions Zsh plugin is installed\n  git:\n    repo: 'https://github.com/zsh-users/zsh-autosuggestions.git'\n    dest: \"{{ ansible_env.HOME }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\"\n\n- name: Ensure dotfiles are installed\n  include_role:\n    name: dotfiles\nIn the dotfiles role, nothing special, no required role, just some unconditional tasks:\nroles/dotfiles/tasks/main.yml\n- name: Ensure repository is cloned\n  git:\n    repo: 'https://github.com/maxwo/dotfiles.git'\n    dest: \"{{ ansible_env.HOME }}/dotfiles\"\n\n- name: Ensure SSH dotfiles symlinks are set with correct permissions\n  file:\n    src: \"{{ ansible_env.HOME}}/dotfiles/ssh/{{ item }}\"\n    dest: \"{{ ansible_env.HOME }}/.ssh/{{ item }}\"\n    mode: 0600\n    state: link\n    force: yes\n  with_items:\n    - authorized_keys\n    - config\n\n- name: Ensure dotfiles standard symlinks are set\n  file:\n    src: \"{{ ansible_env.HOME }}/dotfiles/{{ item.src }}\"\n    dest: \"{{ ansible_env.HOME }}/{{ item.dest }}\"\n    state: link\n    force: yes\n  with_items:\n    - { src: 'gitconfig', dest: '.gitconfig' }\n    - { src: 'zshrc', dest: '.zshrc' }\nEXPECTED RESULTS\nThe last include_role should be played.\nACTUAL RESULTS\nThe last include_role is skipped, whether it is played on a Debian machine or a Mac OS machine.\nWhen I remove the role dependencies, everything is fine.\nTASK [icopp.homebrew : Install Homebrew] *********************************************************************************************************************************\nok: [aki.local]\n\nTASK [homebrew : Ensure Homebrew packages are up to date] ****************************************************************************************************************\nchanged: [aki.local]\n\nTASK [apt : Ensure APT packages are up to date] **************************************************************************************************************************\nskipping: [aki.local]\n\nTASK [shell : Ensure Oh My Zsh required packages are installed] **********************************************************************************************************\nok: [aki.local] => (item=curl)\nok: [aki.local] => (item=git)\nok: [aki.local] => (item=zsh)\n\nTASK [shell : Ensure Zsh is the default shell] ***************************************************************************************************************************\nok: [aki.local]\n\nTASK [shell : Ensure Oh My Zsh is installed] *****************************************************************************************************************************\nok: [aki.local]\n\nTASK [shell : Ensure auto-suggestions Zsh plugin is installed] ***********************************************************************************************************\nok: [aki.local]\n\nTASK [dotfiles : Ensure repository is cloned] ****************************************************************************************************************************\nskipping: [aki.local]\n\nTASK [dotfiles : Ensure SSH dotfiles symlinks are set with correct permissions] ******************************************************************************************\nskipping: [aki.local] => (item=authorized_keys)\nskipping: [aki.local] => (item=config)\n\nTASK [dotfiles : Ensure dotfiles standard symlinks are set] **************************************************************************************************************\nskipping: [aki.local] => (item={u'dest': u'.gitconfig', u'src': u'gitconfig'})\nskipping: [aki.local] => (item={u'dest': u'.zshrc', u'src': u'zshrc'})",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2062,
    "text": "`postgres_user` module doesn't work with AWS RDS databases\n\nIssue Type:\nBug report\nAnsible Version:\nansible 1.7.0\nEnvironment:\nOSX Mavericks 10.9.4\nSummary:\nWhen running against an AWS RDS Postgresql instance, the postgres_user module can't set up new users.\nSteps To Reproduce:\nRun this task:\n- name: Ensure user has access to the database\n  postgresql_user: login_host={{ db_host }}\n                   port={{ db_port }}\n                   login_user={{ db_admin_user }}\n                   login_password={{ db_admin_password }}\n                   db={{ db_name }}\n                   name={{ db_user }}\n                   password={{ db_password }}\n                   priv=ALL\n                   state=present \n\nExpected Results:\nok: [...] => {\"changed\": true, \"db\": \"database\"}\n\nActual Results:\nfailed: [...] => {\"failed\": true, \"parsed\": false}\ninvalid output was: SUDO-SUCCESS-qwpjepgvenunnlewzwjddpnrbfjyptxo\nTraceback (most recent call last):\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user\", line 1869, in <module>\n    main()\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user\", line 497, in main\n    changed = user_alter(cursor, module, user, password, role_attr_flags, encrypted, expires)\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user\", line 201, in user_alter\n    cursor.execute(select, {\"user\": user})\npsycopg2.ProgrammingError: permission denied for relation pg_authid\n\nApparently the pg_authid relation is not available in RDS.\nPossible workaround: if access denied for pg_authid, then always set the password.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2063,
    "text": "letsencrypt module does not create any cert when using DNS-based validation\n\nISSUE TYPE\nBug Report\nCOMPONENT NAME\n\nletsencrypt\nANSIBLE VERSION\nAnsible version is 2.2.1.0\nSUMMARY\nIt looks like the Let's Encrypt Ansible module doesn't create any cert when running with the DNS-based validation.\nSTEPS TO REPRODUCE\n# vars\n---\naws:\n   access_key: \"some_access_key\"\n   secret_key: \"some_secret_key\"\nletsencrypt_account_email: \"some_email@example.com\"\nletsencrypt_key_filename: \"some_filename\"\nletsencrypt_dir: \"some/path\"\nletsencrypt_domains:\n   - \"foo\"\n   - \"bar\"\n\n# tasks\n---\n- name: Install Route53 dependencies\n  pip:\n    name: boto\n\n- name: Create folder\n  file:\n    path: \"{{ letsencrypt_dir }}\"\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n\n- name: Create key\n  shell: 'ssh-keygen -t rsa -b 2048 -C \"{{ letsencrypt_account_email }}\" -f ~/.ssh/{{ letsencrypt_key_filename }} -q -N \"\"'\n  args:\n    creates: \"~/.ssh/{{ letsencrypt_key_filename }}\"\n\n- name: Create domain key\n  shell: 'ssh-keygen -t rsa -b 2048 -C \"{{ letsencrypt_account_email }}\" -f ~/.ssh/id_rsa.{{ item }} -q -N \"\"'\n  args:\n    creates: '~/.ssh/id_rsa.{{ item }}'\n  with_items: \"{{ letsencrypt_domains }}\"\n\n- name: Create CSR (Certificate Signing Request)\n  shell: 'openssl req -new -nodes -key ~/.ssh/{{ letsencrypt_key_filename }} -out {{ letsencrypt_dir }}/{{ item }}.csr -subj \"/CN={{ item }}\"'\n  args:\n    creates: '{{ letsencrypt_dir }}/{{ item }}.csr'\n  with_items: \"{{ letsencrypt_domains }}\"\n\n- name: Create challenge\n  letsencrypt:\n    account_key: '~/.ssh/id_rsa.{{ item }}'\n    challenge: dns-01\n    csr: '{{ letsencrypt_dir }}/{{ item }}.csr'\n    dest: '{{ letsencrypt_dir }}/{{ item }}.crt'\n    remaining_days: 20\n  register: letsencrypt_challenge\n  with_items: \"{{ letsencrypt_domains }}\"\n\n- name: Create Route53 TXT record for DNS-based certificate validation\n  route53:\n    command: create\n    aws_access_key: \"{{ aws.access_key_id }}\"\n    aws_secret_key: \"{{ aws.secret_access_key }}\"\n    zone: 'some_hosted_zone.com'\n    record: \"{{ item.1.challenge_data[item.0]['dns-01']['resource'] }}.some_hosted_zone.com\"\n    retry_interval: 300\n    type: TXT\n    ttl: 7200\n    value: '\"{{ item.1.challenge_data[item.0][\"dns-01\"][\"resource_value\"] }}\"'\n    wait: yes\n  when: \"letsencrypt_challenge|changed\"\n  with_together:\n    - \"{{ letsencrypt_domains }}\"\n    - \"{{ letsencrypt_challenge['results'] }}\"\n  ignore_errors: yes\n\n- name: Validate challenge\n  letsencrypt:\n    account_key: '~/.ssh/id_rsa.{{ item }}'\n    challenge: dns-01\n    csr: '{{ letsencrypt_dir }}/{{ item }}.csr'\n    dest: '{{ letsencrypt_dir }}/{{ item }}.crt'\n    data: '{{ letsencrypt_challenge }}'\n    remaining_days: 20\n  when: '{{ letsencrypt_challenge|changed }}'\n  with_items: \"{{ letsencrypt_domains }}\"\n\n- name: Delete Route53 TXT record for DNS-based certificate validation\n  route53:\n    command: delete\n    aws_access_key: \"{{ aws.access_key_id }}\"\n    aws_secret_key: \"{{ aws.secret_access_key }}\"\n    zone: 'some_hosted_zone.com'\n    record: \"{{ item.1.challenge_data[item.0]['dns-01']['resource'] }}.some_hosted_zone.com\"\n    retry_interval: 300\n    type: TXT\n    ttl: 7200\n    value: '\"{{ item.1.challenge_data[item.0][\"dns-01\"][\"resource_value\"] }}\"'\n    wait: yes\n  when: \"letsencrypt_challenge|changed\"\n  with_together:\n    - \"{{ letsencrypt_domains }}\"\n    - \"{{ letsencrypt_challenge['results'] }}\"\n  ignore_errors: yes\n\n\nACTUAL RESULTS\nNo errors are being thrown or anything, it provisions just fine\nEXPECTED RESULTS\nThere should be a cert generated",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2064,
    "text": "Memory load increased in 2.3.0 compared to 2.2.0.1 (high memory use, high ram use)\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nAnsible\nANSIBLE VERSION\n2.3.0\n\nCONFIGURATION\n[defaults]\nhostfile       = inventory/\nlibrary        = /usr/share/ansible\nremote_tmp     = $HOME/.ansible/tmp\npattern        = *\nforks          = 30\npoll_interval  = 15\nsudo_user      = root\nask_sudo_pass  = True\ntransport      = ssh\nremote_port    = 22\nmodule_lang    = C\nallow_world_readable_tmpfiles = True\ngathering = implicit\nhost_key_checking = False\nstdout_callback = actionable_debug\nsudo_exe = sudo\nsudo_flags = -i\ntimeout = 15\nansible_managed = This file is managed by Ansible.\naction_plugins     = /usr/share/ansible_plugins/action_plugins\ncallback_plugins   = /usr/share/ansible_plugins/callback_plugins:./callback_plugins\nconnection_plugins = /usr/share/ansible_plugins/connection_plugins\nlookup_plugins     = /usr/share/ansible_plugins/lookup_plugins\nvars_plugins       = /usr/share/ansible_plugins/vars_plugins\nfilter_plugins     = /usr/share/ansible_plugins/filter_plugins\nfact_caching = jsonfile\nfact_caching_connection = /tmp/$USER\nretry_files_enabled = False\n\n[privilege_escalation]\n\n[paramiko_connection]\n\n[ssh_connection]\nretries=10\npipelining = False\n\n[accelerate]\n\n[selinux]\n\n[colors]\n\nOS / ENVIRONMENT\nUbuntu 14.04.5 (exclusively) hosts & clients\nSUMMARY\nIncreased memory usage when 2.3.0 is rolled out: http://imgur.com/a/ZgUCe\nRecovery takes place when 2.2.0.1 is reverted.\nSTEPS TO REPRODUCE\n\nInstall Ansible 2.3.0\nRun regular job schedule (6x a day a full run of ~100 roles, several irregular jobs like backups), started from the Rundeck job scheduler over regular SSH.\n\nEXPECTED RESULTS\nSimilar memory usage.\nACTUAL RESULTS\nhttp://imgur.com/a/ZgUCe",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2065,
    "text": "Homebrew module fails silently after fresh Homebrew installs\n\nReproduce (done on a fresh OSX Mavericks install + Command line tools)\n\nRun the homebrew installer script.\nruby -e \"$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)\"\nCompletes. Run brew doctor. Everything looks good.\nNote that the installer does not (and should not) create /usr/local/Cellar, this is created when the first brew package is installed. So right now no Cellar exists.\n\nTry running a simple playbook with localhost inventory:\n\n---\n- name: My playbook\n  user: myusername\n  hosts: all\n  tasks:\n    - name: Install brew packages\n      homebrew: name=ack state=present\n\nansible-playbook myplaybook -i localhost_inventory\nPLAY [My playbook] ******************************************************\nGATHERING FACTS ***************************************************************\nok: [localhost]\nTASK: [Install brew packages] *********************************************\nok: [localhost]\nPLAY RECAP ********************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=0\nYou'll get something that completes instantly, seems to indicate ack was installed(ok), but it wasn't. No Cellar exists. ack is seriously not installed.\nNow go ahead and make the /usr/local/Cellar directory. Run the config again. Boom, suddenly it actually works. There is a noticeable delay as ack is installed and ack immediately works after the playbook completes.\nTry deleting the Cellar completely and you're back to the original broken behavior.\nA clean install seems like a pretty simple edge case that should be supported. What's up with this? Lemme know if I'm going crazy.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2066,
    "text": "ansible 1.5 breaks gather_facts+accelerate\n\nIssue Type:·\nBug report\nAnsible Version:·\nansible 1.5\nEnvironment:\nBoth RHEL6 and Fedora rawhide.·\nSummary:\nAny plays with both:·\ngather_facts: true\naccelerate: true\nFail with:·\nPLAY [foo] ********************************************************************·\nGATHERING FACTS ***************************************************************·\nfatal: [td] => Incorrect permissions on ACCELERATE_KEYS_FILE (/home/kevin/.fireball.keys/td)\nansible never actually connects to the host that I can tell,·\nperhaps it's thinking accelerate is already started when it's not?\nThe directory it refers to doesn't exist (since it never connected to the host)\nSteps To Reproduce:\nCreate a playbook with:·\ngather_facts: true\naccelerate: true\nand at least one host. Run it.·\nExpected Results:\nFacts are gathered and rest of playbook runs.·\nActual Results:\nAnsible-playbook fails at gathering facts and errors out.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2067,
    "text": "Clearing facts (to refresh them) does not work\n\nISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\n\n\nmeta\nsetup\n\nANSIBLE VERSION\n\nansible 2.3.0.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, Jan 12 2017, 17:59:37) [GCC 6.3.1 20161221 (Red Hat 6.3.1-1)]\n\nCONFIGURATION\n\nNone.\nOS / ENVIRONMENT\n\n➤ lsb_release -a\nLSB Version:\t:core-4.1-amd64:core-4.1-noarch\nDistributor ID:\tFedora\nDescription:\tFedora release 25 (Twenty Five)\nRelease:\t25\nCodename:\tTwentyFive\n\nSUMMARY\n\nIf you need to reload some facts (in my case I need the VPN IP after I configured it), it does not work\nSTEPS TO REPRODUCE\n\nOption A:\n\n- name: vpn client must be running\n  service:\n    name: openvpn@example\n    enabled: true\n    state: started\n\n- name: refresh facts\n  setup:\nOption B:\n- name: vpn client must be running\n  service:\n    name: openvpn@example\n    enabled: true\n    state: started\n\n- name: refresh facts\n  meta: clear_facts\n\nEXPECTED RESULTS\n\nFacts reloaded, VPN ip data available.\nACTUAL RESULTS\n\n\nOption A:\nTASK [vpn-client : refresh facts] ********************************************************************************************************************\nok: [server.example.com]\n\n... but later another task fails with:\nfatal: [server.example.com]: FAILED! => {\"changed\": false, \"failed\": true, \"msg\": \"AnsibleUndefinedVariable: 'ansible_tun0' is undefined\"}\n\nOption B:\nTASK [vpn-client : refresh facts] *********************************************************************************************************\nfatal: [server.example.com]: FAILED! => {\"changed\": false, \"failed\": true, \"module_stderr\": \"Shared connection to server.example.com closed.\\r\\n\", \"module_stdout\": \"\", \"msg\": \"MODULE FAILURE\", \"rc\": 0}\n\nWORKAROUND\nRun the playbook again 😞",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2068,
    "text": "jenkins_plugin - incorrect \"changed\" and silent install failures\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\njenkins_plugin\nANSIBLE VERSION\nansible 2.3.1.0\n  config file = /home/ghelling/.ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, May 10 2017, 20:04:28) [GCC 6.3.1 20161221 (Red Hat 6.3.1-1)]\n\nCONFIGURATION\ndefault configuration\nOS / ENVIRONMENT\nThis seems independent of versions, but I see it when running from Fedora while controlling Fedora, CentOS 7,  and RHEL 7 machines with Jenkins 1.651.3 running on them.\nSUMMARY\nSome plugins, after being installed, are either not installed to the latest version (despite no value being specified for the version) or are not installed at all, despite the module reporting success. On subsequent runs, these same plugins continue to report a changed/updated edition despite there being no change in the version being installed.\nAn example of some plugins where this behavior has been noticed:\n\nantisamy-markup-formatter (version 1.1 installed, despite 1.5 being available)\nscriptler (reports installed, but the plugin fails to be installed)\ndynamic-parameter (same as scriptler)\n\nAnd many others. However, the behavior does not affect all plugins.\nSTEPS TO REPRODUCE\nOn clean CentOS system, run the following playbook: https://gist.github.com/greg-hellings/14f58eb19a4992b910f27e53f63573b4\nEXPECTED RESULTS\nPlugins are installed to the latest version if version is specified as latest.\nSpurious \"changed\" values are not reported from the module when nothing gets updated.\nThe module errors when a plugin install error occurs.\nACTUAL RESULTS\nNo version specified results in both plugins reporting back \"changed\" when neither the version updates (antisamy-markup-formatter) or the plugin is not installed at all (scriptler).\nWith the line version: latest added to that final task in the sample file, the scriptler install fails while the antisamy-markup-formatter still reports \"changed\" without actually updating anything.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2069,
    "text": "ec2_group: add tags\n\nFrom @jbrockett on December 4, 2014 15:15\nIssue Type:\nFeature Idea\nComponent name:\n\nec2_group\nAnsible Version:\nansible 2.3\nEnvironment:\nN/A\nSummary:\nPlease add the ability to create and modify tags associated with the security group.  At least being able to set the Name tag would be helpful.",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2070,
    "text": "service: nginx enabled=yes not enabling service ansible 1.7.2\n\nIssue Type:\nBug Report\nAnsible Version:\n1.7.2\n1.6.6\nEnvironment:\nDestination: Ubuntu 12.04.4 LTS\nControl hosts: Mac OSX and Ubuntu 12.04.4 LTS\nSummary:\nWhen trying to enable nginx, which is a typical init.d startup script. It appears to not have any affect.\nHave tried with both ansible versions and with Mac OSX and Ubuntu 12.04 control hosts. Destination host is a Ubuntu 12.04\nSteps To Reproduce:\nInstall nginx from deb and it will create a /etc/init.d/nginx script\nIn a task write:\n  - name: start nginx on boot\n    service: name=nginx state=started enabled=yes\n    tags: nginx_boot\n\nRun the playbook.\nansible-playbook site.yaml -i inventory/prod -l host02.example.com -t nginx_boot\n\nExpected Results:\nService enabled\nActual Results:\nReports changed every time:\nTASK: [nginx | start nginx on boot] *******************************************\nchanged: [host02.example.com]\n\nchkconfig and rc.d directories unchanged:\nroot@host02:/etc# chkconfig nginx\nnginx  off\n\nroot@host02:/etc# ls rc*/*nginx*\nrc0.d/K00nginx  rc1.d/K00nginx  rc2.d/K00nginx  rc3.d/K00nginx  rc4.d/K00nginx  rc5.d/K00nginx  rc6.d/K00nginx",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2071,
    "text": "Include all dependency roles in include search path for role\n\nISSUE TYPE\n\nFeature Idea\n - Bug Report\n\nCOMPONENT NAME\ninclude\nANSIBLE VERSION\nansible 2.2.0.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\nCONFIGURATION\nroles_path set to both my local directory and left as default.\nOS / ENVIRONMENT\nFedora 25\nSUMMARY\nFiles present in a parent role's tasks directory are not found by child tasks' include statements.\nSTEPS TO REPRODUCE\nUnzip ansible-test.zip and execute ansible-playbook test.yml. Note that\n- include: roles/parent/tasks/parent-task.yml\nworks, but\n- include: parent-task.yml\nfails.\nEXPECTED RESULTS\nparent-task.yml should be found as if it were part of the child task that inherits from it. This would follow the standard (single) role behaviour where \"Any copy, script, template or include tasks (in the role) can reference files in roles/x/{files,templates,tasks}/ (dir depends on task) without having to path them relatively or absolutely\".\nACTUAL RESULTS\n$ ANSIBLE_NOCOWS=1 ansible-playbook test.yml -vvvv\nUsing /etc/ansible/ansible.cfg as config file\n [WARNING]: provided hosts list is empty, only localhost is available\n\nstatically included: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml\n[DEPRECATION WARNING]: Included file '/home/pjanes/ansible-test/parent-task.yml'\n not found, however since this include is not explicitly marked as 'static: \nyes', we will try and include it dynamically later. In the future, this will be \nan error unless 'static: no' is used on the include task. If you do not want \nmissing includes to be considered dynamic, use 'static: yes' on the include or \nset the global ansible.cfg options to make all inclues static for tasks and/or \nhandlers.\nThis feature will be removed in a future release. Deprecation warnings\n can be disabled by setting deprecation_warnings=False in ansible.cfg.\nLoading callback plugin default of type stdout, v2.0 from /usr/lib/python2.7/site-packages/ansible/plugins/callback/__init__.pyc\nLoading callback plugin timestamp of type ek3, v2.0 from /usr/lib/python2.7/site-packages/ansible/plugins/callback/__init__.pyc\n\nPLAYBOOK: test.yml *************************************************************\n1 plays in test.yml\n\nPLAY [localhost] ***************************************************************\nFriday 13 January 2017  10:38:41 -0500 (0:00:00.001)       0:00:00.001 ******** \n=============================================================================== \n\nTASK [child : debug] ***********************************************************\ntask path: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml:2\nFriday 13 January 2017  10:38:41 -0500 (0:00:00.030)       0:00:00.032 ******** \nok: [localhost] => {\n    \"msg\": \"task from parent\"\n}\n\nTASK [child : include] *********************************************************\ntask path: /home/pjanes/ansible-test/roles/child/tasks/main.yml:3\nFriday 13 January 2017  10:38:41 -0500 (0:00:00.010)       0:00:00.042 ******** \nfatal: [localhost]: FAILED! => {\n    \"failed\": true, \n    \"reason\": \"the file_name '/home/pjanes/ansible-test/parent-task.yml' does not exist, or is not readable\"\n}\n\tto retry, use: --limit @/home/pjanes/.ansible-retry/test.retry\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=1   \n\nFriday 13 January 2017  10:38:41 -0500 (0:00:00.009)       0:00:00.051 ******** \n===============================================================================\n\nAdding static: yes to avoid the warning:\n$ ANSIBLE_NOCOWS=1 ansible-playbook test.yml -vvvv\nUsing /etc/ansible/ansible.cfg as config file\n [WARNING]: provided hosts list is empty, only localhost is available\n\nstatically included: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml\nERROR! the file_name '/home/pjanes/ansible-test/parent-task.yml' does not exist, or is not readable",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2072,
    "text": "Vault bytes <=> text string API\n\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nlib/ansible/parsing/vault/init.py\nunittests for vault\nANSIBLE VERSION\n\ndevel\n\nSUMMARY\nThe new vault API is mixing bytes and text in inappropriate ways.\nIn working on enabling unittests for Python3, I came across the fact that vault unittests were skipping a lot of tests depending on whether they were running on python3 or python2.  Closer examination revealed that the tests were either giving different values to the API or receiving different values back depending on the version of Python that we are running on.  This is problematic API design that we need to change before release.\nGood API should follow one of these rules for input and output:\n\nTake bytes and return bytes\n\nThe old vault API attempted to use this strategy for \"internal\" functions.  Many of the functions inside of vault were not called from outside of parsing/vault/init.py.  Since they also dealt with operations involving byte strings (encoding, decoding, hexlifying, writing to disk, etc) it made sense that they dealt solely in bytes.\n\n\nTake text and return text\n\nIt is relatively easy to create this sort of API in python3 as combining text and bytes leads to immediate tracebacks.  In python2, it is easy to mix this up with one of the other strategies below as ascii-only bytes and text strings will combine.  Most of ansible's current API does not follow this strategy because we don't trust that the data coming in is going to be the string type we expect.  As we secure our borders (and with python3 tests throwing errors when these are combined inappropriately) we should be able to move more API to this model.\n\n\nTake either bytes or text, normalize internally, and return text\n\nThis is the strategy that a lot of the old external Vault API was taking.  We didn't trust that the rest of Ansible was properly sending us text strings so as the first step we used to_unicode and to_bytes to make sure that the string we were dealing with was the correct type.\n\n\nTake either bytes or text, normalize internally, and return bytes\n\nOld internal vault API may have taken this strategy.  For internal API we should know that our inputs are only text or only bytes but we may have been paranoid.  Since the data we passed around was often pure-ascii, this couldn't cause tracebacks.\n\n\nTake either bytes or text.  If bytes were input then output bytes.  If text was input then output text\n\nI would not recommend this for any of our Vault API.  You'll see this in some Python stdlib API like os.path.abspath() (a good usage of the strategy) or os.listdir() (a bad usage of the strategy).  It is appropriate when the purpose of the function is a straightforward text transformation and there is a need to give callers a version that works with text and a version that works with bytes.  This is not the case for Vault's internal API (where we can adapt the callers to use just one API) and it is not needed for Vault's external API (where we should probably always be returning text).  This strategy can also be used for functions which are operating inside of a larger \"native string\" data model as the code run on python2 should be taking in bytes and outputting bytes while the code on python3 is taking in text and outputting text.  Note, though, that this strategy does not validate the input or output while technically the native string model could validate that only bytes was accepted on python2 and only text was accepted on python3.\n\n\n\nWhichever strategy is followed, be sure that the same strategy is being applied for both Python2 and Python3.  Most projects (including Ansible) are working towards a single code base that runs on both python2 and python3.  Writing API that expects different types and returns different types on one or the other hampers this overall design.\nAlso note, it is tempting to do validation of the strings you receive.  I'd hesitate to do that because all checks have a cost.  Even asserts which are really meant for this purpose invokes the cost because no one really runs ansible in python optimized mode where asserts are stripped out.  My general stance has been if you trust the data coming in, there's no need to validate it.  If you don't trust the data, you should use to_bytes() or to_unicode() to accept either bytes or text and normalize to the type you want to operate on.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2073,
    "text": "hacking/module_formatter build error in Debian [0.9]\n\nI tried to build ansible 0.9 on Debian Squeeze, but the make script fails.\n$ sudo aptitude install python-yaml2 python-jinja2 python-paramiko\n$ wget https://github.com/downloads/ansible/ansible/ansible-0.9.tar.gz\n$ tar -xvzf ansible-0.9.tar.gz\n$ cd ansible-0.9\n\n$ make debian\ncat: VERSION: No such file or directory\nfatal: Not a git repository (or any of the parent directories): .git\nCleaning up distutils stuff\nrm -rf build\nrm -rf dist\nCleaning up byte compiled python stuff\nfind . -type f -regex \".*\\.py[co]$\" -delete\nCleaning up editor backup files\nfind . -type f \\( -name \"*~\" -or -name \"#*\" \\) -delete\nfind . -type f \\( -name \"*.swp\" \\) -delete\nCleaning up manpage stuff\nfind ./docs/man -type f -name \"*.xml\" -delete\nfind ./docs/man -type f -name \"*.asciidoc\" -delete\nfind ./docs/man/man3 -type f -name \"*.3\" -delete\nCleaning up output from test runs\nrm -rf test/test_data\nCleaning up RPM building stuff\nrm -rf MANIFEST rpm-build\nCleaning up Debian building stuff\nrm -rf debian\nrm -rf deb-build\nrm -rf docs/json\nrm -rf docs/js\nPYTHONPATH=./lib hacking/module_formatter.py -A  -t man -o docs/man/man3/ --module-dir=library --template-dir=hacking/templates\n/bin/sh: hacking/module_formatter.py: not found\nmake: *** [modulepages] Error 127",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {},
    "annotation_approver": "jocharan"
  },
  {
    "id": 2074,
    "text": "Add support for \"build-dep\" to apt module\n\nAs a system administrator I want to install build dependency packages for developer and buildhost machines using the same syntax I install normal packages with.\nIt would be useful if one could install build-deps via the apt module instead of mixing and matching apt module and commands for \"apt-get build-dep \"",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2075,
    "text": "sequence lookup shortcut syntax doesn't work and wrong docs\n\nISSUE TYPE\n\nBug Report\nDocumentation Report\n\nCOMPONENT NAME\nsequence lookup plugin\nANSIBLE VERSION\nansible 2.2.1.0\n\n2.1, 2.3 and 2.4 have the same bug.\nCONFIGURATION\nStandard config\nOS / ENVIRONMENT\nN/A\nSUMMARY\nwith_sequence shortcut syntax [start-]end[/stride][:format] is not honoured.\nSTEPS TO REPRODUCE\n---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  tasks:\n    - debug:\n        msg: \"{{ item }}\"\n      with_sequence: '5-6'\n\nEXPECTED RESULTS\nLoop over [5,6]:\nTASK [debug] *******************************************************************\nok: [localhost] => (item=5) => {\n    \"item\": \"5\",\n    \"msg\": \"5\"\n}\nok: [localhost] => (item=6) => {\n    \"item\": \"6\",\n    \"msg\": \"6\"\n}\n\nACTUAL RESULTS\nParameter error:\nTASK [debug] *******************************************************************\nfatal: [localhost]: FAILED! => {\"failed\": true, \"msg\": \"unknown error parsing with_sequence arguments: u'5-6'. Error was: unrecognized arguments to with_sequence: [u'_raw_params']\"}\n\nAlso there is wrong parameters passing in the docs:\n    - user:\n        name: \"{{ item }}\"\n        state: present\n        groups: \"evens\"\n      with_sequence:\n        - start: 0\n        - end: 32\n        - format: testuser%02x\n\nParameters can be passed to with_sequence only as string, not as list or dict.",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {},
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2480,
    "text": "Feature request: Use placeholders to specify the inputs of TFGAN model.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "0",
      "number": "15383",
      "pretext": "",
      "title": "Feature request: Use placeholders to specify the inputs of TFGAN model."
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2481,
    "text": "Win10 C++ TF1.9, error LNK2001, build by bazel  !I have generated the TensorFlowV1.9's .so and .lib file successfully on Win10,  but when I use this in VS2017, it has errors as bellow :\nMFCTestTF1.9.obj : error LNK2001: 无法解析的外部符号 \"char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)\" (?GetVarint32PtrFallback@core@tensorflow@@YAPBDPBD0PAI@Z)\n1>D:\\ProgramData\\VS2017 Project\\MFCTestTF1.9\\Release\\MFCTestTF1.9.exe : fatal error LNK1120: 1 个无法解析的外部命令\n1>已完成生成项目“MFCTestTF1.9.vcxproj”的操作 - 失败。\nAnd I also build TensorFlowV1.8 with CMAKE, it work OK without LNK error.  But V1.9 can not build by CMAKE.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "1",
      "number": "22826",
      "pretext": "I have generated the TensorFlowV1.9's .so and .lib file successfully on Win10,  but when I use this in VS2017, it has errors as bellow :\nMFCTestTF1.9.obj : error LNK2001: 无法解析的外部符号 \"char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)\" (?GetVarint32PtrFallback@core@tensorflow@@YAPBDPBD0PAI@Z)\n1>D:\\ProgramData\\VS2017 Project\\MFCTestTF1.9\\Release\\MFCTestTF1.9.exe : fatal error LNK1120: 1 个无法解析的外部命令\n1>已完成生成项目“MFCTestTF1.9.vcxproj”的操作 - 失败。\nAnd I also build TensorFlowV1.8 with CMAKE, it work OK without LNK error.  But V1.9 can not build by CMAKE.",
      "title": "Win10 C++ TF1.9, error LNK2001, build by bazel  !"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2482,
    "text": "tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directorycan anyone help me??\nTraceback (most recent call last):\nFile \"/Users/meow/generate_tfrecord.py\", line 99, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n_sys.exit(main(argv))\nFile \"/Users/meow/generate_tfrecord.py\", line 85, in main\nwriter = tf.python_io.TFRecordWriter(FLAGS.output_path)\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py\", line 112, in init\ncompat.as_bytes(path), compat.as_bytes(compression_type), status)\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "2",
      "number": "21037",
      "pretext": "can anyone help me??\nTraceback (most recent call last):\nFile \"/Users/meow/generate_tfrecord.py\", line 99, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n_sys.exit(main(argv))\nFile \"/Users/meow/generate_tfrecord.py\", line 85, in main\nwriter = tf.python_io.TFRecordWriter(FLAGS.output_path)\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py\", line 112, in init\ncompat.as_bytes(path), compat.as_bytes(compression_type), status)\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory",
      "title": "tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2483,
    "text": "bazel build error for PolymerElementsI am trying to build tensorflow from source, and bazel is giving some unrelated error\neddie7@albus:~/lab/tensorflow$ git pull\nAlready up-to-date.\neddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n.......\nERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@iron-validatable-behavior//': https://github.com/PolymerElements/iron-validatable-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 129.792s\n\n\nFYI: I have also build bazel from source",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "3",
      "number": "452",
      "pretext": "I am trying to build tensorflow from source, and bazel is giving some unrelated error\neddie7@albus:~/lab/tensorflow$ git pull\nAlready up-to-date.\neddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n.......\nERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@iron-validatable-behavior//': https://github.com/PolymerElements/iron-validatable-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 129.792s\n\n\nFYI: I have also build bazel from source",
      "title": "bazel build error for PolymerElements"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2484,
    "text": "Tensorflow Serving not using multi GPU/CUDA cores I'm using an AWS g3.8xlarge instance which has 2 GPUs.\nTF serving is able to detect both GPUs and initialise them but while running the model it only uses 1 GPU to the maximum.\nWe are on version 1.7, even though the client sends upto 32 requests in parallel, the model server only uses the first GPU\n\n06_09_21",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "4",
      "number": "19640",
      "pretext": "I'm using an AWS g3.8xlarge instance which has 2 GPUs.\nTF serving is able to detect both GPUs and initialise them but while running the model it only uses 1 GPU to the maximum.\nWe are on version 1.7, even though the client sends upto 32 requests in parallel, the model server only uses the first GPU\n\n06_09_21",
      "title": "Tensorflow Serving not using multi GPU/CUDA cores "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2485,
    "text": "tensorflow lite: error when convert frozen model to lite formatSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 14.04\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\n1.3.0\nPython version:\n2.7\nBazel version (if compiling from source):\n0.7.0\nGCC/Compiler version (if compiling from source):\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nCUDA/cuDNN version:\ncuda8.0/cudnn6.0\n\nI tried to convert squeezenet frozen model to lite format with the following command:\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3\"\nthe output is shown below:\n2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)\n2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)\n2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)\n2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.\n2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).\n2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\nThen I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3\"\noutput:\n2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)\n2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)\n2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)\n2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.\n2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).\n2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\nAlthough I installed tensorflow with \"pip install tensorflow-gpu\", in order to convert model to lite format, I git clone the tensorflow files and  configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "5",
      "number": "14761",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 14.04\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\n1.3.0\nPython version:\n2.7\nBazel version (if compiling from source):\n0.7.0\nGCC/Compiler version (if compiling from source):\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nCUDA/cuDNN version:\ncuda8.0/cudnn6.0\n\nI tried to convert squeezenet frozen model to lite format with the following command:\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3\"\nthe output is shown below:\n2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)\n2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)\n2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)\n2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.\n2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).\n2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\nThen I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3\"\noutput:\n2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)\n2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)\n2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)\n2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.\n2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).\n2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\nAlthough I installed tensorflow with \"pip install tensorflow-gpu\", in order to convert model to lite format, I git clone the tensorflow files and  configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!",
      "title": "tensorflow lite: error when convert frozen model to lite format"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2486,
    "text": "configure script hardcodes location of cuda that makes it fail on OSXCuda installation on OSX is at $CUDA_TOOLKIT_PATH/lib (not lib64), and on OSX the shared libraries are end in .dylib (not .so).\n  if [ -e \"$CUDA_TOOLKIT_PATH/lib64/libcudart.so.7.0\" ]; then\n    break\n  fi\n  echo \"Invalid path to CUDA 7.0 toolkit. ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so.7.0 cannot be found\"",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "6",
      "number": "111",
      "pretext": "Cuda installation on OSX is at $CUDA_TOOLKIT_PATH/lib (not lib64), and on OSX the shared libraries are end in .dylib (not .so).\n  if [ -e \"$CUDA_TOOLKIT_PATH/lib64/libcudart.so.7.0\" ]; then\n    break\n  fi\n  echo \"Invalid path to CUDA 7.0 toolkit. ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so.7.0 cannot be found\"",
      "title": "configure script hardcodes location of cuda that makes it fail on OSX"
    },
    "annotation_approver": null
  },
  {
    "id": 2487,
    "text": "Tensorboard scalar summary graph distorts after resizing and toggling log scale on y-axis What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone although a search for distorted image tensorboard doesn't help much...\nEnvironment info\nOperating System: 16.04 LTS\nFirefox: 51.0.1 (64-bit)\nTF: 1.0 (installed via pip)\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$> sudo ls -l /usr/local/cudnn/*\n/usr/local/cudnn/include:\ntotal 100\n-r--r--r-- 1 root root 99658 Feb 20 11:27 cudnn.h\n\n/usr/local/cudnn/lib64:\ntotal 150908\nlrwxrwxrwx 1 root root       13 Feb 20 11:27 libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       35 Feb 16 17:01 libcudnn.so.4 -> /usr/local/cuda/lib64/libcudnn.so.4\nlrwxrwxrwx 1 root root       39 Feb 16 17:01 libcudnn.so.4.0.7 -> /usr/local/cuda/lib64/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       18 Feb 20 11:27 libcudnn.so.5 -> libcudnn.so.5.1.10\n-rwxr-xr-x 1 root root 84163560 Feb 20 11:27 libcudnn.so.5.1.10\n\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nStandard TF pip url.\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\n$> python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n1.0.0\n\nSteps to reproduce (Firefox only)\n\nOn the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg cost or accuracy) by expanding the tab. \nClick on the expand icon \nEnable log scale of y-axis \nDisable log scale of y-axis (note the bug happens regardless of whether you do this) \nClick on expand icon to shrink the graph.\n\nThe graph is now overflowing: \nWhat other attempted solutions have you tried?\nTried to reproduce in Chromium 55.0.2883.87 but unable to.",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "7",
      "number": "8199",
      "pretext": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone although a search for distorted image tensorboard doesn't help much...\nEnvironment info\nOperating System: 16.04 LTS\nFirefox: 51.0.1 (64-bit)\nTF: 1.0 (installed via pip)\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$> sudo ls -l /usr/local/cudnn/*\n/usr/local/cudnn/include:\ntotal 100\n-r--r--r-- 1 root root 99658 Feb 20 11:27 cudnn.h\n\n/usr/local/cudnn/lib64:\ntotal 150908\nlrwxrwxrwx 1 root root       13 Feb 20 11:27 libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       35 Feb 16 17:01 libcudnn.so.4 -> /usr/local/cuda/lib64/libcudnn.so.4\nlrwxrwxrwx 1 root root       39 Feb 16 17:01 libcudnn.so.4.0.7 -> /usr/local/cuda/lib64/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       18 Feb 20 11:27 libcudnn.so.5 -> libcudnn.so.5.1.10\n-rwxr-xr-x 1 root root 84163560 Feb 20 11:27 libcudnn.so.5.1.10\n\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nStandard TF pip url.\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\n$> python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n1.0.0\n\nSteps to reproduce (Firefox only)\n\nOn the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg cost or accuracy) by expanding the tab. \nClick on the expand icon \nEnable log scale of y-axis \nDisable log scale of y-axis (note the bug happens regardless of whether you do this) \nClick on expand icon to shrink the graph.\n\nThe graph is now overflowing: \nWhat other attempted solutions have you tried?\nTried to reproduce in Chromium 55.0.2883.87 but unable to.",
      "title": "Tensorboard scalar summary graph distorts after resizing and toggling log scale on y-axis "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2488,
    "text": "Tensorflow lite 0.1.1 causing Build to failI am trying to use tensrflow-lite in Android. When I add\ncompile 'org.tensorflow:tensorflow-lite:0.1.1'\nI get:\nError:Execution failed for task ':sample:transformClassesWithJarMergingForDebug'.\n> com.android.build.api.transform.TransformException: java.util.zip.ZipException: duplicate entry: R.class\n\nI am using multidex and AGP 2.3.3.\nWhen I take tensorflow-lite off, the app builds correctly. When I put it back, the build fails. I believe this is a bug in the library.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "8",
      "number": "15645",
      "pretext": "I am trying to use tensrflow-lite in Android. When I add\ncompile 'org.tensorflow:tensorflow-lite:0.1.1'\nI get:\nError:Execution failed for task ':sample:transformClassesWithJarMergingForDebug'.\n> com.android.build.api.transform.TransformException: java.util.zip.ZipException: duplicate entry: R.class\n\nI am using multidex and AGP 2.3.3.\nWhen I take tensorflow-lite off, the app builds correctly. When I put it back, the build fails. I believe this is a bug in the library.",
      "title": "Tensorflow lite 0.1.1 causing Build to fail"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2489,
    "text": "How to install tensorflow in python3.7?Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nPython version:\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "9",
      "number": "21851",
      "pretext": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nPython version:\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
      "title": "How to install tensorflow in python3.7?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2490,
    "text": "fatal problem with saving variablesI coded a simple feedforward neural network and it works very well.\nI tried to save the computation time, and created:\nself.total_time = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')\nin the fnn class.\nand i tried to print the total training time per some training epoch.\nI made it to grow with time:\n# Check & Print training time\ntill_now = time.time() - start_time\nself.total_time += till_now\nprint_time(self.total_time.eval())\nand the result look something like this :\nEpoch :   0 | Evaluation :  115 | Learning Rate : 0.50\nTraining Loss :         0.040919\nValidation Loss :      0.0741969\nValidation Accuracy :      97.77%\nTotal time cost : 0.38 seconds\nEpoch :   1 | Evaluation :  116 | Learning Rate : 0.50\nTraining Loss :        0.0417941\nValidation Loss :       0.073841\nValidation Accuracy :      97.73%\nTotal time cost : 0.71 seconds\nEpoch :   2 | Evaluation :  117 | Learning Rate : 0.50\nTraining Loss :        0.0334573\nValidation Loss :      0.0745566\nValidation Accuracy :      97.75%\nTotal time cost : 1.01 seconds\nHowever, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of variable total_time and it initialized as 0 which is the value i first give to.\nI also checked tf.global_variables() include self.total_time.\nWhat is wrong?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "10",
      "number": "10074",
      "pretext": "I coded a simple feedforward neural network and it works very well.\nI tried to save the computation time, and created:\nself.total_time = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')\nin the fnn class.\nand i tried to print the total training time per some training epoch.\nI made it to grow with time:\n# Check & Print training time\ntill_now = time.time() - start_time\nself.total_time += till_now\nprint_time(self.total_time.eval())\nand the result look something like this :\nEpoch :   0 | Evaluation :  115 | Learning Rate : 0.50\nTraining Loss :         0.040919\nValidation Loss :      0.0741969\nValidation Accuracy :      97.77%\nTotal time cost : 0.38 seconds\nEpoch :   1 | Evaluation :  116 | Learning Rate : 0.50\nTraining Loss :        0.0417941\nValidation Loss :       0.073841\nValidation Accuracy :      97.73%\nTotal time cost : 0.71 seconds\nEpoch :   2 | Evaluation :  117 | Learning Rate : 0.50\nTraining Loss :        0.0334573\nValidation Loss :      0.0745566\nValidation Accuracy :      97.75%\nTotal time cost : 1.01 seconds\nHowever, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of variable total_time and it initialized as 0 which is the value i first give to.\nI also checked tf.global_variables() include self.total_time.\nWhat is wrong?",
      "title": "fatal problem with saving variables"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2491,
    "text": "Forward mode ad, directional derivativesSay I have outputs = f(inputs) and g of the same shape as inputs. I'd like to compute the directional derivative of outputs with respect to inputs in the direction g - in other words, the derivative of f(inputs + alpha * g) with respect to alpha at the point alpha=0.\nThis is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "11",
      "number": "4431",
      "pretext": "Say I have outputs = f(inputs) and g of the same shape as inputs. I'd like to compute the directional derivative of outputs with respect to inputs in the direction g - in other words, the derivative of f(inputs + alpha * g) with respect to alpha at the point alpha=0.\nThis is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?",
      "title": "Forward mode ad, directional derivatives"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2492,
    "text": "TPU supportI wanna add support for my Tensor Processing Unit chip in TensorFlow.\nMy TPU is implemented as an accelerator for ARM v7 32bit processor and implements multiple arithmetic kernels, similar to GPU. It implements a simple memory mapped interface, SGDMA and vector instructions over tensors. I added vector extension to GCC 7.1.1 and can run bare metal C++ nets on embedded Ubuntu 16.04\nI also checked the TF port  for Raspberry Pi 3, but it looks outdated and barely supported.\nI'm not currently aware about the scope of work, but believe that should not be that complex, given open examples from GPU vendors and already existing port for Google TPU\nAnyone interested in joining this project is highly welcome! Advise, links and code examples are much appreciated\nThank you",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "12",
      "number": "11937",
      "pretext": "I wanna add support for my Tensor Processing Unit chip in TensorFlow.\nMy TPU is implemented as an accelerator for ARM v7 32bit processor and implements multiple arithmetic kernels, similar to GPU. It implements a simple memory mapped interface, SGDMA and vector instructions over tensors. I added vector extension to GCC 7.1.1 and can run bare metal C++ nets on embedded Ubuntu 16.04\nI also checked the TF port  for Raspberry Pi 3, but it looks outdated and barely supported.\nI'm not currently aware about the scope of work, but believe that should not be that complex, given open examples from GPU vendors and already existing port for Google TPU\nAnyone interested in joining this project is highly welcome! Advise, links and code examples are much appreciated\nThank you",
      "title": "TPU support"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2493,
    "text": "building from source with branch r1.7 gives tf1.5.1 after building wheelPlease go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04.9\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below):1.5.1 after successful building, but i want 1.7\nPython version: 3.6\nBazel version (if compiling from source): build label 0.11.1\nGCC/Compiler version (if compiling from source):5.4.0 when i type gcc --version, 7.2.0 shown in python terminal\nCUDA/cuDNN version:cuda 9, cudnn 7\nGPU model and memory: gtx1080 ti 11 gb\nExact command to reproduce: following this https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nI've make sure i'd pull everything from tensorflow. i remove all other branch and check out r1.7, building was successful. No errors and stuff. The wheel i got says tensorflow-1.5.1-cp36 ... etc. , i go on to install it, tf.version = 1.5.1 . I am confused how to build tf 1.7 from source.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "13",
      "number": "18108",
      "pretext": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04.9\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below):1.5.1 after successful building, but i want 1.7\nPython version: 3.6\nBazel version (if compiling from source): build label 0.11.1\nGCC/Compiler version (if compiling from source):5.4.0 when i type gcc --version, 7.2.0 shown in python terminal\nCUDA/cuDNN version:cuda 9, cudnn 7\nGPU model and memory: gtx1080 ti 11 gb\nExact command to reproduce: following this https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nI've make sure i'd pull everything from tensorflow. i remove all other branch and check out r1.7, building was successful. No errors and stuff. The wheel i got says tensorflow-1.5.1-cp36 ... etc. , i go on to install it, tf.version = 1.5.1 . I am confused how to build tf 1.7 from source.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
      "title": "building from source with branch r1.7 gives tf1.5.1 after building wheel"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2494,
    "text": "AttributeError: 'NoneType' object has no attribute 'rfind'OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n-TensorFlow installed from (source):\ntensorflow v1.6.0-0-gd2e24b6039 1.6.0:\nPython 3.5:\nReproduce\ngit clone tensorflow\npython3 tensorflow/examples/speech_commands/train.py\npython3 tensorflow/examples/speech_commands/freeze.py \n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \n--output_file=/tmp/my_frozen_graph.pb\nError appears:\n/home/lukas/.local/lib/python3.5/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\nfrom ._conv import register_converters as _register_converters\n2018-05-22 21:59:00.562103: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\nConverted 6 variables to const ops.\nTraceback (most recent call last):\nFile \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 180, in \ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\nFile \"/home/lukas/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\nFile \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 124, in main\nos.path.dirname(FLAGS.output_file),\nFile \"/usr/lib/python3.5/posixpath.py\", line 148, in dirname\ni = p.rfind(sep) + 1\nAttributeError: 'NoneType' object has no attribute 'rfind'\nThanks",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "14",
      "number": "19463",
      "pretext": "OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n-TensorFlow installed from (source):\ntensorflow v1.6.0-0-gd2e24b6039 1.6.0:\nPython 3.5:\nReproduce\ngit clone tensorflow\npython3 tensorflow/examples/speech_commands/train.py\npython3 tensorflow/examples/speech_commands/freeze.py \n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \n--output_file=/tmp/my_frozen_graph.pb\nError appears:\n/home/lukas/.local/lib/python3.5/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\nfrom ._conv import register_converters as _register_converters\n2018-05-22 21:59:00.562103: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\nConverted 6 variables to const ops.\nTraceback (most recent call last):\nFile \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 180, in \ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\nFile \"/home/lukas/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\nFile \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 124, in main\nos.path.dirname(FLAGS.output_file),\nFile \"/usr/lib/python3.5/posixpath.py\", line 148, in dirname\ni = p.rfind(sep) + 1\nAttributeError: 'NoneType' object has no attribute 'rfind'\nThanks",
      "title": "AttributeError: 'NoneType' object has no attribute 'rfind'"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2495,
    "text": "Single scalar summary point not visible in the plotWhen only one event is available for a scalar summary, the plot remains empty, as shown below (here the value is 2.0):\n\nIt would be great to see one point corresponding to the value instead. The value does appear in the JSON/CSV file.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "15",
      "number": "459",
      "pretext": "When only one event is available for a scalar summary, the plot remains empty, as shown below (here the value is 2.0):\n\nIt would be great to see one point corresponding to the value instead. The value does appear in the JSON/CSV file.",
      "title": "Single scalar summary point not visible in the plot"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2496,
    "text": "tf.contrib.rnn.decoder does not require explicitly build encoder?As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.\nThe tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "16",
      "number": "10171",
      "pretext": "As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.\nThe tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?",
      "title": "tf.contrib.rnn.decoder does not require explicitly build encoder?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2497,
    "text": "Tensorflow r.0.10, CUDA 8.0, cuDNN 5.1 core dumped, CUDA_ERROR_OUT_OF_MEMORYOn running a benchmark with MNIST data on a CNN (source below) tensorflow first complains about memory allocation and then appears to have trouble using cuDNN 5.1\nDetailed script and output at the bottom.\nProblem appears to affect cuDNN specifically as I could run CUDA examples as well as matmul on tensorflow without problems.\nEnvironment info\nOperating System: ubuntu 16.04\nuname -a\nLinux  4.4.0-34-generic #53-Ubuntu SMP Wed Jul 27 16:06:39 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\nInstalled version of CUDA and cuDNN:\nls -l $CUDA_HOME/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Aug 15 22:51 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Aug 15 22:51 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn_static.a\nEnvironment variables\necho $LD_LIBRARY_PATH\n/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\necho $CUDA_HOME\n/usr/local/cuda\necho $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin/:/usr/local/cuda/bin/\nTensorflow version\nCompiled from source, r0.10, built into pip package and installed this pip wheel\nConfigured with cuDNN path and version set to system default\n\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\nsee /// OUTPUT /// at the end\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nn.a.\nversion r0.10\nThe output of bazel version\nbazel version\nBuild label: 0.3.1-2016-08-15 (@936c2c2)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Sun Aug 14 23:07:32 2016 (1471216052)\nBuild timestamp: 1471216052\nBuild timestamp as int: 1471216052\n\nSteps to reproduce: run this script\nimport numpy\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\nload data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nreshape to be [samples][channels][width][height]\nX_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\nnormalize inputs from 0-255 to 0-1\nX_train = X_train / 255\nX_test = X_test / 255\none hot encode outputs\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\ndefine a simple CNN model\ndef baseline_model():\n##### create model\nmodel = Sequential()\nmodel.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n##### Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nreturn model\nbuild the model\nmodel = baseline_model()\nFit the model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)\nFinal evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n///////////// OUTPUT /////////////////\nUsing TensorFlow backend.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 148.69MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:840] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 148.69M (155910144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/10\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\nAborted (core dumped)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "17",
      "number": "3864",
      "pretext": "On running a benchmark with MNIST data on a CNN (source below) tensorflow first complains about memory allocation and then appears to have trouble using cuDNN 5.1\nDetailed script and output at the bottom.\nProblem appears to affect cuDNN specifically as I could run CUDA examples as well as matmul on tensorflow without problems.\nEnvironment info\nOperating System: ubuntu 16.04\nuname -a\nLinux  4.4.0-34-generic #53-Ubuntu SMP Wed Jul 27 16:06:39 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\nInstalled version of CUDA and cuDNN:\nls -l $CUDA_HOME/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Aug 15 22:51 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Aug 15 22:51 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn_static.a\nEnvironment variables\necho $LD_LIBRARY_PATH\n/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\necho $CUDA_HOME\n/usr/local/cuda\necho $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin/:/usr/local/cuda/bin/\nTensorflow version\nCompiled from source, r0.10, built into pip package and installed this pip wheel\nConfigured with cuDNN path and version set to system default\n\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\nsee /// OUTPUT /// at the end\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nn.a.\nversion r0.10\nThe output of bazel version\nbazel version\nBuild label: 0.3.1-2016-08-15 (@936c2c2)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Sun Aug 14 23:07:32 2016 (1471216052)\nBuild timestamp: 1471216052\nBuild timestamp as int: 1471216052\n\nSteps to reproduce: run this script\nimport numpy\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\nload data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nreshape to be [samples][channels][width][height]\nX_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\nnormalize inputs from 0-255 to 0-1\nX_train = X_train / 255\nX_test = X_test / 255\none hot encode outputs\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\ndefine a simple CNN model\ndef baseline_model():\n##### create model\nmodel = Sequential()\nmodel.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n##### Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nreturn model\nbuild the model\nmodel = baseline_model()\nFit the model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)\nFinal evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n///////////// OUTPUT /////////////////\nUsing TensorFlow backend.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 148.69MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:840] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 148.69M (155910144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/10\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\nAborted (core dumped)",
      "title": "Tensorflow r.0.10, CUDA 8.0, cuDNN 5.1 core dumped, CUDA_ERROR_OUT_OF_MEMORY"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2498,
    "text": "Multiple Classes fails in Eager Mode (\"tf.keras.Model\")System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nBazel version:\nN/A\nCUDA/cuDNN version:\nN/A\nGPU model and memory:\nN/A\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu\nTensorFlow installed from (source or binary):\nInstalled utilizing pip\nTensorFlow version (use command below):\n1.7\nPython version:\n3.6\nExact command to reproduce:\n\nimport tensorflow as tf  \nimport tensorflow.contrib.eager as tfe  \n\ntfe.enable_eager_execution()\n\nclass CustomLayer(tf.keras.Model):\n    def __init__(self):\n        super(CustomLayer, self).__init__()\n        print(\"blah\")\n\nclass CustomNetwork(tf.keras.Model):\n    def __init__(self):\n        super(CustomNetwork, self).__init__()\n        self.custom_layers = CustomLayer()\n\n    def forward(self, x, y=None):\n        x = self.custom_layers(x)\n\nCustomNetwork().forward(tf.convert_to_tensor([1]))\n\nDescribe the problem\nTrying to utilize multiple classes fails in tensorflow eager mode utilizing \"tf.keras.Model\". If I change \"tf.keras.Model\" to \"tfe.Network\" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:\nSource code / logs\nblah\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-12-9afa9b91ddef> in <module>()\n----> 1 CustomNetwork().forward(tf.convert_to_tensor([1]))\n\n<ipython-input-11-484119102aec> in forward(self, x, y)\n      5 \n      6     def forward(self, x, y=None):\n----> 7         x = self.custom_layers(x)\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)\n    237     \"\"\"\n    238     # Actually call the layer (optionally building it).\n--> 239     output = super(Layer, self).__call__(inputs, **kwargs)\n    240     if context.executing_eagerly():\n    241       return output\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    712 \n    713         if not in_deferred_mode:\n--> 714           outputs = self.call(inputs, *args, **kwargs)\n    715           if outputs is None:\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in call(self, inputs, training, mask)\n    635     outputs, _ = self._run_internal_graph(inputs,\n    636                                           training=training,\n--> 637                                           mask=masks)\n    638     return outputs\n    639 \n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\n    770     # does not return a list the same size as `call`\n    771     tensor_map = {}\n--> 772     for x, y, mask in zip(self.inputs, inputs, masks):\n    773       tensor_map[str(id(x))] = (y, mask)\n    774 \n\nTypeError: zip argument #1 must support iteration",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "18",
      "number": "18763",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nBazel version:\nN/A\nCUDA/cuDNN version:\nN/A\nGPU model and memory:\nN/A\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu\nTensorFlow installed from (source or binary):\nInstalled utilizing pip\nTensorFlow version (use command below):\n1.7\nPython version:\n3.6\nExact command to reproduce:\n\nimport tensorflow as tf  \nimport tensorflow.contrib.eager as tfe  \n\ntfe.enable_eager_execution()\n\nclass CustomLayer(tf.keras.Model):\n    def __init__(self):\n        super(CustomLayer, self).__init__()\n        print(\"blah\")\n\nclass CustomNetwork(tf.keras.Model):\n    def __init__(self):\n        super(CustomNetwork, self).__init__()\n        self.custom_layers = CustomLayer()\n\n    def forward(self, x, y=None):\n        x = self.custom_layers(x)\n\nCustomNetwork().forward(tf.convert_to_tensor([1]))\n\nDescribe the problem\nTrying to utilize multiple classes fails in tensorflow eager mode utilizing \"tf.keras.Model\". If I change \"tf.keras.Model\" to \"tfe.Network\" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:\nSource code / logs\nblah\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-12-9afa9b91ddef> in <module>()\n----> 1 CustomNetwork().forward(tf.convert_to_tensor([1]))\n\n<ipython-input-11-484119102aec> in forward(self, x, y)\n      5 \n      6     def forward(self, x, y=None):\n----> 7         x = self.custom_layers(x)\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)\n    237     \"\"\"\n    238     # Actually call the layer (optionally building it).\n--> 239     output = super(Layer, self).__call__(inputs, **kwargs)\n    240     if context.executing_eagerly():\n    241       return output\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    712 \n    713         if not in_deferred_mode:\n--> 714           outputs = self.call(inputs, *args, **kwargs)\n    715           if outputs is None:\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in call(self, inputs, training, mask)\n    635     outputs, _ = self._run_internal_graph(inputs,\n    636                                           training=training,\n--> 637                                           mask=masks)\n    638     return outputs\n    639 \n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\n    770     # does not return a list the same size as `call`\n    771     tensor_map = {}\n--> 772     for x, y, mask in zip(self.inputs, inputs, masks):\n    773       tensor_map[str(id(x))] = (y, mask)\n    774 \n\nTypeError: zip argument #1 must support iteration",
      "title": "Multiple Classes fails in Eager Mode (\"tf.keras.Model\")"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2499,
    "text": "Package not Reslove.Hello,\nI am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:\nimport org.tensorflow.DataType;\nimport org.tensorflow.Graph;\nimport org.tensorflow.Session;\nimport org.tensorflow.Tensor;\nimport org.tensorflow.TensorFlow;\nCan any one help to find out where some thing is missing.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "19",
      "number": "7683",
      "pretext": "Hello,\nI am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:\nimport org.tensorflow.DataType;\nimport org.tensorflow.Graph;\nimport org.tensorflow.Session;\nimport org.tensorflow.Tensor;\nimport org.tensorflow.TensorFlow;\nCan any one help to find out where some thing is missing.",
      "title": "Package not Reslove."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2500,
    "text": "embedding_attention_seq2seq fails / embedding_rnn_seq2seq worksEnvironment info\nOperating System: Ubuntu 14.04\nInstalled version of CUDA and cuDNN: Cuda 7.0 and CUDNN 6.5 v4\nSo when I use a simple Embedding RNN Sequence to Sequence Model like this\n# choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        # embedding model\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n\nThe above implementation works perfectly, but when I just change the model from simple embedding seq2seq to Embedding Attention Seq2Seq, like this,\n\n        # choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n\n\nI get segmentation fault, with absolutely no information. My memory does not run out, neither my CPU, as I tried this with\nbatch_size =1 \nsee.memory_dim = 1\n\nand still got the same segmentation fault.\nI get the same error, and the above configuration can certainly not eat my RAM.\nThis is a potential bug, if I am not getting something worng. The LSTM and GRU cell just takes the size of the hidden layer as parameter, which is a scaler.\nTHE BUG REPORT\nThe Debug result\n(gdb) run train_script_lstm_attn.py \nStarting program: /lusr/bin/python train_script_lstm_attn.py\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n[New Thread 0x7fffd7a25700 (LWP 45650)]\n[New Thread 0x7fffd7224700 (LWP 45651)]\n[New Thread 0x7fffd4a23700 (LWP 45652)]\n[New Thread 0x7fffd2222700 (LWP 45653)]\n[New Thread 0x7fffcfa21700 (LWP 45654)]\n[New Thread 0x7fffcd220700 (LWP 45655)]\n[New Thread 0x7fffcaa1f700 (LWP 45656)]\n[Thread 0x7fffcaa1f700 (LWP 45656) exited]\n[Thread 0x7fffcfa21700 (LWP 45654) exited]\n[Thread 0x7fffd7a25700 (LWP 45650) exited]\n[Thread 0x7fffd2222700 (LWP 45653) exited]\n[Thread 0x7fffd7224700 (LWP 45651) exited]\n[Thread 0x7fffcd220700 (LWP 45655) exited]\n[Thread 0x7fffd4a23700 (LWP 45652) exited]\n[New Thread 0x7fffcaa1f700 (LWP 45661)]\n[New Thread 0x7fffcd220700 (LWP 46103)]\n[New Thread 0x7fffcfa21700 (LWP 46104)]\n[New Thread 0x7fffd2222700 (LWP 46105)]\n[New Thread 0x7ffed22bf700 (LWP 46106)]\n[New Thread 0x7ffed1abe700 (LWP 46107)]\n[New Thread 0x7ffed12bd700 (LWP 46108)]\n[New Thread 0x7ffed0abc700 (LWP 46109)]\n[New Thread 0x7ffec3fff700 (LWP 46110)]\n[New Thread 0x7ffec37fe700 (LWP 46111)]\n[New Thread 0x7ffec2ffd700 (LWP 46112)]\n[New Thread 0x7ffeb77ff700 (LWP 46114)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\n[New Thread 0x7ffeb6ffe700 (LWP 46115)]\n[New Thread 0x7ffeb67fd700 (LWP 46116)]\n[New Thread 0x7ffea2bff700 (LWP 46117)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:42:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:42:00.0)\n[New Thread 0x7ffea23fe700 (LWP 46118)]\n[New Thread 0x7ffea1bfd700 (LWP 46119)]\n[New Thread 0x7ffea13fc700 (LWP 46120)]\n[New Thread 0x7ffea0bfb700 (LWP 46121)]\n[New Thread 0x7ffe8bfff700 (LWP 46122)]\n[New Thread 0x7ffe8b7fe700 (LWP 46123)]\n[New Thread 0x7ffe8affd700 (LWP 46124)]\n[New Thread 0x7ffe8a7fc700 (LWP 46125)]\n[New Thread 0x7ffe89ffb700 (LWP 46126)]\n[New Thread 0x7ffe897fa700 (LWP 46127)]\n[New Thread 0x7ffe88ff9700 (LWP 46128)]\n[New Thread 0x7ffe7bfff700 (LWP 46129)]\n[New Thread 0x7ffe3d23c700 (LWP 46167)]\n\nProgram received signal SIGSEGV, Segmentation fault.\n__memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n2143    ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S: No such file or directory.\n\nThe Backtrace is attached below\n(gdb) backtrace\n#0  __memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n#1  0x00007fffedd56ae1 in tensorflow::Tensor::FromProto(tensorflow::Allocator*, tensorflow::TensorProto const&) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fffedc4577f in tensorflow::ThreadPoolDevice::MakeTensorFromProto(tensorflow::TensorProto const&, tensorflow::AllocatorAttributes, tensorflow::Tensor*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fffecea4f76 in tensorflow::ConstantOp::ConstantOp(tensorflow::OpKernelConstruction*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fffecea50f2 in tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fffedd408bd in tensorflow::CreateOpKernel(tensorflow::DeviceType, tensorflow::DeviceBase*, tensorflow::Allocator*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fffedc1caf4 in tensorflow::CreateNonCachedKernel(tensorflow::Device*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fffedc10c97 in std::_Function_handler<tensorflow::Status (tensorflow::NodeDef const&, tensorflow::OpKernel**), tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*)::{lambda(tensorflow::NodeDef const&, tensorflow::OpKernel**)#2}>::_M_invoke(std::_Any_data const&, tensorflow::NodeDef const&, tensorflow::OpKernel**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fffedc287de in tensorflow::(anonymous namespace)::ExecutorImpl::Initialize() ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fffedc292b3 in tensorflow::NewLocalExecutor(tensorflow::LocalExecutorParams const&, tensorflow::Graph const*, tensorflow::Executor**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fffedc1608d in tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00007fffedc36dda in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Device*, tensorflow::Graph**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00007fffeda06d9d in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00007fffeda07e6a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std---Type <return> to continue, or q <return> to quit---\n::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00007fffeda0a992 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00007fffedc0b7c7 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00007fffedc0bc11 in TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00007fffece8dff5 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00007fffece8e661 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#19 0x00007fffece7a4d7 in _wrap_TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#20 0x000000000049968d in call_function (oparg=<optimized out>, pp_stack=0x7fffffffdb20) at ../Python/ceval.c:4020\n#21 PyEval_EvalFrameEx (f=f@entry=\n    Frame 0xe341a40, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 628, in _run_fn (session=<SwigPyObject at remote 0x7fffb3547b70>, feed_dict={}, fetch_list=[], target_list=['init'], options=None, run_metadata=<TF_Buffer(this=<SwigPyObject at remote 0x7ffe3d2864e0>) at remote 0x7ffe496ff990>), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#22 0x00000000004a1c9a in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, \n    kwcount=<optimized out>, kws=<optimized out>, argcount=238295616, args=<optimized out>, locals=0x0, \n    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252\n#23 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526\n#24 0x0000000000505f96 in PyObject_Call (func=<function at remote 0x7ffe3d39d938>, arg=<optimized out>, kw=<optimized out>)\n    at ../Objects/abstract.c:2529\n#25 0x000000000049b07a in ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, \n    pp_stack=0x7fffffffdd60, func=<function at remote 0x7ffe3d39d938>) at ../Python/ceval.c:4333\n---Type <return> to continue, or q <return> to quit---\n#26 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe341820, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 644, in _do_call (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Ne...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2705\n#27 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25330, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#28 0x000000000049ab45 in fast_function (nk=0, na=8, n=<optimized out>, pp_stack=0x7fffffffdf50, \n    func=<function at remote 0x7fffc3e28de8>) at ../Python/ceval.c:4116\n#29 call_function (oparg=<optimized out>, pp_stack=0x7fffffffdf50) at ../Python/ceval.c:4041\n#30 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe3415e0, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 637, in _do_run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neu...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#31 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25230, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#32 0x000000000049ab45 in fast_function (nk=0, na=7, n=<optimized out>, pp_stack=0x7fffffffe140, \n    func=<function at remote 0x7fffc3e28d70>) at ../Python/ceval.c:4116\n#33 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe140) at ../Python/ceval.c:4041\n#34 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe33d060, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 564, in _run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variabl---Type <return> to continue, or q <return> to quit---\nes': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#35 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25030, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#36 0x000000000049ab45 in fast_function (nk=0, na=6, n=<optimized out>, pp_stack=0x7fffffffe330, \n    func=<function at remote 0x7fffc3e28cf8>) at ../Python/ceval.c:4116\n#37 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe330) at ../Python/ceval.c:4041\n#38 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x1648a420, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 340, in run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#39 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e22a30, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x7fffc3e24608, defcount=3, \n    closure=0x0) at ../Python/ceval.c:3252\n#40 0x0000000000499a52 in fast_function (nk=0, na=2, n=<optimized out>, pp_stack=0x7fffffffe520, \n    func=<function at remote 0x7fffc3e28b18>) at ../Python/ceval.c:4116\n#41 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe520) at ../Python/ceval.c:4041\n#42 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7ffe3d280de0, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 125, in __start_session (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_t---Type <return> to continue, or q <return> to quit---\nype_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#43 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe670, \n    func=<function at remote 0x7fffb4449b18>) at ../Python/ceval.c:4106\n#44 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe670) at ../Python/ceval.c:4041\n#45 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7fffb3511050, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 62, in form_model_graph (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#46 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe7c0, \n    func=<function at remote 0x7fffb4449938>) at ../Python/ceval.c:4106\n#47 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe7c0) at ../Python/ceval.c:4041\n#48 PyEval_EvalFrameEx (f=f@entry=Frame 0x7ffff7ebf7b0, for file train_script_lstm_attn.py, line 11, in <module> (), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#49 0x00000000004a1634 in PyEval_EvalCodeEx (closure=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, \nPython Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n    locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:3252\nPython Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n#50 PyEval_EvalCode (locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:667\n#51 run_mod.42576 (mod=mod@entry=0x9c1f30, filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n---Type <return> to continue, or q <return> to quit---\n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), flags=flags@entry=0x7fffffffe970, \n    arena=arena@entry=0x9aa9c0) at ../Python/pythonrun.c:1370\n#52 0x000000000044e4a5 in PyRun_FileExFlags (fp=fp@entry=0x976cd0, \n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", start=start@entry=257, \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:1356\n#53 0x000000000044ec9f in PyRun_SimpleFileExFlags (fp=fp@entry=0x976cd0, filename=<optimized out>, \n    filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:948\n#54 0x000000000044ed9b in PyRun_AnyFileExFlags (fp=fp@entry=0x976cd0, \n---Type <return> to continue, or q <return> to quit---\n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, \n    flags=flags@entry=0x7fffffffe970) at ../Python/pythonrun.c:752\n#55 0x000000000044f904 in Py_Main (argc=<optimized out>, argv=0x7fffffffeb28) at ../Modules/main.c:640\n#56 0x00007ffff7818ec5 in __libc_start_main (main=0x44f9c2 <main>, argc=2, argv=0x7fffffffeb28, init=<optimized out>, \n    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffeb18) at libc-start.c:287\n#57 0x0000000000578c4e in _start ()\n(gdb)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "20",
      "number": "2138",
      "pretext": "Environment info\nOperating System: Ubuntu 14.04\nInstalled version of CUDA and cuDNN: Cuda 7.0 and CUDNN 6.5 v4\nSo when I use a simple Embedding RNN Sequence to Sequence Model like this\n# choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        # embedding model\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n\nThe above implementation works perfectly, but when I just change the model from simple embedding seq2seq to Embedding Attention Seq2Seq, like this,\n\n        # choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n\n\nI get segmentation fault, with absolutely no information. My memory does not run out, neither my CPU, as I tried this with\nbatch_size =1 \nsee.memory_dim = 1\n\nand still got the same segmentation fault.\nI get the same error, and the above configuration can certainly not eat my RAM.\nThis is a potential bug, if I am not getting something worng. The LSTM and GRU cell just takes the size of the hidden layer as parameter, which is a scaler.\nTHE BUG REPORT\nThe Debug result\n(gdb) run train_script_lstm_attn.py \nStarting program: /lusr/bin/python train_script_lstm_attn.py\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n[New Thread 0x7fffd7a25700 (LWP 45650)]\n[New Thread 0x7fffd7224700 (LWP 45651)]\n[New Thread 0x7fffd4a23700 (LWP 45652)]\n[New Thread 0x7fffd2222700 (LWP 45653)]\n[New Thread 0x7fffcfa21700 (LWP 45654)]\n[New Thread 0x7fffcd220700 (LWP 45655)]\n[New Thread 0x7fffcaa1f700 (LWP 45656)]\n[Thread 0x7fffcaa1f700 (LWP 45656) exited]\n[Thread 0x7fffcfa21700 (LWP 45654) exited]\n[Thread 0x7fffd7a25700 (LWP 45650) exited]\n[Thread 0x7fffd2222700 (LWP 45653) exited]\n[Thread 0x7fffd7224700 (LWP 45651) exited]\n[Thread 0x7fffcd220700 (LWP 45655) exited]\n[Thread 0x7fffd4a23700 (LWP 45652) exited]\n[New Thread 0x7fffcaa1f700 (LWP 45661)]\n[New Thread 0x7fffcd220700 (LWP 46103)]\n[New Thread 0x7fffcfa21700 (LWP 46104)]\n[New Thread 0x7fffd2222700 (LWP 46105)]\n[New Thread 0x7ffed22bf700 (LWP 46106)]\n[New Thread 0x7ffed1abe700 (LWP 46107)]\n[New Thread 0x7ffed12bd700 (LWP 46108)]\n[New Thread 0x7ffed0abc700 (LWP 46109)]\n[New Thread 0x7ffec3fff700 (LWP 46110)]\n[New Thread 0x7ffec37fe700 (LWP 46111)]\n[New Thread 0x7ffec2ffd700 (LWP 46112)]\n[New Thread 0x7ffeb77ff700 (LWP 46114)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\n[New Thread 0x7ffeb6ffe700 (LWP 46115)]\n[New Thread 0x7ffeb67fd700 (LWP 46116)]\n[New Thread 0x7ffea2bff700 (LWP 46117)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:42:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:42:00.0)\n[New Thread 0x7ffea23fe700 (LWP 46118)]\n[New Thread 0x7ffea1bfd700 (LWP 46119)]\n[New Thread 0x7ffea13fc700 (LWP 46120)]\n[New Thread 0x7ffea0bfb700 (LWP 46121)]\n[New Thread 0x7ffe8bfff700 (LWP 46122)]\n[New Thread 0x7ffe8b7fe700 (LWP 46123)]\n[New Thread 0x7ffe8affd700 (LWP 46124)]\n[New Thread 0x7ffe8a7fc700 (LWP 46125)]\n[New Thread 0x7ffe89ffb700 (LWP 46126)]\n[New Thread 0x7ffe897fa700 (LWP 46127)]\n[New Thread 0x7ffe88ff9700 (LWP 46128)]\n[New Thread 0x7ffe7bfff700 (LWP 46129)]\n[New Thread 0x7ffe3d23c700 (LWP 46167)]\n\nProgram received signal SIGSEGV, Segmentation fault.\n__memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n2143    ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S: No such file or directory.\n\nThe Backtrace is attached below\n(gdb) backtrace\n#0  __memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n#1  0x00007fffedd56ae1 in tensorflow::Tensor::FromProto(tensorflow::Allocator*, tensorflow::TensorProto const&) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fffedc4577f in tensorflow::ThreadPoolDevice::MakeTensorFromProto(tensorflow::TensorProto const&, tensorflow::AllocatorAttributes, tensorflow::Tensor*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fffecea4f76 in tensorflow::ConstantOp::ConstantOp(tensorflow::OpKernelConstruction*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fffecea50f2 in tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fffedd408bd in tensorflow::CreateOpKernel(tensorflow::DeviceType, tensorflow::DeviceBase*, tensorflow::Allocator*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fffedc1caf4 in tensorflow::CreateNonCachedKernel(tensorflow::Device*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fffedc10c97 in std::_Function_handler<tensorflow::Status (tensorflow::NodeDef const&, tensorflow::OpKernel**), tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*)::{lambda(tensorflow::NodeDef const&, tensorflow::OpKernel**)#2}>::_M_invoke(std::_Any_data const&, tensorflow::NodeDef const&, tensorflow::OpKernel**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fffedc287de in tensorflow::(anonymous namespace)::ExecutorImpl::Initialize() ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fffedc292b3 in tensorflow::NewLocalExecutor(tensorflow::LocalExecutorParams const&, tensorflow::Graph const*, tensorflow::Executor**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fffedc1608d in tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00007fffedc36dda in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Device*, tensorflow::Graph**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00007fffeda06d9d in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00007fffeda07e6a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std---Type <return> to continue, or q <return> to quit---\n::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00007fffeda0a992 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00007fffedc0b7c7 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00007fffedc0bc11 in TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00007fffece8dff5 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00007fffece8e661 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#19 0x00007fffece7a4d7 in _wrap_TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#20 0x000000000049968d in call_function (oparg=<optimized out>, pp_stack=0x7fffffffdb20) at ../Python/ceval.c:4020\n#21 PyEval_EvalFrameEx (f=f@entry=\n    Frame 0xe341a40, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 628, in _run_fn (session=<SwigPyObject at remote 0x7fffb3547b70>, feed_dict={}, fetch_list=[], target_list=['init'], options=None, run_metadata=<TF_Buffer(this=<SwigPyObject at remote 0x7ffe3d2864e0>) at remote 0x7ffe496ff990>), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#22 0x00000000004a1c9a in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, \n    kwcount=<optimized out>, kws=<optimized out>, argcount=238295616, args=<optimized out>, locals=0x0, \n    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252\n#23 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526\n#24 0x0000000000505f96 in PyObject_Call (func=<function at remote 0x7ffe3d39d938>, arg=<optimized out>, kw=<optimized out>)\n    at ../Objects/abstract.c:2529\n#25 0x000000000049b07a in ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, \n    pp_stack=0x7fffffffdd60, func=<function at remote 0x7ffe3d39d938>) at ../Python/ceval.c:4333\n---Type <return> to continue, or q <return> to quit---\n#26 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe341820, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 644, in _do_call (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Ne...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2705\n#27 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25330, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#28 0x000000000049ab45 in fast_function (nk=0, na=8, n=<optimized out>, pp_stack=0x7fffffffdf50, \n    func=<function at remote 0x7fffc3e28de8>) at ../Python/ceval.c:4116\n#29 call_function (oparg=<optimized out>, pp_stack=0x7fffffffdf50) at ../Python/ceval.c:4041\n#30 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe3415e0, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 637, in _do_run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neu...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#31 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25230, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#32 0x000000000049ab45 in fast_function (nk=0, na=7, n=<optimized out>, pp_stack=0x7fffffffe140, \n    func=<function at remote 0x7fffc3e28d70>) at ../Python/ceval.c:4116\n#33 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe140) at ../Python/ceval.c:4041\n#34 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe33d060, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 564, in _run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variabl---Type <return> to continue, or q <return> to quit---\nes': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#35 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25030, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#36 0x000000000049ab45 in fast_function (nk=0, na=6, n=<optimized out>, pp_stack=0x7fffffffe330, \n    func=<function at remote 0x7fffc3e28cf8>) at ../Python/ceval.c:4116\n#37 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe330) at ../Python/ceval.c:4041\n#38 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x1648a420, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 340, in run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#39 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e22a30, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x7fffc3e24608, defcount=3, \n    closure=0x0) at ../Python/ceval.c:3252\n#40 0x0000000000499a52 in fast_function (nk=0, na=2, n=<optimized out>, pp_stack=0x7fffffffe520, \n    func=<function at remote 0x7fffc3e28b18>) at ../Python/ceval.c:4116\n#41 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe520) at ../Python/ceval.c:4041\n#42 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7ffe3d280de0, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 125, in __start_session (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_t---Type <return> to continue, or q <return> to quit---\nype_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#43 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe670, \n    func=<function at remote 0x7fffb4449b18>) at ../Python/ceval.c:4106\n#44 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe670) at ../Python/ceval.c:4041\n#45 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7fffb3511050, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 62, in form_model_graph (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#46 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe7c0, \n    func=<function at remote 0x7fffb4449938>) at ../Python/ceval.c:4106\n#47 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe7c0) at ../Python/ceval.c:4041\n#48 PyEval_EvalFrameEx (f=f@entry=Frame 0x7ffff7ebf7b0, for file train_script_lstm_attn.py, line 11, in <module> (), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#49 0x00000000004a1634 in PyEval_EvalCodeEx (closure=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, \nPython Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n    locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:3252\nPython Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n#50 PyEval_EvalCode (locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:667\n#51 run_mod.42576 (mod=mod@entry=0x9c1f30, filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n---Type <return> to continue, or q <return> to quit---\n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), flags=flags@entry=0x7fffffffe970, \n    arena=arena@entry=0x9aa9c0) at ../Python/pythonrun.c:1370\n#52 0x000000000044e4a5 in PyRun_FileExFlags (fp=fp@entry=0x976cd0, \n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", start=start@entry=257, \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:1356\n#53 0x000000000044ec9f in PyRun_SimpleFileExFlags (fp=fp@entry=0x976cd0, filename=<optimized out>, \n    filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:948\n#54 0x000000000044ed9b in PyRun_AnyFileExFlags (fp=fp@entry=0x976cd0, \n---Type <return> to continue, or q <return> to quit---\n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, \n    flags=flags@entry=0x7fffffffe970) at ../Python/pythonrun.c:752\n#55 0x000000000044f904 in Py_Main (argc=<optimized out>, argv=0x7fffffffeb28) at ../Modules/main.c:640\n#56 0x00007ffff7818ec5 in __libc_start_main (main=0x44f9c2 <main>, argc=2, argv=0x7fffffffeb28, init=<optimized out>, \n    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffeb18) at libc-start.c:287\n#57 0x0000000000578c4e in _start ()\n(gdb)",
      "title": "embedding_attention_seq2seq fails / embedding_rnn_seq2seq works"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2501,
    "text": "Tensorflow inconsistence results every runPlease go to Stack Overflow for help and support:\nhttp://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug or a feature request.\nThe form below must be filled out.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nBazel version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "21",
      "number": "9570",
      "pretext": "Please go to Stack Overflow for help and support:\nhttp://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug or a feature request.\nThe form below must be filled out.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nBazel version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
      "title": "Tensorflow inconsistence results every run"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2502,
    "text": "tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.Hello,I got the error when i  execute:\"pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\"--------[ tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.].",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "22",
      "number": "96",
      "pretext": "Hello,I got the error when i  execute:\"pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\"--------[ tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.].",
      "title": "tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2503,
    "text": "when connect mnist,download mnist data,show network connection error.I know if the error show ,just our company can not connect to mnist, can i manual download mnist data, and use it? how can i do this?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "23",
      "number": "11633",
      "pretext": "I know if the error show ,just our company can not connect to mnist, can i manual download mnist data, and use it? how can i do this?",
      "title": "when connect mnist,download mnist data,show network connection error."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2504,
    "text": "How to resize one tensor to (e.g., 1.5 * its original shape)?The tensor shape is not fixed, and can change with different input.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "24",
      "number": "2269",
      "pretext": "The tensor shape is not fixed, and can change with different input.",
      "title": "How to resize one tensor to (e.g., 1.5 * its original shape)?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2505,
    "text": "Feature request: add a `local_init_feed_dict` to `tf.train.Scaffold`System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.2.0\nPython version: 3.6.1\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: See below.\n\nDescribe the problem\nFeature request: add a local_init_feed_dict to tf.train.Scaffold. It would be useful to be able to create local variables (which are not saved or restored) and have them initialized by a tf.train.MonitoredTrainingSession with a feed_dict. In the example below, the variable X_var is forced to be part of the GLOBAL_VARIABLES collection in order to be able to initialize the variable with a feed_dict. This has the undesirable consequence that the variable will be saved to disk.\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\n\n# Data that we wish to sample, but not save to disk.\nX = np.eye(15, dtype=np.float32)\n\n# Create a graph that samples rows from X randomly.\ngraph = tf.Graph()\nwith graph.as_default():\n    X_init = tf.placeholder(tf.float32, shape=X.shape)\n    # Here, we want to use tf.GraphKeys.LOCAL_VARIABLES,\n    # but can't because there is no feed_dict for that collection in tf.train.Scaffold.\n    X_var = tf.Variable(X_init, trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n    queue = tf.RandomShuffleQueue(\n        capacity=X.shape[0],\n        min_after_dequeue=1,\n        dtypes=[tf.float32],\n        shapes=[X.shape[1]])\n    enqueue_op = queue.enqueue_many([X_var])\n    row = queue.dequeue()\n\n# Sample a few rows from X.\nwith graph.as_default():\n    sess_params = {\n        'scaffold': tf.train.Scaffold(\n            init_feed_dict={X_init: X},\n            init_fn=lambda scaffold, sess: sess.run(enqueue_op))\n    }\n    with tf.train.MonitoredTrainingSession(**sess_params) as sess:\n        print(sess.run(row))\n        print(sess.run(row))",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "25",
      "number": "11665",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.2.0\nPython version: 3.6.1\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: See below.\n\nDescribe the problem\nFeature request: add a local_init_feed_dict to tf.train.Scaffold. It would be useful to be able to create local variables (which are not saved or restored) and have them initialized by a tf.train.MonitoredTrainingSession with a feed_dict. In the example below, the variable X_var is forced to be part of the GLOBAL_VARIABLES collection in order to be able to initialize the variable with a feed_dict. This has the undesirable consequence that the variable will be saved to disk.\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\n\n# Data that we wish to sample, but not save to disk.\nX = np.eye(15, dtype=np.float32)\n\n# Create a graph that samples rows from X randomly.\ngraph = tf.Graph()\nwith graph.as_default():\n    X_init = tf.placeholder(tf.float32, shape=X.shape)\n    # Here, we want to use tf.GraphKeys.LOCAL_VARIABLES,\n    # but can't because there is no feed_dict for that collection in tf.train.Scaffold.\n    X_var = tf.Variable(X_init, trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n    queue = tf.RandomShuffleQueue(\n        capacity=X.shape[0],\n        min_after_dequeue=1,\n        dtypes=[tf.float32],\n        shapes=[X.shape[1]])\n    enqueue_op = queue.enqueue_many([X_var])\n    row = queue.dequeue()\n\n# Sample a few rows from X.\nwith graph.as_default():\n    sess_params = {\n        'scaffold': tf.train.Scaffold(\n            init_feed_dict={X_init: X},\n            init_fn=lambda scaffold, sess: sess.run(enqueue_op))\n    }\n    with tf.train.MonitoredTrainingSession(**sess_params) as sess:\n        print(sess.run(row))\n        print(sess.run(row))",
      "title": "Feature request: add a `local_init_feed_dict` to `tf.train.Scaffold`"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2506,
    "text": "Udacity Notebook with \"None\" kernelI use Jupyter notebook to open the .ipynb files, but it shows a red \"None\" kernel on top right corner and all lines of code cannot run.\nMethod I use:\n\nBuild a new directory and extract .ipynb files from examples/udacity to the directory\nIn terminal, run jupyter notebook",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "26",
      "number": "3185",
      "pretext": "I use Jupyter notebook to open the .ipynb files, but it shows a red \"None\" kernel on top right corner and all lines of code cannot run.\nMethod I use:\n\nBuild a new directory and extract .ipynb files from examples/udacity to the directory\nIn terminal, run jupyter notebook",
      "title": "Udacity Notebook with \"None\" kernel"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2507,
    "text": "Typo in illustrating figure for XLA/Concatenation operationIllustrating image for Concatenate\nsuggests Concat({ 2x4, 2x8 }, dimension=0) is 2x12. Should be dimension=1, and same for the other examples.",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "27",
      "number": "11099",
      "pretext": "Illustrating image for Concatenate\nsuggests Concat({ 2x4, 2x8 }, dimension=0) is 2x12. Should be dimension=1, and same for the other examples.",
      "title": "Typo in illustrating figure for XLA/Concatenation operation"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2508,
    "text": "the side &deep model   is not good compare with deep model and wide model these days ,I'm learning the wide & deep model ,and run the  wide_n_deep_tutorial.py, so the anwser looks like this:\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=deep\nTraining data is downloaded to /tmp/tmpFB4dsd\nTest data is downloaded to /tmp/tmpomj5Pi\n2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpWei9wK\naccuracy: 0.850071\naccuracy_baseline: 0.763774\nauc: 0.894038\nauc_precision_recall: 0.743199\naverage_loss: 0.393638\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 39.3179\nprediction/mean: 0.242167\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=wide\nTraining data is downloaded to /tmp/tmpFJdWft\nTest data is downloaded to /tmp/tmpjB5nm7\n2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpuPHsDx\naccuracy: 0.835391\naccuracy_baseline: 0.763774\nauc: 0.882763\nauc_precision_recall: 0.694257\naverage_loss: 0.352975\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 35.2563\nprediction/mean: 0.240918\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py\nTraining data is downloaded to /tmp/tmpDdWc_T\nTest data is downloaded to /tmp/tmpFF0PZJ\n2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpq2M7SE\naccuracy: 0.820834\naccuracy_baseline: 0.763774\nauc: 0.850518\nauc_precision_recall: 0.676198\naverage_loss: 0.424271\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 42.3776\nprediction/mean: 0.256489\nI don't know why looks likes this, so someone can help me? thank you.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "28",
      "number": "12764",
      "pretext": "these days ,I'm learning the wide & deep model ,and run the  wide_n_deep_tutorial.py, so the anwser looks like this:\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=deep\nTraining data is downloaded to /tmp/tmpFB4dsd\nTest data is downloaded to /tmp/tmpomj5Pi\n2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpWei9wK\naccuracy: 0.850071\naccuracy_baseline: 0.763774\nauc: 0.894038\nauc_precision_recall: 0.743199\naverage_loss: 0.393638\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 39.3179\nprediction/mean: 0.242167\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=wide\nTraining data is downloaded to /tmp/tmpFJdWft\nTest data is downloaded to /tmp/tmpjB5nm7\n2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpuPHsDx\naccuracy: 0.835391\naccuracy_baseline: 0.763774\nauc: 0.882763\nauc_precision_recall: 0.694257\naverage_loss: 0.352975\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 35.2563\nprediction/mean: 0.240918\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py\nTraining data is downloaded to /tmp/tmpDdWc_T\nTest data is downloaded to /tmp/tmpFF0PZJ\n2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpq2M7SE\naccuracy: 0.820834\naccuracy_baseline: 0.763774\nauc: 0.850518\nauc_precision_recall: 0.676198\naverage_loss: 0.424271\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 42.3776\nprediction/mean: 0.256489\nI don't know why looks likes this, so someone can help me? thank you.",
      "title": "the side &deep model   is not good compare with deep model and wide model "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2509,
    "text": "Distributed training: Evaluation and inference best practicesI understand tensorflow distributed training and I implemented my own script.\nWhat I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.\nLet's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.\nMy intuition to achieve this goal is to do the following:\n...\nelif FLAGS.job_name == \"worker\":\n\n    if FLAGS.task_index <= (len(cluster_dict[\"worker\"][:-2]) - 1):\n         logging.info(\"Training worker started\")\n         ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                train_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.TRAIN\n                )\n               with tf.train.MonitoredTrainingSession(\n                    is_chief=(FLAGS.task_index == 0),\n                    master=server.target,\n                    checkpoint_dir=ckpt_dir,\n                    config=config_proto,\n                    hooks=hooks\n                ) as mon_sess:\n                    while not mon_sess.should_stop():\n                        res = train_model.train(...)\n                        ...\n\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-2]) - 1):\n         logging.info(\"Evaluation worker started\")\n         ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                eval_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.EVAL\n                )\n                ...\n\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-1]) - 1):\n        logging.info(\"Inference worker started\")\n        ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                infer_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.INFER\n                )\n                ...\n\nNow, what about the evaluation and inference sessions?\nFor training, I can use tf.train.MonitoredTrainingSession, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use tf.Session.\nRegarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls  eval_model.eval(...) or  infer_model.infer(...), but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to \"periodically\" is to sleep the thread.\nWhat do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?\nAlberto",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "29",
      "number": "17358",
      "pretext": "I understand tensorflow distributed training and I implemented my own script.\nWhat I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.\nLet's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.\nMy intuition to achieve this goal is to do the following:\n...\nelif FLAGS.job_name == \"worker\":\n\n    if FLAGS.task_index <= (len(cluster_dict[\"worker\"][:-2]) - 1):\n         logging.info(\"Training worker started\")\n         ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                train_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.TRAIN\n                )\n               with tf.train.MonitoredTrainingSession(\n                    is_chief=(FLAGS.task_index == 0),\n                    master=server.target,\n                    checkpoint_dir=ckpt_dir,\n                    config=config_proto,\n                    hooks=hooks\n                ) as mon_sess:\n                    while not mon_sess.should_stop():\n                        res = train_model.train(...)\n                        ...\n\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-2]) - 1):\n         logging.info(\"Evaluation worker started\")\n         ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                eval_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.EVAL\n                )\n                ...\n\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-1]) - 1):\n        logging.info(\"Inference worker started\")\n        ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                infer_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.INFER\n                )\n                ...\n\nNow, what about the evaluation and inference sessions?\nFor training, I can use tf.train.MonitoredTrainingSession, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use tf.Session.\nRegarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls  eval_model.eval(...) or  infer_model.infer(...), but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to \"periodically\" is to sleep the thread.\nWhat do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?\nAlberto",
      "title": "Distributed training: Evaluation and inference best practices"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2510,
    "text": "List of functions could be improved with \"const std::string&\" or \"std::string&\" instead \"std::string\"After a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":\nAfter a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":\n\nc/c_api_function.cc:  static string Normalize(string name);\nSuggestion: string& name;\nc/c_api_function.cc:string NodeNameMapping::Normalize(string name) {\nSuggestion: string& name\n\n\n\ncompiler/jit/graph_to_functiondef.cc:  string NormalizeHelper(string name) const;\nSuggestion: string& name\ncompiler/jit/graph_to_functiondef.cc:  string UniquifyHelper(string name);\nSuggestion: const string&\ncompiler/jit/graph_to_functiondef.cc:string NodeNameMapping::NormalizeHelper(string name) const {\nSuggestion: string& name\ncompiler/jit/graph_to_functiondef.cc:string NodeNameMapping::UniquifyHelper(string name) {\nSuggestion: const string&\ncompiler/tf2xla/dump_graph.cc:string MakeUniquePath(string name) {\nSuggestion: string& name\ncompiler/xla/service/llvm_ir/llvm_util.cc:string IrName(string a) {\nSuggestion: string& a\ncompiler/xla/service/llvm_ir/llvm_util.cc:string SanitizeFunctionName(string function_name) {\nSuggestion: string& function_name\ncompiler/xla/service/llvm_ir/llvm_util.h:string IrName(string a);\nSuggestion: string& a\ncompiler/xla/service/llvm_ir/llvm_util.h:string SanitizeFunctionName(string function_name);\nSuggestion: string& function_name\ncompiler/xla/util.cc:string SanitizeFileName(string file_name) {\nSuggestion: string& file_name\ncompiler/xla/util.h:string SanitizeFileName(string file_name);\nSuggestion: string& file_name\n\n\n\ncontrib/verbs/rdma.cc:RdmaBuffer::RdmaBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaAckBuffer::RdmaAckBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaMessageBuffer::RdmaMessageBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaTensorBuffer::RdmaTensorBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaAckBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaMessageBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaTensorBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\n\n\n\ncore/common_runtime/step_stats_collector.cc:static int ExtractGpuWithStreamAll(string device_name) {\nSuggestion: string& device_name\ncore/common_runtime/step_stats_collector.cc:static int ExtractGpuWithoutStream(string device_name) {\nSuggestion: string& device_name\ncore/kernels/ops_util.cc:string SanitizeThreadSuffix(string suffix) {\nSuggestion: const string& suffix\ncore/kernels/ops_util.h:string SanitizeThreadSuffix(string suffix);\nSuggestion: const string& suffix\ncore/kernels/xsmm_conv2d.cc:static void chk_libxsmm_err(libxsmm_dnn_err_t status, string msg) {\nSuggestion: const string& msg\n\n\n\nstream_executor/platform.cc:PlatformKind PlatformKindFromString(string kind) {\nSuggestion: const string& kind\nstream_executor/platform.h:PlatformKind PlatformKindFromString(string platform_string);\nSuggestion: const string& platform_string",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "30",
      "number": "14170",
      "pretext": "After a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":\nAfter a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":\n\nc/c_api_function.cc:  static string Normalize(string name);\nSuggestion: string& name;\nc/c_api_function.cc:string NodeNameMapping::Normalize(string name) {\nSuggestion: string& name\n\n\n\ncompiler/jit/graph_to_functiondef.cc:  string NormalizeHelper(string name) const;\nSuggestion: string& name\ncompiler/jit/graph_to_functiondef.cc:  string UniquifyHelper(string name);\nSuggestion: const string&\ncompiler/jit/graph_to_functiondef.cc:string NodeNameMapping::NormalizeHelper(string name) const {\nSuggestion: string& name\ncompiler/jit/graph_to_functiondef.cc:string NodeNameMapping::UniquifyHelper(string name) {\nSuggestion: const string&\ncompiler/tf2xla/dump_graph.cc:string MakeUniquePath(string name) {\nSuggestion: string& name\ncompiler/xla/service/llvm_ir/llvm_util.cc:string IrName(string a) {\nSuggestion: string& a\ncompiler/xla/service/llvm_ir/llvm_util.cc:string SanitizeFunctionName(string function_name) {\nSuggestion: string& function_name\ncompiler/xla/service/llvm_ir/llvm_util.h:string IrName(string a);\nSuggestion: string& a\ncompiler/xla/service/llvm_ir/llvm_util.h:string SanitizeFunctionName(string function_name);\nSuggestion: string& function_name\ncompiler/xla/util.cc:string SanitizeFileName(string file_name) {\nSuggestion: string& file_name\ncompiler/xla/util.h:string SanitizeFileName(string file_name);\nSuggestion: string& file_name\n\n\n\ncontrib/verbs/rdma.cc:RdmaBuffer::RdmaBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaAckBuffer::RdmaAckBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaMessageBuffer::RdmaMessageBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaTensorBuffer::RdmaTensorBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaAckBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaMessageBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaTensorBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\n\n\n\ncore/common_runtime/step_stats_collector.cc:static int ExtractGpuWithStreamAll(string device_name) {\nSuggestion: string& device_name\ncore/common_runtime/step_stats_collector.cc:static int ExtractGpuWithoutStream(string device_name) {\nSuggestion: string& device_name\ncore/kernels/ops_util.cc:string SanitizeThreadSuffix(string suffix) {\nSuggestion: const string& suffix\ncore/kernels/ops_util.h:string SanitizeThreadSuffix(string suffix);\nSuggestion: const string& suffix\ncore/kernels/xsmm_conv2d.cc:static void chk_libxsmm_err(libxsmm_dnn_err_t status, string msg) {\nSuggestion: const string& msg\n\n\n\nstream_executor/platform.cc:PlatformKind PlatformKindFromString(string kind) {\nSuggestion: const string& kind\nstream_executor/platform.h:PlatformKind PlatformKindFromString(string platform_string);\nSuggestion: const string& platform_string",
      "title": "List of functions could be improved with \"const std::string&\" or \"std::string&\" instead \"std::string\""
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2511,
    "text": "Feature request: Option to create dataset from a subset of the columns in the CSV file using tf.contrib.data.make_csv_dataset()System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.8\nPython version:  3.6\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory:NA\nExact command to reproduce:\ntf.contrib.data.make_csv_dataset()\n\nDescribe the problem\nThe tf.contrib.data.make_csv_dataset() is a very useful feature which allows us to convert CSV files directly into at dataset without having to use Pandas library (like shown here). However it is missing an important feature which Pandas had, that is to read a subset of the columns in the CSV file.\nFor example the following code:\ndataset=tf.contrib.data.make_csv_dataset(file_pattern='./data/power_data/MISO_power_data1.csv',batch_size=24,shuffle=False)\ndataset = dataset.batch(4)\nX_iter = dataset.make_one_shot_iterator()\nX_batch = X_iter.get_next()\nX_batch\n\nresults in following dataset:\n{'Actual_Load_MWh': <tf.Tensor 'IteratorGetNext_9:0' shape=(?, ?) dtype=float32>,\n 'Hour_Ending': <tf.Tensor 'IteratorGetNext_9:1' shape=(?, ?) dtype=int32>,\n 'Market_Day': <tf.Tensor 'IteratorGetNext_9:2' shape=(?, ?) dtype=int32>,\n 'Wind_MWh': <tf.Tensor 'IteratorGetNext_9:3' shape=(?, ?) dtype=float32>}\n\nHowever I don't want feature columns for 'Hour_Ending'  and  'Market_Day' in my dataset (since they are not relevant training data) . This could be done in Pandas using code below:\ndf_input=pd.read_csv('./data/power_data/MISO_power_data1.csv',\n                         usecols=['Wind_MWh', 'Actual_Load_MWh'], nrows=24)\n\nI know the easy solution would be to create a CSV file having only the feature columns I want. But it would be a great utility feature to add before make_csv_dataset() migrates out of contrib into core TF. I can submit a PR for this if required.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "31",
      "number": "18952",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.8\nPython version:  3.6\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory:NA\nExact command to reproduce:\ntf.contrib.data.make_csv_dataset()\n\nDescribe the problem\nThe tf.contrib.data.make_csv_dataset() is a very useful feature which allows us to convert CSV files directly into at dataset without having to use Pandas library (like shown here). However it is missing an important feature which Pandas had, that is to read a subset of the columns in the CSV file.\nFor example the following code:\ndataset=tf.contrib.data.make_csv_dataset(file_pattern='./data/power_data/MISO_power_data1.csv',batch_size=24,shuffle=False)\ndataset = dataset.batch(4)\nX_iter = dataset.make_one_shot_iterator()\nX_batch = X_iter.get_next()\nX_batch\n\nresults in following dataset:\n{'Actual_Load_MWh': <tf.Tensor 'IteratorGetNext_9:0' shape=(?, ?) dtype=float32>,\n 'Hour_Ending': <tf.Tensor 'IteratorGetNext_9:1' shape=(?, ?) dtype=int32>,\n 'Market_Day': <tf.Tensor 'IteratorGetNext_9:2' shape=(?, ?) dtype=int32>,\n 'Wind_MWh': <tf.Tensor 'IteratorGetNext_9:3' shape=(?, ?) dtype=float32>}\n\nHowever I don't want feature columns for 'Hour_Ending'  and  'Market_Day' in my dataset (since they are not relevant training data) . This could be done in Pandas using code below:\ndf_input=pd.read_csv('./data/power_data/MISO_power_data1.csv',\n                         usecols=['Wind_MWh', 'Actual_Load_MWh'], nrows=24)\n\nI know the easy solution would be to create a CSV file having only the feature columns I want. But it would be a great utility feature to add before make_csv_dataset() migrates out of contrib into core TF. I can submit a PR for this if required.",
      "title": "Feature request: Option to create dataset from a subset of the columns in the CSV file using tf.contrib.data.make_csv_dataset()"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2512,
    "text": "QueueRunner deadlock when using all CPUsI'm building an input pipeline following the guidelines here.  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using tf.py_func), and return the processed results to an output queue.  I'd like to use QueueRunner's ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:\nimport numpy as np\nimport multiprocessing\nimport tensorflow as tf\n\nn_cpus = multiprocessing.cpu_count()\n\nsess = tf.Session()\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\nmult = tf.mul(a, b)\n\ndef python_op(x):\n    print \"python_op called with {}\".format(x)\n    # In my real function, the np.cos and np.sin calls are replaced by\n    # python calculations I can't do in tensorflow\n    y = np.cos(x)\n    z = sess.run(mult, feed_dict={a: y, b: x})\n    print \"intermediate result is {}\".format(z)\n    return np.sin(z)\n\nn_inputs = n_cpus\ninput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nload_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))\n\noutput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nget_result = output_queue.dequeue_many(n_inputs)\n\ndef processing_pipeline():\n    input_value = input_queue.dequeue()\n    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))\n\n# Here's the problem: If we use all CPUs here, the program will deadlock.\n# If we change cpus to (cpus-1), it works as expected.\nrunner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))\n\ncoord = tf.train.Coordinator()\nrunner.create_threads(sess, coord=coord, start=True)\n\nprint \"Loading input\"\nsess.run(load_input)\nsess.run(input_queue.close())\n\ntry:\n    print \"waiting for result\"\n    result = sess.run(get_result)\n    print \"RESULT: {}\".format(result)\nexcept tf.errors.OutOfRangeError:\n    print \"Input exhausted\"\n\ncoord.request_stop()\ncoord.join()\nprint \"Done\"\nThe program above deadlocks waiting for sess.run to complete in python_op:\n$ python queuetest.py                                                                                                                                                                                                                                                  \nLoading input\npython_op called with [ 0.65624136]\n python_op called with [ 0.80651367]\npython_op called with [ 0.31998941]\n python_op called with [ 0.726421]\n python_op called with [ 0.33133706]\npython_op called with [ 0.4912357]\npython_op called with [ 0.27365881]\npython_op called with [ 0.32846987]\n\nThis is running on an 8-core machine; you can see that 8 python_ops are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing (n_cpus) to (n_cpus-1) in the line that creates the tf.train.QueueRunner, then the program runs to completion:\n$ python queuetest.py                                                                                                                                                                                                                                                        \nLoading input\npython_op called with [ 0.40804103]\npython_op called with [ 0.0182138]\npython_op called with [ 0.17579727]\n python_op called with [ 0.29143187]\npython_op called with [ 0.11612369]\n intermediate result is [ 0.37454084]\npython_op called with [ 0.679506]\npython_op called with [ 0.50754625]\nintermediate result is [ 0.01821078]\nintermediate result is [ 0.52857631]\n intermediate result is [ 0.27914321]\nwaiting for result\npython_op called with [ 0.68288684]\n intermediate result is [ 0.44356483]\nintermediate result is [ 0.11534163]\n intermediate result is [ 0.17308778]\nintermediate result is [ 0.52975237]\nRESULT: [[ 0.36584523]\n [ 0.01820978]\n [ 0.50430447]\n [ 0.42916203]\n [ 0.11508605]\n [ 0.27553213]\n [ 0.1722248 ]\n [ 0.50531965]]\nDone\n\nThe program also completes successfully if we pass in fewer examples than CPUs in the input queue.\nI realize it's somewhat awkward for python_op to call back into the tensorflow session.  However, the threading and queues section of the manual states:\n\"The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.\"\nSo, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?\nAs a side note, one option to work around my problems would be to break python_op into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since python_op's real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.\nOS: Linux\nTensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "32",
      "number": "4917",
      "pretext": "I'm building an input pipeline following the guidelines here.  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using tf.py_func), and return the processed results to an output queue.  I'd like to use QueueRunner's ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:\nimport numpy as np\nimport multiprocessing\nimport tensorflow as tf\n\nn_cpus = multiprocessing.cpu_count()\n\nsess = tf.Session()\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\nmult = tf.mul(a, b)\n\ndef python_op(x):\n    print \"python_op called with {}\".format(x)\n    # In my real function, the np.cos and np.sin calls are replaced by\n    # python calculations I can't do in tensorflow\n    y = np.cos(x)\n    z = sess.run(mult, feed_dict={a: y, b: x})\n    print \"intermediate result is {}\".format(z)\n    return np.sin(z)\n\nn_inputs = n_cpus\ninput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nload_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))\n\noutput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nget_result = output_queue.dequeue_many(n_inputs)\n\ndef processing_pipeline():\n    input_value = input_queue.dequeue()\n    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))\n\n# Here's the problem: If we use all CPUs here, the program will deadlock.\n# If we change cpus to (cpus-1), it works as expected.\nrunner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))\n\ncoord = tf.train.Coordinator()\nrunner.create_threads(sess, coord=coord, start=True)\n\nprint \"Loading input\"\nsess.run(load_input)\nsess.run(input_queue.close())\n\ntry:\n    print \"waiting for result\"\n    result = sess.run(get_result)\n    print \"RESULT: {}\".format(result)\nexcept tf.errors.OutOfRangeError:\n    print \"Input exhausted\"\n\ncoord.request_stop()\ncoord.join()\nprint \"Done\"\nThe program above deadlocks waiting for sess.run to complete in python_op:\n$ python queuetest.py                                                                                                                                                                                                                                                  \nLoading input\npython_op called with [ 0.65624136]\n python_op called with [ 0.80651367]\npython_op called with [ 0.31998941]\n python_op called with [ 0.726421]\n python_op called with [ 0.33133706]\npython_op called with [ 0.4912357]\npython_op called with [ 0.27365881]\npython_op called with [ 0.32846987]\n\nThis is running on an 8-core machine; you can see that 8 python_ops are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing (n_cpus) to (n_cpus-1) in the line that creates the tf.train.QueueRunner, then the program runs to completion:\n$ python queuetest.py                                                                                                                                                                                                                                                        \nLoading input\npython_op called with [ 0.40804103]\npython_op called with [ 0.0182138]\npython_op called with [ 0.17579727]\n python_op called with [ 0.29143187]\npython_op called with [ 0.11612369]\n intermediate result is [ 0.37454084]\npython_op called with [ 0.679506]\npython_op called with [ 0.50754625]\nintermediate result is [ 0.01821078]\nintermediate result is [ 0.52857631]\n intermediate result is [ 0.27914321]\nwaiting for result\npython_op called with [ 0.68288684]\n intermediate result is [ 0.44356483]\nintermediate result is [ 0.11534163]\n intermediate result is [ 0.17308778]\nintermediate result is [ 0.52975237]\nRESULT: [[ 0.36584523]\n [ 0.01820978]\n [ 0.50430447]\n [ 0.42916203]\n [ 0.11508605]\n [ 0.27553213]\n [ 0.1722248 ]\n [ 0.50531965]]\nDone\n\nThe program also completes successfully if we pass in fewer examples than CPUs in the input queue.\nI realize it's somewhat awkward for python_op to call back into the tensorflow session.  However, the threading and queues section of the manual states:\n\"The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.\"\nSo, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?\nAs a side note, one option to work around my problems would be to break python_op into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since python_op's real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.\nOS: Linux\nTensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)",
      "title": "QueueRunner deadlock when using all CPUs"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2513,
    "text": "Error:NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nEnvironment info\nOperating System:\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nWhat other attempted solutions have you tried?\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "33",
      "number": "8273",
      "pretext": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nEnvironment info\nOperating System:\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nWhat other attempted solutions have you tried?\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).",
      "title": "Error:"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2514,
    "text": "Feature Request: Provide tf.pow with supporting  broadcasting?Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nPython version:\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "34",
      "number": "21142",
      "pretext": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nPython version:\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
      "title": "Feature Request: Provide tf.pow with supporting  broadcasting?"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2515,
    "text": "Arch doesn't support itAfter sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl or as root, I got:\ntensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\nIt seems it doesn't work with Python 3. What should I do?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "35",
      "number": "1279",
      "pretext": "After sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl or as root, I got:\ntensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\nIt seems it doesn't work with Python 3. What should I do?",
      "title": "Arch doesn't support it"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2516,
    "text": "tf.manip.roll silently ignores negative axesSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\nTensorFlow installed from (source or binary): unknown\nTensorFlow version (use command below): 1.6.0\nPython version: 3.6.3\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce:\n\nimport tensorflow as tf\ntf.InteractiveSession()\nprint(tf.manip.roll(tf.range(5), -1, axis=0).eval())\n# [1 2 3 4 0]\nprint(tf.manip.roll(tf.range(5), -1, axis=-1).eval())\n# [0 1 2 3 4]\n\nDescribe the problem\naxis=-1 and axis=0 should be equivalent, if tf.manip.roll() works like numpy.roll() and other TensorFlow/NumPy functions that accept negative axes. However, instead negative axes are silently ignored. At the very least, TensorFlow should have raised an informative error.",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "36",
      "number": "17877",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\nTensorFlow installed from (source or binary): unknown\nTensorFlow version (use command below): 1.6.0\nPython version: 3.6.3\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce:\n\nimport tensorflow as tf\ntf.InteractiveSession()\nprint(tf.manip.roll(tf.range(5), -1, axis=0).eval())\n# [1 2 3 4 0]\nprint(tf.manip.roll(tf.range(5), -1, axis=-1).eval())\n# [0 1 2 3 4]\n\nDescribe the problem\naxis=-1 and axis=0 should be equivalent, if tf.manip.roll() works like numpy.roll() and other TensorFlow/NumPy functions that accept negative axes. However, instead negative axes are silently ignored. At the very least, TensorFlow should have raised an informative error.",
      "title": "tf.manip.roll silently ignores negative axes"
    },
    "annotation_approver": null
  },
  {
    "id": 2517,
    "text": "invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive]     Hi all,\nI am trying to compile tensorflow-0.11 + bazel 0.3.2 on RHEL 6 with cuda 7.0 + cudnn 7.5.5.0 + gcc 4.9.\nThe compilation command is :\nEXTRA_BAZEL_ARGS=\"--jobs 10\" bazel build -c opt --config=cuda --jobs 10 //tensorflow/tools/pip_package:build_pip_package\nCompilation of rule '//tensorflow/stream_executor:stream_executor' fails with cuda specific message.\nI have latest version of compilers at non standard path , hence i had modified some variables in CROSSTOOL.tpl + third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl. I am attaching compilation error logs, CROSSTOOL.tpl and third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl for your reference.\nThough i am able to compile cpu-only version of tensorflow successfully.\nPlease let me know if any information is needed from my side.\nEagerly awaiting your replies.\ncrosstool_wrapper_driver_is_not_gcc.tpl.txt\nCROSSTOOL.tpl.txt\ntensorflow_build2.log.txt",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "37",
      "number": "5447",
      "pretext": "Hi all,\nI am trying to compile tensorflow-0.11 + bazel 0.3.2 on RHEL 6 with cuda 7.0 + cudnn 7.5.5.0 + gcc 4.9.\nThe compilation command is :\nEXTRA_BAZEL_ARGS=\"--jobs 10\" bazel build -c opt --config=cuda --jobs 10 //tensorflow/tools/pip_package:build_pip_package\nCompilation of rule '//tensorflow/stream_executor:stream_executor' fails with cuda specific message.\nI have latest version of compilers at non standard path , hence i had modified some variables in CROSSTOOL.tpl + third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl. I am attaching compilation error logs, CROSSTOOL.tpl and third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl for your reference.\nThough i am able to compile cpu-only version of tensorflow successfully.\nPlease let me know if any information is needed from my side.\nEagerly awaiting your replies.\ncrosstool_wrapper_driver_is_not_gcc.tpl.txt\nCROSSTOOL.tpl.txt\ntensorflow_build2.log.txt",
      "title": "invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive]     "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2518,
    "text": "Changing computer make the pretrained model failI switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.\nusing TF:1.0",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "38",
      "number": "7908",
      "pretext": "I switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.\nusing TF:1.0",
      "title": "Changing computer make the pretrained model fail"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2519,
    "text": "tf.get_variable without an explicit initializer fails for integer typesThe following fails (shape and name are arbitrary):\ntf.get_variable(name='foo', shape=(42,), dtype=tf.int32)\n\nException: TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.\nIn contrast, using tf.float32 works just fine.\nThe problem appears to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658\nIf an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that sqrt(3)==1.7320...), which of course conflicts with the requested integer type.\nWhile this can be mitigated by doing something like:\ntf.get_variable(name='foo', dtype=tf.int32, initializer=tf.zeros_initializer(shape=(42,), dtype=tf.int32))\n\nit feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).\nTested on the current master.",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "39",
      "number": "4419",
      "pretext": "The following fails (shape and name are arbitrary):\ntf.get_variable(name='foo', shape=(42,), dtype=tf.int32)\n\nException: TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.\nIn contrast, using tf.float32 works just fine.\nThe problem appears to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658\nIf an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that sqrt(3)==1.7320...), which of course conflicts with the requested integer type.\nWhile this can be mitigated by doing something like:\ntf.get_variable(name='foo', dtype=tf.int32, initializer=tf.zeros_initializer(shape=(42,), dtype=tf.int32))\n\nit feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).\nTested on the current master.",
      "title": "tf.get_variable without an explicit initializer fails for integer types"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2520,
    "text": "google-tensorflow",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "40",
      "number": "806",
      "pretext": "",
      "title": "google-tensorflow"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2521,
    "text": "Improving Google Indexing for the DocumentationWhenever I run a Google search on a TensorFlow functionality, say, tf.reshape, it gives me the entire documentation, not the specific documentation related to that functionality.\nCurrently the way I use is  to run a search with ctrl + f to find specific documentation related to what I search for.\nNumpy has that property, i.e. when you run a Google search on np.reshape, you get the specific page.\nIt would be a nice improvement for the documentation if someone fixes the Google indexing for the documentation page, especially for the users who frequently use Google search for the documentation.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "41",
      "number": "4021",
      "pretext": "Whenever I run a Google search on a TensorFlow functionality, say, tf.reshape, it gives me the entire documentation, not the specific documentation related to that functionality.\nCurrently the way I use is  to run a search with ctrl + f to find specific documentation related to what I search for.\nNumpy has that property, i.e. when you run a Google search on np.reshape, you get the specific page.\nIt would be a nice improvement for the documentation if someone fixes the Google indexing for the documentation page, especially for the users who frequently use Google search for the documentation.",
      "title": "Improving Google Indexing for the Documentation"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2522,
    "text": "TfLiteCameraDemo.apk with NNAPICan someone please upload a version of TfLiteCameraDemo.apk that supports NNAPI?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "42",
      "number": "23208",
      "pretext": "Can someone please upload a version of TfLiteCameraDemo.apk that supports NNAPI?",
      "title": "TfLiteCameraDemo.apk with NNAPI"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2523,
    "text": "Links to Android nightly builds on README.md are brokenLinks to Android nightly builds on https://github.com/tensorflow/tensorflow/blob/master/README.md are broken.\n\n-->",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "43",
      "number": "7638",
      "pretext": "Links to Android nightly builds on https://github.com/tensorflow/tensorflow/blob/master/README.md are broken.\n\n-->",
      "title": "Links to Android nightly builds on README.md are broken"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2524,
    "text": "tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nEnvironment info\nOperating System:\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nWhat other attempted solutions have you tried?\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "44",
      "number": "7558",
      "pretext": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nEnvironment info\nOperating System:\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nWhat other attempted solutions have you tried?\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).",
      "title": "tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform."
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2525,
    "text": "Docker build devel-gpu build fails with 8.0-cudnn5-devel (Cannot find cudnn.h under .../lib)Summary: Why is it looking for the header file under the lib directory?\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\n#3989 (and references within)\n\nEnvironment info\nOperating System: Ubuntu 16.04. Docker 1.12.1.\nIf possible, provide a minimal reproducible example\n\nModify devel-gpu to read \"FROM nvidia/cuda:8.0-cudnn5-devel\".\nRun docker build -f Dockerfile.devel-gpu -t tf from the /tensorflow/tools/docker directory. (HEAD @ 4addf4b at time of posting)\n\nWhat other attempted solutions have you tried?\nNone yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.\nLogs or other output that would be helpful\n$ docker build -f Dockerfile.devel-gpu -t tf .\n\n[snip]\n\nStep 23 : RUN ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl\n ---> Running in 1f99527c7748\nNo Google Cloud Platform support will be enabled for TensorFlow\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nExtracting Bazel installation...\n____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.\nConfiguration finished\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/lib/x86_64-linux-gnu.\n____Elapsed time: 0.391s\nThe command '/bin/sh -c ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "45",
      "number": "4397",
      "pretext": "Summary: Why is it looking for the header file under the lib directory?\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\n#3989 (and references within)\n\nEnvironment info\nOperating System: Ubuntu 16.04. Docker 1.12.1.\nIf possible, provide a minimal reproducible example\n\nModify devel-gpu to read \"FROM nvidia/cuda:8.0-cudnn5-devel\".\nRun docker build -f Dockerfile.devel-gpu -t tf from the /tensorflow/tools/docker directory. (HEAD @ 4addf4b at time of posting)\n\nWhat other attempted solutions have you tried?\nNone yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.\nLogs or other output that would be helpful\n$ docker build -f Dockerfile.devel-gpu -t tf .\n\n[snip]\n\nStep 23 : RUN ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl\n ---> Running in 1f99527c7748\nNo Google Cloud Platform support will be enabled for TensorFlow\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nExtracting Bazel installation...\n____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.\nConfiguration finished\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/lib/x86_64-linux-gnu.\n____Elapsed time: 0.391s\nThe command '/bin/sh -c ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1",
      "title": "Docker build devel-gpu build fails with 8.0-cudnn5-devel (Cannot find cudnn.h under .../lib)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2526,
    "text": "\"bazel clean\" will undo \"./configure\" when source configured to build for GPU.Environment info\nOperating System:\nubuntu 14.04\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\ncuda 8.0.44, cudnn 5.5\nIf installed from binary pip package, provide:\nNo\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nc8d4896\nThe output of bazel version\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n./configure   # configure with GPU support)\nbazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one should pass\nbazel clean\nbazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test  # this one fails, complaining no GPU support configured.\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@ae\n3629bd' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@1374ec1e', 'CONFIGURATION_FRAGMENT:com\n.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@3686b55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue\n$ConfigurationFragmentKey@a93d9174')\nat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)\nat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nDavid, Damien, looks like bazel clean undid some output of \"./configure\" at head.\nAny idea where things went wrong?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "46",
      "number": "4848",
      "pretext": "Environment info\nOperating System:\nubuntu 14.04\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\ncuda 8.0.44, cudnn 5.5\nIf installed from binary pip package, provide:\nNo\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nc8d4896\nThe output of bazel version\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n./configure   # configure with GPU support)\nbazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one should pass\nbazel clean\nbazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test  # this one fails, complaining no GPU support configured.\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@ae\n3629bd' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@1374ec1e', 'CONFIGURATION_FRAGMENT:com\n.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@3686b55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue\n$ConfigurationFragmentKey@a93d9174')\nat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)\nat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nDavid, Damien, looks like bazel clean undid some output of \"./configure\" at head.\nAny idea where things went wrong?",
      "title": "\"bazel clean\" will undo \"./configure\" when source configured to build for GPU."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2527,
    "text": "Add -lm (Was: undefined reference to symbol 'ceil@@GLIBC_2.2.5)Environment info\nOperating System:\nepel-release-6-8.noarch\nredhat-release-server-6Server-6.7.0.3.el6.x86_64\nInstalled version of CUDA and cuDNN:\nNone\nIf installed from sources, provide the commit hash:\nf8eb1d7\nSteps to reproduce\n\nbazel clean\n./configure\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).\nERROR: /home/ebice/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command /opt/rh/devtoolset-2/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -no-canonical-prefixes -B/opt/rh/devtoolset-2/root/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 11 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/opt/rh/devtoolset-2/root/usr/bin/ld: /opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/libstdc++_nonshared.a(hashtable_c++0x44.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'\n/opt/rh/devtoolset-2/root/usr/bin/ld: note: 'ceil@@GLIBC_2.2.5' is defined in DSO /lib64/libm.so.6 so try adding it to the linker command line\n/lib64/libm.so.6: could not read symbols: Invalid operation\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "47",
      "number": "2291",
      "pretext": "Environment info\nOperating System:\nepel-release-6-8.noarch\nredhat-release-server-6Server-6.7.0.3.el6.x86_64\nInstalled version of CUDA and cuDNN:\nNone\nIf installed from sources, provide the commit hash:\nf8eb1d7\nSteps to reproduce\n\nbazel clean\n./configure\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).\nERROR: /home/ebice/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command /opt/rh/devtoolset-2/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -no-canonical-prefixes -B/opt/rh/devtoolset-2/root/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 11 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/opt/rh/devtoolset-2/root/usr/bin/ld: /opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/libstdc++_nonshared.a(hashtable_c++0x44.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'\n/opt/rh/devtoolset-2/root/usr/bin/ld: note: 'ceil@@GLIBC_2.2.5' is defined in DSO /lib64/libm.so.6 so try adding it to the linker command line\n/lib64/libm.so.6: could not read symbols: Invalid operation\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build",
      "title": "Add -lm (Was: undefined reference to symbol 'ceil@@GLIBC_2.2.5)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2528,
    "text": "SparseTensor: common unary opsCurrently, we have a slew of common unary ops that work on Tensors, but not SparseTensors (ref):\ntf.pow()\ntf.exp()\ntf.log()\n\n# lower priority?\ntf.abs()\ntf.neg()\ntf.sign()\ntf.inv()\ntf.square()\ntf.round()\ntf.sqrt()\ntf.ceil()\ntf.floor()\n\nand so on.\nWe'd like these ops to work on SparseTensor. These do not change the indices nor shape of SparseTensors, so all that's needed is transform the .values field on Python side in O(1) line.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "48",
      "number": "1828",
      "pretext": "Currently, we have a slew of common unary ops that work on Tensors, but not SparseTensors (ref):\ntf.pow()\ntf.exp()\ntf.log()\n\n# lower priority?\ntf.abs()\ntf.neg()\ntf.sign()\ntf.inv()\ntf.square()\ntf.round()\ntf.sqrt()\ntf.ceil()\ntf.floor()\n\nand so on.\nWe'd like these ops to work on SparseTensor. These do not change the indices nor shape of SparseTensors, so all that's needed is transform the .values field on Python side in O(1) line.",
      "title": "SparseTensor: common unary ops"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2529,
    "text": "How to release GPU memory after sess.close()?hi, all:\nI'm training models iteratively. After each model trained, I run sess.close() and recreate a new session to run a new training process. But it seems that the GPU memory was not relseased and it's increasing constantly.\nI tried tf.reset_default_graph() before run session also typed gc.collect() after sess.close(), but takes no effect.\nHow could I release GPU memory timely to avoid OOM error please?\nThanks!",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "49",
      "number": "19731",
      "pretext": "hi, all:\nI'm training models iteratively. After each model trained, I run sess.close() and recreate a new session to run a new training process. But it seems that the GPU memory was not relseased and it's increasing constantly.\nI tried tf.reset_default_graph() before run session also typed gc.collect() after sess.close(), but takes no effect.\nHow could I release GPU memory timely to avoid OOM error please?\nThanks!",
      "title": "How to release GPU memory after sess.close()?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2530,
    "text": "Flawed memory management: allow_growth=True consumes more memory, causing out-of-memoryTo prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\nHowever, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:\nTraining Epoch 0 ; learning_rate= 0.002 :                                                                                                \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \n\nHowever, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.\nIn principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.\nBelow are my system info:\nOperating System:\nUbuntu 14.04.5 LTS\n\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda-8.0\ncudnn-8.0-linux-x64-v5.1.tgz\n\nIt is installed from binary pip package\n\nA link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n\nThe output from python -c \"import tensorflow; print(tensorflow.version)\"\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "50",
      "number": "6111",
      "pretext": "To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\nHowever, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:\nTraining Epoch 0 ; learning_rate= 0.002 :                                                                                                \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \n\nHowever, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.\nIn principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.\nBelow are my system info:\nOperating System:\nUbuntu 14.04.5 LTS\n\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda-8.0\ncudnn-8.0-linux-x64-v5.1.tgz\n\nIt is installed from binary pip package\n\nA link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n\nThe output from python -c \"import tensorflow; print(tensorflow.version)\"\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0",
      "title": "Flawed memory management: allow_growth=True consumes more memory, causing out-of-memory"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2531,
    "text": "tf.contrib.learn output shapes : shapes (?, 1) and (?,) are incompatibleI tried to train a binary DNNClassifier  similar as the example on https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart.\nI got the following traceback\ntraceback.txt\nI explored the tensorflow source code and found that it may relate to _get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\n\nWhen doing a binary classification,y_shape is something like[batch_size,1].Line 52 changes it into [1].Line 54,55 change it into [].And finally the output_shape is [batch_size,].However the correct ouput shape should be [batch_size,1].\nTo sum up,we do not need to skip 1st dimension if it is 1 and len(y_shape)=1.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "51",
      "number": "5284",
      "pretext": "I tried to train a binary DNNClassifier  similar as the example on https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart.\nI got the following traceback\ntraceback.txt\nI explored the tensorflow source code and found that it may relate to _get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\n\nWhen doing a binary classification,y_shape is something like[batch_size,1].Line 52 changes it into [1].Line 54,55 change it into [].And finally the output_shape is [batch_size,].However the correct ouput shape should be [batch_size,1].\nTo sum up,we do not need to skip 1st dimension if it is 1 and len(y_shape)=1.",
      "title": "tf.contrib.learn output shapes : shapes (?, 1) and (?,) are incompatible"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2532,
    "text": "Inconsistency in supported integer types on GPUSystem information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nnot relevant\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u4 (2018-08-21) x86_64 GNU/Linux\nVERSION_ID=\"9\"\nVERSION=\"9 (stretch)\"\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n\n\nTensorFlow installed from (source or binary):\n1.10.1 (from pip binary)\n\n\nTensorFlow version (use command below):\ntf.VERSION = 1.10.1\ntf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1\ntf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1\n\n\nPython version:\nPython 3.5.3\n\n\nBazel version (if compiling from source):\n\n\nGCC/Compiler version (if compiling from source):\n\n\nCUDA/cuDNN version:\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2017 NVIDIA Corporation\nBuilt on Fri_Nov__3_21:07:56_CDT_2017\nCuda compilation tools, release 9.1, V9.1.85\n\n\nGPU model and memory:\nTesla P100-PCIE-16GB\n\n\nExact command to reproduce:\n\n\nDescribe the problem\nIt appears that the kernel of tf.reduce_sum is not registerd for GPU's if the type of the tensor to be summed is int64 (only registered for tf.int32)\nMoreover the kernel of tf.tile is not registered for the case where the tensor to be tiled is of type tf.int32 (only registered for tf.int64)\nWhy is there this inconsistency?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "52",
      "number": "22249",
      "pretext": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nnot relevant\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u4 (2018-08-21) x86_64 GNU/Linux\nVERSION_ID=\"9\"\nVERSION=\"9 (stretch)\"\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n\n\nTensorFlow installed from (source or binary):\n1.10.1 (from pip binary)\n\n\nTensorFlow version (use command below):\ntf.VERSION = 1.10.1\ntf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1\ntf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1\n\n\nPython version:\nPython 3.5.3\n\n\nBazel version (if compiling from source):\n\n\nGCC/Compiler version (if compiling from source):\n\n\nCUDA/cuDNN version:\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2017 NVIDIA Corporation\nBuilt on Fri_Nov__3_21:07:56_CDT_2017\nCuda compilation tools, release 9.1, V9.1.85\n\n\nGPU model and memory:\nTesla P100-PCIE-16GB\n\n\nExact command to reproduce:\n\n\nDescribe the problem\nIt appears that the kernel of tf.reduce_sum is not registerd for GPU's if the type of the tensor to be summed is int64 (only registered for tf.int32)\nMoreover the kernel of tf.tile is not registered for the case where the tensor to be tiled is of type tf.int32 (only registered for tf.int64)\nWhy is there this inconsistency?",
      "title": "Inconsistency in supported integer types on GPU"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2533,
    "text": "regarding the ValueError: inputs must be a list of at least one Tensor with the same dtype and shapeThere is a program that defines the loss function as follows:\nreg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\nweight_loss = tf.add_n(tf.get_collection(reg_loss_col),name='reg_loss')\n\n\nRunning the program raises the following error message\n\nFile \"/home/ decoder/kitti_multiloss.py\", line 86, in loss\nname='reg_loss')\nFile \"/devl /tensorflow/tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py\", line 1827, in add_n\nraise ValueError(\"inputs must be a list of at least one Tensor with the \"\nValueError: inputs must be a list of at least one Tensor with the same dtype and shape\n\nI am curious how to print out the tensor information of the first parameter tf.get_collection(reg_loss_col) in tf.add_n, so that I can figure out why this cause the error.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "53",
      "number": "8910",
      "pretext": "There is a program that defines the loss function as follows:\nreg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\nweight_loss = tf.add_n(tf.get_collection(reg_loss_col),name='reg_loss')\n\n\nRunning the program raises the following error message\n\nFile \"/home/ decoder/kitti_multiloss.py\", line 86, in loss\nname='reg_loss')\nFile \"/devl /tensorflow/tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py\", line 1827, in add_n\nraise ValueError(\"inputs must be a list of at least one Tensor with the \"\nValueError: inputs must be a list of at least one Tensor with the same dtype and shape\n\nI am curious how to print out the tensor information of the first parameter tf.get_collection(reg_loss_col) in tf.add_n, so that I can figure out why this cause the error.",
      "title": "regarding the ValueError: inputs must be a list of at least one Tensor with the same dtype and shape"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2534,
    "text": "tf.fake_quant_with_min_max_vars returns wrong answerSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): pip install tensorflow-gpu\nTensorFlow version (use command below): 1.5.0\nPython version: 2.7.12\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: CUDA-8.0 CuDNN 6.0\nGPU model and memory: GTX 1080 8GB\nExact command to reproduce:\n\nDescribe the problem\ntf.fake_quant_with_min_max_vars returns wrong answer.\nSource code / logs\nimport tensorflow as tf\n\na =tf.Variable([ 0.09504107, 0.0748544, 0.09333218, 0.106306, 0.09921047, 0.0930253, 0.09277194, 0.08704954, 0.12734564, 0.11479893], dtype=tf.float32)\n\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_variables([a]))\n\nprint a.eval()\nprint tf.fake_quant_with_min_max_vars(inputs=a, min=tf.reduce_min(a), max=tf.reduce_max(a), num_bits=8).eval()\nprint tf.reduce_min(a).eval()\nprint tf.reduce_max(a).eval()\nit prints like below\n[ 0.09504107  0.0748544   0.09333218  0.106306    0.09921047  0.0930253\n  0.09277194  0.08704954  0.12734564  0.11479893]\n[ 0.05249124  0.05249124  0.05249124  0.05249124  0.05249124  0.05249124\n  0.05249124  0.05249124  0.05249124  0.05249124]\n0.0748544\n0.127346",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "54",
      "number": "16995",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): pip install tensorflow-gpu\nTensorFlow version (use command below): 1.5.0\nPython version: 2.7.12\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: CUDA-8.0 CuDNN 6.0\nGPU model and memory: GTX 1080 8GB\nExact command to reproduce:\n\nDescribe the problem\ntf.fake_quant_with_min_max_vars returns wrong answer.\nSource code / logs\nimport tensorflow as tf\n\na =tf.Variable([ 0.09504107, 0.0748544, 0.09333218, 0.106306, 0.09921047, 0.0930253, 0.09277194, 0.08704954, 0.12734564, 0.11479893], dtype=tf.float32)\n\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_variables([a]))\n\nprint a.eval()\nprint tf.fake_quant_with_min_max_vars(inputs=a, min=tf.reduce_min(a), max=tf.reduce_max(a), num_bits=8).eval()\nprint tf.reduce_min(a).eval()\nprint tf.reduce_max(a).eval()\nit prints like below\n[ 0.09504107  0.0748544   0.09333218  0.106306    0.09921047  0.0930253\n  0.09277194  0.08704954  0.12734564  0.11479893]\n[ 0.05249124  0.05249124  0.05249124  0.05249124  0.05249124  0.05249124\n  0.05249124  0.05249124  0.05249124  0.05249124]\n0.0748544\n0.127346",
      "title": "tf.fake_quant_with_min_max_vars returns wrong answer"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2535,
    "text": "Feature Request for the back-propagated errors in intermediate layersAfter the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:\n I->(W1)->C1->(W2)->C2->(W3)->O\n\nI is the input, O is the output, W1,W2,W3 is the weights for 3 layers. C1 and C2 are the outputs for the first two layers. With O and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to C1 and C2?\nI know we could get the parameter operators as follows:\nW1_op = tf.get_default_graph().get_tensor_by_name('W1')\nW1_op = ...\n\nMy final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).\nI know that we could use the tf.test.check_gradient to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.\nIn the Caffe framework, it seems those errors were saved in diff memory for each layer. I want to get these back-propagated errors in each layer. Does anybody know how to get that?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "55",
      "number": "17121",
      "pretext": "After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:\n I->(W1)->C1->(W2)->C2->(W3)->O\n\nI is the input, O is the output, W1,W2,W3 is the weights for 3 layers. C1 and C2 are the outputs for the first two layers. With O and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to C1 and C2?\nI know we could get the parameter operators as follows:\nW1_op = tf.get_default_graph().get_tensor_by_name('W1')\nW1_op = ...\n\nMy final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).\nI know that we could use the tf.test.check_gradient to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.\nIn the Caffe framework, it seems those errors were saved in diff memory for each layer. I want to get these back-propagated errors in each layer. Does anybody know how to get that?",
      "title": "Feature Request for the back-propagated errors in intermediate layers"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2536,
    "text": "Error downloading nasmAs noted in issue #6950 nasm-2.12.02.tar.bz2 is currently unavailable. (www.nasm.us does not accept connections.) @trsaunders observed this for head of r0.12 and I can confirm this for v0.12.0.\nWorkaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "56",
      "number": "6956",
      "pretext": "As noted in issue #6950 nasm-2.12.02.tar.bz2 is currently unavailable. (www.nasm.us does not accept connections.) @trsaunders observed this for head of r0.12 and I can confirm this for v0.12.0.\nWorkaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.",
      "title": "Error downloading nasm"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2537,
    "text": "\"tensorflow: Input iterator is exhausted\" when passing numpy arrays in validation monitorI'm not sure if this is a bug or whether I just don't understand the usage of monitors (in which case I apologize for posting here). I'm trying to use a validation monitor by passing my validation set as numpy array.\nval_monitor = learn.monitors.ValidationMonitor(X_val, Y_val, every_n_steps=100)\nreg.fit(X_train, Y_train, steps=1000, batch_size=200, monitors=[val_monitor])\n\n(X_val and Y_val are numpy arrays)\nThe code runs but only the first validation step is done properly, then I get the following message in the logs:\nINFO:tensorflow:Input iterator is exhausted.\nAny help is welcome!",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "57",
      "number": "3439",
      "pretext": "I'm not sure if this is a bug or whether I just don't understand the usage of monitors (in which case I apologize for posting here). I'm trying to use a validation monitor by passing my validation set as numpy array.\nval_monitor = learn.monitors.ValidationMonitor(X_val, Y_val, every_n_steps=100)\nreg.fit(X_train, Y_train, steps=1000, batch_size=200, monitors=[val_monitor])\n\n(X_val and Y_val are numpy arrays)\nThe code runs but only the first validation step is done properly, then I get the following message in the logs:\nINFO:tensorflow:Input iterator is exhausted.\nAny help is welcome!",
      "title": "\"tensorflow: Input iterator is exhausted\" when passing numpy arrays in validation monitor"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2538,
    "text": "Make NVIDIA library versions to TF Version matrix more visible.This request was a product of the TensorFlow Fall Symposium. Consider talking to the documentation team as well as maybe linking it to the top or near the top of all the release documents.",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "58",
      "number": "22357",
      "pretext": "This request was a product of the TensorFlow Fall Symposium. Consider talking to the documentation team as well as maybe linking it to the top or near the top of all the release documents.",
      "title": "Make NVIDIA library versions to TF Version matrix more visible."
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2539,
    "text": "Variable names created by tf.kera.Model.build() is inconsistent with that by tf.keras.Model.call()System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):  v1.11.0-0-gc19e29306c 1.11.0\nPython version: Python 3.5.2\nBazel version (if compiling from source): No\nGCC/Compiler version (if compiling from source): No\nCUDA/cuDNN version: 7.2.1\nGPU model and memory: GTX 1060 6GB\nExact command to reproduce: Please see the below\n\nDescribe the problem\ntf.keras.Model makes Variables of its weights, when its build() or call() is called first.\nWhile I found call() makes Variables with the prefix \"MyModel/\", build() make Variables without any prefix.\nThat is inconvenient to manage non-object based checkpoint.\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nlayers = keras.layers\n\nclass Model(keras.Model):\n    def __init__(self, name=\"MyModel\"):\n        super().__init__(name=name)\n        self.conv1 = layers.Conv2D(32, [5,5], activation=tf.nn.relu, name=\"conv1\")\n        self.conv2 = layers.Conv2D(64, [5,5], activation=tf.nn.relu, name=\"conv2\")\n\n    def call(self, images):\n        featmap = self.conv1(images)\n        featmap = self.conv2(featmap)\n        return featmap\n\n    def inference(self, images):\n        return self.__call__(images)\n\n\nmodel = Model()\nflags = \"build\"\n\nif flags == \"build\":\n    model.build(input_shape=tf.TensorShape([None, 32, 32, 3]))\n\n    for w in model.weights:\n        print (w.op.name)\nelse:\n    dummy = tf.zeros([1, 32, 32, 3])\n    model(dummy)\n\n    for w in model.weights:\n        print (w.op.name)",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "59",
      "number": "22861",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):  v1.11.0-0-gc19e29306c 1.11.0\nPython version: Python 3.5.2\nBazel version (if compiling from source): No\nGCC/Compiler version (if compiling from source): No\nCUDA/cuDNN version: 7.2.1\nGPU model and memory: GTX 1060 6GB\nExact command to reproduce: Please see the below\n\nDescribe the problem\ntf.keras.Model makes Variables of its weights, when its build() or call() is called first.\nWhile I found call() makes Variables with the prefix \"MyModel/\", build() make Variables without any prefix.\nThat is inconvenient to manage non-object based checkpoint.\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nlayers = keras.layers\n\nclass Model(keras.Model):\n    def __init__(self, name=\"MyModel\"):\n        super().__init__(name=name)\n        self.conv1 = layers.Conv2D(32, [5,5], activation=tf.nn.relu, name=\"conv1\")\n        self.conv2 = layers.Conv2D(64, [5,5], activation=tf.nn.relu, name=\"conv2\")\n\n    def call(self, images):\n        featmap = self.conv1(images)\n        featmap = self.conv2(featmap)\n        return featmap\n\n    def inference(self, images):\n        return self.__call__(images)\n\n\nmodel = Model()\nflags = \"build\"\n\nif flags == \"build\":\n    model.build(input_shape=tf.TensorShape([None, 32, 32, 3]))\n\n    for w in model.weights:\n        print (w.op.name)\nelse:\n    dummy = tf.zeros([1, 32, 32, 3])\n    model(dummy)\n\n    for w in model.weights:\n        print (w.op.name)",
      "title": "Variable names created by tf.kera.Model.build() is inconsistent with that by tf.keras.Model.call()"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2540,
    "text": "Error: Data loss: file is too short to be an sstableHi there,\nI'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm\nI am getting a \"Data loss\" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.\nCurrently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.\nW tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable\nTraceback (most recent call last):\n  File \"single_lm_train.py\", line 38, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 111, in run_eval\n    while ckpt_loader.load_checkpoint():\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 45, in load_checkpoint\n    if load_from_checkpoint(self.saver, self.logdir):\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 27, in load_from_checkpoint\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\n\ntensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]\nCaused by op u'save/RestoreV2_15', defined at:  File \"single_lm_train.py\", line 38, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 98, in run_eval\n    saver = tf.train.Saver(model.avg_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/repl\nica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha\npe_and_slices)]]",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "60",
      "number": "6644",
      "pretext": "Hi there,\nI'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm\nI am getting a \"Data loss\" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.\nCurrently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.\nW tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable\nTraceback (most recent call last):\n  File \"single_lm_train.py\", line 38, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 111, in run_eval\n    while ckpt_loader.load_checkpoint():\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 45, in load_checkpoint\n    if load_from_checkpoint(self.saver, self.logdir):\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 27, in load_from_checkpoint\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\n\ntensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]\nCaused by op u'save/RestoreV2_15', defined at:  File \"single_lm_train.py\", line 38, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 98, in run_eval\n    saver = tf.train.Saver(model.avg_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/repl\nica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha\npe_and_slices)]]",
      "title": "Error: Data loss: file is too short to be an sstable"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2541,
    "text": "PEP 484 Type Annotations (feature request)System information\nN/A\nDescribe the problem\nBackground\nPEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.\nWhen adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the \"untyped\" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.\nBenefits of Adding Type Annotations\n\nThe expected inputs and outputs of functions become much clearer\nCode completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs\nStatic analysis can uncover latent bugs (case study here[5])\n\nDifficulties/Drawbacks\n\nPeople may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages \"Power Features\" [4] I would argue that striving towards code that is explicit is a similar philosophy\nThe protobuf compiler would need to be augmented to generate type annotations.\nThe Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.\nTensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.\n\nFinal thoughts\nI realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.\n[1] https://www.python.org/dev/peps/pep-0484/\n[2] http://mypy-lang.org/\n[3] https://github.com/python/typeshed\n[4] https://google.github.io/styleguide/pyguide.html#Power_Features\n[5] http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/",
    "annotations": [{ "label": 142, "user": 1 }],
    "meta": {
      "": "61",
      "number": "12345",
      "pretext": "System information\nN/A\nDescribe the problem\nBackground\nPEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.\nWhen adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the \"untyped\" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.\nBenefits of Adding Type Annotations\n\nThe expected inputs and outputs of functions become much clearer\nCode completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs\nStatic analysis can uncover latent bugs (case study here[5])\n\nDifficulties/Drawbacks\n\nPeople may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages \"Power Features\" [4] I would argue that striving towards code that is explicit is a similar philosophy\nThe protobuf compiler would need to be augmented to generate type annotations.\nThe Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.\nTensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.\n\nFinal thoughts\nI realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.\n[1] https://www.python.org/dev/peps/pep-0484/\n[2] http://mypy-lang.org/\n[3] https://github.com/python/typeshed\n[4] https://google.github.io/styleguide/pyguide.html#Power_Features\n[5] http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/",
      "title": "PEP 484 Type Annotations (feature request)"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2542,
    "text": "ModuleNotFoundError: No module named 'tensorflow.keras'System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary (CPU linux wheel)\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6.5\n\nDescribe the problem\nUsing Tensorflow 1.8.0, running:\nfrom tensorflow.keras.utils import Progbar\nraises an error:\nModuleNotFoundError: No module named 'tensorflow.keras'\n\nOf course, from tensorflow import keras works fine.\nThis is a minor nit since there's an obvious workaround, but IMO this is pretty unintuitive behavior for how modules work in Python. I'm not sure what kind of sorcery is going on to end up with this result 😆.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "62",
      "number": "20096",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary (CPU linux wheel)\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6.5\n\nDescribe the problem\nUsing Tensorflow 1.8.0, running:\nfrom tensorflow.keras.utils import Progbar\nraises an error:\nModuleNotFoundError: No module named 'tensorflow.keras'\n\nOf course, from tensorflow import keras works fine.\nThis is a minor nit since there's an obvious workaround, but IMO this is pretty unintuitive behavior for how modules work in Python. I'm not sure what kind of sorcery is going on to end up with this result 😆.",
      "title": "ModuleNotFoundError: No module named 'tensorflow.keras'"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2543,
    "text": "Feature request : add weight normalizationcan you implement weight norm ?\nI want to use it as follows.\n    x = tf.layers.conv2d(x, filter_size=32, kernel_size=[3,3], strides=2)\n    x = weight_norm(x)\nIs it possible?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "63",
      "number": "14070",
      "pretext": "can you implement weight norm ?\nI want to use it as follows.\n    x = tf.layers.conv2d(x, filter_size=32, kernel_size=[3,3], strides=2)\n    x = weight_norm(x)\nIs it possible?",
      "title": "Feature request : add weight normalization"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2544,
    "text": "TF Learn TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\ntf.cast()\nEnvironment info\nOperating System:\nmasOSSierra\njupyter notebook\ntensforflow v0.12.1\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nNo\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: pip install tensorflow\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".tensforflow v0.12.1\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nI try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py\nWhat other attempted solutions have you tried?\ntry to cast DataFrame into float32 with\nX_train = X_train.astype(np.float32)\ntry to cast each column with\ntf.cast(col, tf.float32)\nbut after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)\nfeature_columns dtype just turn to tf.float64\n(tried and didn't find attribute from source code that I can change dtype here)\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\n\nTypeError                                 Traceback (most recent call last)\n in ()\n----> 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])\n/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)\n189             _call_location(), decorator_utils.get_qualified_name(func),\n190             func.module, arg_name, date, instructions)\n--> 191       return func(*args, **kwargs)\n192     new_func.doc = _add_deprecated_arg_notice_to_docstring(\n193         func.doc, date, instructions)\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\n353                              steps=steps,\n354                              monitors=monitors,\n--> 355                              max_steps=max_steps)\n356     logging.info('Loss for final step: %s.', loss)\n357     return self\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\n697       # cases, but will soon be deleted after the subclasses are updated.\n698       # TODO(b/32664904): Update subclasses and delete the else-statement.\n--> 699       train_ops = self._get_train_ops(features, labels)\n700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature\n701         train_op = train_ops.train_op\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)\n1050       ModelFnOps object.\n1051     \"\"\"\n-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\n1053\n1054   def _get_eval_ops(self, features, labels, metrics):\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)\n1019                                           params=self.params)\n1020       else:\n-> 1021         model_fn_results = self._model_fn(features, labels, mode=mode)\n1022     else:\n1023       model_fn_results = self._model_fn(features, labels)\n in conv_model(feature, target, mode)\n10                                     activation_fn=tf.nn.relu)\n11\n---> 12         h_pool1 = max_pool_2x2(h_conv1)\n13\n14     with tf.variable_scope('conv_layer2'):\n in max_pool_2x2(tensor_in)\n1 def max_pool_2x2(tensor_in):\n----> 2     return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)\n1615                                 padding=padding,\n1616                                 data_format=data_format,\n-> 1617                                 name=name)\n1618\n1619\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)\n1596   result = _op_def_lib.apply_op(\"MaxPool\", input=input, ksize=ksize,\n1597                                 strides=strides, padding=padding,\n-> 1598                                 data_format=data_format, name=name)\n1599   return result\n1600\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n580             for base_type in base_types:\n581               _SatisfiesTypeConstraint(base_type,\n--> 582                                        _Attr(op_def, input_arg.type_attr))\n583             attrs[input_arg.type_attr] = attr_value\n584             inferred_from[input_arg.type_attr] = input_name\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)\n58           \"DataType %s for attr '%s' not in list of allowed values: %s\" %\n59           (dtypes.as_dtype(dtype).name, attr_def.name,\n---> 60            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n61\n62\nTypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16\nMany thanks.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "64",
      "number": "6769",
      "pretext": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\ntf.cast()\nEnvironment info\nOperating System:\nmasOSSierra\njupyter notebook\ntensforflow v0.12.1\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nNo\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: pip install tensorflow\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".tensforflow v0.12.1\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nI try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py\nWhat other attempted solutions have you tried?\ntry to cast DataFrame into float32 with\nX_train = X_train.astype(np.float32)\ntry to cast each column with\ntf.cast(col, tf.float32)\nbut after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)\nfeature_columns dtype just turn to tf.float64\n(tried and didn't find attribute from source code that I can change dtype here)\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\n\nTypeError                                 Traceback (most recent call last)\n in ()\n----> 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])\n/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)\n189             _call_location(), decorator_utils.get_qualified_name(func),\n190             func.module, arg_name, date, instructions)\n--> 191       return func(*args, **kwargs)\n192     new_func.doc = _add_deprecated_arg_notice_to_docstring(\n193         func.doc, date, instructions)\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\n353                              steps=steps,\n354                              monitors=monitors,\n--> 355                              max_steps=max_steps)\n356     logging.info('Loss for final step: %s.', loss)\n357     return self\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\n697       # cases, but will soon be deleted after the subclasses are updated.\n698       # TODO(b/32664904): Update subclasses and delete the else-statement.\n--> 699       train_ops = self._get_train_ops(features, labels)\n700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature\n701         train_op = train_ops.train_op\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)\n1050       ModelFnOps object.\n1051     \"\"\"\n-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\n1053\n1054   def _get_eval_ops(self, features, labels, metrics):\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)\n1019                                           params=self.params)\n1020       else:\n-> 1021         model_fn_results = self._model_fn(features, labels, mode=mode)\n1022     else:\n1023       model_fn_results = self._model_fn(features, labels)\n in conv_model(feature, target, mode)\n10                                     activation_fn=tf.nn.relu)\n11\n---> 12         h_pool1 = max_pool_2x2(h_conv1)\n13\n14     with tf.variable_scope('conv_layer2'):\n in max_pool_2x2(tensor_in)\n1 def max_pool_2x2(tensor_in):\n----> 2     return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)\n1615                                 padding=padding,\n1616                                 data_format=data_format,\n-> 1617                                 name=name)\n1618\n1619\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)\n1596   result = _op_def_lib.apply_op(\"MaxPool\", input=input, ksize=ksize,\n1597                                 strides=strides, padding=padding,\n-> 1598                                 data_format=data_format, name=name)\n1599   return result\n1600\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n580             for base_type in base_types:\n581               _SatisfiesTypeConstraint(base_type,\n--> 582                                        _Attr(op_def, input_arg.type_attr))\n583             attrs[input_arg.type_attr] = attr_value\n584             inferred_from[input_arg.type_attr] = input_name\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)\n58           \"DataType %s for attr '%s' not in list of allowed values: %s\" %\n59           (dtypes.as_dtype(dtype).name, attr_def.name,\n---> 60            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n61\n62\nTypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16\nMany thanks.",
      "title": "TF Learn TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2545,
    "text": "TensorFlow upgrade to 1.0.1I upgraded my server from 1.0.0 (ubuntu):\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\nBut I get the following discrepancy:\n(tensorflow)$ pip list | grep tensorflow\ntensorflow (1.0.0)\n(tensorflow)$ python -c 'import tensorflow as tf; print(tf.version)'\n1.0.1",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "65",
      "number": "8586",
      "pretext": "I upgraded my server from 1.0.0 (ubuntu):\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\nBut I get the following discrepancy:\n(tensorflow)$ pip list | grep tensorflow\ntensorflow (1.0.0)\n(tensorflow)$ python -c 'import tensorflow as tf; print(tf.version)'\n1.0.1",
      "title": "TensorFlow upgrade to 1.0.1"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2546,
    "text": "Beam Search Decoder API@ebrevdo Why is it that the user needs to call tile_batch explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided initial_state in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.\nThank you!",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "66",
      "number": "15323",
      "pretext": "@ebrevdo Why is it that the user needs to call tile_batch explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided initial_state in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.\nThank you!",
      "title": "Beam Search Decoder API"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2547,
    "text": "[doc] link to \"How to Use t-SNE Effectively\" from embeddings is brokenSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7:\nTensorFlow installed from (source or binary) binary:\n**TensorFlow version (use command below) 1.5.0rc0 **:\nPython version  3.5.1:\nBazel version (if compiling from source) NOT USED:\nGCC/Compiler version (if compiling from source) NOT USED:\nCUDA/cuDNN version NOT USED:\n**GPU model and memory NOT USED **:\nExact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/:\n\nDescribe the problem\n\nLink to to \"How to Use t-SNE Effectively\" is broken.\nThe page link is follows (before junmping)\n\nhttps://www.tensorflow.org/programmers_guide/embedding\n404 page is following URL\n\nhttps://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/\n\n\n\n\nOriginal page should be follows. (the URL in embedding.md should rewrite to follows)\n\nhttps://distill.pub/2016/misread-tsne/\n\n\n\nSource code / logs\n\nThe problem code is follows.\n\nhttps://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "67",
      "number": "16400",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7:\nTensorFlow installed from (source or binary) binary:\n**TensorFlow version (use command below) 1.5.0rc0 **:\nPython version  3.5.1:\nBazel version (if compiling from source) NOT USED:\nGCC/Compiler version (if compiling from source) NOT USED:\nCUDA/cuDNN version NOT USED:\n**GPU model and memory NOT USED **:\nExact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/:\n\nDescribe the problem\n\nLink to to \"How to Use t-SNE Effectively\" is broken.\nThe page link is follows (before junmping)\n\nhttps://www.tensorflow.org/programmers_guide/embedding\n404 page is following URL\n\nhttps://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/\n\n\n\n\nOriginal page should be follows. (the URL in embedding.md should rewrite to follows)\n\nhttps://distill.pub/2016/misread-tsne/\n\n\n\nSource code / logs\n\nThe problem code is follows.\n\nhttps://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123",
      "title": "[doc] link to \"How to Use t-SNE Effectively\" from embeddings is broken"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2548,
    "text": "While loop no gradients provided for any variableAfter i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.\n\nwhile_loop\ndef dynamic_pointing_decoder(self, U, mask):\ndef _HMN(ut, h, us, ue):\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\nut_r = tf.concat([ut, r], axis=1) #batch,3d\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\nhmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\nhmn = tf.reduce_max(hmn, axis=2) #batch 1\nhmn = tf.reshape(hmn, [-1]) #batch\nreturn hmn\n    def body(time_step, p1s, p2s, alphas, betas, us, ue, state):\n        us_ue = tf.concat([us, ue], axis=1)  # batch 4d\n        h, state = cell(inputs=us_ue, state=state)  # batch * d\n\n        with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):\n            alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_start = tf.argmax(alpha, axis=1)  # batch\n        i_start = tf.cast(i_start, tf.int32)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        us = tf.gather_nd(U, s_idx)  # batch 2d\n\n        with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):\n            beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_end = tf.argmax(beta, axis=1)  # batch\n        i_end = tf.cast(i_end, tf.int32)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        ue = tf.gather_nd(U, e_idx)  # batch 2d\n\n        p1s.write(time_step, i_start)\n        p2s.write(time_step, i_end)\n        alphas.write(time_step, alpha)\n        betas.write(time_step, beta)\n        return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)\n\n    def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):\n        return tf.less(time_step, 4)\n\n\n    with tf.variable_scope('dynamic_pointing_decoder'):\n        cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\n        i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        idx = tf.range(0, tf.shape(U)[0], 1)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        us = tf.gather_nd(U, s_idx) #batch 2d\n        ue = tf.gather_nd(U, e_idx) #batch 2d\n        p1s = tf.TensorArray(tf.int32, size=4)\n        p2s = tf.TensorArray(tf.int32, size=4)\n        alphas = tf.TensorArray(tf.float32, size=4)\n        betas = tf.TensorArray(tf.float32, size=4)\n        state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\n                                          tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\n        U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\n        #time_step, p1s, p2s, us, ue, state\n        time_step = 0\n        time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,\n                                                                          loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),\n                                                                          maximum_iterations=4)\n        self.p1s = tf.transpose(p1s.stack()) #batch*4\n        self.p2s = tf.transpose(p2s.stack())\n        print(\n            \"p1s  shape : {0}\".format(np.shape(self.p1s))\n        )\n        self.p1 = tf.unstack(self.p1s,axis=0)[-1]\n        print(\"p1 shape : {0}\".format(np.shape(self.p1)))\n        self.p2 = tf.unstack(self.p2s, axis=0)[-1]\n        alphas = tf.unstack(alphas.stack(), axis=0)\n        betas = tf.unstack(betas.stack(), axis=0)\n        print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))\n        return alphas, betas\n\n#no while loop\ndef dynamic_pointing_decoder(self, U, mask):\ndef _HMN(ut, h, us, ue):\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\nut_r = tf.concat([ut, r], axis=1) #batch,3d\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\nhmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\nhmn = tf.reduce_max(hmn, axis=2) #batch 1\nhmn = tf.reshape(hmn, [-1]) #batch\nreturn hmn\nwith tf.variable_scope('dynamic_pointing_decoder'):\n#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)\n#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])\n#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)\ncell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\ni_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\ni_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\nidx = tf.range(0, tf.shape(U)[0], 1)\ns_idx = tf.stack([idx, i_start], axis=1)\ne_idx = tf.stack([idx, i_end], axis=1)\nus = tf.gather_nd(U, s_idx) #batch 2d\nue = tf.gather_nd(U, e_idx) #batch 2d\nalphas, betas = [], []\nstate = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\ntf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\nU_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\nfor time_step in range(4):\nif time_step >= 1:\ntf.get_variable_scope().reuse_variables()\nus_ue = tf.concat([us,ue], axis=1) #batch 4d\nh, state = cell(inputs=us_ue, state=state) #batch * d\n            with tf.variable_scope('alpha_HMN'):\n                if time_step >= 1:\n                    tf.get_variable_scope().reuse_variables()\n                alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_start = tf.argmax(alpha, axis=1) #batch\n            i_start = tf.cast(i_start, tf.int32)\n            s_idx = tf.stack([idx, i_start], axis=1)\n            us = tf.gather_nd(U, s_idx) #batch 2d\n\n            with tf.variable_scope('betas_HMN'):\n                if time_step >= 1:\n                    tf.get_variable_scope().reuse_variables()\n                beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_end = tf.argmax(alpha, axis=1) #batch\n            i_end = tf.cast(i_end, tf.int32)\n            e_idx = tf.stack([idx, i_end], axis=1)\n            ue = tf.gather_nd(U, e_idx) #batch 2d\n\n            alphas.append(alpha)\n            betas.append(beta)\n            #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):\n            #    break\n            #else:\n            #    pre_start = i_start\n            #    pre_end = i_end\n        return alpha, beta\n\nanyone can help me? Thank you very much",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "68",
      "number": "21985",
      "pretext": "After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.\n\nwhile_loop\ndef dynamic_pointing_decoder(self, U, mask):\ndef _HMN(ut, h, us, ue):\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\nut_r = tf.concat([ut, r], axis=1) #batch,3d\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\nhmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\nhmn = tf.reduce_max(hmn, axis=2) #batch 1\nhmn = tf.reshape(hmn, [-1]) #batch\nreturn hmn\n    def body(time_step, p1s, p2s, alphas, betas, us, ue, state):\n        us_ue = tf.concat([us, ue], axis=1)  # batch 4d\n        h, state = cell(inputs=us_ue, state=state)  # batch * d\n\n        with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):\n            alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_start = tf.argmax(alpha, axis=1)  # batch\n        i_start = tf.cast(i_start, tf.int32)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        us = tf.gather_nd(U, s_idx)  # batch 2d\n\n        with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):\n            beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_end = tf.argmax(beta, axis=1)  # batch\n        i_end = tf.cast(i_end, tf.int32)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        ue = tf.gather_nd(U, e_idx)  # batch 2d\n\n        p1s.write(time_step, i_start)\n        p2s.write(time_step, i_end)\n        alphas.write(time_step, alpha)\n        betas.write(time_step, beta)\n        return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)\n\n    def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):\n        return tf.less(time_step, 4)\n\n\n    with tf.variable_scope('dynamic_pointing_decoder'):\n        cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\n        i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        idx = tf.range(0, tf.shape(U)[0], 1)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        us = tf.gather_nd(U, s_idx) #batch 2d\n        ue = tf.gather_nd(U, e_idx) #batch 2d\n        p1s = tf.TensorArray(tf.int32, size=4)\n        p2s = tf.TensorArray(tf.int32, size=4)\n        alphas = tf.TensorArray(tf.float32, size=4)\n        betas = tf.TensorArray(tf.float32, size=4)\n        state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\n                                          tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\n        U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\n        #time_step, p1s, p2s, us, ue, state\n        time_step = 0\n        time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,\n                                                                          loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),\n                                                                          maximum_iterations=4)\n        self.p1s = tf.transpose(p1s.stack()) #batch*4\n        self.p2s = tf.transpose(p2s.stack())\n        print(\n            \"p1s  shape : {0}\".format(np.shape(self.p1s))\n        )\n        self.p1 = tf.unstack(self.p1s,axis=0)[-1]\n        print(\"p1 shape : {0}\".format(np.shape(self.p1)))\n        self.p2 = tf.unstack(self.p2s, axis=0)[-1]\n        alphas = tf.unstack(alphas.stack(), axis=0)\n        betas = tf.unstack(betas.stack(), axis=0)\n        print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))\n        return alphas, betas\n\n#no while loop\ndef dynamic_pointing_decoder(self, U, mask):\ndef _HMN(ut, h, us, ue):\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\nut_r = tf.concat([ut, r], axis=1) #batch,3d\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\nhmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\nhmn = tf.reduce_max(hmn, axis=2) #batch 1\nhmn = tf.reshape(hmn, [-1]) #batch\nreturn hmn\nwith tf.variable_scope('dynamic_pointing_decoder'):\n#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)\n#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])\n#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)\ncell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\ni_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\ni_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\nidx = tf.range(0, tf.shape(U)[0], 1)\ns_idx = tf.stack([idx, i_start], axis=1)\ne_idx = tf.stack([idx, i_end], axis=1)\nus = tf.gather_nd(U, s_idx) #batch 2d\nue = tf.gather_nd(U, e_idx) #batch 2d\nalphas, betas = [], []\nstate = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\ntf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\nU_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\nfor time_step in range(4):\nif time_step >= 1:\ntf.get_variable_scope().reuse_variables()\nus_ue = tf.concat([us,ue], axis=1) #batch 4d\nh, state = cell(inputs=us_ue, state=state) #batch * d\n            with tf.variable_scope('alpha_HMN'):\n                if time_step >= 1:\n                    tf.get_variable_scope().reuse_variables()\n                alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_start = tf.argmax(alpha, axis=1) #batch\n            i_start = tf.cast(i_start, tf.int32)\n            s_idx = tf.stack([idx, i_start], axis=1)\n            us = tf.gather_nd(U, s_idx) #batch 2d\n\n            with tf.variable_scope('betas_HMN'):\n                if time_step >= 1:\n                    tf.get_variable_scope().reuse_variables()\n                beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_end = tf.argmax(alpha, axis=1) #batch\n            i_end = tf.cast(i_end, tf.int32)\n            e_idx = tf.stack([idx, i_end], axis=1)\n            ue = tf.gather_nd(U, e_idx) #batch 2d\n\n            alphas.append(alpha)\n            betas.append(beta)\n            #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):\n            #    break\n            #else:\n            #    pre_start = i_start\n            #    pre_end = i_end\n        return alpha, beta\n\nanyone can help me? Thank you very much",
      "title": "While loop no gradients provided for any variable"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2549,
    "text": "[cmake] Build error in dependency re2Somehow the re2 dependency does not get build correctly.\nI think its because of re2_INCLUDE_DIR in re.cmake holding two directories. Then COMMAND ${CMAKE_COMMAND} -E make_directory ${re2_INCLUDE_DIR} fails since cmake -E make_directory only takes one arg. I'm just comiping and might add a PR if it works.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "69",
      "number": "4027",
      "pretext": "Somehow the re2 dependency does not get build correctly.\nI think its because of re2_INCLUDE_DIR in re.cmake holding two directories. Then COMMAND ${CMAKE_COMMAND} -E make_directory ${re2_INCLUDE_DIR} fails since cmake -E make_directory only takes one arg. I'm just comiping and might add a PR if it works.",
      "title": "[cmake] Build error in dependency re2"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2550,
    "text": "XLA AOT tfcompile failure due to undeclared inclusions in cc_binary ruleThis happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:\nERROR: tensorflow/BUILD:13:1: undeclared inclusion(s) in rule '//:model':\nthis rule is missing dependency declarations for the following files included by 'graph.cc':\n  'tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h'\n  'tensorflow/compiler/tf2xla/xla_local_runtime_context.h'\n  'tensorflow/core/platform/macros.h'\n  '/tensorflow/core/platform/types.h'\n  '/tensorflow/core/platform/platform.h'\n  '/tensorflow/core/platform/default/integral_types.h'\n  '/tensorflow/compiler/xla/executable_run_options.h'\ngraph.cc pretty much just does #include \"graph.h\" as per the tfcompile tutorial and it's weird because these headers seem to be included in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.\nThis is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):\nload(\"@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\n\ntf_library(\n  name = \"graph\",\n  cpp_class = \"Graph\",\n  graph = \"graph.pb\",\n  config = \"graph.config.pb\",\n)\n\ncc_binary(\n  name = \"model\",\n  srcs = [\"graph.cc\"],\n  deps = [\":graph\", \"//third_party/eigen3\"],\n  linkopts = [\"-lpthread\"]\n)\nI'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "70",
      "number": "14797",
      "pretext": "This happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:\nERROR: tensorflow/BUILD:13:1: undeclared inclusion(s) in rule '//:model':\nthis rule is missing dependency declarations for the following files included by 'graph.cc':\n  'tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h'\n  'tensorflow/compiler/tf2xla/xla_local_runtime_context.h'\n  'tensorflow/core/platform/macros.h'\n  '/tensorflow/core/platform/types.h'\n  '/tensorflow/core/platform/platform.h'\n  '/tensorflow/core/platform/default/integral_types.h'\n  '/tensorflow/compiler/xla/executable_run_options.h'\ngraph.cc pretty much just does #include \"graph.h\" as per the tfcompile tutorial and it's weird because these headers seem to be included in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.\nThis is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):\nload(\"@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\n\ntf_library(\n  name = \"graph\",\n  cpp_class = \"Graph\",\n  graph = \"graph.pb\",\n  config = \"graph.config.pb\",\n)\n\ncc_binary(\n  name = \"model\",\n  srcs = [\"graph.cc\"],\n  deps = [\":graph\", \"//third_party/eigen3\"],\n  linkopts = [\"-lpthread\"]\n)\nI'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.",
      "title": "XLA AOT tfcompile failure due to undeclared inclusions in cc_binary rule"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2551,
    "text": "Native GPU version of `tf.dynamic_stitch`Environment info\nOperating System: Windows 10\nInstalled version of CUDA and cuDNN: 8.0, 5105\ntensorflow release 0.12.1\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport tensorflow as tf\nfrom tensorflow.python.client.timeline import Timeline\n\nwith tf.device(\"/gpu:0\"):\n    x = tf.ones(100)\n    idxs = tf.range(100)\n\n    for _ in range(10):\n        y = tf.identity(x)\n        x = tf.dynamic_stitch([idxs, idxs], [x, y])\n        # x = tf.gather(y, idxs)\n\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    metadata = tf.RunMetadata()\n    sess.run(x, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n             run_metadata=metadata)\n\ntimeline = Timeline(metadata.step_stats)\nwith open(\"profile.json\", \"w\") as f:\n    f.write(timeline.generate_chrome_trace_format())\nThe log_device_placement output shows that everything is assigned to the GPU, as expected.  However, inspecting the trace output shows that data is being copied on and off the GPU for each call to dynamic_stitch.  This is something specific to the dynamic_stitch implementation, because using tf.gather (a similar indexed read operation, and functionally equivalent in this case), doesn't show this behaviour.\nIs this intended behaviour for dynamic_stitch (i.e., the copying to and from the GPU is necessary)?  Or is this a bug?  If it isn't a bug, is there some equivalent solution that doesn't require the data to be copied back and forth?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "71",
      "number": "7251",
      "pretext": "Environment info\nOperating System: Windows 10\nInstalled version of CUDA and cuDNN: 8.0, 5105\ntensorflow release 0.12.1\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport tensorflow as tf\nfrom tensorflow.python.client.timeline import Timeline\n\nwith tf.device(\"/gpu:0\"):\n    x = tf.ones(100)\n    idxs = tf.range(100)\n\n    for _ in range(10):\n        y = tf.identity(x)\n        x = tf.dynamic_stitch([idxs, idxs], [x, y])\n        # x = tf.gather(y, idxs)\n\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    metadata = tf.RunMetadata()\n    sess.run(x, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n             run_metadata=metadata)\n\ntimeline = Timeline(metadata.step_stats)\nwith open(\"profile.json\", \"w\") as f:\n    f.write(timeline.generate_chrome_trace_format())\nThe log_device_placement output shows that everything is assigned to the GPU, as expected.  However, inspecting the trace output shows that data is being copied on and off the GPU for each call to dynamic_stitch.  This is something specific to the dynamic_stitch implementation, because using tf.gather (a similar indexed read operation, and functionally equivalent in this case), doesn't show this behaviour.\nIs this intended behaviour for dynamic_stitch (i.e., the copying to and from the GPU is necessary)?  Or is this a bug?  If it isn't a bug, is there some equivalent solution that doesn't require the data to be copied back and forth?",
      "title": "Native GPU version of `tf.dynamic_stitch`"
    },
    "annotation_approver": null
  },
  {
    "id": 2552,
    "text": "Java Api String tensors supportfrom Tensor.java:\nnon-scalar DataType.STRING tensors are not supported yet\nIs there a plan for adding them for the Java interface?",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "72",
      "number": "10019",
      "pretext": "from Tensor.java:\nnon-scalar DataType.STRING tensors are not supported yet\nIs there a plan for adding them for the Java interface?",
      "title": "Java Api String tensors support"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2553,
    "text": "How to make statistics script using summary?Hi, all\nI want parameter distribution analysis script for pretrained models.\nI do not want special script for each model, just want single program to do it.\nSome person advised me to use the summary graph.\ntensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc\nI check the code and fell that the code does not support extracting parameters from pb file.\nI wrote a draft code to analyze;\nhttps://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py\nAny suggestion is welcome, and I am beginner, please explain softly.\nBest,\nSyouyu",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "73",
      "number": "19814",
      "pretext": "Hi, all\nI want parameter distribution analysis script for pretrained models.\nI do not want special script for each model, just want single program to do it.\nSome person advised me to use the summary graph.\ntensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc\nI check the code and fell that the code does not support extracting parameters from pb file.\nI wrote a draft code to analyze;\nhttps://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py\nAny suggestion is welcome, and I am beginner, please explain softly.\nBest,\nSyouyu",
      "title": "How to make statistics script using summary?"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2554,
    "text": "tensorflow lite converter(toco) build error System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10(64bit)\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below): tensorflow 1.5.0\nPython version: Python 2.7/3.6\nBazel version (if compiling from source):  bazel 0.9.0\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: No GPU model\n\nDescribe the problem\nI try to build the toco that is tensorflow lite converter.\nBut I can not success to build. please see below for the details.\nSource code / logs\nC:\\tensorflow>bazel build //tensorflow/contrib/lite/toco:toco\nThe following error message appears.\n\nERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\nFile \"C:/tensorflow/third_party/repo.bzl\", line 88\n_apply_patch(ctx, ctx.attr.patch_file)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\n_execute_and_check_ret_code(ctx, cmd)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\nfail(\"Non-zero return code({1}) when ...))\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\nWARNING: Target pattern parsing failed.\nERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\nFile \"C:/tensorflow/third_party/repo.bzl\", line 88\n_apply_patch(ctx, ctx.attr.patch_file)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\n_execute_and_check_ret_code(ctx, cmd)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\nfail(\"Non-zero return code({1}) when ...))\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\nINFO: Elapsed time: 27.852s\nFAILED: Build did NOT complete successfully (0 packages loaded)\ncurrently loading: tensorflow/contrib/lite/toco\n\n\nplus info.\nThe following message appears when I input like this in command line. (for test)\npatch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch\n\n\npatching file src/google/protobuf/compiler/cpp/cpp_file.cc\nAssertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\n\npatch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary\n\npatching file src/google/protobuf/compiler/cpp/cpp_file.cc\nHunk #1 succeeded at 750 with fuzz 1 (offset 193 lines).\nHunk #2 succeeded at 825 (offset 169 lines).\nHunk #3 succeeded at 906 with fuzz 2 (offset 169 lines).\n\nI don't know how to add --binary option to script...\nref. #10435",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "74",
      "number": "16946",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10(64bit)\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below): tensorflow 1.5.0\nPython version: Python 2.7/3.6\nBazel version (if compiling from source):  bazel 0.9.0\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: No GPU model\n\nDescribe the problem\nI try to build the toco that is tensorflow lite converter.\nBut I can not success to build. please see below for the details.\nSource code / logs\nC:\\tensorflow>bazel build //tensorflow/contrib/lite/toco:toco\nThe following error message appears.\n\nERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\nFile \"C:/tensorflow/third_party/repo.bzl\", line 88\n_apply_patch(ctx, ctx.attr.patch_file)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\n_execute_and_check_ret_code(ctx, cmd)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\nfail(\"Non-zero return code({1}) when ...))\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\nWARNING: Target pattern parsing failed.\nERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\nFile \"C:/tensorflow/third_party/repo.bzl\", line 88\n_apply_patch(ctx, ctx.attr.patch_file)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\n_execute_and_check_ret_code(ctx, cmd)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\nfail(\"Non-zero return code({1}) when ...))\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\nINFO: Elapsed time: 27.852s\nFAILED: Build did NOT complete successfully (0 packages loaded)\ncurrently loading: tensorflow/contrib/lite/toco\n\n\nplus info.\nThe following message appears when I input like this in command line. (for test)\npatch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch\n\n\npatching file src/google/protobuf/compiler/cpp/cpp_file.cc\nAssertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\n\npatch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary\n\npatching file src/google/protobuf/compiler/cpp/cpp_file.cc\nHunk #1 succeeded at 750 with fuzz 1 (offset 193 lines).\nHunk #2 succeeded at 825 (offset 169 lines).\nHunk #3 succeeded at 906 with fuzz 2 (offset 169 lines).\n\nI don't know how to add --binary option to script...\nref. #10435",
      "title": "tensorflow lite converter(toco) build error "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2555,
    "text": "ImportError: cannot import name model_fnI tried run cnn_mnist.py and I got the following error.\nTraceback (most recent call last):\nFile \" cnn_mnist.py\", line 13, in \nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\nImportError: cannot import name model_fn\ncuda veriosn\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Sun_Sep__4_22:14:01_CDT_2016\nCuda compilation tools, release 8.0, V8.0.44\ntensorflow version\ntensorflow (0.10.0)\npython version\nPython 2.7.12",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "75",
      "number": "7959",
      "pretext": "I tried run cnn_mnist.py and I got the following error.\nTraceback (most recent call last):\nFile \" cnn_mnist.py\", line 13, in \nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\nImportError: cannot import name model_fn\ncuda veriosn\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Sun_Sep__4_22:14:01_CDT_2016\nCuda compilation tools, release 8.0, V8.0.44\ntensorflow version\ntensorflow (0.10.0)\npython version\nPython 2.7.12",
      "title": "ImportError: cannot import name model_fn"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2556,
    "text": "Killed during checkpoint save (v0.8)Environment info\nOperating System: Ubuntu 15.10\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root   322936 aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a lrwxrwxrwx 1 root root       16 aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5 lrwxrwxrwx 1 root root       19 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18 -rwxr-xr-x 1 root root   383336 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18 -rw-r--r-- 1 root root   720192 aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4 -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48 -rw-r--r-- 1 root root 62025862 mar  6 15:08 /usr/local/cuda/lib64/libcudnn_static.a\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\nThe output from python -c \"import tensorflow; print(tensorflow.version)\".\n\n0.8.0rc0\nRelevant code:\nUnfortunately I can't upload everything, but the code is based on your CIFAR-10 example. Here is the structure of my network:\nhttp://pastebin.com/5QEJWqtm\nAfter running for some time, I save a checkpoint:\nsaver.save(sess, checkpoint_path, global_step=step)\nWhich sometimes allocates all memory on my system and gets killed. I have 8gb RAM and 8gb swap. For the first few checkpoints it seems fine, it allocates 2gb RAM (equal to checkpoint file size), but after some time it locks up my entire system and gets killed automatically.\nDidn't have any issues in 0.7.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "76",
      "number": "1962",
      "pretext": "Environment info\nOperating System: Ubuntu 15.10\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root   322936 aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a lrwxrwxrwx 1 root root       16 aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5 lrwxrwxrwx 1 root root       19 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18 -rwxr-xr-x 1 root root   383336 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18 -rw-r--r-- 1 root root   720192 aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4 -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48 -rw-r--r-- 1 root root 62025862 mar  6 15:08 /usr/local/cuda/lib64/libcudnn_static.a\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\nThe output from python -c \"import tensorflow; print(tensorflow.version)\".\n\n0.8.0rc0\nRelevant code:\nUnfortunately I can't upload everything, but the code is based on your CIFAR-10 example. Here is the structure of my network:\nhttp://pastebin.com/5QEJWqtm\nAfter running for some time, I save a checkpoint:\nsaver.save(sess, checkpoint_path, global_step=step)\nWhich sometimes allocates all memory on my system and gets killed. I have 8gb RAM and 8gb swap. For the first few checkpoints it seems fine, it allocates 2gb RAM (equal to checkpoint file size), but after some time it locks up my entire system and gets killed automatically.\nDidn't have any issues in 0.7.",
      "title": "Killed during checkpoint save (v0.8)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2557,
    "text": "tf.cond not working with depedenciestf.cond seems to have a bug if one of the condition have a dependency. (Dependencies are run, whatever tf.cond arg is True or False).\nTo illustrate:\nimport tensorflow as tf\n\na = tf.Variable(0)\nincr = a.count_up_to(1)\n\ndef todo_if_true():\n  with tf.control_dependencies([incr]):\n    return tf.identity(a)\ndef todo_if_false():\n  return tf.identity(a)\n\ng = tf.cond(tf.constant(False), todo_if_true, todo_if_false)\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  sess.run(init)\n  print(sess.run(g))\n\nOutput:\n1 #But should be 0",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "77",
      "number": "2062",
      "pretext": "tf.cond seems to have a bug if one of the condition have a dependency. (Dependencies are run, whatever tf.cond arg is True or False).\nTo illustrate:\nimport tensorflow as tf\n\na = tf.Variable(0)\nincr = a.count_up_to(1)\n\ndef todo_if_true():\n  with tf.control_dependencies([incr]):\n    return tf.identity(a)\ndef todo_if_false():\n  return tf.identity(a)\n\ng = tf.cond(tf.constant(False), todo_if_true, todo_if_false)\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  sess.run(init)\n  print(sess.run(g))\n\nOutput:\n1 #But should be 0",
      "title": "tf.cond not working with depedencies"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2558,
    "text": "Include netstat in the tensorflow docker containerDescribe the problem\nThis is a feature request to add net-tools to the Tensorflow docker containers.  Having netstat in the Tensorflow container will make it easier to find open ports in a multi-tenant environment when launching Tensorflow Distributed or Tensorboard.\nNote, I have found how to add netstat (see URL below), but would prefer not having to change or maintain a modified version of the Tensorflow container.\nhttps://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):NA\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):tensorflow/tensorflow:1.3.0 docker container\nTensorFlow installed from (source or binary):docker container\nTensorFlow version (use command below):1.3.0\nPython version: 2.7.12\nBazel version (if compiling from source):NA\nGCC/Compiler version (if compiling from source):NA\nCUDA/cuDNN version:NA\nGPU model and memory:NA\nExact command to reproduce:netstat",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "78",
      "number": "16226",
      "pretext": "Describe the problem\nThis is a feature request to add net-tools to the Tensorflow docker containers.  Having netstat in the Tensorflow container will make it easier to find open ports in a multi-tenant environment when launching Tensorflow Distributed or Tensorboard.\nNote, I have found how to add netstat (see URL below), but would prefer not having to change or maintain a modified version of the Tensorflow container.\nhttps://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):NA\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):tensorflow/tensorflow:1.3.0 docker container\nTensorFlow installed from (source or binary):docker container\nTensorFlow version (use command below):1.3.0\nPython version: 2.7.12\nBazel version (if compiling from source):NA\nGCC/Compiler version (if compiling from source):NA\nCUDA/cuDNN version:NA\nGPU model and memory:NA\nExact command to reproduce:netstat",
      "title": "Include netstat in the tensorflow docker container"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2559,
    "text": "Please provide an example how to use a model trained from scratch for image classificationThe following documentation of TensorFlow-Slim contains how to train a model from scratch, but it's not explained how to use the resulting checkpoint files for image classification.\nhttps://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch\nAlthough it's possible to load the pre-trained models (https://github.com/tensorflow/models/tree/master/slim#pre-trained-models) and use it for image classification with the example given in https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb, it seems not possible to simply use the checkpoint files generated by \"training from scratch\" in the same way.\nAny example on how to use the newly generated checkpoints for image classification (for example with inception) would be appreciated.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "79",
      "number": "8633",
      "pretext": "The following documentation of TensorFlow-Slim contains how to train a model from scratch, but it's not explained how to use the resulting checkpoint files for image classification.\nhttps://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch\nAlthough it's possible to load the pre-trained models (https://github.com/tensorflow/models/tree/master/slim#pre-trained-models) and use it for image classification with the example given in https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb, it seems not possible to simply use the checkpoint files generated by \"training from scratch\" in the same way.\nAny example on how to use the newly generated checkpoints for image classification (for example with inception) would be appreciated.",
      "title": "Please provide an example how to use a model trained from scratch for image classification"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2560,
    "text": "TensorFlow op to copy weights of Keras modelI am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)\nwith tf.device(tf.train.replica_device_setter(...):\n      model = ##create model by keras\n      clone_model = ## create the same model by keras but now a stateful one\n\nafter calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling\n clone_model.set_weights(model.get_weights())\n\ndoes not work.\nI understand I need to define this weight copy as an op and then call session(run) of that op\nCan you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?",
    "annotations": [{ "label": 145, "user": 3 }],
    "meta": {
      "": "80",
      "number": "16584",
      "pretext": "I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)\nwith tf.device(tf.train.replica_device_setter(...):\n      model = ##create model by keras\n      clone_model = ## create the same model by keras but now a stateful one\n\nafter calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling\n clone_model.set_weights(model.get_weights())\n\ndoes not work.\nI understand I need to define this weight copy as an op and then call session(run) of that op\nCan you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?",
      "title": "TensorFlow op to copy weights of Keras model"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2561,
    "text": "TensorRT engine binding errorSystem information*\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson TX2, Linux4Tegra Xenial 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.8.0\nPython version: 2.7.12\nBazel version (if compiling from source): 0.18.0\nGCC/Compiler version (if compiling from source): gcc5\nCUDA/cuDNN version: 9.0/7.1.5\nGPU model and memory: Jetson tx2 8GB\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the current behavior:\nI have frozen a graph that contains the following operations:\n\ntf.layers.conv2d(*args, **kwargs)\ntf.layers.batch_normalization(net, **batch_norm)\n\nI will try to provide a minimal graph that have this problem.\nDescribe the expected behavior:\nI create a trt_graph by using the function trt.create_inference_graph and it creates a graph successfully  but whenever I try to make an inference I encounter:\n2018-10-22 15:29:17.537843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with│\n 363 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)                                            │\n2018-10-22 15:29:20.426303: F tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:76] TensorRT engine cannot find binding: ModelBase/Conv2dBatchNorm/Relu     \nAborted (core dumped)\n\nAny guideline on how to provide a better log issue?\nThanks",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "81",
      "number": "23165",
      "pretext": "System information*\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson TX2, Linux4Tegra Xenial 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.8.0\nPython version: 2.7.12\nBazel version (if compiling from source): 0.18.0\nGCC/Compiler version (if compiling from source): gcc5\nCUDA/cuDNN version: 9.0/7.1.5\nGPU model and memory: Jetson tx2 8GB\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the current behavior:\nI have frozen a graph that contains the following operations:\n\ntf.layers.conv2d(*args, **kwargs)\ntf.layers.batch_normalization(net, **batch_norm)\n\nI will try to provide a minimal graph that have this problem.\nDescribe the expected behavior:\nI create a trt_graph by using the function trt.create_inference_graph and it creates a graph successfully  but whenever I try to make an inference I encounter:\n2018-10-22 15:29:17.537843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with│\n 363 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)                                            │\n2018-10-22 15:29:20.426303: F tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:76] TensorRT engine cannot find binding: ModelBase/Conv2dBatchNorm/Relu     \nAborted (core dumped)\n\nAny guideline on how to provide a better log issue?\nThanks",
      "title": "TensorRT engine binding error"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2562,
    "text": "gradient_override_map should raise an error if passed invalid gradient names.Currently, gradient_override_map does not complain if passed in nonsensical information (apart from the simple check to make sure the map is a map from strings to strings).\nFor instance, the following lines of code run without issue:\n with graph.gradient_override_map({\"nonsense\": \"more_nonsense\"}): input = tf.sign(input)\nA more subtle point (that happened with me), when attempting to override sign's gradient, the following typo ran without problem:\n with graph.gradient_override_map({\"sign\": \"Identity\"}): input = tf.sign(input)\n(\"sign\" should be \"Sign\").\nSeems like a fairly simple issue, but I am not quite versed enough in the Tensorflow backend to suggest a fix to this problem.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "82",
      "number": "8138",
      "pretext": "Currently, gradient_override_map does not complain if passed in nonsensical information (apart from the simple check to make sure the map is a map from strings to strings).\nFor instance, the following lines of code run without issue:\n with graph.gradient_override_map({\"nonsense\": \"more_nonsense\"}): input = tf.sign(input)\nA more subtle point (that happened with me), when attempting to override sign's gradient, the following typo ran without problem:\n with graph.gradient_override_map({\"sign\": \"Identity\"}): input = tf.sign(input)\n(\"sign\" should be \"Sign\").\nSeems like a fairly simple issue, but I am not quite versed enough in the Tensorflow backend to suggest a fix to this problem.",
      "title": "gradient_override_map should raise an error if passed invalid gradient names."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2563,
    "text": "[ distribution ] How to use multiple GPU on each replica ?The Code Here shows how to set each replica which has a single tower that uses one GPU. I'm wondering if there is a way changing this code a little bit to make use of multiple GPU on one machine like that example.\nThe way I currently used for using all GPU on a worker machine is starting the number of workers that equal to the number of GPUs. then the workers can communicate to each other as if they are not on one machine. That is slower than if I can start a woker that control more than one GPU.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "83",
      "number": "2106",
      "pretext": "The Code Here shows how to set each replica which has a single tower that uses one GPU. I'm wondering if there is a way changing this code a little bit to make use of multiple GPU on one machine like that example.\nThe way I currently used for using all GPU on a worker machine is starting the number of workers that equal to the number of GPUs. then the workers can communicate to each other as if they are not on one machine. That is slower than if I can start a woker that control more than one GPU.",
      "title": "[ distribution ] How to use multiple GPU on each replica ?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2564,
    "text": "custom CUDA op example returns random valuesWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNothing\nEnvironment info\nOperating System:\n$ uname -a\nLinux n-62-18-47 2.6.32-642.6.1.el6.x86_64 #1 SMP Wed Oct 5 08:48:31 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: CUDA: 8, cuDNN 5.1\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$ ls -l /appl/cuda/8.0/lib64/libcud*\n-rw-r--r-- 1 sebo root 560184 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 sebo root     16 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 sebo root     19 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 sebo root 394472 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 sebo root 737516 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart_static.a\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD) 5a5a25e\nThe output of bazel version\n\n$ bazel version\nBuild label: 0.3.2- (@non-git)\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 21 15:09:04 2016 (1477062544)\nBuild timestamp: 1477062544\nBuild timestamp as int: 1477062544\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nUsing the CUDA example from: https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op\n\ncompile example\n\nexport TF_INC=/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/include\n\nnvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \\\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\n\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \\\ncuda_op_kernel.cu.o -I $TF_INC -fPIC -L /appl/cuda/8.0/lib64 -L /appl/cudnn/v5.1-prod/lib64 -lcudart\n\n\nedit tensorflow.g3doc.how_tos.adding_an_op import cuda_op to import cuda_op in cuda_op_test.py.\n\nWhat other attempted solutions have you tried?\n\nI tried a non CUDA example, worked fine.\nI tried a diffrent cuda kernel (square operator) also failed.\nI added printf to the kernel launcher and made sure it was executed.\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\nCUDA_VISIBLE_DEVICES=3 python3 cuda_op_test.py\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:944] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:02:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:965] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\nnot equal where =  (array([0, 1, 2, 3, 4]),)\nnot equal lhs =  [ 280541332  143397048 2031878174 1533025280 1612453930]\nnot equal rhs =  [6 5 4 3 2]\nF.\n======================================================================\nFAIL: test (__main__.AddOneTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"cuda_op_test.py\", line 31, in test\n    self.assertAllEqual(result.eval(), [6, 5, 4, 3, 2])\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 499, in assertAllEqual\n    np.testing.assert_array_equal(a, b)\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 813, in assert_array_equal\n    verbose=verbose, header='Arrays are not equal')\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError:\nArrays are not equal\n\n(mismatch 100.0%)\n x: array([ 280541332,  143397048, 2031878174, 1533025280, 1612453930], dtype=int32)\n y: array([6, 5, 4, 3, 2])\n\n----------------------------------------------------------------------\nRan 2 tests in 0.213s\n\nFAILED (failures=1)\n\n\nIt looks like the output just contains random memory. Perhaps the GPU memory isn't copied back to the host memory.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "84",
      "number": "5122",
      "pretext": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNothing\nEnvironment info\nOperating System:\n$ uname -a\nLinux n-62-18-47 2.6.32-642.6.1.el6.x86_64 #1 SMP Wed Oct 5 08:48:31 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: CUDA: 8, cuDNN 5.1\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$ ls -l /appl/cuda/8.0/lib64/libcud*\n-rw-r--r-- 1 sebo root 560184 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 sebo root     16 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 sebo root     19 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 sebo root 394472 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 sebo root 737516 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart_static.a\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD) 5a5a25e\nThe output of bazel version\n\n$ bazel version\nBuild label: 0.3.2- (@non-git)\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 21 15:09:04 2016 (1477062544)\nBuild timestamp: 1477062544\nBuild timestamp as int: 1477062544\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nUsing the CUDA example from: https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op\n\ncompile example\n\nexport TF_INC=/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/include\n\nnvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \\\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\n\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \\\ncuda_op_kernel.cu.o -I $TF_INC -fPIC -L /appl/cuda/8.0/lib64 -L /appl/cudnn/v5.1-prod/lib64 -lcudart\n\n\nedit tensorflow.g3doc.how_tos.adding_an_op import cuda_op to import cuda_op in cuda_op_test.py.\n\nWhat other attempted solutions have you tried?\n\nI tried a non CUDA example, worked fine.\nI tried a diffrent cuda kernel (square operator) also failed.\nI added printf to the kernel launcher and made sure it was executed.\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\nCUDA_VISIBLE_DEVICES=3 python3 cuda_op_test.py\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:944] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:02:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:965] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\nnot equal where =  (array([0, 1, 2, 3, 4]),)\nnot equal lhs =  [ 280541332  143397048 2031878174 1533025280 1612453930]\nnot equal rhs =  [6 5 4 3 2]\nF.\n======================================================================\nFAIL: test (__main__.AddOneTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"cuda_op_test.py\", line 31, in test\n    self.assertAllEqual(result.eval(), [6, 5, 4, 3, 2])\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 499, in assertAllEqual\n    np.testing.assert_array_equal(a, b)\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 813, in assert_array_equal\n    verbose=verbose, header='Arrays are not equal')\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError:\nArrays are not equal\n\n(mismatch 100.0%)\n x: array([ 280541332,  143397048, 2031878174, 1533025280, 1612453930], dtype=int32)\n y: array([6, 5, 4, 3, 2])\n\n----------------------------------------------------------------------\nRan 2 tests in 0.213s\n\nFAILED (failures=1)\n\n\nIt looks like the output just contains random memory. Perhaps the GPU memory isn't copied back to the host memory.",
      "title": "custom CUDA op example returns random values"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2565,
    "text": "Assert randomly fails when training with multiple threadsSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04.2 LTS\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0-rc2\nPython version:  2.7.12\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: no\nGPU model and memory: no\nExact command to reproduce: for ((n=0;n<100;n++)); do python mnist_softmax_parallel_issue.py; done\n\nDescribe the problem\nThe following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads.\nSource code / logs\nmnist_softmax_device_issue.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\nimport threading\nimport numpy as np\nimport json\nimport os\nimport time\n\nFLAGS = None\n\nINTER_OP_PARALLELISM = 76\nINTRA_OP_PARALLELISM = 1\nBATCH_SIZE = 100\nITERATIONS = 1000\nTRAINING_THREADS = 46\n\nthreads = [None] * TRAINING_THREADS\n\ndef train_function(thread_idx, mnist, sess, train_step, x, y_, y):\n  iterations = int(ITERATIONS/TRAINING_THREADS)\n  for i in range(iterations):\n    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\ndef main(_):\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n  x = tf.placeholder(tf.float32, [None, 784])\n  W = tf.Variable(tf.zeros([784, 10]))\n  b = tf.Variable(tf.zeros([10]))\n  y = tf.matmul(x, W) + b\n\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  cross_entropy = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n  train_step = tf.train.GradientDescentOptimizer(0.5, use_locking=True).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads = INTRA_OP_PARALLELISM, inter_op_parallelism_threads= INTER_OP_PARALLELISM))\n  sess.run(tf.global_variables_initializer())\n\n  for i in range(TRAINING_THREADS):\n      threads[i] = threading.Thread(target=train_function, args=[i, mnist, sess, train_step, x, y_, y])\n\n  for thread in threads:\n      thread.start()\n  for thread in threads:\n      thread.join()\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str, default='mnist-data',\n                      help='Directory for storing input data')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\n\nTraceback\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::T ensorEvaluator(const XprType&, const Device&) [with Broadcast = const Eigen::IndexList<Eigen::type2index<1l>, int>; ArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::XprType = Eigen::TensorBroadcastingOp<const Eigen::IndexList<Eigen::type2index<1l>, int>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer> >]: Assertion input_dims[i] > $' failed.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "85",
      "number": "12078",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04.2 LTS\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0-rc2\nPython version:  2.7.12\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: no\nGPU model and memory: no\nExact command to reproduce: for ((n=0;n<100;n++)); do python mnist_softmax_parallel_issue.py; done\n\nDescribe the problem\nThe following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads.\nSource code / logs\nmnist_softmax_device_issue.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\nimport threading\nimport numpy as np\nimport json\nimport os\nimport time\n\nFLAGS = None\n\nINTER_OP_PARALLELISM = 76\nINTRA_OP_PARALLELISM = 1\nBATCH_SIZE = 100\nITERATIONS = 1000\nTRAINING_THREADS = 46\n\nthreads = [None] * TRAINING_THREADS\n\ndef train_function(thread_idx, mnist, sess, train_step, x, y_, y):\n  iterations = int(ITERATIONS/TRAINING_THREADS)\n  for i in range(iterations):\n    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\ndef main(_):\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n  x = tf.placeholder(tf.float32, [None, 784])\n  W = tf.Variable(tf.zeros([784, 10]))\n  b = tf.Variable(tf.zeros([10]))\n  y = tf.matmul(x, W) + b\n\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  cross_entropy = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n  train_step = tf.train.GradientDescentOptimizer(0.5, use_locking=True).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads = INTRA_OP_PARALLELISM, inter_op_parallelism_threads= INTER_OP_PARALLELISM))\n  sess.run(tf.global_variables_initializer())\n\n  for i in range(TRAINING_THREADS):\n      threads[i] = threading.Thread(target=train_function, args=[i, mnist, sess, train_step, x, y_, y])\n\n  for thread in threads:\n      thread.start()\n  for thread in threads:\n      thread.join()\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str, default='mnist-data',\n                      help='Directory for storing input data')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\n\nTraceback\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::T ensorEvaluator(const XprType&, const Device&) [with Broadcast = const Eigen::IndexList<Eigen::type2index<1l>, int>; ArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::XprType = Eigen::TensorBroadcastingOp<const Eigen::IndexList<Eigen::type2index<1l>, int>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer> >]: Assertion input_dims[i] > $' failed.",
      "title": "Assert randomly fails when training with multiple threads"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2566,
    "text": "the weights of tf.contrib.rnn.BasicLSTMCell can't be updatedEnvironment\nOS: Ubuntu 16.04\nTensorflow-gpu: 1.8\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n        num_units=self.config.num_lstm_units, state_is_tuple=True)\n    if self.mode == \"train\":\n      lstm_cell = tf.contrib.rnn.DropoutWrapper(\n          lstm_cell,\n          input_keep_prob=self.config.lstm_dropout_keep_prob,\n          output_keep_prob=self.config.lstm_dropout_keep_prob)\n\n    with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n      # Feed the image embeddings to set the initial LSTM state.\n      zero_state = lstm_cell.zero_state(\n          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\n\n      # Allow the LSTM variables to be reused.\n      #lstm_scope.reuse_variables()\n\n      ........\n\n      scores = tf.Variable(tf.random_normal(shape=[K, self.config.batch_size, C]), name=\"scores\")\n\n      M = tf.Variable(tf.random_normal(shape=[K+1, self.config.batch_size, 2, 3]), name=\"M\")\n      tf.assign(M[0], tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.]]))\n\n      lstm_input_size = 14\n      zk_size = 4096\n\n      hidden = zero_state\n\n      for k in range(0, K+1):\n          .......\n\n          lstm_outputs, hidden = lstm_cell(f_k, hidden) \n\nM and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module？",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "86",
      "number": "21200",
      "pretext": "Environment\nOS: Ubuntu 16.04\nTensorflow-gpu: 1.8\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n        num_units=self.config.num_lstm_units, state_is_tuple=True)\n    if self.mode == \"train\":\n      lstm_cell = tf.contrib.rnn.DropoutWrapper(\n          lstm_cell,\n          input_keep_prob=self.config.lstm_dropout_keep_prob,\n          output_keep_prob=self.config.lstm_dropout_keep_prob)\n\n    with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n      # Feed the image embeddings to set the initial LSTM state.\n      zero_state = lstm_cell.zero_state(\n          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\n\n      # Allow the LSTM variables to be reused.\n      #lstm_scope.reuse_variables()\n\n      ........\n\n      scores = tf.Variable(tf.random_normal(shape=[K, self.config.batch_size, C]), name=\"scores\")\n\n      M = tf.Variable(tf.random_normal(shape=[K+1, self.config.batch_size, 2, 3]), name=\"M\")\n      tf.assign(M[0], tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.]]))\n\n      lstm_input_size = 14\n      zk_size = 4096\n\n      hidden = zero_state\n\n      for k in range(0, K+1):\n          .......\n\n          lstm_outputs, hidden = lstm_cell(f_k, hidden) \n\nM and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module？",
      "title": "the weights of tf.contrib.rnn.BasicLSTMCell can't be updated"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2567,
    "text": "Failed to compile tensorflow offline with '--fetch=false' after all external dependencies fetched by bazelSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.2\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0 from master branch\nPython version: Python 2.7.12\nBazel version (if compiling from source): 0.5.4\nCUDA/cuDNN version: null\nGPU model and memory: null\nExact command to reproduce:\n\n\nFetch all external dependencies by docker image with internet access.\n\nbazel fetch //tensorflow/tools/pip_package:build_pip_package\n\n\nComplie tensorflow offline without internet access with --fetch=false.\n\nbazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\n\nDescribe the problem\nI fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub.\n#bazel fetch //tensorflow/tools/pip_package:build_pip_package\nINFO: All external dependencies fetched successfully.\n\nAfter that, I tried to compile tensorflow offline by using the image with bazel build --fetch=false since there is no internet access on my server, but it failed with the error \"no such package '@xxx'\", as follows:\n#bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:323:3: External repository 'six_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:173:3: External repository 'eigen_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\nERROR: /home/admin/src/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': BUILD file not found on package path and referenced by '//third_party/eigen3:eigen3'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 0.349s\n\nIn fact, be sure that the package eigen_archive was existing in the container, as below:\n#ls /root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/external/ | grep eigen_archive\neigen_archive\n\nAnd the ps stdout of bazel process was as follows:\n#ps aux | grep bazel\nroot        664  0.8  0.8 20850492 570628 ?     Ssl  16:39   0:29 bazel(src) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/A-server.jar --max_idle_secs=10800 --connect_timeout_secs=10 --install_base=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f --install_md5=2211725bdc2c34f807246fe9fb601a7f --output_base=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a --workspace_directory=/home/admin/src/tensorflow --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --client_debug=false --product_name=Bazel --option_sources=\n\nSo, please help for that and let me know if something wrong with my operation, thanks.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "87",
      "number": "13207",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.2\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0 from master branch\nPython version: Python 2.7.12\nBazel version (if compiling from source): 0.5.4\nCUDA/cuDNN version: null\nGPU model and memory: null\nExact command to reproduce:\n\n\nFetch all external dependencies by docker image with internet access.\n\nbazel fetch //tensorflow/tools/pip_package:build_pip_package\n\n\nComplie tensorflow offline without internet access with --fetch=false.\n\nbazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\n\nDescribe the problem\nI fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub.\n#bazel fetch //tensorflow/tools/pip_package:build_pip_package\nINFO: All external dependencies fetched successfully.\n\nAfter that, I tried to compile tensorflow offline by using the image with bazel build --fetch=false since there is no internet access on my server, but it failed with the error \"no such package '@xxx'\", as follows:\n#bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:323:3: External repository 'six_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:173:3: External repository 'eigen_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\nERROR: /home/admin/src/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': BUILD file not found on package path and referenced by '//third_party/eigen3:eigen3'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 0.349s\n\nIn fact, be sure that the package eigen_archive was existing in the container, as below:\n#ls /root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/external/ | grep eigen_archive\neigen_archive\n\nAnd the ps stdout of bazel process was as follows:\n#ps aux | grep bazel\nroot        664  0.8  0.8 20850492 570628 ?     Ssl  16:39   0:29 bazel(src) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/A-server.jar --max_idle_secs=10800 --connect_timeout_secs=10 --install_base=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f --install_md5=2211725bdc2c34f807246fe9fb601a7f --output_base=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a --workspace_directory=/home/admin/src/tensorflow --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --client_debug=false --product_name=Bazel --option_sources=\n\nSo, please help for that and let me know if something wrong with my operation, thanks.",
      "title": "Failed to compile tensorflow offline with '--fetch=false' after all external dependencies fetched by bazel"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2568,
    "text": "Error Building from source on Windows / my CPU doesn't have AVX System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below):1.11\nPython version:3.6.6\nBazel version (if compiling from source): 0.17.2\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\n\nDescribe the problem\nI can't build from source as it gives me the error\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command\nPerhaps because my CPU doesnt have AVX instructions set\non my CPU Supported Instructions sets\tMMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T\nLack of AVX don't allow me to pip install tf>1.5\nMy question is how to install from source without AVX instructions set?\nSource code / logs\n\nC:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\nStarting local Bazel server and connecting to it...\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan🚋 target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\nWARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).\nINFO: Found 1 target...\nINFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:\ncl : Command line warning D9025 : overriding '/w' with '/W3'\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command\ncd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin\nSET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe\nSET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages\nSET TF_DOWNLOAD_CLANG=0\nSET TF_NEED_CUDA=0\nSET TF_NEED_OPENCL_SYCL=0\nSET TF_NEED_ROCM=0\nC:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command\ncd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin\nSET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe\nSET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages\nSET TF_DOWNLOAD_CLANG=0\nSET TF_NEED_CUDA=0\nSET TF_NEED_OPENCL_SYCL=0\nSET TF_NEED_ROCM=0\nC:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\n/usr/bin/bash: line 1:  7128 Illegal instruction     bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 74.199s, Critical Path: 2.64s\nINFO: 42 processes: 42 local.\nFAILED: Build did NOT complete successfully",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "88",
      "number": "22954",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below):1.11\nPython version:3.6.6\nBazel version (if compiling from source): 0.17.2\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\n\nDescribe the problem\nI can't build from source as it gives me the error\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command\nPerhaps because my CPU doesnt have AVX instructions set\non my CPU Supported Instructions sets\tMMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T\nLack of AVX don't allow me to pip install tf>1.5\nMy question is how to install from source without AVX instructions set?\nSource code / logs\n\nC:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\nStarting local Bazel server and connecting to it...\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan🚋 target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\nWARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).\nINFO: Found 1 target...\nINFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:\ncl : Command line warning D9025 : overriding '/w' with '/W3'\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command\ncd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin\nSET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe\nSET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages\nSET TF_DOWNLOAD_CLANG=0\nSET TF_NEED_CUDA=0\nSET TF_NEED_OPENCL_SYCL=0\nSET TF_NEED_ROCM=0\nC:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command\ncd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin\nSET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe\nSET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages\nSET TF_DOWNLOAD_CLANG=0\nSET TF_NEED_CUDA=0\nSET TF_NEED_OPENCL_SYCL=0\nSET TF_NEED_ROCM=0\nC:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\n/usr/bin/bash: line 1:  7128 Illegal instruction     bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 74.199s, Critical Path: 2.64s\nINFO: 42 processes: 42 local.\nFAILED: Build did NOT complete successfully",
      "title": "Error Building from source on Windows / my CPU doesn't have AVX "
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2569,
    "text": "TocoConvertor: converting keras models to tflite doesn't support custom objectsHave I written custom code: No\nOS Platform and Distribution: Mac\nTensorFlow installed from: pip\nTensorFlow version: 1.10-rc1\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)\nMobile device: N/A\n\nThis is required to convert models that use custom layers or loss functions\nthis is the fix (will submit a PR soon):\nFrom 2c2179765cc9006762cf75c6a1b587e06895b869 Mon Sep 17 00:00:00 2001\nFrom: Ophir Yoktan <ophir@ziprecruiter.com>\nDate: Wed, 1 Aug 2018 10:16:10 +0300\nSubject: [PATCH] add support for custom_objects when loading keras model for\n conversion\n\n---\n tensorflow/contrib/lite/python/lite.py | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/tensorflow/contrib/lite/python/lite.py b/tensorflow/contrib/lite/python/lite.py\nindex 2f9b9d469a2..c12617df061 100644\n--- a/tensorflow/contrib/lite/python/lite.py\n+++ b/tensorflow/contrib/lite/python/lite.py\n@@ -274,7 +274,8 @@ def from_keras_model_file(cls,\n                             model_file,\n                             input_arrays=None,\n                             input_shapes=None,\n-                            output_arrays=None):\n+                            output_arrays=None,\n+                            custom_objects=None):\n     \"\"\"Creates a TocoConverter class from a tf.keras model file.\n \n     Args:\n@@ -293,7 +294,7 @@ def from_keras_model_file(cls,\n     \"\"\"\n     _keras.backend.clear_session()\n     _keras.backend.set_learning_phase(False)\n-    keras_model = _keras.models.load_model(model_file)\n+    keras_model = _keras.models.load_model(model_file, custom_objects=custom_objects)\n     sess = _keras.backend.get_session()\n \n     # Get input and output tensors.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "89",
      "number": "21429",
      "pretext": "Have I written custom code: No\nOS Platform and Distribution: Mac\nTensorFlow installed from: pip\nTensorFlow version: 1.10-rc1\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)\nMobile device: N/A\n\nThis is required to convert models that use custom layers or loss functions\nthis is the fix (will submit a PR soon):\nFrom 2c2179765cc9006762cf75c6a1b587e06895b869 Mon Sep 17 00:00:00 2001\nFrom: Ophir Yoktan <ophir@ziprecruiter.com>\nDate: Wed, 1 Aug 2018 10:16:10 +0300\nSubject: [PATCH] add support for custom_objects when loading keras model for\n conversion\n\n---\n tensorflow/contrib/lite/python/lite.py | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/tensorflow/contrib/lite/python/lite.py b/tensorflow/contrib/lite/python/lite.py\nindex 2f9b9d469a2..c12617df061 100644\n--- a/tensorflow/contrib/lite/python/lite.py\n+++ b/tensorflow/contrib/lite/python/lite.py\n@@ -274,7 +274,8 @@ def from_keras_model_file(cls,\n                             model_file,\n                             input_arrays=None,\n                             input_shapes=None,\n-                            output_arrays=None):\n+                            output_arrays=None,\n+                            custom_objects=None):\n     \"\"\"Creates a TocoConverter class from a tf.keras model file.\n \n     Args:\n@@ -293,7 +294,7 @@ def from_keras_model_file(cls,\n     \"\"\"\n     _keras.backend.clear_session()\n     _keras.backend.set_learning_phase(False)\n-    keras_model = _keras.models.load_model(model_file)\n+    keras_model = _keras.models.load_model(model_file, custom_objects=custom_objects)\n     sess = _keras.backend.get_session()\n \n     # Get input and output tensors.",
      "title": "TocoConvertor: converting keras models to tflite doesn't support custom objects"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2570,
    "text": "Undefined reference to CheckOpMessageBuilder::NewString() when linking libtensorflow_cc.soI am trying to use the TensorFlow Session C++ API (Python-free) to load a pre-trained model for inference. For build-time considerations, I am trying to deploy TensorFlow as a \"system\" package by linking against libtensorflow_cc.so and including headers into my Bazel-based workspace which has its own copies of protobuf and Eigen. I am almost there except that I have run into linker errors for missing implementations of tensorflow::internal::CheckOpMessageBuilder::NewString(). The symbols appear to be exported by libtensorflow_cc.so and it does seem to all be linking correctly, just not this symbol.\nAny help fixing this issue or suggestions for a better way of doing this would be greatly appreciated.\nThanks,\nHemal\nMy setup is the following:\nDocker image from ubuntu:16.04 using gcc5.\nBazel 0.3.1 (needed to upgrade from 0.3.0 because of other Tensorflow build issues)\nI matched the Eigen version but the protobuf used to build the Tensorflow wheel below is installed via apt-get and there is another copy (3.0.0) within my workspace's third_party directory.\nThe following is in my Dockerfile to build and \"deploy\" Tensorflow:\nRUN git clone https://github.com/tensorflow/tensorflow.git /tmp/tensorflow \\\n&& cd /tmp/tensorflow && git checkout r0.11 \\\n&& yes '' | ./configure \\\n&& bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \\\n&& /tmp/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \\\n&& pip2 install --quiet --upgrade /tmp/tensorflow_pkg/*.whl \\\n&& bazel build -c opt //tensorflow:libtensorflow_cc.so \\\n&& cp /tmp/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so /usr/lib/libtensorflow_cc.so \\\n&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow /usr/include/tensorflow \\\n&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/third_party /usr/include/third_party\n164:1: Linking of rule '//estimation/detection:playback_ground_truth' failed: clang-3.6 failed: error executing command\n(cd /code/.cache/bazel/_bazel_hemalshah/6fa7a91faa1abdfbb41bc875fa66f0f6/execroot/robotics && \nexec env - \n/usr/bin/clang-3.6 -o bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth -L/usr/lib/python2.7/config-x86_64-linux-gnu -L/usr/lib -Wl,-O1 -Wl,-Bsymbolic-functions -pthread -B/usr/bin/ -Wl,@bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "90",
      "number": "5179",
      "pretext": "I am trying to use the TensorFlow Session C++ API (Python-free) to load a pre-trained model for inference. For build-time considerations, I am trying to deploy TensorFlow as a \"system\" package by linking against libtensorflow_cc.so and including headers into my Bazel-based workspace which has its own copies of protobuf and Eigen. I am almost there except that I have run into linker errors for missing implementations of tensorflow::internal::CheckOpMessageBuilder::NewString(). The symbols appear to be exported by libtensorflow_cc.so and it does seem to all be linking correctly, just not this symbol.\nAny help fixing this issue or suggestions for a better way of doing this would be greatly appreciated.\nThanks,\nHemal\nMy setup is the following:\nDocker image from ubuntu:16.04 using gcc5.\nBazel 0.3.1 (needed to upgrade from 0.3.0 because of other Tensorflow build issues)\nI matched the Eigen version but the protobuf used to build the Tensorflow wheel below is installed via apt-get and there is another copy (3.0.0) within my workspace's third_party directory.\nThe following is in my Dockerfile to build and \"deploy\" Tensorflow:\nRUN git clone https://github.com/tensorflow/tensorflow.git /tmp/tensorflow \\\n&& cd /tmp/tensorflow && git checkout r0.11 \\\n&& yes '' | ./configure \\\n&& bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \\\n&& /tmp/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \\\n&& pip2 install --quiet --upgrade /tmp/tensorflow_pkg/*.whl \\\n&& bazel build -c opt //tensorflow:libtensorflow_cc.so \\\n&& cp /tmp/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so /usr/lib/libtensorflow_cc.so \\\n&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow /usr/include/tensorflow \\\n&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/third_party /usr/include/third_party\n164:1: Linking of rule '//estimation/detection:playback_ground_truth' failed: clang-3.6 failed: error executing command\n(cd /code/.cache/bazel/_bazel_hemalshah/6fa7a91faa1abdfbb41bc875fa66f0f6/execroot/robotics && \nexec env - \n/usr/bin/clang-3.6 -o bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth -L/usr/lib/python2.7/config-x86_64-linux-gnu -L/usr/lib -Wl,-O1 -Wl,-Bsymbolic-functions -pthread -B/usr/bin/ -Wl,@bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'",
      "title": "Undefined reference to CheckOpMessageBuilder::NewString() when linking libtensorflow_cc.so"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2571,
    "text": "protobuf message overflow on trying distributed I'm trying to build an RNN on multi-machines following the Distributed Tensorflow.\nwhen I use \"with sv.managed_session(server.target) as sess:\", it shows error:\nAttributeError: 'Supervisor' object has no attribute 'managed_session'\nSo I follow the code of \"Inception\":\nwith sv.prepare_or_wait_for_session(server.target, config = sess_config) as sess :\nThen it starts to run, but hangs immediately after reporting the following error:\n[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:569] Reading dangerously large protocol message.  If the message turns out to be larger than 67108864 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 67108864\nE tensorflow/core/framework/tensor.cc:105] Input size was 67108839 and expected 72000800\nWould you please help me on this?\nThanks a lot in advance!",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "91",
      "number": "2233",
      "pretext": "I'm trying to build an RNN on multi-machines following the Distributed Tensorflow.\nwhen I use \"with sv.managed_session(server.target) as sess:\", it shows error:\nAttributeError: 'Supervisor' object has no attribute 'managed_session'\nSo I follow the code of \"Inception\":\nwith sv.prepare_or_wait_for_session(server.target, config = sess_config) as sess :\nThen it starts to run, but hangs immediately after reporting the following error:\n[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:569] Reading dangerously large protocol message.  If the message turns out to be larger than 67108864 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 67108864\nE tensorflow/core/framework/tensor.cc:105] Input size was 67108839 and expected 72000800\nWould you please help me on this?\nThanks a lot in advance!",
      "title": "protobuf message overflow on trying distributed "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2572,
    "text": "Tensorboard feature request - Text summaryWould it be reasonable to add a basic text summary feature to Tensorboard? Personally I've run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where I could put some arbitrary text where I just wrote the key differences in my runs.\nFor example, on the Events page (or somewhere else) there would be a dropdown, similar to the summaries on the Events and Histograms page, with text I added (either hardcoded or as a script argument) that says what I did differently this run. Maybe I would print out the argument values for each run as well, that would be pretty useful, but basically something where I can say \"What did I do with this run again? Why was it different than the one before? Oh yeah I changed the batch size\" or \"Oh yeah I used my other dataset instead.\"\nObviously if it's arbitrary text you could maybe use it to write up a description of the network or whatever you want.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "92",
      "number": "2510",
      "pretext": "Would it be reasonable to add a basic text summary feature to Tensorboard? Personally I've run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where I could put some arbitrary text where I just wrote the key differences in my runs.\nFor example, on the Events page (or somewhere else) there would be a dropdown, similar to the summaries on the Events and Histograms page, with text I added (either hardcoded or as a script argument) that says what I did differently this run. Maybe I would print out the argument values for each run as well, that would be pretty useful, but basically something where I can say \"What did I do with this run again? Why was it different than the one before? Oh yeah I changed the batch size\" or \"Oh yeah I used my other dataset instead.\"\nObviously if it's arbitrary text you could maybe use it to write up a description of the network or whatever you want.",
      "title": "Tensorboard feature request - Text summary"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2573,
    "text": "grappler: memory optimizer fails with: No output shape in Conv2DBackpropInput op / Conv2DBackpropFilter op.System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below):  v1.5.0-0-g37aa430d84\nPython version: 3.5\nBazel version (if compiling from source): 0.10.0\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: 9.1 / 7\nGPU model and memory: TITAN Xp, 12196MiB\nExact command to reproduce: -\n\nDescribe the problem\nWhen I enable the memory optimizer in grappler, it fails with the following errors:\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\n\nMy network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.\nIs this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "93",
      "number": "16669",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below):  v1.5.0-0-g37aa430d84\nPython version: 3.5\nBazel version (if compiling from source): 0.10.0\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: 9.1 / 7\nGPU model and memory: TITAN Xp, 12196MiB\nExact command to reproduce: -\n\nDescribe the problem\nWhen I enable the memory optimizer in grappler, it fails with the following errors:\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\n\nMy network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.\nIs this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?",
      "title": "grappler: memory optimizer fails with: No output shape in Conv2DBackpropInput op / Conv2DBackpropFilter op."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2574,
    "text": "How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)I want to limit the total memory of each GPU in mnist,\nhttps://www.tensorflow.org/tutorials/using_gpu\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nsession = tf.Session(config=config, ...)\nand I added the above code to the mnis.py\nhttps://github.com/tensorflow/models/tree/master/official/mnist\nhere is the modified code in mnis.py :\ndef main(unused_argv):\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nmnist_classifier = tf.estimator.Estimator(\nmodel_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)\nbut I get the below error:\nTraceback (most recent call last):\nFile \"mnist.py\", line 231, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\nFile \"mnist.py\", line 206, in main\nmodel_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 142, in init\nconfig)\nValueError: config must be an instance of RunConfig, but provided gpu_options {\nper_process_gpu_memory_fraction: 0.4\n}\n.\nMy question is :\nHow to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "94",
      "number": "13458",
      "pretext": "I want to limit the total memory of each GPU in mnist,\nhttps://www.tensorflow.org/tutorials/using_gpu\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nsession = tf.Session(config=config, ...)\nand I added the above code to the mnis.py\nhttps://github.com/tensorflow/models/tree/master/official/mnist\nhere is the modified code in mnis.py :\ndef main(unused_argv):\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nmnist_classifier = tf.estimator.Estimator(\nmodel_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)\nbut I get the below error:\nTraceback (most recent call last):\nFile \"mnist.py\", line 231, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\nFile \"mnist.py\", line 206, in main\nmodel_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 142, in init\nconfig)\nValueError: config must be an instance of RunConfig, but provided gpu_options {\nper_process_gpu_memory_fraction: 0.4\n}\n.\nMy question is :\nHow to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)",
      "title": "How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2575,
    "text": "Error 404 when downloading Tensorflow on WindowsThe links provided on the website to the .whl used to install Tensorflow on Windows seem to be broken : https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows\nI'm getting an HTTP Error 404.\nI found the CPU Build elsewhere but I can't find the GPU build which I would like tu use.",
    "annotations": [{ "label": 141, "user": 3 }],
    "meta": {
      "": "95",
      "number": "6314",
      "pretext": "The links provided on the website to the .whl used to install Tensorflow on Windows seem to be broken : https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows\nI'm getting an HTTP Error 404.\nI found the CPU Build elsewhere but I can't find the GPU build which I would like tu use.",
      "title": "Error 404 when downloading Tensorflow on Windows"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2576,
    "text": "Feature request: Patch extracting given location implementation needed.Hello there,\nFeature\nI consider a lot but finally decide put this request here.\nI know there are some matrix operation and image operations like image_patch_extract and image.extract_glimpse().\nBut what I want is extract the patches given several locations.\nInput Tensor:\n         batch image: [batch_size, image_height, image_width, image_channel]\n         batch locations: [batch_size, num_patches, 2]\nOutput Tensor:\n        [batch_size, num_patches, patch_height, patch_width, image_channel]\nor \n        [batch_size * num_patches, patch_height, patch_width, image_channel]\nReference:\nGeorgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.\nhttps://github.com/trigeorgis/tensorflow\n\nBut when I use it in v0.10.0, it requires me to define a shape function.\nI really want to use it in the future, so I am glad it can be added as a new feature.\nOther solution I tried\nI have tried use extract_glimpse() instead.\noutput_list = [[] for _ in range(batch_size)]   # create n_patch list\nlocations = locations/image_size  # normalize the location\nfor j in range(n_patch):\n    patch_one = tf.image.glimpse(batch_image, tf.constant([20, 20]), locations[:, j, :], centered=False)\n    for i in range(batch_size):\n          output_list[i].append(patch_one[j])  # add tensor to each list\npatches = tf.pack(output_list)  # pack the list to tensor, size = [batch_size, n_patch, patch_size, patch_height, image_channel]",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "96",
      "number": "4474",
      "pretext": "Hello there,\nFeature\nI consider a lot but finally decide put this request here.\nI know there are some matrix operation and image operations like image_patch_extract and image.extract_glimpse().\nBut what I want is extract the patches given several locations.\nInput Tensor:\n         batch image: [batch_size, image_height, image_width, image_channel]\n         batch locations: [batch_size, num_patches, 2]\nOutput Tensor:\n        [batch_size, num_patches, patch_height, patch_width, image_channel]\nor \n        [batch_size * num_patches, patch_height, patch_width, image_channel]\nReference:\nGeorgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.\nhttps://github.com/trigeorgis/tensorflow\n\nBut when I use it in v0.10.0, it requires me to define a shape function.\nI really want to use it in the future, so I am glad it can be added as a new feature.\nOther solution I tried\nI have tried use extract_glimpse() instead.\noutput_list = [[] for _ in range(batch_size)]   # create n_patch list\nlocations = locations/image_size  # normalize the location\nfor j in range(n_patch):\n    patch_one = tf.image.glimpse(batch_image, tf.constant([20, 20]), locations[:, j, :], centered=False)\n    for i in range(batch_size):\n          output_list[i].append(patch_one[j])  # add tensor to each list\npatches = tf.pack(output_list)  # pack the list to tensor, size = [batch_size, n_patch, patch_size, patch_height, image_channel]",
      "title": "Feature request: Patch extracting given location implementation needed."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2577,
    "text": "LinearClassifier feature_columns overwritten in LinearClassifier.fittensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit\neffectively returns\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit\nEstimator.fit calls:\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model\nwhich has these lines:\n      features, targets = input_fn()\n      self._check_inputs(features, targets)\n      train_op, loss_op = self._get_train_ops(features, targets)\n\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs\n    if self._features_info is not None:\n      ...\n    else:\n      self._features_info = tensor_signature.create_signatures(features)\n\nSo we get to a point where features, as derived from the input_fn, is treated as our feature columns set.\nIn pseudo code:\ndef input_function()\n    return [foo, bar, baz], quux\n\nlc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be\nlc.fit(input_function) # run fit on out input function\nlc._feature_columns # repr _feature_columns on the instantiated classifier\n    [foo]\nlc.estimator._features_info # _features_info on the instantiated classifiers instantiated estimator\n    [foo, bar, baz]\n\nThe issue is:\nAlthough this line appears to indicate that we will be making an estimation based on the feature columns supplied:\nlc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be\nWhat happens is that the passed in feature_columns is unused and instead the return from the input_function supplied to fit are used.\nAm I correct in thinking that if the feature_columns arg is supplied that only those columns should be used by the classifiers estimator?\nThat when we instantiate the classifier we are setting the feature_columns we expect to be used?\nThe work around for this is simply to only return the columns you need from your input function however I found this misleading.\nPoint in the tutorial:\nEither the code or the tutorial need to be changed.\nhttps://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model\nError raised:\nWARNING:tensorflow:Setting feature info to\nas per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "97",
      "number": "4675",
      "pretext": "tensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit\neffectively returns\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit\nEstimator.fit calls:\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model\nwhich has these lines:\n      features, targets = input_fn()\n      self._check_inputs(features, targets)\n      train_op, loss_op = self._get_train_ops(features, targets)\n\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs\n    if self._features_info is not None:\n      ...\n    else:\n      self._features_info = tensor_signature.create_signatures(features)\n\nSo we get to a point where features, as derived from the input_fn, is treated as our feature columns set.\nIn pseudo code:\ndef input_function()\n    return [foo, bar, baz], quux\n\nlc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be\nlc.fit(input_function) # run fit on out input function\nlc._feature_columns # repr _feature_columns on the instantiated classifier\n    [foo]\nlc.estimator._features_info # _features_info on the instantiated classifiers instantiated estimator\n    [foo, bar, baz]\n\nThe issue is:\nAlthough this line appears to indicate that we will be making an estimation based on the feature columns supplied:\nlc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be\nWhat happens is that the passed in feature_columns is unused and instead the return from the input_function supplied to fit are used.\nAm I correct in thinking that if the feature_columns arg is supplied that only those columns should be used by the classifiers estimator?\nThat when we instantiate the classifier we are setting the feature_columns we expect to be used?\nThe work around for this is simply to only return the columns you need from your input function however I found this misleading.\nPoint in the tutorial:\nEither the code or the tutorial need to be changed.\nhttps://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model\nError raised:\nWARNING:tensorflow:Setting feature info to\nas per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613",
      "title": "LinearClassifier feature_columns overwritten in LinearClassifier.fit"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2578,
    "text": "LayoutOptimizer optimizes to unsupported data_format for max_pool on CPUSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):  v1.11.0-rc2-4-gc19e29306c 1.11.0\nPython version: 3.6.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0.176/7.1.3\nGPU model and memory: NVIDIA Quadro M2200\n\nDescribe the current behavior\nTensorFlow automatically replaces my MaxPoolingOp by one using another data format which is not supported, subsequently.\nThis throws the InvalidArgumentError as seen below.\nThe problem arose when upgrading from TensorFlow 1.8 to 1.11 and from the error it seems to be caused by a TransposeNHWCToNCHW-LayoutOptimizer. When isolating the issue to reproduce it, it seems that max_pool, dataset and squeeze are involved to raise the error.  The only related (closed) issue I could find: #19497 \"NHWC convolution sometimes incorrectly considered NCHW\"\n InvalidArgumentError (see above for traceback): Default MaxPoolingOp only supports NHWC on device type CPU [[{{node label_image_dilated}} = MaxPool[T=DT_INT32, data_format=\"NCHW\", ksize=[1, 1, 3, 3], padding=\"SAME\", strides=[1, 1, 1, 1]](label_image_dilated-0-TransposeNHWCToNCHW-LayoutOptimizer)]] [[{{node OneShotIterator_2}} = OneShotIterator[container=\"\", dataset_factory=_make_dataset_UaZD9hBkHvg[], output_shapes=[[?,?]], output_types=[DT_INT32], shared_name=\"\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nDescribe the expected behavior\nTo not throw the error as was the case for TensorFlow 1.8.\nCode to reproduce the issue\nimport tensorflow as tf\nimport numpy as np\n\ndataset_np = {'height_or_width': 512, \n              'indices': np.random.randint(low=0, high=512, size=(1000,2),dtype=np.int64),\n              'values' : np.random.randint(low=0, high=1000, size=(1000,),dtype=np.int32)}\ndataset = tf.data.Dataset.from_tensors(dataset_np)\n\ndef densify(element):\n    label_image_sparse = tf.SparseTensor(indices = element['indices'], \n                                         values = element['values'], \n                                         dense_shape = tf.cast(tf.stack([element['height_or_width'],\n                                                                         element['height_or_width']]),tf.int64))\n    label_image = tf.sparse_tensor_to_dense(label_image_sparse, \n                                            default_value=-1, \n                                            validate_indices=False, \n                                            name='label_image')\n    label_image_dilated = tf.squeeze(tf.nn.max_pool([tf.expand_dims(label_image, axis=-1)], \n                                                     data_format=\"NHWC\", \n                                                     ksize= [1,3,3,1], \n                                                     strides = [1,1,1,1], \n                                                     padding='SAME', \n                                                     name='label_image_dilated'),[0,-1])\n    return {'label_image_dilated':label_image_dilated}\n\ndataset = dataset.map(densify)\nelement = dataset.make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n    result = sess.run(element)\n    print(result)\nOther info / logs",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "98",
      "number": "23344",
      "pretext": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):  v1.11.0-rc2-4-gc19e29306c 1.11.0\nPython version: 3.6.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0.176/7.1.3\nGPU model and memory: NVIDIA Quadro M2200\n\nDescribe the current behavior\nTensorFlow automatically replaces my MaxPoolingOp by one using another data format which is not supported, subsequently.\nThis throws the InvalidArgumentError as seen below.\nThe problem arose when upgrading from TensorFlow 1.8 to 1.11 and from the error it seems to be caused by a TransposeNHWCToNCHW-LayoutOptimizer. When isolating the issue to reproduce it, it seems that max_pool, dataset and squeeze are involved to raise the error.  The only related (closed) issue I could find: #19497 \"NHWC convolution sometimes incorrectly considered NCHW\"\n InvalidArgumentError (see above for traceback): Default MaxPoolingOp only supports NHWC on device type CPU [[{{node label_image_dilated}} = MaxPool[T=DT_INT32, data_format=\"NCHW\", ksize=[1, 1, 3, 3], padding=\"SAME\", strides=[1, 1, 1, 1]](label_image_dilated-0-TransposeNHWCToNCHW-LayoutOptimizer)]] [[{{node OneShotIterator_2}} = OneShotIterator[container=\"\", dataset_factory=_make_dataset_UaZD9hBkHvg[], output_shapes=[[?,?]], output_types=[DT_INT32], shared_name=\"\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nDescribe the expected behavior\nTo not throw the error as was the case for TensorFlow 1.8.\nCode to reproduce the issue\nimport tensorflow as tf\nimport numpy as np\n\ndataset_np = {'height_or_width': 512, \n              'indices': np.random.randint(low=0, high=512, size=(1000,2),dtype=np.int64),\n              'values' : np.random.randint(low=0, high=1000, size=(1000,),dtype=np.int32)}\ndataset = tf.data.Dataset.from_tensors(dataset_np)\n\ndef densify(element):\n    label_image_sparse = tf.SparseTensor(indices = element['indices'], \n                                         values = element['values'], \n                                         dense_shape = tf.cast(tf.stack([element['height_or_width'],\n                                                                         element['height_or_width']]),tf.int64))\n    label_image = tf.sparse_tensor_to_dense(label_image_sparse, \n                                            default_value=-1, \n                                            validate_indices=False, \n                                            name='label_image')\n    label_image_dilated = tf.squeeze(tf.nn.max_pool([tf.expand_dims(label_image, axis=-1)], \n                                                     data_format=\"NHWC\", \n                                                     ksize= [1,3,3,1], \n                                                     strides = [1,1,1,1], \n                                                     padding='SAME', \n                                                     name='label_image_dilated'),[0,-1])\n    return {'label_image_dilated':label_image_dilated}\n\ndataset = dataset.map(densify)\nelement = dataset.make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n    result = sess.run(element)\n    print(result)\nOther info / logs",
      "title": "LayoutOptimizer optimizes to unsupported data_format for max_pool on CPU"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2579,
    "text": "No GPU OpKernel for tf.exp() operation for Complex64I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea'  on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:\nIt seems that there is no OpKernel on device='GPU'  for the tf.exp() operation applied to complex numbers in eager mode.  This can be reproduced with the below code:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution()\n\nwith tf.device('/gpu:0'):\n  g = tf.spectral.rfft(tf.ones(64))\n  \n  tf.exp(g)\n\nwhich results in\n\t (OpKernel was found, but attributes didn't match)\n\t.  Registered:  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_HALF]\n  device='GPU'; T in [DT_FLOAT]\n [Op:Exp]\n\na more practical example that would lead to this same error:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n\nframe_length=256\nframe_step=64\nn_mels = 64\nsr=16000\nfilename = 'path/to/a.wav'\n\nsome_signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(filename), \n                                     file_format='wav', \n                                     samples_per_second=16000, \n                                     channel_count=1)\n\nwith tf.device('/gpu:0'):\n  stft = tf.contrib.signal.stft(tf.transpose(some_signal), frame_length=frame_length, \n                                  frame_step=frame_step, fft_length=frame_length)\n\n  linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n      n_mels, 1+frame_length//2, sr)\n\n  magnitude_spectrograms = tf.abs(stft)\n  log_mel_spec = tf.log(1e-6+ tf.tensordot(magnitude_spectrograms,\n                                           linear_to_mel_weight_matrix, \n                                           axes = [[2], [0]]))\n\n  mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spec)\n\nKeeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "99",
      "number": "15103",
      "pretext": "I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea'  on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:\nIt seems that there is no OpKernel on device='GPU'  for the tf.exp() operation applied to complex numbers in eager mode.  This can be reproduced with the below code:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution()\n\nwith tf.device('/gpu:0'):\n  g = tf.spectral.rfft(tf.ones(64))\n  \n  tf.exp(g)\n\nwhich results in\n\t (OpKernel was found, but attributes didn't match)\n\t.  Registered:  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_HALF]\n  device='GPU'; T in [DT_FLOAT]\n [Op:Exp]\n\na more practical example that would lead to this same error:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n\nframe_length=256\nframe_step=64\nn_mels = 64\nsr=16000\nfilename = 'path/to/a.wav'\n\nsome_signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(filename), \n                                     file_format='wav', \n                                     samples_per_second=16000, \n                                     channel_count=1)\n\nwith tf.device('/gpu:0'):\n  stft = tf.contrib.signal.stft(tf.transpose(some_signal), frame_length=frame_length, \n                                  frame_step=frame_step, fft_length=frame_length)\n\n  linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n      n_mels, 1+frame_length//2, sr)\n\n  magnitude_spectrograms = tf.abs(stft)\n  log_mel_spec = tf.log(1e-6+ tf.tensordot(magnitude_spectrograms,\n                                           linear_to_mel_weight_matrix, \n                                           axes = [[2], [0]]))\n\n  mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spec)\n\nKeeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks",
      "title": "No GPU OpKernel for tf.exp() operation for Complex64"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2580,
    "text": "Indirect dependency on golang test code introduced unintended CLI flags in Kubernetes binariesFrom @eparis's comment in the Cinder PR: #13367 (comment)\nPR #13367, Cinder Volume Plugin introduced a dependency on\n\ngithub.com/rackspace/gophercloud/openstack/blockstorage/v1/volumes\ngithub.com/rackspace/gophercloud/openstack/compute/v2/extensions/volumeattach\n\nwhich in turn has a dependency on github.com/golang/go/src/testing\nwhich declares a bunch of command line args (see https://github.com/golang/go/blob/master/src/testing/testing.go#L187).\nWhich means that kubernetes binaries, like kubelet now have flags for golang test code like --test.memprofilerate, --chatty, etc.",
    "annotations": [{ "label": 145, "user": 3 }],
    "meta": {
      "": "0",
      "number": "13565",
      "pretext": "From @eparis's comment in the Cinder PR: #13367 (comment)\nPR #13367, Cinder Volume Plugin introduced a dependency on\n\ngithub.com/rackspace/gophercloud/openstack/blockstorage/v1/volumes\ngithub.com/rackspace/gophercloud/openstack/compute/v2/extensions/volumeattach\n\nwhich in turn has a dependency on github.com/golang/go/src/testing\nwhich declares a bunch of command line args (see https://github.com/golang/go/blob/master/src/testing/testing.go#L187).\nWhich means that kubernetes binaries, like kubelet now have flags for golang test code like --test.memprofilerate, --chatty, etc.",
      "title": "Indirect dependency on golang test code introduced unintended CLI flags in Kubernetes binaries"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2581,
    "text": "Kubelet on a node dead right before pod is created and assigned to that node cause pod disappear on apiserverThe steps to reproduce the issue, on a working gce cluster\n\nChoose a node, edit /usr/sbin/kubelet-checker.sh, change sleep time to a large number (600s)\nCreate a pod and make sure it is assigned to the node you chose in step 1\nkill kubelet process (kubelet process will be restarted automatically after > 5mins )\nCheck pod status, at the beginning, it is ContainerCreating/(or pending), after a few mins, it disappears (kubectl get pods no long shows it)",
    "annotations": [{ "label": 145, "user": 3 }],
    "meta": {
      "": "1",
      "number": "28318",
      "pretext": "The steps to reproduce the issue, on a working gce cluster\n\nChoose a node, edit /usr/sbin/kubelet-checker.sh, change sleep time to a large number (600s)\nCreate a pod and make sure it is assigned to the node you chose in step 1\nkill kubelet process (kubelet process will be restarted automatically after > 5mins )\nCheck pod status, at the beginning, it is ContainerCreating/(or pending), after a few mins, it disappears (kubectl get pods no long shows it)",
      "title": "Kubelet on a node dead right before pod is created and assigned to that node cause pod disappear on apiserver"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2582,
    "text": "Configure master node same as slave nodeThis is pre-requirement for running all master components in a pod. #3853 was filed to run etcd as a pod. To make sure we really run etcd as a pod using Kubernete network model without specifying HostPort from spec, we need config the master node:\n\ncreate networking bridge called cbr0\nconfig docker run with \"--bridge cbr0 --iptables=false\"\nadd default route for master node (on GCE). Need to figure out other cloud providers.",
    "annotations": [{ "label": 145, "user": 3 }],
    "meta": {
      "": "2",
      "number": "4128",
      "pretext": "This is pre-requirement for running all master components in a pod. #3853 was filed to run etcd as a pod. To make sure we really run etcd as a pod using Kubernete network model without specifying HostPort from spec, we need config the master node:\n\ncreate networking bridge called cbr0\nconfig docker run with \"--bridge cbr0 --iptables=false\"\nadd default route for master node (on GCE). Need to figure out other cloud providers.",
      "title": "Configure master node same as slave node"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2583,
    "text": "Alert someone when kubernetes-test-history stops running, and add report dateThe software that generates http://storage.googleapis.com/kubernetes-test-history/static/tests-e2e.html has crashed/stopped running a few times, and nobody knew.  Please add an alert.\nAlso, the above report does not indicate what date the report pertains to, which makes it very confusing, and difficult to tell whether the report is up to date (e.g. I looked today, and it was a week old, although this was entirely non-obvious).",
    "annotations": [{ "label": 141, "user": 3 }],
    "meta": {
      "": "3",
      "number": "23062",
      "pretext": "The software that generates http://storage.googleapis.com/kubernetes-test-history/static/tests-e2e.html has crashed/stopped running a few times, and nobody knew.  Please add an alert.\nAlso, the above report does not indicate what date the report pertains to, which makes it very confusing, and difficult to tell whether the report is up to date (e.g. I looked today, and it was a week old, although this was entirely non-obvious).",
      "title": "Alert someone when kubernetes-test-history stops running, and add report date"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2584,
    "text": "Add generic documentationHi,\nI'm happy to contribute the missing documentation but I've spend now quite some time on trying to get kubernetes working on a plain docker host and had no success so far.\nI've read the design doc, shell scripts and salt states and this is how I deployed it:\n\nkubelet -config /etc/kubelet.conf -address=0.0.0.0\netcd -peer-addr 10.0.1.115:7001 -addr 10.0.1.115:4001 -discovery https://discovery.etcd.io/\napiserver -address 0.0.0.0 -etcd_servers=http://10.0.1.115:4001 -machines=10.0.1.115\ncontroller-manager -master localhost:8080 -etcd_servers=http://10.0.1.115:4001\n\nKubelet log shows (I've created an empty kubelet.conf to stop it throwing errors, not sure if necessary though):\n2014/06/17 13:01:12 Desired:[]api.ContainerManifest{}\n2014/06/17 13:01:12 Existing:\n[]string{} Desired: map[string]bool{}\n\napiserver prints nothing at all.\ncontroller-manger logs:\netcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=false&sorted=false]\netcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false  | method  GET]\netcd 2014/06/17 13:13:17 DEBUG: watch [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]\netcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]\netcd 2014/06/17 13:13:17 DEBUG: [recv.response.from http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]\netcd 2014/06/17 13:13:17 DEBUG: [recv.success. http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]\n2014/06/17 13:13:17 Synchronization error &etcd.EtcdError{ErrorCode:100, Message:\"Key not found\", Cause:\"/registry\", Index:0x2}\n\nNow running cloudcfg fails:\n./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 run dockerfile/nginx 2 myNginx\n2014/06/17 15:16:03 Error: &errors.errorString{s:\"request [POST http://localhost:10250/api/v1beta1/replicationControllers] failed (404) 404 Not Found\"}\n\nIf I try to create a pod I get:\n./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 -c api/examples/pod.json create /pods\n2014/06/17 15:17:14 Failed to print: &json.SyntaxError{msg:\"invalid character 'N' looking for beginning of value\", Offset:1}",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "4",
      "number": "132",
      "pretext": "Hi,\nI'm happy to contribute the missing documentation but I've spend now quite some time on trying to get kubernetes working on a plain docker host and had no success so far.\nI've read the design doc, shell scripts and salt states and this is how I deployed it:\n\nkubelet -config /etc/kubelet.conf -address=0.0.0.0\netcd -peer-addr 10.0.1.115:7001 -addr 10.0.1.115:4001 -discovery https://discovery.etcd.io/\napiserver -address 0.0.0.0 -etcd_servers=http://10.0.1.115:4001 -machines=10.0.1.115\ncontroller-manager -master localhost:8080 -etcd_servers=http://10.0.1.115:4001\n\nKubelet log shows (I've created an empty kubelet.conf to stop it throwing errors, not sure if necessary though):\n2014/06/17 13:01:12 Desired:[]api.ContainerManifest{}\n2014/06/17 13:01:12 Existing:\n[]string{} Desired: map[string]bool{}\n\napiserver prints nothing at all.\ncontroller-manger logs:\netcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=false&sorted=false]\netcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false  | method  GET]\netcd 2014/06/17 13:13:17 DEBUG: watch [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]\netcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]\netcd 2014/06/17 13:13:17 DEBUG: [recv.response.from http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]\netcd 2014/06/17 13:13:17 DEBUG: [recv.success. http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]\n2014/06/17 13:13:17 Synchronization error &etcd.EtcdError{ErrorCode:100, Message:\"Key not found\", Cause:\"/registry\", Index:0x2}\n\nNow running cloudcfg fails:\n./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 run dockerfile/nginx 2 myNginx\n2014/06/17 15:16:03 Error: &errors.errorString{s:\"request [POST http://localhost:10250/api/v1beta1/replicationControllers] failed (404) 404 Not Found\"}\n\nIf I try to create a pod I get:\n./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 -c api/examples/pod.json create /pods\n2014/06/17 15:17:14 Failed to print: &json.SyntaxError{msg:\"invalid character 'N' looking for beginning of value\", Offset:1}",
      "title": "Add generic documentation"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2585,
    "text": "`rolling-update` requires a unique name when specifying a manifestPer a conversation in slack with @jimmidyson, rolling-update should be able to do the same rename dance when a manifest is specified as when --image is used.\nRight now, it errors out with a message like: error: pods/catalog.yml cannot have the same name as the existing ReplicationController catalog",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "5",
      "number": "14281",
      "pretext": "Per a conversation in slack with @jimmidyson, rolling-update should be able to do the same rename dance when a manifest is specified as when --image is used.\nRight now, it errors out with a message like: error: pods/catalog.yml cannot have the same name as the existing ReplicationController catalog",
      "title": "`rolling-update` requires a unique name when specifying a manifest"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2586,
    "text": "What do we do with 1.0 tests that fail when run against 1.1?This is a problem we're going to run into as we're running 1.0 e2e tests against 1.1 clusters, (which we do because we need to make sure that 1.1 clusters still operate the way 1.0 clusters did).  If a test fails, but the failure is due to a bad test rather than a problem in 1.1, what do we do?\nFor example, Services/Nodeport e2es are failing on upgrade due to change in error message.  They fail when running 1.0 e2es against a HEAD master (in jobs kubernetes-upgrade-gke-step3-e2e-old and kubernetes-upgrade-gke-step5-e2e-old):\n\nKubernetes e2e suite.Services should check NodePort out-of-range\n\nExpected\n    <string>: Service \"nodeport-range-test\" is invalid: spec.ports[0].nodePort: invalid value '53127', Details: provided port is not in the valid range\nto equal\n    <string>: Service \"nodeport-range-test\" is invalid: spec.ports[0].nodePort: invalid value '53127': provided port is not in the valid range\n\n\nKubernetes e2e suite.Services should prevent NodePort collisions\n\nExpected\n    <string>: Service \"nodeport-collision2\" is invalid: spec.ports[0].nodePort: invalid value '31641', Details: provided port is already allocated\nto equal\n    <string>: Service \"nodeport-collision2\" is invalid: spec.ports[0].nodePort: invalid value '31641': provided port is already allocated\n\n@ixdy @quinton-hoole Any ideas about how to fix this?  This definitely isn't a regression, but it's probably not a good idea to just disable these tests.  My best thought is to cherry-pick the 1.1 test changes into 1.0, and somehow pull these tests HEAD of the 1.0 branch.  That's a lot of mucking around though, and I'm not sure it's worth it.\nFor now, I think we should punt on these specific tests until we have a better idea of how widespread this kind of version-skew problem is going to be.",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "6",
      "number": "15116",
      "pretext": "This is a problem we're going to run into as we're running 1.0 e2e tests against 1.1 clusters, (which we do because we need to make sure that 1.1 clusters still operate the way 1.0 clusters did).  If a test fails, but the failure is due to a bad test rather than a problem in 1.1, what do we do?\nFor example, Services/Nodeport e2es are failing on upgrade due to change in error message.  They fail when running 1.0 e2es against a HEAD master (in jobs kubernetes-upgrade-gke-step3-e2e-old and kubernetes-upgrade-gke-step5-e2e-old):\n\nKubernetes e2e suite.Services should check NodePort out-of-range\n\nExpected\n    <string>: Service \"nodeport-range-test\" is invalid: spec.ports[0].nodePort: invalid value '53127', Details: provided port is not in the valid range\nto equal\n    <string>: Service \"nodeport-range-test\" is invalid: spec.ports[0].nodePort: invalid value '53127': provided port is not in the valid range\n\n\nKubernetes e2e suite.Services should prevent NodePort collisions\n\nExpected\n    <string>: Service \"nodeport-collision2\" is invalid: spec.ports[0].nodePort: invalid value '31641', Details: provided port is already allocated\nto equal\n    <string>: Service \"nodeport-collision2\" is invalid: spec.ports[0].nodePort: invalid value '31641': provided port is already allocated\n\n@ixdy @quinton-hoole Any ideas about how to fix this?  This definitely isn't a regression, but it's probably not a good idea to just disable these tests.  My best thought is to cherry-pick the 1.1 test changes into 1.0, and somehow pull these tests HEAD of the 1.0 branch.  That's a lot of mucking around though, and I'm not sure it's worth it.\nFor now, I think we should punt on these specific tests until we have a better idea of how widespread this kind of version-skew problem is going to be.",
      "title": "What do we do with 1.0 tests that fail when run against 1.1?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2587,
    "text": "GKE cluster running nginx seems to break nodePortSetting up a cluster on GKE via the gcloud cli tool, and then following this tutorial https://cloud.google.com/container-engine/docs/tutorials/http-balancer, I came across two issues - the first is here #13073.\nIn the tutorial we set up the nginx service with a nodePort like so\nkubectl expose rc my-nginx --target-port=80 --type=NodePort\n\nOf all the nodes in the cluster (I tried 3 and 4 node clusters), the only one who responded to\ncurl (node external ip):(node_port)\n\nwas the node where the nginx container was actually hosted. The other nodes all showed TCP open at the nodePort in nmap but dropped the connection right away and nothing ever got through to nginx.\nNot sure if this is expected behaviour ? From the tutorial it seems like all nodes should return nginx responses\nNext, create a Container Engine service which exposes this nginx Pod on each node in your cluster:",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "7",
      "number": "14237",
      "pretext": "Setting up a cluster on GKE via the gcloud cli tool, and then following this tutorial https://cloud.google.com/container-engine/docs/tutorials/http-balancer, I came across two issues - the first is here #13073.\nIn the tutorial we set up the nginx service with a nodePort like so\nkubectl expose rc my-nginx --target-port=80 --type=NodePort\n\nOf all the nodes in the cluster (I tried 3 and 4 node clusters), the only one who responded to\ncurl (node external ip):(node_port)\n\nwas the node where the nginx container was actually hosted. The other nodes all showed TCP open at the nodePort in nmap but dropped the connection right away and nothing ever got through to nginx.\nNot sure if this is expected behaviour ? From the tutorial it seems like all nodes should return nginx responses\nNext, create a Container Engine service which exposes this nginx Pod on each node in your cluster:",
      "title": "GKE cluster running nginx seems to break nodePort"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2588,
    "text": "Docker k8s setup instructions to have DNS optionPlease add a way to add DNS support to the setup instructions for getting a kubernetes cluster using docker.\nThe SkyDNS server already has a yaml file  and would exist as a pod and service just as the current apiserver pods exist.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "8",
      "number": "10774",
      "pretext": "Please add a way to add DNS support to the setup instructions for getting a kubernetes cluster using docker.\nThe SkyDNS server already has a yaml file  and would exist as a pod and service just as the current apiserver pods exist.",
      "title": "Docker k8s setup instructions to have DNS option"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2589,
    "text": "Check local copy of the golang docker image is always failed.The golang docker image is existed\n$ docker images docker.io/golang\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\ndocker.io/golang    1.4                 124e2127157f        8 days ago          517.2 MB\n\nBut it always say \"You don't have a local copy of the golang docker image\".\n$ sudo make release\nbuild/release.sh\n+++ [0723 10:58:17] Verifying Prerequisites....\nYou don't have a local copy of the golang docker image. This image is 450MB.\nDownload it now? [y/n] n\n\n#11284 fixes this.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "9",
      "number": "11732",
      "pretext": "The golang docker image is existed\n$ docker images docker.io/golang\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\ndocker.io/golang    1.4                 124e2127157f        8 days ago          517.2 MB\n\nBut it always say \"You don't have a local copy of the golang docker image\".\n$ sudo make release\nbuild/release.sh\n+++ [0723 10:58:17] Verifying Prerequisites....\nYou don't have a local copy of the golang docker image. This image is 450MB.\nDownload it now? [y/n] n\n\n#11284 fixes this.",
      "title": "Check local copy of the golang docker image is always failed."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2590,
    "text": "[mesos/docker] Flakey Smoke Test - TLS handshake timeouterror: couldn't read version from server: Get https://172.17.0.79:6443/api: net/http: TLS handshake timeout\n\nhttps://teamcity.mesosphere.io/viewLog.html?buildId=56436&buildTypeId=Oss_KubernetesMesos_5SmokeTestsDockerMesos&tab=buildLog&guest=1#_focus=614\nFirst time I've seen this. Documenting for searchability.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "10",
      "number": "15412",
      "pretext": "error: couldn't read version from server: Get https://172.17.0.79:6443/api: net/http: TLS handshake timeout\n\nhttps://teamcity.mesosphere.io/viewLog.html?buildId=56436&buildTypeId=Oss_KubernetesMesos_5SmokeTestsDockerMesos&tab=buildLog&guest=1#_focus=614\nFirst time I've seen this. Documenting for searchability.",
      "title": "[mesos/docker] Flakey Smoke Test - TLS handshake timeout"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2591,
    "text": "Services with the same name in different namespaces cause kube-proxy to bug outShould it be possible to have services with the same name in different namespaces? Nothing prevents it and I suspect that it should be possible but there are assumptions in code that cause this not to function correctly.\nHere are some failing tests: https://github.com/mikesimons/kubernetes/compare/proxy_namespace_clash\nAnd here is what lead me to investigate:\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703463   19433 config.go:233] Setting services {Services:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d1e4b42e-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:51 +0000 UTC Labels:map[role:service name:read] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.236.240 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}] Op:0}\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703801   19433 config.go:138] Setting endpoints {Endpoints:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:8080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:7080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink:/api/v1beta1/endpoints/syslog?namespace=default UID:4c837d1f-b929-11e4-bc1b-080027205d4b ResourceVersion:12 CreationTimestamp:2015-02-20 17:52:55 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[172.17.0.3:514]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d24c9595-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:52 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:fc253202-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:41:02 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]}] Op:0}\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703923   19433 proxier.go:459] Received update notice: [{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}]\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.704086   19433 proxier.go:470] Something changed for service \"redis\": stopping it\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.726654   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.729142   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.733688   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.736540   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739764   19433 proxier.go:575] Closed iptables portals for service \"redis\"\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739798   19433 proxier.go:480] Adding new service \"redis\" at 119.9.235.183:6379/TCP (local :0)\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739880   19433 proxier.go:443] Proxying for service \"redis\" on TCP port 37841\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739890   19433 proxier.go:492] info: &{portalIP:[0 0 0 0 0 0 0 0 0 0 255 255 119 9 235 183] portalPort:6379 protocol:TCP proxyPort:37841 socket:0xc20810c4a0 timeout:60000000000 publicIP:[] sessionAffinityType:None stickyMaxAgeMinutes:180}\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742221   19433 proxier.go:103] Accept failed: accept tcp [::]:55814: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742248   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742257   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742261   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742265   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742269   19433 proxier.go:103] Accept failed: use of closed network connection\n\nThe Accept failed: use of closed network connection is spat out indefinitely at a very high rate. All services cease to function (but timeout rather than refuse connection).\nThe thing of note in the logs is that in the \"Setting services\" message both redis instances are present where-as in the \"Recieved update\" message, only the project2 instance is present.\nI believe that this is due to the fact that several places in pkg/proxy/config/config.go create maps keyed on service name only. Here is an example https://github.com/mikesimons/kubernetes/blob/master/pkg/proxy/config/config.go#L214\nAssuming I am correct, what would be the best way to fix this? It doesn't look like the keys of the maps are used anywhere so does this need to be a map at all?",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "11",
      "number": "4695",
      "pretext": "Should it be possible to have services with the same name in different namespaces? Nothing prevents it and I suspect that it should be possible but there are assumptions in code that cause this not to function correctly.\nHere are some failing tests: https://github.com/mikesimons/kubernetes/compare/proxy_namespace_clash\nAnd here is what lead me to investigate:\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703463   19433 config.go:233] Setting services {Services:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d1e4b42e-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:51 +0000 UTC Labels:map[role:service name:read] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.236.240 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}] Op:0}\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703801   19433 config.go:138] Setting endpoints {Endpoints:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:8080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:7080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink:/api/v1beta1/endpoints/syslog?namespace=default UID:4c837d1f-b929-11e4-bc1b-080027205d4b ResourceVersion:12 CreationTimestamp:2015-02-20 17:52:55 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[172.17.0.3:514]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d24c9595-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:52 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:fc253202-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:41:02 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]}] Op:0}\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703923   19433 proxier.go:459] Received update notice: [{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}]\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.704086   19433 proxier.go:470] Something changed for service \"redis\": stopping it\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.726654   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.729142   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.733688   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.736540   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739764   19433 proxier.go:575] Closed iptables portals for service \"redis\"\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739798   19433 proxier.go:480] Adding new service \"redis\" at 119.9.235.183:6379/TCP (local :0)\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739880   19433 proxier.go:443] Proxying for service \"redis\" on TCP port 37841\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739890   19433 proxier.go:492] info: &{portalIP:[0 0 0 0 0 0 0 0 0 0 255 255 119 9 235 183] portalPort:6379 protocol:TCP proxyPort:37841 socket:0xc20810c4a0 timeout:60000000000 publicIP:[] sessionAffinityType:None stickyMaxAgeMinutes:180}\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742221   19433 proxier.go:103] Accept failed: accept tcp [::]:55814: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742248   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742257   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742261   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742265   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742269   19433 proxier.go:103] Accept failed: use of closed network connection\n\nThe Accept failed: use of closed network connection is spat out indefinitely at a very high rate. All services cease to function (but timeout rather than refuse connection).\nThe thing of note in the logs is that in the \"Setting services\" message both redis instances are present where-as in the \"Recieved update\" message, only the project2 instance is present.\nI believe that this is due to the fact that several places in pkg/proxy/config/config.go create maps keyed on service name only. Here is an example https://github.com/mikesimons/kubernetes/blob/master/pkg/proxy/config/config.go#L214\nAssuming I am correct, what would be the best way to fix this? It doesn't look like the keys of the maps are used anywhere so does this need to be a map at all?",
      "title": "Services with the same name in different namespaces cause kube-proxy to bug out"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2592,
    "text": "Want create-or-update command for kubectlIf you want to build a shell script to reconcile a config file with the state on the apiserver, it is convenient to be able to do \"create or update\" in a single command.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "12",
      "number": "7047",
      "pretext": "If you want to build a shell script to reconcile a config file with the state on the apiserver, it is convenient to be able to do \"create or update\" in a single command.",
      "title": "Want create-or-update command for kubectl"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2593,
    "text": "Create a command-line tool to query yaml and json filesExample usage (details to be decided):\n$ manifest-query -f mypod.yaml  -t {{.metadata.name}}\n\ngolang templates (http://golang.org/pkg/text/template/#pkg-overview) may not be sufficient because they don't handle all characters (e.g. there have been problems with /)\nThe tool will be useful for:\n\nbash scripts that have to manipulate yamls (e.g. for fixing #9849)\nexamples, e.g. here: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/accessing-the-cluster.md#without-kubectl-proxy\n\ncc. @zmerlynn",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "13",
      "number": "9937",
      "pretext": "Example usage (details to be decided):\n$ manifest-query -f mypod.yaml  -t {{.metadata.name}}\n\ngolang templates (http://golang.org/pkg/text/template/#pkg-overview) may not be sufficient because they don't handle all characters (e.g. there have been problems with /)\nThe tool will be useful for:\n\nbash scripts that have to manipulate yamls (e.g. for fixing #9849)\nexamples, e.g. here: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/accessing-the-cluster.md#without-kubectl-proxy\n\ncc. @zmerlynn",
      "title": "Create a command-line tool to query yaml and json files"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2594,
    "text": "Vagrant:Fedora 21 repo failingHello,\nI was trying to bringing up the cluster from the github master branch which is using fedora 21. But I am getting the following errors:\n==> master: Installing, enabling prerequisites\n==> master:\n==> master:\n==> master:  One of the configured repositories failed (Fedora 21 - x86_64),\n==> master:  and yum doesn't have enough cached data to continue. At this point the only\n==> master:  safe thing yum can do is fail. There are a few ways to work \"fix\" this:\n==> master:\n==> master:      1. Contact the upstream for the repository and get them to fix the problem.\n==> master:\n==> master:      2. Reconfigure the baseurl/etc. for the repository, to point to a working\n==> master:         upstream. This is most often useful if you are using a newer\n==> master:         distribution release than is supported by the repository (and the\n==> master:         packages for the previous distribution release still work).\n==> master:\n==> master:      3. Disable the repository, so yum won't use it by default. Yum will then\n==> master:         just ignore the repository until you permanently enable it again or use\n==> master:         --enablerepo for temporary usage:\n==> master:\n==> master:             yum-config-manager --disable fedora\n==> master:\n==> master:      4. Configure the failing repository to be skipped, if it is unavailable.\n==> master:         Note that yum will try to contact the repo. when it runs most commands,\n==> master:         so will have to try and fail each time (and thus. yum will be be much\n==> master:         slower). If it is a very temporary problem though, this is often a nice\n==> master:         compromise:\n==> master:\n==> master:             yum-config-manager --save --setopt=fedora.skip_if_unavailable=true\n==> master:\n==> master: Cannot retrieve metalink for repository: fedora/21/x86_64. Please verify its path and try again\nAny idea?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "14",
      "number": "9459",
      "pretext": "Hello,\nI was trying to bringing up the cluster from the github master branch which is using fedora 21. But I am getting the following errors:\n==> master: Installing, enabling prerequisites\n==> master:\n==> master:\n==> master:  One of the configured repositories failed (Fedora 21 - x86_64),\n==> master:  and yum doesn't have enough cached data to continue. At this point the only\n==> master:  safe thing yum can do is fail. There are a few ways to work \"fix\" this:\n==> master:\n==> master:      1. Contact the upstream for the repository and get them to fix the problem.\n==> master:\n==> master:      2. Reconfigure the baseurl/etc. for the repository, to point to a working\n==> master:         upstream. This is most often useful if you are using a newer\n==> master:         distribution release than is supported by the repository (and the\n==> master:         packages for the previous distribution release still work).\n==> master:\n==> master:      3. Disable the repository, so yum won't use it by default. Yum will then\n==> master:         just ignore the repository until you permanently enable it again or use\n==> master:         --enablerepo for temporary usage:\n==> master:\n==> master:             yum-config-manager --disable fedora\n==> master:\n==> master:      4. Configure the failing repository to be skipped, if it is unavailable.\n==> master:         Note that yum will try to contact the repo. when it runs most commands,\n==> master:         so will have to try and fail each time (and thus. yum will be be much\n==> master:         slower). If it is a very temporary problem though, this is often a nice\n==> master:         compromise:\n==> master:\n==> master:             yum-config-manager --save --setopt=fedora.skip_if_unavailable=true\n==> master:\n==> master: Cannot retrieve metalink for repository: fedora/21/x86_64. Please verify its path and try again\nAny idea?",
      "title": "Vagrant:Fedora 21 repo failing"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2595,
    "text": "Too many daemons created by DaemonSetI created a DaemonSet a few days ago on my 4 node cluster.  I noticed today that I have 6596 Pending pods, and 4 running pods.\nThis is not expected.\nThe running pods are on nodes that have existed for 3 or 4 days.\nThe pending pods all have unset nodeName.  I would not have expected the DaemonSet controller to ever create a pod with this value unset.  I assume they are pending due to port conflicts with the existing pods, as they use a hostPort.",
    "annotations": [{ "label": 143, "user": 1 }],
    "meta": {
      "": "15",
      "number": "14528",
      "pretext": "I created a DaemonSet a few days ago on my 4 node cluster.  I noticed today that I have 6596 Pending pods, and 4 running pods.\nThis is not expected.\nThe running pods are on nodes that have existed for 3 or 4 days.\nThe pending pods all have unset nodeName.  I would not have expected the DaemonSet controller to ever create a pod with this value unset.  I assume they are pending due to port conflicts with the existing pods, as they use a hostPort.",
      "title": "Too many daemons created by DaemonSet"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2596,
    "text": "kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytesI'm getting an error while running the cluster/kube-up.sh with\nexport KUBERNETES_PROVIDER=vagrant\nexport VAGRANT_DEFAULT_PROVIDER=virtualbox\nexport NUM_MINIONS=1\n\nerror:\n==> master: ‘/srv/salt-overlay/salt/kube-apiserver/basic_auth.csv’ -> ‘/srv/salt-new/salt/kube-apiserver/basic_auth.csv’\n==> master: ‘/srv/salt-overlay/pillar/cluster-params.sls’ -> ‘/srv/salt-new/pillar/cluster-params.sls’\n==> master: +++ Install binaries from tar: kubernetes-server-linux-amd64.tar.gz\n==> master: tar: kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes\n==> master: tar: Exiting with failure status due to previous errors\nThe SSH command responded with a non-zero exit status. Vagrant\nassumes that this means the command failed. The output for this command\nshould be in the log above. Please read the output to determine what\nwent wrong.\n\nam I missing something?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "16",
      "number": "14743",
      "pretext": "I'm getting an error while running the cluster/kube-up.sh with\nexport KUBERNETES_PROVIDER=vagrant\nexport VAGRANT_DEFAULT_PROVIDER=virtualbox\nexport NUM_MINIONS=1\n\nerror:\n==> master: ‘/srv/salt-overlay/salt/kube-apiserver/basic_auth.csv’ -> ‘/srv/salt-new/salt/kube-apiserver/basic_auth.csv’\n==> master: ‘/srv/salt-overlay/pillar/cluster-params.sls’ -> ‘/srv/salt-new/pillar/cluster-params.sls’\n==> master: +++ Install binaries from tar: kubernetes-server-linux-amd64.tar.gz\n==> master: tar: kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes\n==> master: tar: Exiting with failure status due to previous errors\nThe SSH command responded with a non-zero exit status. Vagrant\nassumes that this means the command failed. The output for this command\nshould be in the log above. Please read the output to determine what\nwent wrong.\n\nam I missing something?",
      "title": "kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2597,
    "text": "Please provide a feedback mechanism on kubernetes.io --> github.com/kubernetes/kubernetesI may be missing something (!) but, when I wish to file bugs against kubernetes.io, instead of a \"Click here to submit feedback\", I'm resorting to Googling a section of text on the kubernetes.io to find the relevant page on github in order to reference the github content for github issues!\nE.g.\nFrom here:\nhttp://kubernetes.io/v1.1/docs/user-guide/connecting-applications.html#securing-the-service\nMust Google to help find this:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/connecting-applications.md#securing-the-service\nFrom here:\nhttp://kubernetes.io/v1.1/examples/https-nginx/README.html\nMust Google to help find this:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.1/examples/https-nginx/README.md\nRecommend providing a feedback feature that facilitates submitting bugs against kubenetes.io pages by either auto-referencing or facilitating finding the reference on the github page.\nAlternatively, do you just accept kubernetes.io references in github issues?",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "17",
      "number": "21635",
      "pretext": "I may be missing something (!) but, when I wish to file bugs against kubernetes.io, instead of a \"Click here to submit feedback\", I'm resorting to Googling a section of text on the kubernetes.io to find the relevant page on github in order to reference the github content for github issues!\nE.g.\nFrom here:\nhttp://kubernetes.io/v1.1/docs/user-guide/connecting-applications.html#securing-the-service\nMust Google to help find this:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/connecting-applications.md#securing-the-service\nFrom here:\nhttp://kubernetes.io/v1.1/examples/https-nginx/README.html\nMust Google to help find this:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.1/examples/https-nginx/README.md\nRecommend providing a feedback feature that facilitates submitting bugs against kubenetes.io pages by either auto-referencing or facilitating finding the reference on the github page.\nAlternatively, do you just accept kubernetes.io references in github issues?",
      "title": "Please provide a feedback mechanism on kubernetes.io --> github.com/kubernetes/kubernetes"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2598,
    "text": "Broken on soak cluster: Kubectl client Update Demo should do a rolling update of a replication controller [Conformance]http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-soak-continuous-e2e-gce/3939/",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "18",
      "number": "17600",
      "pretext": "http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-soak-continuous-e2e-gce/3939/",
      "title": "Broken on soak cluster: Kubectl client Update Demo should do a rolling update of a replication controller [Conformance]"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2599,
    "text": "'--api-prefix' option in 'kubectl proxy' documentation is wrongWithout --api-prefix, kubectl proxy works just fine,\n$ curl localhost:8001/api/\n{\n  \"versions\": [\n    \"v1beta1\",\n    \"v1beta2\",\n    \"v1beta3\"\n  ]\n}\n\nWith '--api-prefix=xxx-api', it stops work:\n$ curl localhost:8001/xxx-api/\n404 page not found\n\nMy kubectl version:\n$ kubectl version\nClient Version: version.Info{Major:\"0\", Minor:\"17+\", GitVersion:\"v0.17.1-878-g851f6b754241c0-dirty\", GitCommit:\"851f6b754241c01f30c43cddd2f9ca8c7c3f42f5\", GitTreeState:\"dirty\"}\nServer Version: version.Info{Major:\"0\", Minor:\"17+\", GitVersion:\"v0.17.1-893-g58b683fe296da8-dirty\", GitCommit:\"58b683fe296da870fdbc6eb1a32dcf50ed94a8e3\", GitTreeState:\"dirty\"}\n\n@jlowdermilk, could you take a look? Thanks.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "19",
      "number": "8943",
      "pretext": "Without --api-prefix, kubectl proxy works just fine,\n$ curl localhost:8001/api/\n{\n  \"versions\": [\n    \"v1beta1\",\n    \"v1beta2\",\n    \"v1beta3\"\n  ]\n}\n\nWith '--api-prefix=xxx-api', it stops work:\n$ curl localhost:8001/xxx-api/\n404 page not found\n\nMy kubectl version:\n$ kubectl version\nClient Version: version.Info{Major:\"0\", Minor:\"17+\", GitVersion:\"v0.17.1-878-g851f6b754241c0-dirty\", GitCommit:\"851f6b754241c01f30c43cddd2f9ca8c7c3f42f5\", GitTreeState:\"dirty\"}\nServer Version: version.Info{Major:\"0\", Minor:\"17+\", GitVersion:\"v0.17.1-893-g58b683fe296da8-dirty\", GitCommit:\"58b683fe296da870fdbc6eb1a32dcf50ed94a8e3\", GitTreeState:\"dirty\"}\n\n@jlowdermilk, could you take a look? Thanks.",
      "title": "'--api-prefix' option in 'kubectl proxy' documentation is wrong"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2600,
    "text": "AllowUnconditionalUpdate is very frighteningWe provide an option to violate the semantics of our API, and on top of that, it is the default if you don't pass a resourceVersion. This has the ability to do very bad things.\nIf we want to encourage people to start writing their own controllers on top of kubernetes, we shouldn't make it so easy for them to shoot themselves in the face. Is there a reason this option exists?",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "20",
      "number": "21330",
      "pretext": "We provide an option to violate the semantics of our API, and on top of that, it is the default if you don't pass a resourceVersion. This has the ability to do very bad things.\nIf we want to encourage people to start writing their own controllers on top of kubernetes, we shouldn't make it so easy for them to shoot themselves in the face. Is there a reason this option exists?",
      "title": "AllowUnconditionalUpdate is very frightening"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2601,
    "text": "Bash completion fails to autocomplete files or directoriesI've enabled kubectl bash completion as per the instructions:\nsource ./contrib/completions/bash/kubectl\nNow autocompletion works for the kubectl commands. However, if I try to autocomplete a file or directory name, it fails.\n\nkubectl create -f ./ser\nTab to autocomplete directory name\nkubectl create -f ./ser-bash: _filedir: command not found\n\nI am using OSX.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "21",
      "number": "24139",
      "pretext": "I've enabled kubectl bash completion as per the instructions:\nsource ./contrib/completions/bash/kubectl\nNow autocompletion works for the kubectl commands. However, if I try to autocomplete a file or directory name, it fails.\n\nkubectl create -f ./ser\nTab to autocomplete directory name\nkubectl create -f ./ser-bash: _filedir: command not found\n\nI am using OSX.",
      "title": "Bash completion fails to autocomplete files or directories"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2602,
    "text": "Document / rationalize CNI plugin distributionI believe the correct place to download the CNI plugins is https://storage.googleapis.com/kubernetes-release/network-plugins/cni-c864f0e1ea73719b8f4582402b0847064f9883b0.tar.gz\nA few challenges with that:\n\nIt's not clear which version is the latest of the handful in that directory (they all the same date, and the hash doesn't give any clues)\nWe should probably bundle it instead with the k8s version with which that k8s version is tested\nI'm not entirely sure how this tar file was built\n\nIt would be nice if they were available as individual files also (i.e. expanded form), just like we distribute the key k8s binaries in expanded form and in the the kubernetes.tar.gz file.  On that note they appear to actually depend on each other though, so perhaps they can't be split.",
    "annotations": [{ "label": 141, "user": 3 }],
    "meta": {
      "": "22",
      "number": "30338",
      "pretext": "I believe the correct place to download the CNI plugins is https://storage.googleapis.com/kubernetes-release/network-plugins/cni-c864f0e1ea73719b8f4582402b0847064f9883b0.tar.gz\nA few challenges with that:\n\nIt's not clear which version is the latest of the handful in that directory (they all the same date, and the hash doesn't give any clues)\nWe should probably bundle it instead with the k8s version with which that k8s version is tested\nI'm not entirely sure how this tar file was built\n\nIt would be nice if they were available as individual files also (i.e. expanded form), just like we distribute the key k8s binaries in expanded form and in the the kubernetes.tar.gz file.  On that note they appear to actually depend on each other though, so perhaps they can't be split.",
      "title": "Document / rationalize CNI plugin distribution"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2603,
    "text": "v1.3.0-alpha.2 binaries not pushedIt seems like v1.3.0-alpha.2 binaries are not pushed.\n$ curl -I https://storage.googleapis.com/kubernetes-release/release/v1.3.0-alpha.2/bin/linux/amd64/kubectl\nHTTP/1.1 404 Not Found\nX-GUploader-UploadID: AEnB2UqOJabwiZ7oFEUJjl-os0mvuOaqsNMaKo9pQr6RJtv1SjStRaTQ6_XwWj3duuLF1Q7bymVfgcSNvoRQoxNzLHHUOdgUiQ\nContent-Type: application/xml; charset=UTF-8\nContent-Length: 127\nDate: Wed, 20 Apr 2016 16:01:18 GMT\nExpires: Wed, 20 Apr 2016 16:01:18 GMT\nCache-Control: private, max-age=0\nServer: UploadServer\nAlternate-Protocol: 443:quic\nAlt-Svc: quic=\":443\"; ma=2592000; v=\"32,31,30,29,28,27,26,25\"\n@david-mcmahon @bgrant0607 @ixdy",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "23",
      "number": "24534",
      "pretext": "It seems like v1.3.0-alpha.2 binaries are not pushed.\n$ curl -I https://storage.googleapis.com/kubernetes-release/release/v1.3.0-alpha.2/bin/linux/amd64/kubectl\nHTTP/1.1 404 Not Found\nX-GUploader-UploadID: AEnB2UqOJabwiZ7oFEUJjl-os0mvuOaqsNMaKo9pQr6RJtv1SjStRaTQ6_XwWj3duuLF1Q7bymVfgcSNvoRQoxNzLHHUOdgUiQ\nContent-Type: application/xml; charset=UTF-8\nContent-Length: 127\nDate: Wed, 20 Apr 2016 16:01:18 GMT\nExpires: Wed, 20 Apr 2016 16:01:18 GMT\nCache-Control: private, max-age=0\nServer: UploadServer\nAlternate-Protocol: 443:quic\nAlt-Svc: quic=\":443\"; ma=2592000; v=\"32,31,30,29,28,27,26,25\"\n@david-mcmahon @bgrant0607 @ixdy",
      "title": "v1.3.0-alpha.2 binaries not pushed"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2604,
    "text": "Investigate alternative JSON parserse.g., I saw this on reddit: http://ugorji.net/blog/go-codecgen\nThe primary reason why this would be worth our time at the moment is to get better error messages when people do things like pass a string to an array, or misspell a field name. Performance will eventually become important as we scale up, but serialization performance is a very tiny issue compared with e.g. the serial health checking when you list minions.",
    "annotations": [{ "label": 143, "user": 1 }],
    "meta": {
      "": "24",
      "number": "3338",
      "pretext": "e.g., I saw this on reddit: http://ugorji.net/blog/go-codecgen\nThe primary reason why this would be worth our time at the moment is to get better error messages when people do things like pass a string to an array, or misspell a field name. Performance will eventually become important as we scale up, but serialization performance is a very tiny issue compared with e.g. the serial health checking when you list minions.",
      "title": "Investigate alternative JSON parsers"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2605,
    "text": "ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke/15018/\nFailed: ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134\nSep  7 14:58:43.497: Couldn't delete ns: \"e2e-tests-thirdparty-dn7z5\": unable to retrieve the complete list of server APIs: company.com/v1: the server could not find the requested resource (&discovery.ErrGroupDiscoveryFailed{Groups:map[unversioned.GroupVersion]error{unversioned.GroupVersion{Group:\"company.com\", Version:\"v1\"}:(*errors.StatusError)(0xc820cece80)}})\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:338",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "25",
      "number": "32237",
      "pretext": "https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke/15018/\nFailed: ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134\nSep  7 14:58:43.497: Couldn't delete ns: \"e2e-tests-thirdparty-dn7z5\": unable to retrieve the complete list of server APIs: company.com/v1: the server could not find the requested resource (&discovery.ErrGroupDiscoveryFailed{Groups:map[unversioned.GroupVersion]error{unversioned.GroupVersion{Group:\"company.com\", Version:\"v1\"}:(*errors.StatusError)(0xc820cece80)}})\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:338",
      "title": "ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2606,
    "text": "Secure kubelet port 10250The kubelet exposes an unauthenticated endpoint on port 10250. The issues with this:\n\nthere are the debug handlers /exec/ and /run/ that run code in any container on the host\nthese debug handlers are enabled by default\nthe code run in the container runs with full root capabilities (compared to docker's root with a capability bounding set)",
    "annotations": [{ "label": 140, "user": 1 }],
    "meta": {
      "": "26",
      "number": "7965",
      "pretext": "The kubelet exposes an unauthenticated endpoint on port 10250. The issues with this:\n\nthere are the debug handlers /exec/ and /run/ that run code in any container on the host\nthese debug handlers are enabled by default\nthe code run in the container runs with full root capabilities (compared to docker's root with a capability bounding set)",
      "title": "Secure kubelet port 10250"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2607,
    "text": "Rescheduler e2e should not scale kube-dns pods.In rescheduler e2e test, for It(\"should ensure that critical pod is scheduled in case there is no resources available\"), kube-dns is used as the critical pod target and being scaled out and in.\nHowever, we plan to enable the dns horizontal autoscaling feature in the near future, such as this WIP. If this feature is turned on, the corresponding autoscaler will fight with this rescheduler e2e and maintain the desired number of replicas. Hence this test will be very likely to fail.\nSo probably we should use another critical pod here, or either create a new critical pod that does not exist before in order to protect the ongoing functionalities.\n@piosz  @thockin",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "27",
      "number": "33289",
      "pretext": "In rescheduler e2e test, for It(\"should ensure that critical pod is scheduled in case there is no resources available\"), kube-dns is used as the critical pod target and being scaled out and in.\nHowever, we plan to enable the dns horizontal autoscaling feature in the near future, such as this WIP. If this feature is turned on, the corresponding autoscaler will fight with this rescheduler e2e and maintain the desired number of replicas. Hence this test will be very likely to fail.\nSo probably we should use another critical pod here, or either create a new critical pod that does not exist before in order to protect the ongoing functionalities.\n@piosz  @thockin",
      "title": "Rescheduler e2e should not scale kube-dns pods."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2608,
    "text": "E2E: Audit density.go iterator and other timeout iteratorsIn my last PR @brendanburns duly noted  that you can do declarative style of timeout iteration, which is an important part of the e2e's, rather than a for loop.\n\nlets use the style of iteration in soak_k8petstore.go (pending merge now) with switch -> case statements in density.go as well, and possibly other places.    There are also other examples of this online, (i.e.  https://code.google.com/p/go-wiki/wiki/Timeouts)\nwhile we're at it lets audit util.go and see if it is being used wherever possible in tests to maximize code reuse.  there are now a lot of new utils in it (like RunRC and createNS so on) which weren't there when e2e's were originally created",
    "annotations": [{ "label": 142, "user": 1 }],
    "meta": {
      "": "28",
      "number": "7989",
      "pretext": "In my last PR @brendanburns duly noted  that you can do declarative style of timeout iteration, which is an important part of the e2e's, rather than a for loop.\n\nlets use the style of iteration in soak_k8petstore.go (pending merge now) with switch -> case statements in density.go as well, and possibly other places.    There are also other examples of this online, (i.e.  https://code.google.com/p/go-wiki/wiki/Timeouts)\nwhile we're at it lets audit util.go and see if it is being used wherever possible in tests to maximize code reuse.  there are now a lot of new utils in it (like RunRC and createNS so on) which weren't there when e2e's were originally created",
      "title": "E2E: Audit density.go iterator and other timeout iterators"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2609,
    "text": "update Google Cloud API client import paths and moreThe Google Cloud API client libraries for Go are making some breaking changes:\n\nThe import paths are changing from google.golang.org/cloud/... to\ncloud.google.com/go/.... For example, if your code imports the BigQuery client\nit currently reads\nimport \"google.golang.org/cloud/bigquery\"\nIt should be changed to\nimport \"cloud.google.com/go/bigquery\"\nClient options are also moving, from google.golang.org/cloud to\ngoogle.golang.org/api/option. Two have also been renamed:\n\nWithBaseGRPC is now WithGRPCConn\nWithBaseHTTP is now WithHTTPClient\n\n\nThe cloud.WithContext and cloud.NewContext methods are gone, as are the\ndeprecated pubsub and container functions that required them. Use the Client\nmethods of these packages instead.\n\nYou should make these changes before September 12, 2016, when the packages at\ngoogle.golang.org/cloud will go away.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "29",
      "number": "30069",
      "pretext": "The Google Cloud API client libraries for Go are making some breaking changes:\n\nThe import paths are changing from google.golang.org/cloud/... to\ncloud.google.com/go/.... For example, if your code imports the BigQuery client\nit currently reads\nimport \"google.golang.org/cloud/bigquery\"\nIt should be changed to\nimport \"cloud.google.com/go/bigquery\"\nClient options are also moving, from google.golang.org/cloud to\ngoogle.golang.org/api/option. Two have also been renamed:\n\nWithBaseGRPC is now WithGRPCConn\nWithBaseHTTP is now WithHTTPClient\n\n\nThe cloud.WithContext and cloud.NewContext methods are gone, as are the\ndeprecated pubsub and container functions that required them. Use the Client\nmethods of these packages instead.\n\nYou should make these changes before September 12, 2016, when the packages at\ngoogle.golang.org/cloud will go away.",
      "title": "update Google Cloud API client import paths and more"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2610,
    "text": "Copy relevant useful docs from https://cloud.google.com/container-engine/docs/They should have been in github in the first place.\ncc @mansoorj",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "30",
      "number": "16938",
      "pretext": "They should have been in github in the first place.\ncc @mansoorj",
      "title": "Copy relevant useful docs from https://cloud.google.com/container-engine/docs/"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2611,
    "text": "Missing service environment variables while starting podWhile debugging #5091 I noticed that when I'm creating guestbook application by running ./cluster/kubectl.sh create -f examples/guestbook there is a chance that frontend pods come up before information about environment variables of redis is propagated. In such case frontend can't connect to database during its whole life.\nIt's actually more general problem, since most of complex application might be affected by this. Also I think create a set of resource (especially by specifying the directory of their config files) should just work no matter of the order of creation.\nI can see few solutions of such problem:\n\nGet rid off HOST:PORT stuff and start using DNS service instead.\nAdd ability to wait for such information being propagated.\nInjecting environment variables to container somehow(?).",
    "annotations": [{ "label": 143, "user": 1 }],
    "meta": {
      "": "31",
      "number": "5184",
      "pretext": "While debugging #5091 I noticed that when I'm creating guestbook application by running ./cluster/kubectl.sh create -f examples/guestbook there is a chance that frontend pods come up before information about environment variables of redis is propagated. In such case frontend can't connect to database during its whole life.\nIt's actually more general problem, since most of complex application might be affected by this. Also I think create a set of resource (especially by specifying the directory of their config files) should just work no matter of the order of creation.\nI can see few solutions of such problem:\n\nGet rid off HOST:PORT stuff and start using DNS service instead.\nAdd ability to wait for such information being propagated.\nInjecting environment variables to container somehow(?).",
      "title": "Missing service environment variables while starting pod"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2612,
    "text": "registered.EnabledVersions returns all registered versions#20846 introduced a bug where registered.EnabledVersions doesn't consult enabledVersions anymore... I think. I was trying to debug another problem and spotted this.\nI think only clients use this function. APIServer has an entirely different mechanism for tracking enabled/disabled things.\ncc @kubernetes/sig-api-machinery",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "32",
      "number": "32224",
      "pretext": "#20846 introduced a bug where registered.EnabledVersions doesn't consult enabledVersions anymore... I think. I was trying to debug another problem and spotted this.\nI think only clients use this function. APIServer has an entirely different mechanism for tracking enabled/disabled things.\ncc @kubernetes/sig-api-machinery",
      "title": "registered.EnabledVersions returns all registered versions"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2613,
    "text": "Add a `shellcheck` based pre-submitshellcheck is a bash script linter which could help catch common problems in our (numerous) bash scripts. I know we'd like to reduce our use of bash scripts, but until then I think being stricter about script quality would be helpful.\n/cc @zmerlynn",
    "annotations": [{ "label": 142, "user": 1 }],
    "meta": {
      "": "33",
      "number": "24614",
      "pretext": "shellcheck is a bash script linter which could help catch common problems in our (numerous) bash scripts. I know we'd like to reduce our use of bash scripts, but until then I think being stricter about script quality would be helpful.\n/cc @zmerlynn",
      "title": "Add a `shellcheck` based pre-submit"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2614,
    "text": "Remove the generated client for ScaleScale is a sub-resource, we shouldn't generate a typed client for it.\ncc @lavalamp @madhusudancs",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "34",
      "number": "21932",
      "pretext": "Scale is a sub-resource, we shouldn't generate a typed client for it.\ncc @lavalamp @madhusudancs",
      "title": "Remove the generated client for Scale"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2615,
    "text": "AWS: Check that dynamic volumes are deletedIt was reported that dynamic volumes were not deleted on AWS.  Maybe we need to enable the PersistentVolumeRecycler somehow?",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "35",
      "number": "22907",
      "pretext": "It was reported that dynamic volumes were not deleted on AWS.  Maybe we need to enable the PersistentVolumeRecycler somehow?",
      "title": "AWS: Check that dynamic volumes are deleted"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2616,
    "text": "Merge NamespaceExists and NamespaceLifecycle admission controllersWe should have a single NamespaceLifecycle plugin that enforces the Namespace rules now that all providers are off NamespaceAutoProvision.\nWe may also want to make this no longer a user choice to configure and hard-wire the server to always run this check first to simplify configuration errors.\nFor an example\n#12039",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "36",
      "number": "12053",
      "pretext": "We should have a single NamespaceLifecycle plugin that enforces the Namespace rules now that all providers are off NamespaceAutoProvision.\nWe may also want to make this no longer a user choice to configure and hard-wire the server to always run this check first to simplify configuration errors.\nFor an example\n#12039",
      "title": "Merge NamespaceExists and NamespaceLifecycle admission controllers"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2617,
    "text": "Scaling clusters: 1000's of services in env varsClearly you ought to be able to make more services in a k8s cluster than it is reasonable to pass to pods in env vars.\nPossible solutions:\n\nSegment by namespace\nRequire predeclarations\n\n@bgrant0607 I know you hate env vars; do you have a preferred solution for this?",
    "annotations": [{ "label": 145, "user": 3 }],
    "meta": {
      "": "37",
      "number": "3345",
      "pretext": "Clearly you ought to be able to make more services in a k8s cluster than it is reasonable to pass to pods in env vars.\nPossible solutions:\n\nSegment by namespace\nRequire predeclarations\n\n@bgrant0607 I know you hate env vars; do you have a preferred solution for this?",
      "title": "Scaling clusters: 1000's of services in env vars"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2618,
    "text": "Pod dependencies on servicesOne thing that is a bad experience at the moment is the bring-up behaviour of one pod that depends on another the services of another pod. For example, in my logging work the Kibana viewer (pod, service) depends on the Elasticsearch (pod, service). When I try and bring them up together from my Makefile I have an intermediate sate like this for quite a while:\nNAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS\ninflux-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending\nheapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running\nsynthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running\nelasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Pending\nkibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Failed\n\ni.e. the Kibana viewer fails to start up because Elasticsearch is not ready yet. Eventually things start to look better:\nNAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS\ninflux-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending\nheapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running\nsynthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running\nelasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Running\nkibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Running\nkubectl.sh get services\n\nbut even though the pods are marked as Running they are still not quite ready yet and it takes another five minutes or so before one can make queries to Elasticsearch and see log output in Kibana.\nIt would be nice to describe in a pod declaration its dependencies on other services so this can be taken into account during scheudling. For example:\napiVersion: v1beta1\nkind: Pod\nid: kibana-pod\ndesiredState:\n  manifest:\n    version: v1beta1\n    id: kibana-server\n    containers:\n      - name: kibana-image\n        image: kubernetes/kibana:latest\n        ports:\n          - name: kibana-port\n            containerPort: 80\n        dependencies: [elasticsearch]\nlabels:\n  app: kibana-viewer\n\nThis would delay the scheduling of this pod until the pod(s) identified by the elasticsearch service are all in the running state.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "38",
      "number": "2385",
      "pretext": "One thing that is a bad experience at the moment is the bring-up behaviour of one pod that depends on another the services of another pod. For example, in my logging work the Kibana viewer (pod, service) depends on the Elasticsearch (pod, service). When I try and bring them up together from my Makefile I have an intermediate sate like this for quite a while:\nNAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS\ninflux-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending\nheapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running\nsynthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running\nelasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Pending\nkibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Failed\n\ni.e. the Kibana viewer fails to start up because Elasticsearch is not ready yet. Eventually things start to look better:\nNAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS\ninflux-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending\nheapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running\nsynthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running\nelasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Running\nkibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Running\nkubectl.sh get services\n\nbut even though the pods are marked as Running they are still not quite ready yet and it takes another five minutes or so before one can make queries to Elasticsearch and see log output in Kibana.\nIt would be nice to describe in a pod declaration its dependencies on other services so this can be taken into account during scheudling. For example:\napiVersion: v1beta1\nkind: Pod\nid: kibana-pod\ndesiredState:\n  manifest:\n    version: v1beta1\n    id: kibana-server\n    containers:\n      - name: kibana-image\n        image: kubernetes/kibana:latest\n        ports:\n          - name: kibana-port\n            containerPort: 80\n        dependencies: [elasticsearch]\nlabels:\n  app: kibana-viewer\n\nThis would delay the scheduling of this pod until the pod(s) identified by the elasticsearch service are all in the running state.",
      "title": "Pod dependencies on services"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2619,
    "text": "service account kube-system/default was not found in ubuntuhi\ni'm trying to run with k8s version 1.1.7\ni updated config-default  script and removed DenyEscalatingExec key from ADMISSION_CONTROL(otherwise its not working)\nafter that i tried to run kube-ui (or any other pod with kube-system namespace)  without any success\nin  kube-controller-manager.log  i saw the following error\nunable to create pod replica: Pod \"kube-ui-v4-\" is forbidden: service account kube-system/default was not found, retry after the service account is created\ni googled it  and  found the following  solution\n\nGenerate a signing key:\nopenssl genrsa -out /tmp/serviceaccount.key 2048\nUpdate /etc/kubernetes/apiserver:\nKUBE_API_ARGS=\"--service_account_key_file=/tmp/serviceaccount.key\"\nUpdate /etc/kubernetes/controller-manager:\nKUBE_CONTROLLER_MANAGER_ARGS=\"--service_account_private_key_file=/etc/kubernetes/serviceaccount.key\"\n\nbut  i couldn't found the keys in the config files\ni tried to update util.sh\nand set the following flags (thats the only places that i found those keys)\n--tls-private-key-file=/etc/kubernetes/serviceaccount.key\"\n--service-account-private-key-file=/etc/kubernetes/serviceaccount.key \\\nnote: doing kubectl serviceaccount for default namespace returns 1 entry\nBUT kubectl serviceaccount --namespace=kube-system returns NO ENTRIES!\ni'm really desperate :) does anyone have a  clue how to fix this issue\ntks a lot",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "39",
      "number": "20556",
      "pretext": "hi\ni'm trying to run with k8s version 1.1.7\ni updated config-default  script and removed DenyEscalatingExec key from ADMISSION_CONTROL(otherwise its not working)\nafter that i tried to run kube-ui (or any other pod with kube-system namespace)  without any success\nin  kube-controller-manager.log  i saw the following error\nunable to create pod replica: Pod \"kube-ui-v4-\" is forbidden: service account kube-system/default was not found, retry after the service account is created\ni googled it  and  found the following  solution\n\nGenerate a signing key:\nopenssl genrsa -out /tmp/serviceaccount.key 2048\nUpdate /etc/kubernetes/apiserver:\nKUBE_API_ARGS=\"--service_account_key_file=/tmp/serviceaccount.key\"\nUpdate /etc/kubernetes/controller-manager:\nKUBE_CONTROLLER_MANAGER_ARGS=\"--service_account_private_key_file=/etc/kubernetes/serviceaccount.key\"\n\nbut  i couldn't found the keys in the config files\ni tried to update util.sh\nand set the following flags (thats the only places that i found those keys)\n--tls-private-key-file=/etc/kubernetes/serviceaccount.key\"\n--service-account-private-key-file=/etc/kubernetes/serviceaccount.key \\\nnote: doing kubectl serviceaccount for default namespace returns 1 entry\nBUT kubectl serviceaccount --namespace=kube-system returns NO ENTRIES!\ni'm really desperate :) does anyone have a  clue how to fix this issue\ntks a lot",
      "title": "service account kube-system/default was not found in ubuntu"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2620,
    "text": "etcd arguments are different for systemd vs non-systemdkubernetes/cluster/saltbase/salt/etcd/default (used for systemd) and kubernetes/cluster/saltbase/salt/etcd/initd (used for initd) have diverged; in particular DAEMON_ARGS has a different bind_addr.\nAlso, I'm not sure whether etcd.conf is still used at all, but it has another value for the bind addresses (hard-coded to 0.0.0.0).",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "40",
      "number": "4024",
      "pretext": "kubernetes/cluster/saltbase/salt/etcd/default (used for systemd) and kubernetes/cluster/saltbase/salt/etcd/initd (used for initd) have diverged; in particular DAEMON_ARGS has a different bind_addr.\nAlso, I'm not sure whether etcd.conf is still used at all, but it has another value for the bind addresses (hard-coded to 0.0.0.0).",
      "title": "etcd arguments are different for systemd vs non-systemd"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2621,
    "text": "Incorrect Login For MasterHello,\nFor a brand new (first time) installation using the Vagrant install fails (different reason, suspect python lib issue).  I then switched to using the AWS tutorial, it succeeds after some undocumented fixes (will add new bug for that in docs).  After it starts, if you visit the AWS EC2 master, the authentication is requiring the login from the previous Vagrant install.  I suspect it didn't override with the new AWS settings.  I will keep investigating and try to see if this is a real bug or poor docs.\nRegards.\nRon",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "41",
      "number": "13060",
      "pretext": "Hello,\nFor a brand new (first time) installation using the Vagrant install fails (different reason, suspect python lib issue).  I then switched to using the AWS tutorial, it succeeds after some undocumented fixes (will add new bug for that in docs).  After it starts, if you visit the AWS EC2 master, the authentication is requiring the login from the previous Vagrant install.  I suspect it didn't override with the new AWS settings.  I will keep investigating and try to see if this is a real bug or poor docs.\nRegards.\nRon",
      "title": "Incorrect Login For Master"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2622,
    "text": "e2e flake: Kubernetes e2e suite.Kubectl client Kubectl apply should apply a new configuration to an existing RCRecent failure on #23287 seems flaky. That PR didn't change anything but delete a few dead\nJenkins jobs.\nkubernetes-pull-build-test-e2e-gce/33368/\nStacktrace\n\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:463\nExpected error:\n    <*errors.errorString | 0xc208266480>: {\n        s: \"Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\\n{\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"kind\\\\\\\":\\\\\\\"ReplicationController\\\\\\\",\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"metadata\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"replicas\\\\\\\":1,\\\\\\\"selector\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"},\\\\\\\"template\\\\\\\":{\\\\\\\"metadata\\\\\\\":{\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"containers\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"image\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"ports\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-server\\\\\\\",\\\\\\\"containerPort\\\\\\\":6379}],\\\\\\\"resources\\\\\\\":{}}]}}},\\\\\\\"status\\\\\\\":{\\\\\\\"replicas\\\\\\\":0}}\\\"},\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}}}}}\\nto:\\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\\nfor: \\\"STDIN\\\": replicationControllers \\\"redis-master\\\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\\n [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\\nCommand stdout:\\n\\nstderr:\\nError from server: error when applying patch:\\n{\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"kind\\\\\\\":\\\\\\\"ReplicationController\\\\\\\",\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"metadata\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"replicas\\\\\\\":1,\\\\\\\"selector\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"},\\\\\\\"template\\\\\\\":{\\\\\\\"metadata\\\\\\\":{\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"containers\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"image\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"ports\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-server\\\\\\\",\\\\\\\"containerPort\\\\\\\":6379}],\\\\\\\"resources\\\\\\\":{}}]}}},\\\\\\\"status\\\\\\\":{\\\\\\\"replicas\\\\\\\":0}}\\\"},\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}}}}}\\nto:\\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\\nfor: \\\"STDIN\\\": replicationControllers \\\"redis-master\\\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\\n\\n\",\n    }\n    Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\n    {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"kind\\\":\\\"ReplicationController\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"redis-master\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"redis-master\\\",\\\"image\\\":\\\"redis\\\",\\\"ports\\\":[{\\\"name\\\":\\\"redis-server\\\",\\\"containerPort\\\":6379}],\\\"resources\\\":{}}]}}},\\\"status\\\":{\\\"replicas\\\":0}}\"},\"creationTimestamp\":null,\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}},\"spec\":{\"selector\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"},\"template\":{\"metadata\":{\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}}}}}\n    to:\n    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\n    for: \"STDIN\": replicationControllers \"redis-master\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n     [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\n    Command stdout:\n\n    stderr:\n    Error from server: error when applying patch:\n    {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"kind\\\":\\\"ReplicationController\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"redis-master\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"redis-master\\\",\\\"image\\\":\\\"redis\\\",\\\"ports\\\":[{\\\"name\\\":\\\"redis-server\\\",\\\"containerPort\\\":6379}],\\\"resources\\\":{}}]}}},\\\"status\\\":{\\\"replicas\\\":0}}\"},\"creationTimestamp\":null,\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}},\"spec\":{\"selector\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"},\"template\":{\"metadata\":{\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}}}}}\n    to:\n    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\n    for: \"STDIN\": replicationControllers \"redis-master\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n\n\nnot to have occurred\n\n@spxtr to delegate.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "42",
      "number": "23389",
      "pretext": "Recent failure on #23287 seems flaky. That PR didn't change anything but delete a few dead\nJenkins jobs.\nkubernetes-pull-build-test-e2e-gce/33368/\nStacktrace\n\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:463\nExpected error:\n    <*errors.errorString | 0xc208266480>: {\n        s: \"Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\\n{\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"kind\\\\\\\":\\\\\\\"ReplicationController\\\\\\\",\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"metadata\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"replicas\\\\\\\":1,\\\\\\\"selector\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"},\\\\\\\"template\\\\\\\":{\\\\\\\"metadata\\\\\\\":{\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"containers\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"image\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"ports\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-server\\\\\\\",\\\\\\\"containerPort\\\\\\\":6379}],\\\\\\\"resources\\\\\\\":{}}]}}},\\\\\\\"status\\\\\\\":{\\\\\\\"replicas\\\\\\\":0}}\\\"},\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}}}}}\\nto:\\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\\nfor: \\\"STDIN\\\": replicationControllers \\\"redis-master\\\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\\n [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\\nCommand stdout:\\n\\nstderr:\\nError from server: error when applying patch:\\n{\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"kind\\\\\\\":\\\\\\\"ReplicationController\\\\\\\",\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"metadata\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"replicas\\\\\\\":1,\\\\\\\"selector\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"},\\\\\\\"template\\\\\\\":{\\\\\\\"metadata\\\\\\\":{\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"containers\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"image\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"ports\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-server\\\\\\\",\\\\\\\"containerPort\\\\\\\":6379}],\\\\\\\"resources\\\\\\\":{}}]}}},\\\\\\\"status\\\\\\\":{\\\\\\\"replicas\\\\\\\":0}}\\\"},\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}}}}}\\nto:\\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\\nfor: \\\"STDIN\\\": replicationControllers \\\"redis-master\\\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\\n\\n\",\n    }\n    Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\n    {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"kind\\\":\\\"ReplicationController\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"redis-master\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"redis-master\\\",\\\"image\\\":\\\"redis\\\",\\\"ports\\\":[{\\\"name\\\":\\\"redis-server\\\",\\\"containerPort\\\":6379}],\\\"resources\\\":{}}]}}},\\\"status\\\":{\\\"replicas\\\":0}}\"},\"creationTimestamp\":null,\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}},\"spec\":{\"selector\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"},\"template\":{\"metadata\":{\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}}}}}\n    to:\n    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\n    for: \"STDIN\": replicationControllers \"redis-master\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n     [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\n    Command stdout:\n\n    stderr:\n    Error from server: error when applying patch:\n    {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"kind\\\":\\\"ReplicationController\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"redis-master\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"redis-master\\\",\\\"image\\\":\\\"redis\\\",\\\"ports\\\":[{\\\"name\\\":\\\"redis-server\\\",\\\"containerPort\\\":6379}],\\\"resources\\\":{}}]}}},\\\"status\\\":{\\\"replicas\\\":0}}\"},\"creationTimestamp\":null,\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}},\"spec\":{\"selector\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"},\"template\":{\"metadata\":{\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}}}}}\n    to:\n    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\n    for: \"STDIN\": replicationControllers \"redis-master\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n\n\nnot to have occurred\n\n@spxtr to delegate.",
      "title": "e2e flake: Kubernetes e2e suite.Kubectl client Kubectl apply should apply a new configuration to an existing RC"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2623,
    "text": "rkt: retrieve image size from rkt api serviceAs rkt/rkt#1916 is merged, we should be able to return the image size information to kubelet.\ncc @derekparker @sjpotter @jonboulle",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "43",
      "number": "19494",
      "pretext": "As rkt/rkt#1916 is merged, we should be able to return the image size information to kubelet.\ncc @derekparker @sjpotter @jonboulle",
      "title": "rkt: retrieve image size from rkt api service"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2624,
    "text": "Pods Killed Every 5 Minutes After Upgrading to 1.2We've successfully been running 1.1.7 on AWS for several months.  After upgrading to 1.2 today nearly all of our pods are killed every 5 minutes.  In the logs we see Killing container with docker id 661a6ded11b9: Need to kill pod., but beyond that there doesn't seem to be a clear cause.  I'm at a bit of loss for where to look next and how to debug this, and love any suggestions / recommendations.",
    "annotations": [{ "label": 145, "user": 3 }],
    "meta": {
      "": "44",
      "number": "23170",
      "pretext": "We've successfully been running 1.1.7 on AWS for several months.  After upgrading to 1.2 today nearly all of our pods are killed every 5 minutes.  In the logs we see Killing container with docker id 661a6ded11b9: Need to kill pod., but beyond that there doesn't seem to be a clear cause.  I'm at a bit of loss for where to look next and how to debug this, and love any suggestions / recommendations.",
      "title": "Pods Killed Every 5 Minutes After Upgrading to 1.2"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2625,
    "text": "Named Ports not creating _name._protocol.<service> SRV entriesHi,\nI have the following service definition for use on GCE:\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth\n  labels:\n    name: auth\nspec:\n  type: LoadBalancer\n  ports:\n  - name: api\n    protocol: TCP\n    port: 8000\n    targetPort: 8000\n  selector:\n    name: auth\n\nWhen i do a SRV lookup to get the port (as per instructions here) for that service using:\ndig SRV _api._TCP.auth.default.cluster.local.\nI get:\nroot@ubuntu:/# dig SRV _api._tcp.auth.default.cluster.local.\n\n; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV _api._tcp.auth.default.cluster.local.\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11979\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;_api._tcp.auth.default.cluster.local. IN SRV\n\n;; AUTHORITY SECTION:\ncluster.local.      60  IN  SOA ns.dns.cluster.local. hostmaster.skydns.local. 1438578000 28800 7200 604800 60\n\n;; Query time: 5 msec\n;; SERVER: 10.111.240.10#53(10.111.240.10)\n;; WHEN: Mon Aug 03 05:44:55 UTC 2015\n;; MSG SIZE  rcvd: 123\n\nThe service does get registered properly and is responding. I can look up the service if I use:\ndig SRV default.cluster.local.\nI get:\n; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV default.cluster.local.\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 54040\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 2\n\n;; QUESTION SECTION:\n;default.cluster.local.     IN  SRV\n\n;; ANSWER SECTION:\ndefault.cluster.local.  30  IN  SRV 10 50 0 kubernetes.default.cluster.local.\ndefault.cluster.local.  30  IN  SRV 10 50 0 auth.default.cluster.local.\n\n;; ADDITIONAL SECTION:\nkubernetes.default.cluster.local. 30 IN A   10.111.240.1\nauth.default.cluster.local. 30 IN A 10.111.247.242\n\n;; Query time: 7 msec\n;; SERVER: 10.111.240.10#53(10.111.240.10)\n;; WHEN: Mon Aug 03 05:46:53 UTC 2015\n;; MSG SIZE  rcvd: 201\n\nWhy doesn't the port number get returned when I lookup the named port via an SRV record?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "45",
      "number": "12137",
      "pretext": "Hi,\nI have the following service definition for use on GCE:\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth\n  labels:\n    name: auth\nspec:\n  type: LoadBalancer\n  ports:\n  - name: api\n    protocol: TCP\n    port: 8000\n    targetPort: 8000\n  selector:\n    name: auth\n\nWhen i do a SRV lookup to get the port (as per instructions here) for that service using:\ndig SRV _api._TCP.auth.default.cluster.local.\nI get:\nroot@ubuntu:/# dig SRV _api._tcp.auth.default.cluster.local.\n\n; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV _api._tcp.auth.default.cluster.local.\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11979\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;_api._tcp.auth.default.cluster.local. IN SRV\n\n;; AUTHORITY SECTION:\ncluster.local.      60  IN  SOA ns.dns.cluster.local. hostmaster.skydns.local. 1438578000 28800 7200 604800 60\n\n;; Query time: 5 msec\n;; SERVER: 10.111.240.10#53(10.111.240.10)\n;; WHEN: Mon Aug 03 05:44:55 UTC 2015\n;; MSG SIZE  rcvd: 123\n\nThe service does get registered properly and is responding. I can look up the service if I use:\ndig SRV default.cluster.local.\nI get:\n; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV default.cluster.local.\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 54040\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 2\n\n;; QUESTION SECTION:\n;default.cluster.local.     IN  SRV\n\n;; ANSWER SECTION:\ndefault.cluster.local.  30  IN  SRV 10 50 0 kubernetes.default.cluster.local.\ndefault.cluster.local.  30  IN  SRV 10 50 0 auth.default.cluster.local.\n\n;; ADDITIONAL SECTION:\nkubernetes.default.cluster.local. 30 IN A   10.111.240.1\nauth.default.cluster.local. 30 IN A 10.111.247.242\n\n;; Query time: 7 msec\n;; SERVER: 10.111.240.10#53(10.111.240.10)\n;; WHEN: Mon Aug 03 05:46:53 UTC 2015\n;; MSG SIZE  rcvd: 201\n\nWhy doesn't the port number get returned when I lookup the named port via an SRV record?",
      "title": "Named Ports not creating _name._protocol.<service> SRV entries"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2626,
    "text": "Support setting env vars in kubectl runSomething like kubectl run --env=\"VAR=value\" image. Multiple --env flags should be accepted. Comma-separate values would get into an escape rathole, so I'd like to avoid that.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "46",
      "number": "12828",
      "pretext": "Something like kubectl run --env=\"VAR=value\" image. Multiple --env flags should be accepted. Comma-separate values would get into an escape rathole, so I'd like to avoid that.",
      "title": "Support setting env vars in kubectl run"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2627,
    "text": "Node e2e reportingIt would be useful to have daily or per-pr reports about the health of each distro and publish it to a dashboard somewhere",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "47",
      "number": "26220",
      "pretext": "It would be useful to have daily or per-pr reports about the health of each distro and publish it to a dashboard somewhere",
      "title": "Node e2e reporting"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2628,
    "text": "code.google.com/p/go.tools/cmd/cover moved to godoc.org/golang.org/x/tools/cmd/coverWhen I tried to install Kubernetes, I met an issue: 'package code.google.com/p/go.tools/cmd/cover: Get https://code.google.com/p/go/source/checkout?repo=tools: dial tcp 173.194.127.104:443: connection timed out'\nThen I did some research and found the path of covert has been moved to https://godoc.org/golang.org/x/tools/cmd/cover\nHowever in https://github.com/GoogleCloudPlatform/kubernetes/blob/e5e4c8a7d35a0bb981155d84d766eec3e2cd6ffa/Godeps/_workspace/src/github.com/google/gofuzz/.travis.yml, it is still use code.google.com/p/go.tools/cmd/cover, should we update it?\nBest Regards\nSimon",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "48",
      "number": "2728",
      "pretext": "When I tried to install Kubernetes, I met an issue: 'package code.google.com/p/go.tools/cmd/cover: Get https://code.google.com/p/go/source/checkout?repo=tools: dial tcp 173.194.127.104:443: connection timed out'\nThen I did some research and found the path of covert has been moved to https://godoc.org/golang.org/x/tools/cmd/cover\nHowever in https://github.com/GoogleCloudPlatform/kubernetes/blob/e5e4c8a7d35a0bb981155d84d766eec3e2cd6ffa/Godeps/_workspace/src/github.com/google/gofuzz/.travis.yml, it is still use code.google.com/p/go.tools/cmd/cover, should we update it?\nBest Regards\nSimon",
      "title": "code.google.com/p/go.tools/cmd/cover moved to godoc.org/golang.org/x/tools/cmd/cover"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2629,
    "text": "kubectl should return more information on failureAfter making a kubectl create call with bad JSON, kubectl would read-back the master's understanding of what I wrote and show me a diff.  Something like that",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "49",
      "number": "6132",
      "pretext": "After making a kubectl create call with bad JSON, kubectl would read-back the master's understanding of what I wrote and show me a diff.  Something like that",
      "title": "kubectl should return more information on failure"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2630,
    "text": "/minions not available for api v1beta3Just got a clean deployment of 0.17.1, and there seem to be an issue with kubectl:\nroot@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 get minions\nI0524 08:52:29.291509   19049 selector.go:53] Unable to list \"minions\": the server could not find the requested resource\n\nIndeed, requesting https://10.241.1.1:6443/api/v1beta3/minions returns:\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1beta3\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"the server could not find the requested resource\",\n  \"reason\": \"NotFound\",\n  \"details\": {},\n  \"code\": 404\n}\nSpecifying older api version works though:\nroot@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 --api-version='v1beta2' get minions\nNAME         LABELS    STATUS\n10.243.0.1   <none>    NotReady",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "50",
      "number": "8753",
      "pretext": "Just got a clean deployment of 0.17.1, and there seem to be an issue with kubectl:\nroot@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 get minions\nI0524 08:52:29.291509   19049 selector.go:53] Unable to list \"minions\": the server could not find the requested resource\n\nIndeed, requesting https://10.241.1.1:6443/api/v1beta3/minions returns:\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1beta3\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"the server could not find the requested resource\",\n  \"reason\": \"NotFound\",\n  \"details\": {},\n  \"code\": 404\n}\nSpecifying older api version works though:\nroot@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 --api-version='v1beta2' get minions\nNAME         LABELS    STATUS\n10.243.0.1   <none>    NotReady",
      "title": "/minions not available for api v1beta3"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2631,
    "text": "kube-up.sh fails and misreports status using AWS (KUBE_MANIFESTS_TAR_URL: unbound variable)This is, essentially, a deliberate duplicate of #30495, which was closed by the requester before the underlying issue was resolved. In short, current versions of Kubernetes (I'm using 1.3.7) fail when using kube-up.sh to create a cluster under AWS, but incorrectly report success. This leaves some AWS resources (VPCs, security groups, etc.) in place, but with no actual nodes.\nThe relevant bit of output is as follows:\n./cluster/../cluster/../cluster/aws/../../cluster/common.sh: line 528: KUBE_MANIFESTS_TAR_URL: unbound variable\nKubernetes binaries at /Users/jon/kubernetes/cluster/\nYou may want to add this directory to your PATH in $HOME/.profile\nInstallation successful!\n\nA viable workaround has been described in #30495 (I believe this is why the original author closed the issue), but the issue itself has not actually been fixed. Please accept my apologies for the goofy paperwork shenanigans here, but I did want to make sure the issue didn't get lost.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "51",
      "number": "33202",
      "pretext": "This is, essentially, a deliberate duplicate of #30495, which was closed by the requester before the underlying issue was resolved. In short, current versions of Kubernetes (I'm using 1.3.7) fail when using kube-up.sh to create a cluster under AWS, but incorrectly report success. This leaves some AWS resources (VPCs, security groups, etc.) in place, but with no actual nodes.\nThe relevant bit of output is as follows:\n./cluster/../cluster/../cluster/aws/../../cluster/common.sh: line 528: KUBE_MANIFESTS_TAR_URL: unbound variable\nKubernetes binaries at /Users/jon/kubernetes/cluster/\nYou may want to add this directory to your PATH in $HOME/.profile\nInstallation successful!\n\nA viable workaround has been described in #30495 (I believe this is why the original author closed the issue), but the issue itself has not actually been fixed. Please accept my apologies for the goofy paperwork shenanigans here, but I did want to make sure the issue didn't get lost.",
      "title": "kube-up.sh fails and misreports status using AWS (KUBE_MANIFESTS_TAR_URL: unbound variable)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2632,
    "text": "kubectl proxy failed to run as a service or sudo in GCEKubernetes version (use kubectl version):\nClient Version: version.Info{Major:\"1\", Minor:\"3\", GitVersion:\"v1.3.4\", GitCommit:\"dd6b458ef8dbf24aff55795baa68f83383c9b3a9\", GitTreeState:\"clean\", BuildDate:\"2016-08-01T16:45:16Z\", GoVersion:\"go1.6.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"3\", GitVersion:\"v1.3.5\", GitCommit:\"b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5\", GitTreeState:\"clean\", BuildDate:\"2016-08-11T20:21:58Z\", GoVersion:\"go1.6.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\nEnvironment:\n\nCloud provider or hardware configuration: GCP VM & GCP container cluster\nOS (e.g. from /etc/os-release):\n\nNAME=\"CentOS Linux\"\nVERSION=\"7 (Core)\"\nID=\"centos\"\nID_LIKE=\"rhel fedora\"\nVERSION_ID=\"7\"\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\nANSI_COLOR=\"0;31\"\nCPE_NAME=\"cpe:/o:centos:centos:7\"\nHOME_URL=\"https://www.centos.org/\"\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\n\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\nREDHAT_SUPPORT_PRODUCT=\"centos\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\n\n\nKernel (e.g. uname -a):\n\nLinux cm-1 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nWhat happened:\nIn a GCE instance, failed to run kubectl proxy as a service or sudo, the same command ran successfully in command line as current user.\nOn Mac, I can run \"kubectl proxy --port=8080\" or \"sudo kubectl proxy --port=8080\" without problem.\nRun as a service, failed:\n$ sudo systemctl restart kubectlproxy\nJob for kubectlproxy.service failed because the control process exited with error code. See \"systemctl status kubectlproxy.service\" and \"journalctl -xe\" for details.\n\n$ sudo systemctl -l status kubectlproxy.service\n● kubectlproxy.service - kubectl proxy Service\n   Loaded: loaded (/opt/cm/kubectlproxy.service; linked; vendor preset: disabled)\n   Active: failed (Result: exit-code) since Fri 2016-08-26 02:09:04 UTC; 5s ago\n  Process: 1982 ExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080 (code=exited, status=1/FAILURE)\n Main PID: 1982 (code=exited, status=1/FAILURE)\n\nAug 26 02:09:04 cm-1 systemd[1]: Starting kubectl proxy Service...\nAug 26 02:09:04 cm-1 kubectl[1982]: The connection to the server localhost:8080 was refused - did you specify the right host or port?\nAug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service: main process exited, code=exited, status=1/FAILURE\nAug 26 02:09:04 cm-1 systemd[1]: Failed to start kubectl proxy Service.\nAug 26 02:09:04 cm-1 systemd[1]: Unit kubectlproxy.service entered failed state.\nAug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service failed.\n\nRun as sudo, failed:\n$ sudo /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\nRun as current user, succeeded:\n$ /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080\nStarting to serve on 127.0.0.1:8080\n\nBelow is the systemd service file:\n[Unit]\nDescription=kubectl proxy Service\nAfter=network.target\n\n[Service]\nType=notify\nUser=root\nExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --p\nort=8080\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "52",
      "number": "31488",
      "pretext": "Kubernetes version (use kubectl version):\nClient Version: version.Info{Major:\"1\", Minor:\"3\", GitVersion:\"v1.3.4\", GitCommit:\"dd6b458ef8dbf24aff55795baa68f83383c9b3a9\", GitTreeState:\"clean\", BuildDate:\"2016-08-01T16:45:16Z\", GoVersion:\"go1.6.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"3\", GitVersion:\"v1.3.5\", GitCommit:\"b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5\", GitTreeState:\"clean\", BuildDate:\"2016-08-11T20:21:58Z\", GoVersion:\"go1.6.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\nEnvironment:\n\nCloud provider or hardware configuration: GCP VM & GCP container cluster\nOS (e.g. from /etc/os-release):\n\nNAME=\"CentOS Linux\"\nVERSION=\"7 (Core)\"\nID=\"centos\"\nID_LIKE=\"rhel fedora\"\nVERSION_ID=\"7\"\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\nANSI_COLOR=\"0;31\"\nCPE_NAME=\"cpe:/o:centos:centos:7\"\nHOME_URL=\"https://www.centos.org/\"\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\n\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\nREDHAT_SUPPORT_PRODUCT=\"centos\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\n\n\nKernel (e.g. uname -a):\n\nLinux cm-1 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nWhat happened:\nIn a GCE instance, failed to run kubectl proxy as a service or sudo, the same command ran successfully in command line as current user.\nOn Mac, I can run \"kubectl proxy --port=8080\" or \"sudo kubectl proxy --port=8080\" without problem.\nRun as a service, failed:\n$ sudo systemctl restart kubectlproxy\nJob for kubectlproxy.service failed because the control process exited with error code. See \"systemctl status kubectlproxy.service\" and \"journalctl -xe\" for details.\n\n$ sudo systemctl -l status kubectlproxy.service\n● kubectlproxy.service - kubectl proxy Service\n   Loaded: loaded (/opt/cm/kubectlproxy.service; linked; vendor preset: disabled)\n   Active: failed (Result: exit-code) since Fri 2016-08-26 02:09:04 UTC; 5s ago\n  Process: 1982 ExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080 (code=exited, status=1/FAILURE)\n Main PID: 1982 (code=exited, status=1/FAILURE)\n\nAug 26 02:09:04 cm-1 systemd[1]: Starting kubectl proxy Service...\nAug 26 02:09:04 cm-1 kubectl[1982]: The connection to the server localhost:8080 was refused - did you specify the right host or port?\nAug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service: main process exited, code=exited, status=1/FAILURE\nAug 26 02:09:04 cm-1 systemd[1]: Failed to start kubectl proxy Service.\nAug 26 02:09:04 cm-1 systemd[1]: Unit kubectlproxy.service entered failed state.\nAug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service failed.\n\nRun as sudo, failed:\n$ sudo /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\nRun as current user, succeeded:\n$ /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080\nStarting to serve on 127.0.0.1:8080\n\nBelow is the systemd service file:\n[Unit]\nDescription=kubectl proxy Service\nAfter=network.target\n\n[Service]\nType=notify\nUser=root\nExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --p\nort=8080\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target",
      "title": "kubectl proxy failed to run as a service or sudo in GCE"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2633,
    "text": "Kubernetes should support cross-zone clusterMost applications should be running in multiple zones to increase availability. Kubernetes should support it. I imagine this to work in the following way:\n\nUser sets up cluster in a region in a way that minions are spread evenly across all available zones\nUser creates replication controller for the application with size > 1\nScheduler spreads pods within the same replication controller across available zones\n\nThat way user gets regional availability for free.\nAFAIU this will mostly require changes in how we create cluster and how we schedule pods.\ncc @wojtek-t",
    "annotations": [{ "label": 141, "user": 3 }],
    "meta": {
      "": "53",
      "number": "4235",
      "pretext": "Most applications should be running in multiple zones to increase availability. Kubernetes should support it. I imagine this to work in the following way:\n\nUser sets up cluster in a region in a way that minions are spread evenly across all available zones\nUser creates replication controller for the application with size > 1\nScheduler spreads pods within the same replication controller across available zones\n\nThat way user gets regional availability for free.\nAFAIU this will mostly require changes in how we create cluster and how we schedule pods.\ncc @wojtek-t",
      "title": "Kubernetes should support cross-zone cluster"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2634,
    "text": "Validate Docker v1.11Docker v1.11 rcs are starting to show up. Its time to get started with the validation.\nhttps://github.com/docker/docker/releases/tag/v1.11.0-rc1\ncc @kubernetes/sig-node\nEDIT(timstclair):\nTASKS:\n\n e2e tests pass\n performance analysis\n startup tests (bootstrap, restart)\n live upgrade successful\n 1 week soak tests",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "54",
      "number": "23397",
      "pretext": "Docker v1.11 rcs are starting to show up. Its time to get started with the validation.\nhttps://github.com/docker/docker/releases/tag/v1.11.0-rc1\ncc @kubernetes/sig-node\nEDIT(timstclair):\nTASKS:\n\n e2e tests pass\n performance analysis\n startup tests (bootstrap, restart)\n live upgrade successful\n 1 week soak tests",
      "title": "Validate Docker v1.11"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2635,
    "text": "Figure out how to handle code in multiple reposA large monorepo works for Google, but not on github.\nWe hit the ceiling of achievable velocity of a single github repo in early 2015:\nhttps://github.com/kubernetes/kubernetes/graphs/contributors\nThere are many reasons: ACLs, notification management, issue triage, PR reviews, sequentialized submit testing, merge conflicts, etc.\nWe're chipping away at these issues, but we need more than incremental improvement.\nWe've discussed moving a number of things to other repos:\n\nKubelet: #444\nGeneric API infrastructure: #2742\nClient libraries: #5660\nMisc. utilities: #24156\nkubectl\nscheduler\nexamples\ncloud providers + cluster code + \"getting-started guides\"\nauth plugins\nnetwork and storage plugins?\ne2e tests\ncontributor documentation\n\nWe need to seriously think about how to do this.\nKnown issues that need to be addressed:\n\nWe need to get sprawl and package dependencies under control: #4851\nWe need to make most components ordinary clients of the API: #20193\nWe need to figure out dependency management and integration testing\n\nAn example of a Go project on github with good repo hygiene:\nhttps://github.com/deis\nI have no illusions that breaking the project into separate repos will be a silver bullet: it's necessary, but not sufficient. I also know that it will cause some pain. But that pain already exists: cadvisor, heapster, dashboard, contrib, docs, ....\nSpeaking of contrib, it needs to be broken up, too: kubernetes-retired/contrib#762\n@thockin @smarterclayton @lavalamp @mikedanese @dchen1107 @davidopp @ixdy",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "55",
      "number": "24343",
      "pretext": "A large monorepo works for Google, but not on github.\nWe hit the ceiling of achievable velocity of a single github repo in early 2015:\nhttps://github.com/kubernetes/kubernetes/graphs/contributors\nThere are many reasons: ACLs, notification management, issue triage, PR reviews, sequentialized submit testing, merge conflicts, etc.\nWe're chipping away at these issues, but we need more than incremental improvement.\nWe've discussed moving a number of things to other repos:\n\nKubelet: #444\nGeneric API infrastructure: #2742\nClient libraries: #5660\nMisc. utilities: #24156\nkubectl\nscheduler\nexamples\ncloud providers + cluster code + \"getting-started guides\"\nauth plugins\nnetwork and storage plugins?\ne2e tests\ncontributor documentation\n\nWe need to seriously think about how to do this.\nKnown issues that need to be addressed:\n\nWe need to get sprawl and package dependencies under control: #4851\nWe need to make most components ordinary clients of the API: #20193\nWe need to figure out dependency management and integration testing\n\nAn example of a Go project on github with good repo hygiene:\nhttps://github.com/deis\nI have no illusions that breaking the project into separate repos will be a silver bullet: it's necessary, but not sufficient. I also know that it will cause some pain. But that pain already exists: cadvisor, heapster, dashboard, contrib, docs, ....\nSpeaking of contrib, it needs to be broken up, too: kubernetes-retired/contrib#762\n@thockin @smarterclayton @lavalamp @mikedanese @dchen1107 @davidopp @ixdy",
      "title": "Figure out how to handle code in multiple repos"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2636,
    "text": "Modify E2E tests to use the GCE API instead of gcloud execPR #17747 laid the ground work to enable E2E tests to use the GCE API instead of gcloud exec. That PR only modified the PD tests to do so.\nIn order to make E2E more robust, we should switch over all other instances of gcloud exec in E2E tests to use the GCE API instead.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "56",
      "number": "20885",
      "pretext": "PR #17747 laid the ground work to enable E2E tests to use the GCE API instead of gcloud exec. That PR only modified the PD tests to do so.\nIn order to make E2E more robust, we should switch over all other instances of gcloud exec in E2E tests to use the GCE API instead.",
      "title": "Modify E2E tests to use the GCE API instead of gcloud exec"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2637,
    "text": "Add the \"when\" and \"why\" to use the Downward APIWe need to add more content around the Downward API to better clarify when and why to use it. We should also try to revise the \"how\" info we provide today to improve/clarify what the specific steps.\n(for example: Prerequisites are xyx. To use the downward API: 1. do this. 2....3...etc..).\nSome details we should make clear:\n\nWhen do i want containers to consume info about the system without coupling to k8s client or REST API?\nDo we say \"use the downward API when your containers need access to information about the cluster in which it resides\"?\nWe should also tie together/link/mention the other content we have on using environment variables and the downward api example.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "57",
      "number": "10130",
      "pretext": "We need to add more content around the Downward API to better clarify when and why to use it. We should also try to revise the \"how\" info we provide today to improve/clarify what the specific steps.\n(for example: Prerequisites are xyx. To use the downward API: 1. do this. 2....3...etc..).\nSome details we should make clear:\n\nWhen do i want containers to consume info about the system without coupling to k8s client or REST API?\nDo we say \"use the downward API when your containers need access to information about the cluster in which it resides\"?\nWe should also tie together/link/mention the other content we have on using environment variables and the downward api example.",
      "title": "Add the \"when\" and \"why\" to use the Downward API"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2638,
    "text": "rkt: hostPath mounts to non-existent directories failA pod referencing a nonexistent host path, such as the one below, does not run under the rkt container runtime.. However, it runs just fine under docker (which, by default, creates nonexistent paths as empty directories when a mount references them).\nThe pod also does not show up in kubectl get pods in any state, though it can still be deleted.\nThe latter is definitely something that should be fixed; the former is an intentional behavioural difference between rkt and docker and might be up to debate.\n\nShould the pod fail? Do we need consistent behavior with docker here? Do we think that blindly creating directories on the host masks typos and is surprising?\n1b) Should rkt change to this behavior, or should we create the directory before referencing it in a bindmount? (e.g. an ExecStartPre=mkdir -p <host directories>)\nPods that fail like this should still be visible in get pods; I think this is jut that rkt failed during stage1 and there's some over-aggressive error handling in our code that masks this pod, but I haven't dived very deeply there yet.\n\nExample failing pod\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: mount-dn\n  name: mount-dne\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: dne\n    hostPath:\n      path: /does/not/exist\n  containers:\n    - name: exit\n      image: busybox\n      command: [\"sh\", \"-c\", \"ls /test; sleep 60\"]\n      volumeMounts:\n      - mountPath: /test\n        name: dne\ncc @yifan-gu @tmrts",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "58",
      "number": "26816",
      "pretext": "A pod referencing a nonexistent host path, such as the one below, does not run under the rkt container runtime.. However, it runs just fine under docker (which, by default, creates nonexistent paths as empty directories when a mount references them).\nThe pod also does not show up in kubectl get pods in any state, though it can still be deleted.\nThe latter is definitely something that should be fixed; the former is an intentional behavioural difference between rkt and docker and might be up to debate.\n\nShould the pod fail? Do we need consistent behavior with docker here? Do we think that blindly creating directories on the host masks typos and is surprising?\n1b) Should rkt change to this behavior, or should we create the directory before referencing it in a bindmount? (e.g. an ExecStartPre=mkdir -p <host directories>)\nPods that fail like this should still be visible in get pods; I think this is jut that rkt failed during stage1 and there's some over-aggressive error handling in our code that masks this pod, but I haven't dived very deeply there yet.\n\nExample failing pod\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: mount-dn\n  name: mount-dne\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: dne\n    hostPath:\n      path: /does/not/exist\n  containers:\n    - name: exit\n      image: busybox\n      command: [\"sh\", \"-c\", \"ls /test; sleep 60\"]\n      volumeMounts:\n      - mountPath: /test\n        name: dne\ncc @yifan-gu @tmrts",
      "title": "rkt: hostPath mounts to non-existent directories fail"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2639,
    "text": "fixing detecting docker versionIn the getting-started-guides matches the detection of the docker version not with a version 1.1x.\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/master.sh#L108\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/worker.sh#L105",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "59",
      "number": "22485",
      "pretext": "In the getting-started-guides matches the detection of the docker version not with a version 1.1x.\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/master.sh#L108\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/worker.sh#L105",
      "title": "fixing detecting docker version"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2640,
    "text": "test-cmd flake: first kubectl command (get nodes) failsEncountered on #18901\n!!! Error in ./hack/test-cmd.sh:203\n  '[ \"$(kubectl get nodes -o go-template='{{ .apiVersion }}' \"${kube_flags[@]}\")\" == \"v1\" ]' exited with status 1\nCall stack:\n  1: ./hack/test-cmd.sh:203 runTests(...)\n  2: ./hack/test-cmd.sh:1352 main(...)\nExiting with status 1\n\ncc @kargakis",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "60",
      "number": "20154",
      "pretext": "Encountered on #18901\n!!! Error in ./hack/test-cmd.sh:203\n  '[ \"$(kubectl get nodes -o go-template='{{ .apiVersion }}' \"${kube_flags[@]}\")\" == \"v1\" ]' exited with status 1\nCall stack:\n  1: ./hack/test-cmd.sh:203 runTests(...)\n  2: ./hack/test-cmd.sh:1352 main(...)\nExiting with status 1\n\ncc @kargakis",
      "title": "test-cmd flake: first kubectl command (get nodes) fails"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2641,
    "text": "Use bandwidth shaping on the masterLimit outbound and inbound bandwidth, using the tc tool, see:\nhttps://www.iplocation.net/traffic-control",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "61",
      "number": "11965",
      "pretext": "Limit outbound and inbound bandwidth, using the tc tool, see:\nhttps://www.iplocation.net/traffic-control",
      "title": "Use bandwidth shaping on the master"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2642,
    "text": "Add an optional \"why\" clause to ValidationErrorI find myself wanting to explain WHY a validation error failed, and there's just no way to pass that down to users.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "62",
      "number": "1988",
      "pretext": "I find myself wanting to explain WHY a validation error failed, and there's just no way to pass that down to users.",
      "title": "Add an optional \"why\" clause to ValidationError"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2643,
    "text": "Add go_vet to presubmitWe'd like to stay vet clean, and aren't too far away.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "63",
      "number": "22524",
      "pretext": "We'd like to stay vet clean, and aren't too far away.",
      "title": "Add go_vet to presubmit"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2644,
    "text": "Per-Container Inode Accountingcc: @derekwaynecarr (how can we get devicemapper support for this?)\nwhen a node has reached its eviction-hard threshold for imagefs.inodes or nodefs.inodes, the eviction manager does not behave optimally.\nThis is because we do not keep track of inodes used per-container, but rather just overall.  In some cases, the pod which is not using many inodes is evicted before a pod that is using many.\nProgress:\n\n Create an end to end test that demonstrates that a pod using a normal amount of disk capacity is evicted before a pod that uses all remaining inodes (but little disk capacity).\n Modify cadvisor api and kubernetes api to include per-container inode usage\n Modify cadvisor to publish per-container inode usage\n Modify kubelet eviction manager to take per-container inode usage into account when evicting",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "64",
      "number": "33382",
      "pretext": "cc: @derekwaynecarr (how can we get devicemapper support for this?)\nwhen a node has reached its eviction-hard threshold for imagefs.inodes or nodefs.inodes, the eviction manager does not behave optimally.\nThis is because we do not keep track of inodes used per-container, but rather just overall.  In some cases, the pod which is not using many inodes is evicted before a pod that is using many.\nProgress:\n\n Create an end to end test that demonstrates that a pod using a normal amount of disk capacity is evicted before a pod that uses all remaining inodes (but little disk capacity).\n Modify cadvisor api and kubernetes api to include per-container inode usage\n Modify cadvisor to publish per-container inode usage\n Modify kubelet eviction manager to take per-container inode usage into account when evicting",
      "title": "Per-Container Inode Accounting"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2645,
    "text": "[docker 1.10] create syscall filters for k8s-supplied componentsWith docker 1.10, you can create a filter for syscalls that the container is allowed to execute, mainly to reduce the kernel attack surface and make it harder to use a privilege escalation vulnerability in the kernel code.\nFor containers that we provide (master components, add-ons) where we know the expected syscall surface we should explore locking them down by limiting the system calls that they can make.\n/cc @stephenR",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {
      "": "65",
      "number": "20820",
      "pretext": "With docker 1.10, you can create a filter for syscalls that the container is allowed to execute, mainly to reduce the kernel attack surface and make it harder to use a privilege escalation vulnerability in the kernel code.\nFor containers that we provide (master components, add-ons) where we know the expected syscall surface we should explore locking them down by limiting the system calls that they can make.\n/cc @stephenR",
      "title": "[docker 1.10] create syscall filters for k8s-supplied components"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2646,
    "text": "What kind of aws roles do I need to prepare for kubernetesHi,\niam has a few roles. Is it \"Grant API access to SAML providers\"?\nThank you.\nskwok",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {
      "": "66",
      "number": "7493",
      "pretext": "Hi,\niam has a few roles. Is it \"Grant API access to SAML providers\"?\nThank you.\nskwok",
      "title": "What kind of aws roles do I need to prepare for kubernetes"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2647,
    "text": "kubectl: rationalize which messages to stderr/stdoutA user reports to google-containers@googlegroups.com (https://groups.google.com/forum/#!topic/google-containers/qHDR-mvh7sM) the following.  I verified similar behavior in kubectl v0.9.1.\nI am using Kubernetes v0.7.0, when I run \"kubectl\" to create pod, I found:\n$ kubectl create -s http://192.168.122.136:8080 -f ./mariadb-pod.yaml\nI0128 22:09:44.258267   30016 restclient.go:133] Waiting for completion of operation 19    --> this is to stderr\nmariadb    --> this is to stdout\nI do not know why the message \"...Waiting for completion ...\" is considered as an error, I think it should be to stdout too.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "67",
      "number": "3894",
      "pretext": "A user reports to google-containers@googlegroups.com (https://groups.google.com/forum/#!topic/google-containers/qHDR-mvh7sM) the following.  I verified similar behavior in kubectl v0.9.1.\nI am using Kubernetes v0.7.0, when I run \"kubectl\" to create pod, I found:\n$ kubectl create -s http://192.168.122.136:8080 -f ./mariadb-pod.yaml\nI0128 22:09:44.258267   30016 restclient.go:133] Waiting for completion of operation 19    --> this is to stderr\nmariadb    --> this is to stdout\nI do not know why the message \"...Waiting for completion ...\" is considered as an error, I think it should be to stdout too.",
      "title": "kubectl: rationalize which messages to stderr/stdout"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2648,
    "text": "Version is missing in Heapster configurationcc: @piosz",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "68",
      "number": "22938",
      "pretext": "cc: @piosz",
      "title": "Version is missing in Heapster configuration"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2649,
    "text": "Kubelet is reporting Node cpu capacity as negative valueI have some reports of a Node reporting a a resource capacity for cpu as a negative value.\nosc describe node `hostname -f`\nName:                   <omitted>\nLabels:                 <none>\nCreationTimestamp:      Thu, 07 May 2015 09:50:48 -0400\nConditions:\n  Type          Status  LastHeartbeatTime\nLastTransitionTime                      Reason\n        Message\n  Ready         True    Thu, 07 May 2015 09:52:58 -0400         Thu, 07\nMay 2015 09:51:54 -0400         kubelet is posting ready status\nAddresses:      <omitted>\nCapacity:\n cpu:           -1\n memory:        1884432Ki\nVersion:\n Kernel Version:                3.10.0-229.el7.x86_64\n OS Image:                      Red Hat Enterprise Linux Server 7.1 (Maipo)\n Container Runtime Version:     docker://1.6.0\n Kubelet Version:               v0.14.1-582-gb12d75d\n Kube-Proxy Version:            v0.14.1-582-gb12d75d\nExternalID:                     <omitted>\nPods:                           (0 in total)\nNo events.\n\nAny idea why cpu could report as a negative value?  Is this an expected normal outcome?  In its current state, it prevents any pods from being scheduled to that Node.  Tips on how to debug further are appreciated.\n@dchen1107 @vmarmol - any ideas?",
    "annotations": [{ "label": 143, "user": 1 }],
    "meta": {
      "": "69",
      "number": "7906",
      "pretext": "I have some reports of a Node reporting a a resource capacity for cpu as a negative value.\nosc describe node `hostname -f`\nName:                   <omitted>\nLabels:                 <none>\nCreationTimestamp:      Thu, 07 May 2015 09:50:48 -0400\nConditions:\n  Type          Status  LastHeartbeatTime\nLastTransitionTime                      Reason\n        Message\n  Ready         True    Thu, 07 May 2015 09:52:58 -0400         Thu, 07\nMay 2015 09:51:54 -0400         kubelet is posting ready status\nAddresses:      <omitted>\nCapacity:\n cpu:           -1\n memory:        1884432Ki\nVersion:\n Kernel Version:                3.10.0-229.el7.x86_64\n OS Image:                      Red Hat Enterprise Linux Server 7.1 (Maipo)\n Container Runtime Version:     docker://1.6.0\n Kubelet Version:               v0.14.1-582-gb12d75d\n Kube-Proxy Version:            v0.14.1-582-gb12d75d\nExternalID:                     <omitted>\nPods:                           (0 in total)\nNo events.\n\nAny idea why cpu could report as a negative value?  Is this an expected normal outcome?  In its current state, it prevents any pods from being scheduled to that Node.  Tips on how to debug further are appreciated.\n@dchen1107 @vmarmol - any ideas?",
      "title": "Kubelet is reporting Node cpu capacity as negative value"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2650,
    "text": "apiserver proxy: no endpoints available error if Service.spec.ports[*].name is specifiedHello everyone!\nI noticed this strange behaviour today when I was trying to connect to my service via the apiserver proxy:\nspec:\n  ports:\n  - port: 80\n    name: foo\n    targetPort: 3000\n$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"no endpoints available for service \\\"gogs\\\"\",\n  \"reason\": \"ServiceUnavailable\",\n  \"code\": 503\n}\nBut when I deleted the Service.spec.ports[*].name field, it worked as it should:\nspec:\n  ports:\n  - port: 80\n    targetPort: 3000\n$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar\nOK\nIs this a known bug? Or is it made this way \"by design\"?\nThis also makes services with two or more ports inaccessible via apiserver proxy\n@ArtfulCoder @thockin",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "70",
      "number": "20070",
      "pretext": "Hello everyone!\nI noticed this strange behaviour today when I was trying to connect to my service via the apiserver proxy:\nspec:\n  ports:\n  - port: 80\n    name: foo\n    targetPort: 3000\n$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"no endpoints available for service \\\"gogs\\\"\",\n  \"reason\": \"ServiceUnavailable\",\n  \"code\": 503\n}\nBut when I deleted the Service.spec.ports[*].name field, it worked as it should:\nspec:\n  ports:\n  - port: 80\n    targetPort: 3000\n$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar\nOK\nIs this a known bug? Or is it made this way \"by design\"?\nThis also makes services with two or more ports inaccessible via apiserver proxy\n@ArtfulCoder @thockin",
      "title": "apiserver proxy: no endpoints available error if Service.spec.ports[*].name is specified"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2651,
    "text": "Implement e2e tests for horizontal pod autoscaling of deploymentsWe need to write e2e tests which combines horizontal pod autoscaling with deployments (scale sub-resourcer should point to a deployment object).\nThe tests should be similar to https://github.com/kubernetes/kubernetes/blob/master/test/e2e/horizontal_pod_autoscaling.go. The library from https://github.com/kubernetes/kubernetes/blob/master/test/e2e/autoscaling_utils.go should be extended and allow creation of a resource consumer as a deployment (currently it is always replication controller). The new tests should create a deployment resource consumer and pass a scale-sub-resource to the deployment to autoscaler.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "71",
      "number": "15474",
      "pretext": "We need to write e2e tests which combines horizontal pod autoscaling with deployments (scale sub-resourcer should point to a deployment object).\nThe tests should be similar to https://github.com/kubernetes/kubernetes/blob/master/test/e2e/horizontal_pod_autoscaling.go. The library from https://github.com/kubernetes/kubernetes/blob/master/test/e2e/autoscaling_utils.go should be extended and allow creation of a resource consumer as a deployment (currently it is always replication controller). The new tests should create a deployment resource consumer and pass a scale-sub-resource to the deployment to autoscaler.",
      "title": "Implement e2e tests for horizontal pod autoscaling of deployments"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2652,
    "text": "How Webhook works, where can find the details? thanks1 Is webhook plugin stable?\n2 How webhook works?\n3 How can I use webhook and where can find detailed information.\nThank you very much!",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "72",
      "number": "26513",
      "pretext": "1 Is webhook plugin stable?\n2 How webhook works?\n3 How can I use webhook and where can find detailed information.\nThank you very much!",
      "title": "How Webhook works, where can find the details? thanks"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2653,
    "text": "Unit tests are EXTREMELY flaky latelyIt's bad. kubertest-test-go suite failed 5 consecutive times because of it.\n@thockin @davidopp @brendandburns @bgrant0607 @dchen1107 @fgrzadkowski @wojtek-t @nikhiljindal @aronchick\nKnown Issues:\n\n pkg/master #19141 (fixed by @wojtek-t in #19195)\n pkg/util/wait #19067 (maybe fixed by @wojtek-t in #19196)\n pkg/storage/etcd, pkg/registry/generic/etcd #18928\n pkg/apiserver #19176\n pkg/client/record/event_test.go #19151\n k8s.io/kubernetes/contrib/mesos/pkg/runtime #19186\n pkg/util/wait: #19223\n pkg/client/unversioned/portforward #19230\n plugin/pkg/scheduler/factory #19229\n pkg/storage #19254\n pkg/client/record #19268",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "73",
      "number": "19167",
      "pretext": "It's bad. kubertest-test-go suite failed 5 consecutive times because of it.\n@thockin @davidopp @brendandburns @bgrant0607 @dchen1107 @fgrzadkowski @wojtek-t @nikhiljindal @aronchick\nKnown Issues:\n\n pkg/master #19141 (fixed by @wojtek-t in #19195)\n pkg/util/wait #19067 (maybe fixed by @wojtek-t in #19196)\n pkg/storage/etcd, pkg/registry/generic/etcd #18928\n pkg/apiserver #19176\n pkg/client/record/event_test.go #19151\n k8s.io/kubernetes/contrib/mesos/pkg/runtime #19186\n pkg/util/wait: #19223\n pkg/client/unversioned/portforward #19230\n plugin/pkg/scheduler/factory #19229\n pkg/storage #19254\n pkg/client/record #19268",
      "title": "Unit tests are EXTREMELY flaky lately"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2654,
    "text": "Fix l7 controller watch on namespacesObserved odd flakyness when watching Ingresses from the controller scoped to a namespace. Need to get to the bottom of it, but I suspect it's a bug higher up in the stack. For now it isn't that important because the controller watches all namespaces.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "74",
      "number": "17805",
      "pretext": "Observed odd flakyness when watching Ingresses from the controller scoped to a namespace. Need to get to the bottom of it, but I suspect it's a bug higher up in the stack. For now it isn't that important because the controller watches all namespaces.",
      "title": "Fix l7 controller watch on namespaces"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2655,
    "text": "support for docker links env variables formatWhen exposing services through environments variable to running containers, we should support the standard docker format used by links (see below). So developers don't have to modify their discovery code to connect to a service.\n    $ sudo docker run --rm --name web2 --link db:db training/webapp env\n    . . .\n    DB_NAME=/web2/db\n    DB_PORT=tcp://172.17.0.5:5432\n    DB_PORT_5000_TCP=tcp://172.17.0.5:5432\n    DB_PORT_5000_TCP_PROTO=tcp\n    DB_PORT_5000_TCP_PORT=5432\n    DB_PORT_5000_TCP_ADDR=172.17.0.5\n\nThe code writing the env var from service definition is here:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/779cb84625fda1ffd0b87437c2cc002033b2d2fe/pkg/registry/service_registry.go#L46\nI believe it could be easily adapted to follow the same standard defined by docker.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "75",
      "number": "424",
      "pretext": "When exposing services through environments variable to running containers, we should support the standard docker format used by links (see below). So developers don't have to modify their discovery code to connect to a service.\n    $ sudo docker run --rm --name web2 --link db:db training/webapp env\n    . . .\n    DB_NAME=/web2/db\n    DB_PORT=tcp://172.17.0.5:5432\n    DB_PORT_5000_TCP=tcp://172.17.0.5:5432\n    DB_PORT_5000_TCP_PROTO=tcp\n    DB_PORT_5000_TCP_PORT=5432\n    DB_PORT_5000_TCP_ADDR=172.17.0.5\n\nThe code writing the env var from service definition is here:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/779cb84625fda1ffd0b87437c2cc002033b2d2fe/pkg/registry/service_registry.go#L46\nI believe it could be easily adapted to follow the same standard defined by docker.",
      "title": "support for docker links env variables format"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2656,
    "text": "Add e2e test for etcd failures handlingWe ran into a lot of issues when etcd crashes. We need tests to ensure all components / daemons are doing the right things etcd is down, and recover later.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "76",
      "number": "4783",
      "pretext": "We ran into a lot of issues when etcd crashes. We need tests to ensure all components / daemons are doing the right things etcd is down, and recover later.",
      "title": "Add e2e test for etcd failures handling"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2657,
    "text": "Remove --auth-path from kubectlSince the .kubeconfig file was introduced, there is a new way to describe the information contained inside of the existing .kubernetes_auth format.  .kubernetes_auth combined information that described how to recognize the api-server with information about how to authenticate the user to the api-server.  .kubeconfig separates those two concepts into discretely re-useable chunks, but --auth-path was kept for backwards compatibility.\nIf .kubernetes_auth is eliminated, there will be one way to express that information and that will simplify the explanation of how the information is built.  Right now, allowing references to .kuberentes_auth and defaulting to looking at the ~/.kubernetes_auth makes it harder to describe exactly where authentication information is coming from.\nThis would be a breaking change that has ripples affecting e2e tests, so I'd like to be sure there is agreement to the concept before starting a change.\n/cc @jlowdermilk @smarterclayton @liggitt",
    "annotations": [{ "label": 140, "user": 3 }],
    "meta": {
      "": "77",
      "number": "3800",
      "pretext": "Since the .kubeconfig file was introduced, there is a new way to describe the information contained inside of the existing .kubernetes_auth format.  .kubernetes_auth combined information that described how to recognize the api-server with information about how to authenticate the user to the api-server.  .kubeconfig separates those two concepts into discretely re-useable chunks, but --auth-path was kept for backwards compatibility.\nIf .kubernetes_auth is eliminated, there will be one way to express that information and that will simplify the explanation of how the information is built.  Right now, allowing references to .kuberentes_auth and defaulting to looking at the ~/.kubernetes_auth makes it harder to describe exactly where authentication information is coming from.\nThis would be a breaking change that has ripples affecting e2e tests, so I'd like to be sure there is agreement to the concept before starting a change.\n/cc @jlowdermilk @smarterclayton @liggitt",
      "title": "Remove --auth-path from kubectl"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2658,
    "text": "kube-proxy's udp proxy deadEDIT: kubernetes version 1.0.3\nOne of our kubernete machine not able to connect to dns server, and after some trouble shooting, it seems kube-proxy's udp socket dead.\n\nIt stops reading from socket. I don't know how to re-pro yet. All other tcp sockets are working normally. I am trying to trouble shoot this, but I has no knowledge on how to debug a running golang process.\niptables:\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-PORTALS-CONTAINER  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */\nDOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL\nKUBE-NODEPORT-CONTAINER  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */\n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-PORTALS-HOST  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */\nDOCKER     all  --  anywhere            !127.0.0.0/8          ADDRTYPE match dst-type LOCAL\nKUBE-NODEPORT-HOST  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */\n\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nMASQUERADE  all  --  172.16.70.0/24       anywhere\n\nChain DOCKER (2 references)\ntarget     prot opt source               destination\n\nChain KUBE-NODEPORT-CONTAINER (1 references)\ntarget     prot opt source               destination\nREDIRECT   tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 redir ports 46087\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 redir ports 48104\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 redir ports 42337\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 redir ports 32768\n\nChain KUBE-NODEPORT-HOST (1 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 to:192.168.1.30:46087\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 to:192.168.1.30:48104\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 to:192.168.1.30:42337\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 to:192.168.1.30:32768\n\nChain KUBE-PORTALS-CONTAINER (1 references)\ntarget     prot opt source               destination\nREDIRECT   tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https redir ports 37726\nREDIRECT   tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 redir ports 46087\nREDIRECT   udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain redir ports 38046\nREDIRECT   tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain redir ports 47397\nREDIRECT   tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http redir ports 48270\nREDIRECT   tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http redir ports 40069\nREDIRECT   tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http redir ports 54624\nREDIRECT   tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 redir ports 33381\nREDIRECT   tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 redir ports 41223\nREDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 redir ports 52521\nREDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 redir ports 45287\nREDIRECT   tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 redir ports 48104\nREDIRECT   tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 redir ports 42337\nREDIRECT   tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 redir ports 32768\n\nChain KUBE-PORTALS-HOST (1 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https to:192.168.1.30:37726\nDNAT       tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 to:192.168.1.30:46087\nDNAT       udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain to:192.168.1.30:38046\nDNAT       tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain to:192.168.1.30:47397\nDNAT       tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http to:192.168.1.30:48270\nDNAT       tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http to:192.168.1.30:40069\nDNAT       tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http to:192.168.1.30:54624\nDNAT       tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 to:192.168.1.30:33381\nDNAT       tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 to:192.168.1.30:41223\nDNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 to:192.168.1.30:52521\nDNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 to:192.168.1.30:45287\nDNAT       tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 to:192.168.1.30:48104\nDNAT       tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 to:192.168.1.30:42337\nDNAT       tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 to:192.168.1.30:32768\n\nNothing interesting in logs. So I just skip them.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "78",
      "number": "13709",
      "pretext": "EDIT: kubernetes version 1.0.3\nOne of our kubernete machine not able to connect to dns server, and after some trouble shooting, it seems kube-proxy's udp socket dead.\n\nIt stops reading from socket. I don't know how to re-pro yet. All other tcp sockets are working normally. I am trying to trouble shoot this, but I has no knowledge on how to debug a running golang process.\niptables:\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-PORTALS-CONTAINER  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */\nDOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL\nKUBE-NODEPORT-CONTAINER  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */\n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-PORTALS-HOST  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */\nDOCKER     all  --  anywhere            !127.0.0.0/8          ADDRTYPE match dst-type LOCAL\nKUBE-NODEPORT-HOST  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */\n\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nMASQUERADE  all  --  172.16.70.0/24       anywhere\n\nChain DOCKER (2 references)\ntarget     prot opt source               destination\n\nChain KUBE-NODEPORT-CONTAINER (1 references)\ntarget     prot opt source               destination\nREDIRECT   tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 redir ports 46087\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 redir ports 48104\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 redir ports 42337\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 redir ports 32768\n\nChain KUBE-NODEPORT-HOST (1 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 to:192.168.1.30:46087\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 to:192.168.1.30:48104\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 to:192.168.1.30:42337\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 to:192.168.1.30:32768\n\nChain KUBE-PORTALS-CONTAINER (1 references)\ntarget     prot opt source               destination\nREDIRECT   tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https redir ports 37726\nREDIRECT   tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 redir ports 46087\nREDIRECT   udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain redir ports 38046\nREDIRECT   tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain redir ports 47397\nREDIRECT   tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http redir ports 48270\nREDIRECT   tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http redir ports 40069\nREDIRECT   tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http redir ports 54624\nREDIRECT   tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 redir ports 33381\nREDIRECT   tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 redir ports 41223\nREDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 redir ports 52521\nREDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 redir ports 45287\nREDIRECT   tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 redir ports 48104\nREDIRECT   tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 redir ports 42337\nREDIRECT   tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 redir ports 32768\n\nChain KUBE-PORTALS-HOST (1 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https to:192.168.1.30:37726\nDNAT       tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 to:192.168.1.30:46087\nDNAT       udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain to:192.168.1.30:38046\nDNAT       tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain to:192.168.1.30:47397\nDNAT       tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http to:192.168.1.30:48270\nDNAT       tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http to:192.168.1.30:40069\nDNAT       tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http to:192.168.1.30:54624\nDNAT       tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 to:192.168.1.30:33381\nDNAT       tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 to:192.168.1.30:41223\nDNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 to:192.168.1.30:52521\nDNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 to:192.168.1.30:45287\nDNAT       tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 to:192.168.1.30:48104\nDNAT       tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 to:192.168.1.30:42337\nDNAT       tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 to:192.168.1.30:32768\n\nNothing interesting in logs. So I just skip them.",
      "title": "kube-proxy's udp proxy dead"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2659,
    "text": "kubernetes-e2e-gke-staging: broken test runFailed: https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke-staging/6333/\nRun so broken it didn't make JUnit output!",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "79",
      "number": "30962",
      "pretext": "Failed: https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke-staging/6333/\nRun so broken it didn't make JUnit output!",
      "title": "kubernetes-e2e-gke-staging: broken test run"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2660,
    "text": "make clean ; make WHAT=test/e2e/e2e.test\" failswhen building e2e_test.go using 'make WHAT=test/e2e/e2e' fails with:\n(20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test\nbuild/make-clean.sh\n+++ [0525 20:52:03] Verifying Prerequisites....\n+++ [0525 20:52:03] Cleaning out local _output directory\nrm -rf _output\nrm -rf Godeps/_workspace/pkg\nhack/build-go.sh test/e2e/e2e.test\n+++ [0525 20:52:04] Building go targets for darwin/amd64:\ntest/e2e/e2e.test\n/Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh: line 367: pushd: /Users/bc/dev/git/t/kubernetes/_output/local/go/bin: No such file or directory\n!!! Error in /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361\n'pushd \"$(dirname ${outfile})\" > /dev/null' exited with status 1\nCall stack:\n1: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361 kube::golang::build_binaries_for_platform(...)\n2: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:488 kube::golang::build_binaries(...)\n3: hack/build-go.sh:26 main(...)\nExiting with status 1\nmake: *** [all] Error 1\nHowever 'make all; make (20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test' works",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "80",
      "number": "8792",
      "pretext": "when building e2e_test.go using 'make WHAT=test/e2e/e2e' fails with:\n(20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test\nbuild/make-clean.sh\n+++ [0525 20:52:03] Verifying Prerequisites....\n+++ [0525 20:52:03] Cleaning out local _output directory\nrm -rf _output\nrm -rf Godeps/_workspace/pkg\nhack/build-go.sh test/e2e/e2e.test\n+++ [0525 20:52:04] Building go targets for darwin/amd64:\ntest/e2e/e2e.test\n/Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh: line 367: pushd: /Users/bc/dev/git/t/kubernetes/_output/local/go/bin: No such file or directory\n!!! Error in /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361\n'pushd \"$(dirname ${outfile})\" > /dev/null' exited with status 1\nCall stack:\n1: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361 kube::golang::build_binaries_for_platform(...)\n2: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:488 kube::golang::build_binaries(...)\n3: hack/build-go.sh:26 main(...)\nExiting with status 1\nmake: *** [all] Error 1\nHowever 'make all; make (20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test' works",
      "title": "make clean ; make WHAT=test/e2e/e2e.test\" fails"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2661,
    "text": "Pods hang in pending state indefinitelyI've been working with a 6 node cluster for the last few weeks without issue. Earlier today we ran into an open file issue (https://github.com/kubernetes/kubernetes/pull/12443/files) and I patched and restarted kube-proxy. Since then, all pods deployed to all BUT node-01 get stuck in pending state and there log messages stating the cause.\nI've taking a look at both the following similar issues and they don't appear to be the cause\n#4891\n#3185\nCluster is running v1.0.3\nHere's an example of the state\ndocker run --rm -it lachie83/kubectl:prod get pods --namespace=kube-system -o wide\nNAME                READY     STATUS    RESTARTS   AGE       NODE\nkube-dns-v8-i0yac   0/4       Pending   0          4s        10.1.1.35\nkube-dns-v8-jti2e   0/4       Pending   0          4s        10.1.1.34\n\nget events\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-i0yac\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-i0yac                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-i0yac to 10.1.1.35\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-jti2e                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-jti2e to 10.1.1.34\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-jti2e\n\nscheduler log\nI0916 06:25:42.897814   10076 event.go:203] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"kube-dns-v8-jti2e\", UID:\"c1cafebe-5c3b-11e5-b3c4-020443b6797d\", APIVersion:\"v1\", ResourceVersion:\"670117\", FieldPath:\"\"}): reason: 'scheduled' Successfully assigned kube-dns-v8-jti2e to 10.1.1.34\nI0916 06:25:42.904195   10076 event.go:203] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"kube-dns-v8-i0yac\", UID:\"c1cafc69-5c3b-11e5-b3c4-020443b6797d\", APIVersion:\"v1\", ResourceVersion:\"670118\", FieldPath:\"\"}): reason: 'scheduled' Successfully assigned kube-dns-v8-i0yac to 10.1.1.35\n\ntailing kubelet log file during pod create\ntail -f kubelet.kube-node-03.root.log.INFO.20150916-060744.10668\nI0916 06:25:04.448916   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:25:24.449253   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:25:44.449522   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:04.449774   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:24.450400   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:44.450995   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:04.451501   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:24.451910   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:44.452511   10668 config.go:253] Setting pods for source file : {[] 0 file}\n\nkubelet process\nroot@kube-node-03:/var/log/kubernetes# ps -ef | grep kubelet\nroot     10668     1  1 06:07 ?        00:00:13 /opt/bin/kubelet --address=10.1.1.34 --port=10250 --hostname_override=10.1.1.34 --api_servers=https://kube-master-01.sj.lithium.com:6443 --logtostderr=false --log_dir=/var/log/kubernetes --cluster_dns=10.1.2.53 --config=/etc/kubelet/conf --cluster_domain=prod-kube-sjc1-1.internal --v=4 --tls-cert-file=/etc/kubelet/certs/kubelet.pem --tls-private-key-file=/etc/kubelet/certs/kubelet-key.pem\n\nnode list\ndocker run --rm -it lachie83/kubectl:prod get nodes\nNAME            LABELS                                             STATUS\n10.1.1.30   kubernetes.io/hostname=10.1.1.30,name=node-1   Ready\n10.1.1.32   kubernetes.io/hostname=10.1.1.32,name=node-2   Ready\n10.1.1.34   kubernetes.io/hostname=10.1.1.34,name=node-3   Ready\n10.1.1.35   kubernetes.io/hostname=10.1.1.35,name=node-4   Ready\n10.1.1.42   kubernetes.io/hostname=10.1.1.42,name=node-5   Ready\n10.1.1.43   kubernetes.io/hostname=10.1.1.43,name=node-6   Ready",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "81",
      "number": "14023",
      "pretext": "I've been working with a 6 node cluster for the last few weeks without issue. Earlier today we ran into an open file issue (https://github.com/kubernetes/kubernetes/pull/12443/files) and I patched and restarted kube-proxy. Since then, all pods deployed to all BUT node-01 get stuck in pending state and there log messages stating the cause.\nI've taking a look at both the following similar issues and they don't appear to be the cause\n#4891\n#3185\nCluster is running v1.0.3\nHere's an example of the state\ndocker run --rm -it lachie83/kubectl:prod get pods --namespace=kube-system -o wide\nNAME                READY     STATUS    RESTARTS   AGE       NODE\nkube-dns-v8-i0yac   0/4       Pending   0          4s        10.1.1.35\nkube-dns-v8-jti2e   0/4       Pending   0          4s        10.1.1.34\n\nget events\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-i0yac\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-i0yac                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-i0yac to 10.1.1.35\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-jti2e                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-jti2e to 10.1.1.34\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-jti2e\n\nscheduler log\nI0916 06:25:42.897814   10076 event.go:203] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"kube-dns-v8-jti2e\", UID:\"c1cafebe-5c3b-11e5-b3c4-020443b6797d\", APIVersion:\"v1\", ResourceVersion:\"670117\", FieldPath:\"\"}): reason: 'scheduled' Successfully assigned kube-dns-v8-jti2e to 10.1.1.34\nI0916 06:25:42.904195   10076 event.go:203] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"kube-dns-v8-i0yac\", UID:\"c1cafc69-5c3b-11e5-b3c4-020443b6797d\", APIVersion:\"v1\", ResourceVersion:\"670118\", FieldPath:\"\"}): reason: 'scheduled' Successfully assigned kube-dns-v8-i0yac to 10.1.1.35\n\ntailing kubelet log file during pod create\ntail -f kubelet.kube-node-03.root.log.INFO.20150916-060744.10668\nI0916 06:25:04.448916   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:25:24.449253   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:25:44.449522   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:04.449774   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:24.450400   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:44.450995   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:04.451501   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:24.451910   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:44.452511   10668 config.go:253] Setting pods for source file : {[] 0 file}\n\nkubelet process\nroot@kube-node-03:/var/log/kubernetes# ps -ef | grep kubelet\nroot     10668     1  1 06:07 ?        00:00:13 /opt/bin/kubelet --address=10.1.1.34 --port=10250 --hostname_override=10.1.1.34 --api_servers=https://kube-master-01.sj.lithium.com:6443 --logtostderr=false --log_dir=/var/log/kubernetes --cluster_dns=10.1.2.53 --config=/etc/kubelet/conf --cluster_domain=prod-kube-sjc1-1.internal --v=4 --tls-cert-file=/etc/kubelet/certs/kubelet.pem --tls-private-key-file=/etc/kubelet/certs/kubelet-key.pem\n\nnode list\ndocker run --rm -it lachie83/kubectl:prod get nodes\nNAME            LABELS                                             STATUS\n10.1.1.30   kubernetes.io/hostname=10.1.1.30,name=node-1   Ready\n10.1.1.32   kubernetes.io/hostname=10.1.1.32,name=node-2   Ready\n10.1.1.34   kubernetes.io/hostname=10.1.1.34,name=node-3   Ready\n10.1.1.35   kubernetes.io/hostname=10.1.1.35,name=node-4   Ready\n10.1.1.42   kubernetes.io/hostname=10.1.1.42,name=node-5   Ready\n10.1.1.43   kubernetes.io/hostname=10.1.1.43,name=node-6   Ready",
      "title": "Pods hang in pending state indefinitely"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2662,
    "text": "Fix wait.Until tests to actually verify something other than \"not hanging\"In review for #26301, it was pointed out that my copy-pasta test was verifying one thing, but then the second half of the test is more of the \"run this code and hope it doesn't hang\" variety.\nFollow-up issue.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "82",
      "number": "26304",
      "pretext": "In review for #26301, it was pointed out that my copy-pasta test was verifying one thing, but then the second half of the test is more of the \"run this code and hope it doesn't hang\" variety.\nFollow-up issue.",
      "title": "Fix wait.Until tests to actually verify something other than \"not hanging\""
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2663,
    "text": "Document how to add a new API groupAFAIK, we don't have a documentation on how to add a new API group. Currently we have #16621 and #13146 that try to add a new API group. I will draft a guide to ease the future endeavor of adding groups. And probably by writing this guide, I will see how can we make the API group machinery easier to use.\ncc @mikedanese @timstclair @lavalamp",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "83",
      "number": "16626",
      "pretext": "AFAIK, we don't have a documentation on how to add a new API group. Currently we have #16621 and #13146 that try to add a new API group. I will draft a guide to ease the future endeavor of adding groups. And probably by writing this guide, I will see how can we make the API group machinery easier to use.\ncc @mikedanese @timstclair @lavalamp",
      "title": "Document how to add a new API group"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2664,
    "text": "Add a user guide readme for deployments",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "84",
      "number": "16124",
      "pretext": "",
      "title": "Add a user guide readme for deployments"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2665,
    "text": "Investigate possibly broken Docker SIGTERM delivery@rsokolowski and I found the following behavior last week when we were working with someone who was trying to run some stuff on Kubernetes. Note that this appears to be a Docker problem, not a Kubernetes problem, but I'm filing it here for now so we can verify that it's reproducible before filing a bug against Docker.\nStart two containers on the same machine, each a shell script that traps SIGTERM and prints something like “I received SIGTERM” and quits. Then do “docker stop” and observe that one container exits via the SIGTERM handler path and the other just gets SIGKILL.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "85",
      "number": "5585",
      "pretext": "@rsokolowski and I found the following behavior last week when we were working with someone who was trying to run some stuff on Kubernetes. Note that this appears to be a Docker problem, not a Kubernetes problem, but I'm filing it here for now so we can verify that it's reproducible before filing a bug against Docker.\nStart two containers on the same machine, each a shell script that traps SIGTERM and prints something like “I received SIGTERM” and quits. Then do “docker stop” and observe that one container exits via the SIGTERM handler path and the other just gets SIGKILL.",
      "title": "Investigate possibly broken Docker SIGTERM delivery"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2666,
    "text": "Loosen label and annotation validation and related testsFrom #4486\nProposed:\n\nlabel values: restrict to [A-Za-z0-9_-.]*\nannotation values: no restriction\nqualified names (label and annotation keys, resource names, volume plugins): loosen restrictions to ([A-Za-z0-9_-.]+/)?[A-Za-z0-9_-.]+ - this should be a strict superset of\nwhat we allow today.  We can say that convention is to use dns-compatible\ndomain + label, but still allow things like FOO/B_A.R.  This DOES NOT allow for foo.com/bat/bat - is that an important affordance to anyone?  We could allow that, but we need to be clear that \"bar/bat\" means (bar, bat) while \"foo.com/bar/bat\" means (foo.com, bar/bat).\n\n@smarterclayton does that give you enough freedom?\n@bgrant0607 does that give you enough consistency?\n@quinton-hoole-google does this make the qualified name type now viable for your use case in kube-proxy?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "86",
      "number": "4628",
      "pretext": "From #4486\nProposed:\n\nlabel values: restrict to [A-Za-z0-9_-.]*\nannotation values: no restriction\nqualified names (label and annotation keys, resource names, volume plugins): loosen restrictions to ([A-Za-z0-9_-.]+/)?[A-Za-z0-9_-.]+ - this should be a strict superset of\nwhat we allow today.  We can say that convention is to use dns-compatible\ndomain + label, but still allow things like FOO/B_A.R.  This DOES NOT allow for foo.com/bat/bat - is that an important affordance to anyone?  We could allow that, but we need to be clear that \"bar/bat\" means (bar, bat) while \"foo.com/bar/bat\" means (foo.com, bar/bat).\n\n@smarterclayton does that give you enough freedom?\n@bgrant0607 does that give you enough consistency?\n@quinton-hoole-google does this make the qualified name type now viable for your use case in kube-proxy?",
      "title": "Loosen label and annotation validation and related tests"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2667,
    "text": "`validate-cluster.sh` should be using `/validate`: wants `kubectl healthy`?We have a /validate endpoint that GKE has been using to validate the health of running clusters after the API server is available, similar to the kube-up.sh flow of validate-cluster.sh. There's no reason that validate-cluster.sh should actually do this work manually anymore - we should just go through common source. We should probably introduce a kubectl healthy or kubectl healthcheck?",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "87",
      "number": "5946",
      "pretext": "We have a /validate endpoint that GKE has been using to validate the health of running clusters after the API server is available, similar to the kube-up.sh flow of validate-cluster.sh. There's no reason that validate-cluster.sh should actually do this work manually anymore - we should just go through common source. We should probably introduce a kubectl healthy or kubectl healthcheck?",
      "title": "`validate-cluster.sh` should be using `/validate`: wants `kubectl healthy`?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2668,
    "text": "Running k8s locally via Docker & 1.2.0-alpha.6 does not stand up apiserverFollowing the all-in-one install via kubelet on Docker as listed in the docs is not working. I hit a \"no cloud provider specified\" and the controller and apiserver never come up and am plagued with many connection refused errors when hitting 127.0.0.1:8080 even though I set KUBERNETES_PROVIDER https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md#run-it\nI'm running on an Ubuntu VM and don't need cloud provider resources like the LoadBalancer type for Services - I just want a local dev k8s environment.\nHere is a snippet of the logs for the mod that enables nsenter (seeing how there were issues with Secrets not working in hyperkube - see #19069 for related info )\n\ndefault\ndocker-compose file:\nmaster:\n  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\n  net: host\n  pid: host\n  privileged: true\n  volumes:\n    - /:/rootfs:ro\n    - /sys:/sys:ro\n    - /dev:/dev\n    - /var/lib/docker/:/var/lib/docker:rw\n    - /var/lib/kubelet/:/var/lib/kubelet:rw\n    - /var/run:/var/run:rw\n  command: ['./hyperkube', 'kubelet', '--containerized', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local', '--allow-privileged=true', '--v=10']\n\nlogs:\nAttaching to aiok8s_master_1\n[36mmaster_1 | [0mI0202 21:15:53.489776    6562 server.go:132] Running kubelet in containerized mode (experimental)\n[36mmaster_1 | [0mI0202 21:15:54.248665    6562 server.go:351] Using self-signed cert (/var/run/kubernetes/kubelet.crt, /var/run/kubernetes/kubelet.key)\n[36mmaster_1 | [0mW0202 21:15:54.248985    6562 server.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.\n[36mmaster_1 | [0mW0202 21:15:54.249026    6562 server.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.\n[36mmaster_1 | [0mI0202 21:15:54.249194    6562 plugins.go:71] No cloud provider specified.\n[36mmaster_1 | [0mI0202 21:15:54.249212    6562 server.go:289] Successfully initialized cloud provider: \"\" from the config file: \"\"\n[36mmaster_1 | [0mI0202 21:15:54.249332    6562 manager.go:128] cAdvisor running in container: \"/docker-daemon/docker/808781db3c41aa36cafc5229b705611b694d581a5c661f03094a875dd9a8ff6e\"\n[36mmaster_1 | [0mI0202 21:15:54.465357    6562 fs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/rootfs major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/rootfs/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/rootfs/mystore major:202 minor:66 fsType: blockSize:0}]\n[36mmaster_1 | [0mI0202 21:15:54.494458    6562 machine.go:50] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\n[36mmaster_1 | [0mI0202 21:15:54.494534    6562 manager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID: SystemUUID:7ae5ca5c485b47b88d89d96a71601d80 BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968} {Device:/dev/xvda1 Capacity:42140499968}] DiskMap:map[43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline} 43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\n[36mmaster_1 | [0mI0202 21:15:54.495665    6562 manager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\n[36mmaster_1 | [0mI0202 21:15:54.496462    6562 server.go:312] Using root directory: /var/lib/kubelet\n[36mmaster_1 | [0mI0202 21:15:54.496691    6562 server.go:571] Sending events to api server.\n[36mmaster_1 | [0mI0202 21:15:54.496862    6562 server.go:636] Adding manifest file: etc/kubernetes/manifests\n[36mmaster_1 | [0mI0202 21:15:54.496911    6562 file.go:47] Watching path \"etc/kubernetes/manifests\"\n[36mmaster_1 | [0mI0202 21:15:54.496925    6562 server.go:646] Watching apiserver\n[36mmaster_1 | [0mI0202 21:15:54.497132    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/etcd.json\"\n[36mmaster_1 | [0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0\n[36mmaster_1 | [0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/services?resourceVersion=0\n[36mmaster_1 | [0mI0202 21:15:54.501520    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0\n[36mmaster_1 | [0mI0202 21:15:54.501968    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0  in 1 milliseconds\n[36mmaster_1 | [0mI0202 21:15:54.502198    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:54.502042    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/services?resourceVersion=0  in 1 milliseconds\n[36mmaster_1 | [0mI0202 21:15:54.502666    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:54.502225    6562 common.go:52] Generated UID \"e171033ec56ced6f29a4417f8b61cbf0\" pod \"k8s-etcd\" from etc/kubernetes/manifests/etcd.json\n[36mmaster_1 | [0mI0202 21:15:54.502880    6562 common.go:56] Generated Name \"k8s-etcd-127.0.0.1\" for UID \"e171033ec56ced6f29a4417f8b61cbf0\" from URL etc/kubernetes/manifests/etcd.json\n[36mmaster_1 | [0mI0202 21:15:54.502892    6562 common.go:61] Using namespace \"default\" for pod \"k8s-etcd-127.0.0.1\" from etc/kubernetes/manifests/etcd.json\n[36mmaster_1 | [0mI0202 21:15:54.503109    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/kube-proxy.json\"\n[36mmaster_1 | [0mI0202 21:15:54.503393    6562 common.go:52] Generated UID \"e513e62ea641585218de8b3495fc7a21\" pod \"k8s-proxy\" from etc/kubernetes/manifests/kube-proxy.json\n[36mmaster_1 | [0mI0202 21:15:54.503419    6562 common.go:56] Generated Name \"k8s-proxy-127.0.0.1\" for UID \"e513e62ea641585218de8b3495fc7a21\" from URL etc/kubernetes/manifests/kube-proxy.json\n[36mmaster_1 | [0mI0202 21:15:54.503429    6562 common.go:61] Using namespace \"default\" for pod \"k8s-proxy-127.0.0.1\" from etc/kubernetes/manifests/kube-proxy.json\n[36mmaster_1 | [0mI0202 21:15:54.503514    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/master.json\"\n[36mmaster_1 | [0mI0202 21:15:54.503983    6562 common.go:52] Generated UID \"eaf8ef5e21f965406a198841c5faa403\" pod \"k8s-master\" from etc/kubernetes/manifests/master.json\n[36mmaster_1 | [0mI0202 21:15:54.504018    6562 common.go:56] Generated Name \"k8s-master-127.0.0.1\" for UID \"eaf8ef5e21f965406a198841c5faa403\" from URL etc/kubernetes/manifests/master.json\n[36mmaster_1 | [0mI0202 21:15:54.504029    6562 common.go:61] Using namespace \"default\" for pod \"k8s-master-127.0.0.1\" from etc/kubernetes/manifests/master.json\n[36mmaster_1 | [0mI0202 21:15:54.504172    6562 config.go:265] Setting pods for source file\n[36mmaster_1 | [0mI0202 21:15:54.504433    6562 config.go:412] Receiving a new pod \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\"\n[36mmaster_1 | [0mI0202 21:15:54.504492    6562 config.go:412] Receiving a new pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\"\n[36mmaster_1 | [0mI0202 21:15:54.504507    6562 config.go:412] Receiving a new pod \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n[36mmaster_1 | [0mI0202 21:15:54.505051    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0  in 3 milliseconds\n[36mmaster_1 | [0mI0202 21:15:54.505072    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:54.708688    6562 manager.go:191] Setting dockerRoot to /var/lib/docker\n[36mmaster_1 | [0mI0202 21:15:54.759597    6562 plugins.go:56] Registering credential provider: .dockercfg\n[36mmaster_1 | [0mI0202 21:15:54.772006    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/aws-ebs\"\n[36mmaster_1 | [0mI0202 21:15:54.772056    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/empty-dir\"\n[36mmaster_1 | [0mI0202 21:15:54.772081    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/gce-pd\"\n[36mmaster_1 | [0mI0202 21:15:54.772103    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/git-repo\"\n[36mmaster_1 | [0mI0202 21:15:54.772125    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/host-path\"\n[36mmaster_1 | [0mI0202 21:15:54.772149    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/nfs\"\n[36mmaster_1 | [0mI0202 21:15:54.772170    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/secret\"\n[36mmaster_1 | [0mI0202 21:15:54.772185    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/iscsi\"\n[36mmaster_1 | [0mI0202 21:15:54.772204    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/glusterfs\"\n[36mmaster_1 | [0mI0202 21:15:54.772227    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/persistent-claim\"\n[36mmaster_1 | [0mI0202 21:15:54.772242    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/rbd\"\n[36mmaster_1 | [0mI0202 21:15:54.772263    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/cinder\"\n[36mmaster_1 | [0mI0202 21:15:54.772284    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/cephfs\"\n[36mmaster_1 | [0mI0202 21:15:54.772327    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/downward-api\"\n[36mmaster_1 | [0mI0202 21:15:54.772351    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/fc\"\n[36mmaster_1 | [0mI0202 21:15:54.772373    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/flocker\"\n[36mmaster_1 | [0mI0202 21:15:54.772498    6562 server.go:608] Started kubelet\n[36mmaster_1 | [0mI0202 21:15:54.772569    6562 server.go:104] Starting to listen on 0.0.0.0:10250\n[36mmaster_1 | [0mE0202 21:15:54.772810    6562 kubelet.go:868] Image garbage collection failed: unable to find data for container /\n[36mmaster_1 | [0mI0202 21:15:54.773100    6562 request.go:546] Request Body: {\"kind\":\"Event\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"127.0.0.1.142f3c86e792e0e6\",\"namespace\":\"default\",\"creationTimestamp\":null},\"involvedObject\":{\"kind\":\"Node\",\"name\":\"127.0.0.1\",\"uid\":\"127.0.0.1\"},\"reason\":\"Starting\",\"message\":\"Starting kubelet.\",\"source\":{\"component\":\"kubelet\",\"host\":\"127.0.0.1\"},\"firstTimestamp\":\"2016-02-02T21:15:54Z\",\"lastTimestamp\":\"2016-02-02T21:15:54Z\",\"count\":1,\"type\":\"Normal\"}\n[36mmaster_1 | [0mI0202 21:15:54.773187    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" -H \"Content-Type: application/json\" http://localhost:8080/api/v1/namespaces/default/events\n[36mmaster_1 | [0mI0202 21:15:54.773216    6562 server.go:121] Starting to listen read-only on 0.0.0.0:10255\n[36mmaster_1 | [0mI0202 21:15:54.773351    6562 server.go:569] Event(api.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"127.0.0.1\", UID:\"127.0.0.1\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'Starting' Starting kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.773845    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/events  in 0 milliseconds\n[36mmaster_1 | [0mI0202 21:15:54.773867    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mE0202 21:15:54.774060    6562 event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\n[36mmaster_1 | [0mI0202 21:15:54.781503    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781526    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781535    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781542    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781549    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781556    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781562    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781569    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781575    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781582    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781589    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.784869    6562 kubelet.go:898] Running in container \"/kubelet\"\n[36mmaster_1 | [0mI0202 21:15:54.788967    6562 container_manager_linux.go:207] Configure resource-only container /docker-daemon with memory limit: 11043009331\n[36mmaster_1 | [0mI0202 21:15:54.789028    6562 manager.go:124] Starting to sync pod status with apiserver\n[36mmaster_1 | [0mI0202 21:15:54.789054    6562 kubelet.go:2246] Starting kubelet main sync loop.\n[36mmaster_1 | [0mI0202 21:15:54.792327    6562 generic.go:106] GenericPLEG: Relisting\n[36mmaster_1 | [0mI0202 21:15:54.819530    6562 kubelet.go:2278] SyncLoop (ADD, \"file\"): \"\"\n[36mmaster_1 | [0mI0202 21:15:54.820696    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820720    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820728    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820735    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820743    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820749    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820756    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820763    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820770    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820776    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820783    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820971    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.820996    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821006    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821016    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821024    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/b98145a18fd89935c205528d910e094b6c178ec657f1be1637f922949dc2cd7d: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821034    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/3baa25cf80a8301be5f04d31d181b17023c23ed0e4c7a36df72f174b8b74896f: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821042    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/e94c8a2adb00d32038bde655ce418d482fe3810b81465e26395b2f2abb767047: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821051    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/8da0f1f1d20e5f3b00b0b0bfa994e8682a7b48f294453c15455fa88b1d6ca90b: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821060    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f98816a30a176e1dc9da2d980e4f961e2fffa056d109fe099af5dc6233edc540: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821070    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/c3f9389f06f5c520d0e57a197b284d5af182dad8f6d1f90e179a22edbbf6e132: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821079    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/6882bb8bc6e182316fe58110d3586738e9479a288604fe73396176151595024d: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821088    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/ea35c7a7f264f71ff0c7fc8d724cda79d272994f4dcb841d08944c6dc39d3528: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821096    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/351e59a4c698f8412365798aa8fce50e9029c22d3065f9531e2d9e4be5e3720c: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821105    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821116    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821126    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821134    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821144    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821154    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/6ba5339964aec260e37272fb2d67597a170f2bd203e09e3b7736b72684814f31: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821162    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/e45b33bd1c357e7a18b8f73ba755c568e7563aa854b3351595e0fadbb59b5748: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821171    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/54b9ec78c213e0309daf88c259138a3d4bf5e1ed6309bd63503c666f65105064: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821180    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/5a3cdba1e62db46b6c5440c443439571bfeb5720032c251e3ce7aaac764fec10: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.851415    6562 kubelet.go:2301] SyncLoop (PLEG): ignore irrelevant event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f\"}\n[36mmaster_1 | [0mI0202 21:15:54.880172    6562 kubelet.go:2281] SyncLoop (UPDATE, \"file\"): \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0), k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21), k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n[36mmaster_1 | [0mI0202 21:15:54.952533    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18\"}\n[36mmaster_1 | [0mI0202 21:15:55.029929    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.030066    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\"\n[36mmaster_1 | [0mI0202 21:15:55.030158    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n[36mmaster_1 | [0mI0202 21:15:55.030284    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\"\n[36mmaster_1 | [0mI0202 21:15:55.031309    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-proxy-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default\",\"uid\":\"e513e62ea641585218de8b3495fc7a21\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"e513e62ea641585218de8b3495fc7a21\",\"kubernetes.io/config.mirror\":\"e513e62ea641585218de8b3495fc7a21\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504500123Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"containers\":[{\"name\":\"kube-proxy\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"proxy\",\"--master=http://127.0.0.1:8080\",\"--v=2\",\"--resource-container=\\\"\\\"\"],\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\",\"securityContext\":{\"privileged\":true}}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n[36mmaster_1 | [0mI0202 21:15:55.031354    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-etcd-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-etcd-127.0.0.1/default\",\"uid\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"kubernetes.io/config.mirror\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504450902Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"volumes\":[{\"name\":\"varetcd\",\"emptyDir\":{}}],\"containers\":[{\"name\":\"etcd\",\"image\":\"gcr.io/google_containers/etcd:2.2.1\",\"command\":[\"/usr/local/bin/etcd\",\"--listen-client-urls=http://127.0.0.1:4001\",\"--advertise-client-urls=http://127.0.0.1:4001\",\"--data-dir=/var/etcd/data\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"varetcd\",\"mountPath\":\"/var/etcd\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n[36mmaster_1 | [0mI0202 21:15:55.031372    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" -H \"Content-Type: application/json\" http://localhost:8080/api/v1/namespaces/default/pods\n[36mmaster_1 | [0mI0202 21:15:55.031378    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-master-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-master-127.0.0.1/default\",\"uid\":\"eaf8ef5e21f965406a198841c5faa403\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"eaf8ef5e21f965406a198841c5faa403\",\"kubernetes.io/config.mirror\":\"eaf8ef5e21f965406a198841c5faa403\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504513882Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"volumes\":[{\"name\":\"data\",\"emptyDir\":{}}],\"containers\":[{\"name\":\"controller-manager\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"controller-manager\",\"--master=127.0.0.1:8080\",\"--min-resync-period=3m\",\"--service-account-private-key-file=/srv/kubernetes/server.key\",\"--root-ca-file=/srv/kubernetes/ca.crt\",\"--v=2\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/srv/kubernetes\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"apiserver\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"apiserver\",\"--service-cluster-ip-range=10.0.0.1/24\",\"--insecure-bind-address=127.0.0.1\",\"--etcd-servers=http://127.0.0.1:4001\",\"--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota\",\"--client-ca-file=/srv/kubernetes/ca.crt\",\"--basic-auth-file=/srv/kubernetes/basic_auth.csv\",\"--min-request-timeout=300\",\"--tls-cert-file=/srv/kubernetes/server.cert\",\"--tls-private-key-file=/srv/kubernetes/server.key\",\"--token-auth-file=/srv/kubernetes/known_tokens.csv\",\"--allow-privileged=true\",\"--v=4\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/srv/kubernetes\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"scheduler\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"scheduler\",\"--master=127.0.0.1:8080\",\"--v=2\"],\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"setup\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/setup-files.sh\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/data\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n[36mmaster_1 | [0mI0202 21:15:55.031432    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"Content-Type: application/json\" -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/namespaces/default/pods\n[36mmaster_1 | [0mI0202 21:15:55.031439    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"Content-Type: application/json\" -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/namespaces/default/pods\n[36mmaster_1 | [0mI0202 21:15:55.031799    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n[36mmaster_1 | [0mI0202 21:15:55.031802    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n[36mmaster_1 | [0mI0202 21:15:55.031814    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:55.031819    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:55.031800    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n[36mmaster_1 | [0mI0202 21:15:55.031841    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mE0202 21:15:55.033798    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n[36mmaster_1 | [0mE0202 21:15:55.033828    6562 kubelet.go:1626] Mirror pod not available\n[36mmaster_1 | [0mE0202 21:15:55.033824    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n[36mmaster_1 | [0mE0202 21:15:55.033852    6562 kubelet.go:1626] Mirror pod not available\n[36mmaster_1 | [0mE0202 21:15:55.033930    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n[36mmaster_1 | [0mE0202 21:15:55.033960    6562 kubelet.go:1626] Mirror pod not available\n[36mmaster_1 | [0mI0202 21:15:55.034347    6562 volumes.go:114] Used volume plugin \"kubernetes.io/empty-dir\" for varetcd\n[36mmaster_1 | [0mI0202 21:15:55.034395    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd]\n[36mmaster_1 | [0mE0202 21:15:55.044950    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1\n[36mmaster_1 | [0mI0202 21:15:55.046068    6562 volumes.go:114] Used volume plugin \"kubernetes.io/empty-dir\" for data\n[36mmaster_1 | [0mI0202 21:15:55.046118    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data]\n[36mmaster_1 | [0mE0202 21:15:55.053737    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1\n[36mmaster_1 | [0mI0202 21:15:55.078701    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078723    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078731    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078738    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078745    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078752    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078758    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078765    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078772    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078778    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078785    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.079802    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.141312    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05\"}\n[36mmaster_1 | [0mI0202 21:15:55.171516    6562 manager.go:339] Container inspect result: {ID:40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Created:2016-02-02 19:45:35.144413858 +0000 UTC Path:/usr/local/bin/etcd Args:[--listen-client-urls=http://127.0.0.1:4001 --advertise-client-urls=http://127.0.0.1:4001 --data-dir=/var/etcd/data] Config:0xc20824b380 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:772 ExitCode:0 Error: StartedAt:2016-02-02 19:45:35.353220983 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:fbea2d67e6339d5aac386091030eb8c5bd7c82e9f0a3d29d4254dd4ed6f725d5 Node:<nil> NetworkSettings:0xc20866f300 SysInitPath: ResolvConfPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/resolv.conf HostnamePath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hostname HostsPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hosts LogPath:/var/lib/docker/containers/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540-json.log Name:/k8s_etcd.7e452b0b_k8s-etcd-127.0.0.1_default_e171033ec56ced6f29a4417f8b61cbf0_361132f4 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd Destination:/var/etcd Mode: RW:true} {Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/containers/etcd/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208318280 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n[36mmaster_1 | [0mI0202 21:15:55.171926    6562 manager.go:339] Container inspect result: {ID:1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Created:2016-02-02 21:12:56.37330813 +0000 UTC Path:/hyperkube Args:[proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=\"\"] Config:0xc20876e340 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4969 ExitCode:0 Error: StartedAt:2016-02-02 21:12:56.66700108 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc208338800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03-json.log Name:/k8s_kube-proxy.4e393dc3_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_07ffcf9b Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e513e62ea641585218de8b3495fc7a21/containers/kube-proxy/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc2084cd180 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n[36mmaster_1 | [0mI0202 21:15:55.172408    6562 manager.go:339] Container inspect result: {ID:db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Created:2016-02-02 21:14:59.392327351 +0000 UTC Path:/setup-files.sh Args:[] Config:0xc2084bd1e0 State:{Running:false Paused:false Restarting:false OOMKilled:false Pid:0 ExitCode:1 Error: StartedAt:2016-02-02 21:14:59.601273593 +0000 UTC FinishedAt:2016-02-02 21:15:00.138896081 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc20846c800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/resolv.conf HostnamePath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hostname HostsPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hosts LogPath:/var/lib/docker/containers/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f-json.log Name:/k8s_setup.7df70a9a_k8s-master-127.0.0.1_default_eaf8ef5e21f965406a198841c5faa403_f5904769 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data Destination:/data Mode: RW:true} {Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/containers/setup/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208206500 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n[36mmaster_1 | [0mI0202 21:15:55.175011    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.231588    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28\"}\n[36mmaster_1 | [0mI0202 21:15:55.232542    6562 manager.go:339] Container inspect result: {ID:212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455 Created:2016-02-02 21:12:55.664721048 +0000 UTC Path:/pause Args:[] Config:0xc208036820 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4952 ExitCode:0 Error: StartedAt:2016-02-02 21:12:55.935180921 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:8950680a606cf7a0b7916dbf4a435b35d28d75c705999847eddb5ed38eb53204 Node:<nil> NetworkSettings:0xc20866f500 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455-json.log Name:/k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_32c906a6 Driver:aufs Mounts:[] Volumes:map[] VolumesRW:map[] HostConfig:0xc208319180 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n[36mmaster_1 | [0mI0202 21:15:55.232693    6562 manager.go:1630] Syncing Pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\": &{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:k8s-proxy-127.0.0.1 GenerateName: Namespace:default SelfLink:/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default UID:e513e62ea641585218de8b3495fc7a21 ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[kubernetes.io/config.source:file kubernetes.io/config.seen:2016-02-02T21:15:54.504500123Z kubernetes.io/config.hash:e513e62ea641585218de8b3495fc7a21]} Spec:{Volumes:[] Containers:[{Name:kube-proxy Image:gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6 Command:[/hyperkube proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=\"\"] Args:[] WorkingDir: Ports:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[] LivenessProbe:<nil> ReadinessProbe:<nil> Lifecycle:<nil> TerminationMessagePath:/dev/termination-log ImagePullPolicy:IfNotPresent SecurityContext:0xc208422600 Stdin:false StdinOnce:false TTY:false}] RestartPolicy:Always TerminationGracePeriodSeconds:0xc20841d548 ActiveDeadlineSeconds:<nil> DNSPolicy:ClusterFirst NodeSelector:map[] ServiceAccountName: NodeName:127.0.0.1 SecurityContext:0xc2083b8080 ImagePullSecrets:[]} Status:{Phase: Conditions:[] Message: Reason: HostIP: PodIP: StartTime:<nil> ContainerStatuses:[]}}\n\n\nwith nsenter modification\ndocker-compose file:\nmaster:\n  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\n  net: host\n  pid: host\n  privileged: true\n  volumes:\n    - /sys:/sys:ro\n    - /dev:/dev\n    - /var/lib/docker/:/var/lib/docker:rw\n    - /var/lib/kubelet/:/var/lib/kubelet:rw\n    - /var/run:/var/run:rw\n  command: ['nsenter', '--target=1', '--mount', '--wd=.', '--', './hyperkube', 'kubelet', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local']\n\nlogs:\nAttaching to aiok8s_master_1\nserver.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.\nserver.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.\nplugins.go:71] No cloud provider specified.\nmanager.go:128] cAdvisor running in container: \"/docker-daemon/docker/307029613f92612249c358cfcb9796055c4d264503e63f3806dac25175a10177\"\nfs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/ major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/mystore major:202 minor:66 fsType: blockSize:0}]\nmachine.go:93] Failed to get system UUID: open /etc/machine-id: no such file or directory\nmanager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID:1463833d523b452349b56f17534ffabe SystemUUID: BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvda1 Capacity:42140499968} {Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968}] DiskMap:map[43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\nmanager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Ubuntu 14.04.3 LTS DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\nserver.go:636] Adding manifest file: etc/kubernetes/manifests\nserver.go:646] Watching apiserver\nmanager.go:191] Setting dockerRoot to /var/lib/docker\nplugins.go:56] Registering credential provider: .dockercfg\nserver.go:608] Started kubelet\nkubelet.go:868] Image garbage collection failed: unable to find data for container /\nserver.go:104] Starting to listen on 0.0.0.0:10250\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nkubelet.go:898] Running in container \"/kubelet\"\nmanager.go:124] Starting to sync pod status with apiserver\nkubelet.go:2246] Starting kubelet main sync loop.\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nfactory.go:245] Registering Docker factory\nfactory.go:94] Registering Raw factory\nmanager.go:1005] Started watching for new ooms in manager\noomparser.go:198] OOM parser using kernel log file: \"/var/log/kern.log\"\nmanager.go:249] Starting recovery of all containers\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:254] Recovery completed\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 1\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=controller-manager pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 3\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "88",
      "number": "20514",
      "pretext": "Following the all-in-one install via kubelet on Docker as listed in the docs is not working. I hit a \"no cloud provider specified\" and the controller and apiserver never come up and am plagued with many connection refused errors when hitting 127.0.0.1:8080 even though I set KUBERNETES_PROVIDER https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md#run-it\nI'm running on an Ubuntu VM and don't need cloud provider resources like the LoadBalancer type for Services - I just want a local dev k8s environment.\nHere is a snippet of the logs for the mod that enables nsenter (seeing how there were issues with Secrets not working in hyperkube - see #19069 for related info )\n\ndefault\ndocker-compose file:\nmaster:\n  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\n  net: host\n  pid: host\n  privileged: true\n  volumes:\n    - /:/rootfs:ro\n    - /sys:/sys:ro\n    - /dev:/dev\n    - /var/lib/docker/:/var/lib/docker:rw\n    - /var/lib/kubelet/:/var/lib/kubelet:rw\n    - /var/run:/var/run:rw\n  command: ['./hyperkube', 'kubelet', '--containerized', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local', '--allow-privileged=true', '--v=10']\n\nlogs:\nAttaching to aiok8s_master_1\n[36mmaster_1 | [0mI0202 21:15:53.489776    6562 server.go:132] Running kubelet in containerized mode (experimental)\n[36mmaster_1 | [0mI0202 21:15:54.248665    6562 server.go:351] Using self-signed cert (/var/run/kubernetes/kubelet.crt, /var/run/kubernetes/kubelet.key)\n[36mmaster_1 | [0mW0202 21:15:54.248985    6562 server.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.\n[36mmaster_1 | [0mW0202 21:15:54.249026    6562 server.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.\n[36mmaster_1 | [0mI0202 21:15:54.249194    6562 plugins.go:71] No cloud provider specified.\n[36mmaster_1 | [0mI0202 21:15:54.249212    6562 server.go:289] Successfully initialized cloud provider: \"\" from the config file: \"\"\n[36mmaster_1 | [0mI0202 21:15:54.249332    6562 manager.go:128] cAdvisor running in container: \"/docker-daemon/docker/808781db3c41aa36cafc5229b705611b694d581a5c661f03094a875dd9a8ff6e\"\n[36mmaster_1 | [0mI0202 21:15:54.465357    6562 fs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/rootfs major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/rootfs/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/rootfs/mystore major:202 minor:66 fsType: blockSize:0}]\n[36mmaster_1 | [0mI0202 21:15:54.494458    6562 machine.go:50] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\n[36mmaster_1 | [0mI0202 21:15:54.494534    6562 manager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID: SystemUUID:7ae5ca5c485b47b88d89d96a71601d80 BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968} {Device:/dev/xvda1 Capacity:42140499968}] DiskMap:map[43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline} 43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\n[36mmaster_1 | [0mI0202 21:15:54.495665    6562 manager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\n[36mmaster_1 | [0mI0202 21:15:54.496462    6562 server.go:312] Using root directory: /var/lib/kubelet\n[36mmaster_1 | [0mI0202 21:15:54.496691    6562 server.go:571] Sending events to api server.\n[36mmaster_1 | [0mI0202 21:15:54.496862    6562 server.go:636] Adding manifest file: etc/kubernetes/manifests\n[36mmaster_1 | [0mI0202 21:15:54.496911    6562 file.go:47] Watching path \"etc/kubernetes/manifests\"\n[36mmaster_1 | [0mI0202 21:15:54.496925    6562 server.go:646] Watching apiserver\n[36mmaster_1 | [0mI0202 21:15:54.497132    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/etcd.json\"\n[36mmaster_1 | [0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0\n[36mmaster_1 | [0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/services?resourceVersion=0\n[36mmaster_1 | [0mI0202 21:15:54.501520    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0\n[36mmaster_1 | [0mI0202 21:15:54.501968    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0  in 1 milliseconds\n[36mmaster_1 | [0mI0202 21:15:54.502198    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:54.502042    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/services?resourceVersion=0  in 1 milliseconds\n[36mmaster_1 | [0mI0202 21:15:54.502666    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:54.502225    6562 common.go:52] Generated UID \"e171033ec56ced6f29a4417f8b61cbf0\" pod \"k8s-etcd\" from etc/kubernetes/manifests/etcd.json\n[36mmaster_1 | [0mI0202 21:15:54.502880    6562 common.go:56] Generated Name \"k8s-etcd-127.0.0.1\" for UID \"e171033ec56ced6f29a4417f8b61cbf0\" from URL etc/kubernetes/manifests/etcd.json\n[36mmaster_1 | [0mI0202 21:15:54.502892    6562 common.go:61] Using namespace \"default\" for pod \"k8s-etcd-127.0.0.1\" from etc/kubernetes/manifests/etcd.json\n[36mmaster_1 | [0mI0202 21:15:54.503109    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/kube-proxy.json\"\n[36mmaster_1 | [0mI0202 21:15:54.503393    6562 common.go:52] Generated UID \"e513e62ea641585218de8b3495fc7a21\" pod \"k8s-proxy\" from etc/kubernetes/manifests/kube-proxy.json\n[36mmaster_1 | [0mI0202 21:15:54.503419    6562 common.go:56] Generated Name \"k8s-proxy-127.0.0.1\" for UID \"e513e62ea641585218de8b3495fc7a21\" from URL etc/kubernetes/manifests/kube-proxy.json\n[36mmaster_1 | [0mI0202 21:15:54.503429    6562 common.go:61] Using namespace \"default\" for pod \"k8s-proxy-127.0.0.1\" from etc/kubernetes/manifests/kube-proxy.json\n[36mmaster_1 | [0mI0202 21:15:54.503514    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/master.json\"\n[36mmaster_1 | [0mI0202 21:15:54.503983    6562 common.go:52] Generated UID \"eaf8ef5e21f965406a198841c5faa403\" pod \"k8s-master\" from etc/kubernetes/manifests/master.json\n[36mmaster_1 | [0mI0202 21:15:54.504018    6562 common.go:56] Generated Name \"k8s-master-127.0.0.1\" for UID \"eaf8ef5e21f965406a198841c5faa403\" from URL etc/kubernetes/manifests/master.json\n[36mmaster_1 | [0mI0202 21:15:54.504029    6562 common.go:61] Using namespace \"default\" for pod \"k8s-master-127.0.0.1\" from etc/kubernetes/manifests/master.json\n[36mmaster_1 | [0mI0202 21:15:54.504172    6562 config.go:265] Setting pods for source file\n[36mmaster_1 | [0mI0202 21:15:54.504433    6562 config.go:412] Receiving a new pod \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\"\n[36mmaster_1 | [0mI0202 21:15:54.504492    6562 config.go:412] Receiving a new pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\"\n[36mmaster_1 | [0mI0202 21:15:54.504507    6562 config.go:412] Receiving a new pod \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n[36mmaster_1 | [0mI0202 21:15:54.505051    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0  in 3 milliseconds\n[36mmaster_1 | [0mI0202 21:15:54.505072    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:54.708688    6562 manager.go:191] Setting dockerRoot to /var/lib/docker\n[36mmaster_1 | [0mI0202 21:15:54.759597    6562 plugins.go:56] Registering credential provider: .dockercfg\n[36mmaster_1 | [0mI0202 21:15:54.772006    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/aws-ebs\"\n[36mmaster_1 | [0mI0202 21:15:54.772056    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/empty-dir\"\n[36mmaster_1 | [0mI0202 21:15:54.772081    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/gce-pd\"\n[36mmaster_1 | [0mI0202 21:15:54.772103    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/git-repo\"\n[36mmaster_1 | [0mI0202 21:15:54.772125    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/host-path\"\n[36mmaster_1 | [0mI0202 21:15:54.772149    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/nfs\"\n[36mmaster_1 | [0mI0202 21:15:54.772170    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/secret\"\n[36mmaster_1 | [0mI0202 21:15:54.772185    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/iscsi\"\n[36mmaster_1 | [0mI0202 21:15:54.772204    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/glusterfs\"\n[36mmaster_1 | [0mI0202 21:15:54.772227    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/persistent-claim\"\n[36mmaster_1 | [0mI0202 21:15:54.772242    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/rbd\"\n[36mmaster_1 | [0mI0202 21:15:54.772263    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/cinder\"\n[36mmaster_1 | [0mI0202 21:15:54.772284    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/cephfs\"\n[36mmaster_1 | [0mI0202 21:15:54.772327    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/downward-api\"\n[36mmaster_1 | [0mI0202 21:15:54.772351    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/fc\"\n[36mmaster_1 | [0mI0202 21:15:54.772373    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/flocker\"\n[36mmaster_1 | [0mI0202 21:15:54.772498    6562 server.go:608] Started kubelet\n[36mmaster_1 | [0mI0202 21:15:54.772569    6562 server.go:104] Starting to listen on 0.0.0.0:10250\n[36mmaster_1 | [0mE0202 21:15:54.772810    6562 kubelet.go:868] Image garbage collection failed: unable to find data for container /\n[36mmaster_1 | [0mI0202 21:15:54.773100    6562 request.go:546] Request Body: {\"kind\":\"Event\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"127.0.0.1.142f3c86e792e0e6\",\"namespace\":\"default\",\"creationTimestamp\":null},\"involvedObject\":{\"kind\":\"Node\",\"name\":\"127.0.0.1\",\"uid\":\"127.0.0.1\"},\"reason\":\"Starting\",\"message\":\"Starting kubelet.\",\"source\":{\"component\":\"kubelet\",\"host\":\"127.0.0.1\"},\"firstTimestamp\":\"2016-02-02T21:15:54Z\",\"lastTimestamp\":\"2016-02-02T21:15:54Z\",\"count\":1,\"type\":\"Normal\"}\n[36mmaster_1 | [0mI0202 21:15:54.773187    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" -H \"Content-Type: application/json\" http://localhost:8080/api/v1/namespaces/default/events\n[36mmaster_1 | [0mI0202 21:15:54.773216    6562 server.go:121] Starting to listen read-only on 0.0.0.0:10255\n[36mmaster_1 | [0mI0202 21:15:54.773351    6562 server.go:569] Event(api.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"127.0.0.1\", UID:\"127.0.0.1\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'Starting' Starting kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.773845    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/events  in 0 milliseconds\n[36mmaster_1 | [0mI0202 21:15:54.773867    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mE0202 21:15:54.774060    6562 event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\n[36mmaster_1 | [0mI0202 21:15:54.781503    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781526    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781535    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781542    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781549    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781556    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781562    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781569    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781575    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781582    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.781589    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.784869    6562 kubelet.go:898] Running in container \"/kubelet\"\n[36mmaster_1 | [0mI0202 21:15:54.788967    6562 container_manager_linux.go:207] Configure resource-only container /docker-daemon with memory limit: 11043009331\n[36mmaster_1 | [0mI0202 21:15:54.789028    6562 manager.go:124] Starting to sync pod status with apiserver\n[36mmaster_1 | [0mI0202 21:15:54.789054    6562 kubelet.go:2246] Starting kubelet main sync loop.\n[36mmaster_1 | [0mI0202 21:15:54.792327    6562 generic.go:106] GenericPLEG: Relisting\n[36mmaster_1 | [0mI0202 21:15:54.819530    6562 kubelet.go:2278] SyncLoop (ADD, \"file\"): \"\"\n[36mmaster_1 | [0mI0202 21:15:54.820696    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820720    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820728    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820735    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820743    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820749    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820756    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820763    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820770    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820776    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820783    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:54.820971    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.820996    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821006    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821016    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821024    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/b98145a18fd89935c205528d910e094b6c178ec657f1be1637f922949dc2cd7d: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821034    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/3baa25cf80a8301be5f04d31d181b17023c23ed0e4c7a36df72f174b8b74896f: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821042    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/e94c8a2adb00d32038bde655ce418d482fe3810b81465e26395b2f2abb767047: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821051    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/8da0f1f1d20e5f3b00b0b0bfa994e8682a7b48f294453c15455fa88b1d6ca90b: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821060    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f98816a30a176e1dc9da2d980e4f961e2fffa056d109fe099af5dc6233edc540: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821070    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/c3f9389f06f5c520d0e57a197b284d5af182dad8f6d1f90e179a22edbbf6e132: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821079    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/6882bb8bc6e182316fe58110d3586738e9479a288604fe73396176151595024d: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821088    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/ea35c7a7f264f71ff0c7fc8d724cda79d272994f4dcb841d08944c6dc39d3528: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821096    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/351e59a4c698f8412365798aa8fce50e9029c22d3065f9531e2d9e4be5e3720c: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821105    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821116    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821126    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821134    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821144    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15: unknown -> running\n[36mmaster_1 | [0mI0202 21:15:54.821154    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/6ba5339964aec260e37272fb2d67597a170f2bd203e09e3b7736b72684814f31: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821162    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/e45b33bd1c357e7a18b8f73ba755c568e7563aa854b3351595e0fadbb59b5748: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821171    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/54b9ec78c213e0309daf88c259138a3d4bf5e1ed6309bd63503c666f65105064: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.821180    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/5a3cdba1e62db46b6c5440c443439571bfeb5720032c251e3ce7aaac764fec10: unknown -> exited\n[36mmaster_1 | [0mI0202 21:15:54.851415    6562 kubelet.go:2301] SyncLoop (PLEG): ignore irrelevant event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f\"}\n[36mmaster_1 | [0mI0202 21:15:54.880172    6562 kubelet.go:2281] SyncLoop (UPDATE, \"file\"): \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0), k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21), k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n[36mmaster_1 | [0mI0202 21:15:54.952533    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18\"}\n[36mmaster_1 | [0mI0202 21:15:55.029929    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.030066    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\"\n[36mmaster_1 | [0mI0202 21:15:55.030158    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n[36mmaster_1 | [0mI0202 21:15:55.030284    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\"\n[36mmaster_1 | [0mI0202 21:15:55.031309    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-proxy-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default\",\"uid\":\"e513e62ea641585218de8b3495fc7a21\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"e513e62ea641585218de8b3495fc7a21\",\"kubernetes.io/config.mirror\":\"e513e62ea641585218de8b3495fc7a21\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504500123Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"containers\":[{\"name\":\"kube-proxy\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"proxy\",\"--master=http://127.0.0.1:8080\",\"--v=2\",\"--resource-container=\\\"\\\"\"],\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\",\"securityContext\":{\"privileged\":true}}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n[36mmaster_1 | [0mI0202 21:15:55.031354    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-etcd-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-etcd-127.0.0.1/default\",\"uid\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"kubernetes.io/config.mirror\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504450902Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"volumes\":[{\"name\":\"varetcd\",\"emptyDir\":{}}],\"containers\":[{\"name\":\"etcd\",\"image\":\"gcr.io/google_containers/etcd:2.2.1\",\"command\":[\"/usr/local/bin/etcd\",\"--listen-client-urls=http://127.0.0.1:4001\",\"--advertise-client-urls=http://127.0.0.1:4001\",\"--data-dir=/var/etcd/data\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"varetcd\",\"mountPath\":\"/var/etcd\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n[36mmaster_1 | [0mI0202 21:15:55.031372    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" -H \"Content-Type: application/json\" http://localhost:8080/api/v1/namespaces/default/pods\n[36mmaster_1 | [0mI0202 21:15:55.031378    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-master-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-master-127.0.0.1/default\",\"uid\":\"eaf8ef5e21f965406a198841c5faa403\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"eaf8ef5e21f965406a198841c5faa403\",\"kubernetes.io/config.mirror\":\"eaf8ef5e21f965406a198841c5faa403\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504513882Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"volumes\":[{\"name\":\"data\",\"emptyDir\":{}}],\"containers\":[{\"name\":\"controller-manager\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"controller-manager\",\"--master=127.0.0.1:8080\",\"--min-resync-period=3m\",\"--service-account-private-key-file=/srv/kubernetes/server.key\",\"--root-ca-file=/srv/kubernetes/ca.crt\",\"--v=2\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/srv/kubernetes\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"apiserver\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"apiserver\",\"--service-cluster-ip-range=10.0.0.1/24\",\"--insecure-bind-address=127.0.0.1\",\"--etcd-servers=http://127.0.0.1:4001\",\"--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota\",\"--client-ca-file=/srv/kubernetes/ca.crt\",\"--basic-auth-file=/srv/kubernetes/basic_auth.csv\",\"--min-request-timeout=300\",\"--tls-cert-file=/srv/kubernetes/server.cert\",\"--tls-private-key-file=/srv/kubernetes/server.key\",\"--token-auth-file=/srv/kubernetes/known_tokens.csv\",\"--allow-privileged=true\",\"--v=4\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/srv/kubernetes\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"scheduler\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"scheduler\",\"--master=127.0.0.1:8080\",\"--v=2\"],\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"setup\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/setup-files.sh\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/data\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n[36mmaster_1 | [0mI0202 21:15:55.031432    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"Content-Type: application/json\" -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/namespaces/default/pods\n[36mmaster_1 | [0mI0202 21:15:55.031439    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"Content-Type: application/json\" -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/namespaces/default/pods\n[36mmaster_1 | [0mI0202 21:15:55.031799    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n[36mmaster_1 | [0mI0202 21:15:55.031802    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n[36mmaster_1 | [0mI0202 21:15:55.031814    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:55.031819    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mI0202 21:15:55.031800    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n[36mmaster_1 | [0mI0202 21:15:55.031841    6562 round_trippers.go:286] Response Headers:\n[36mmaster_1 | [0mE0202 21:15:55.033798    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n[36mmaster_1 | [0mE0202 21:15:55.033828    6562 kubelet.go:1626] Mirror pod not available\n[36mmaster_1 | [0mE0202 21:15:55.033824    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n[36mmaster_1 | [0mE0202 21:15:55.033852    6562 kubelet.go:1626] Mirror pod not available\n[36mmaster_1 | [0mE0202 21:15:55.033930    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n[36mmaster_1 | [0mE0202 21:15:55.033960    6562 kubelet.go:1626] Mirror pod not available\n[36mmaster_1 | [0mI0202 21:15:55.034347    6562 volumes.go:114] Used volume plugin \"kubernetes.io/empty-dir\" for varetcd\n[36mmaster_1 | [0mI0202 21:15:55.034395    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd]\n[36mmaster_1 | [0mE0202 21:15:55.044950    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1\n[36mmaster_1 | [0mI0202 21:15:55.046068    6562 volumes.go:114] Used volume plugin \"kubernetes.io/empty-dir\" for data\n[36mmaster_1 | [0mI0202 21:15:55.046118    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data]\n[36mmaster_1 | [0mE0202 21:15:55.053737    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1\n[36mmaster_1 | [0mI0202 21:15:55.078701    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078723    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078731    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078738    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078745    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078752    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078758    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078765    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078772    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078778    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.078785    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.079802    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.141312    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05\"}\n[36mmaster_1 | [0mI0202 21:15:55.171516    6562 manager.go:339] Container inspect result: {ID:40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Created:2016-02-02 19:45:35.144413858 +0000 UTC Path:/usr/local/bin/etcd Args:[--listen-client-urls=http://127.0.0.1:4001 --advertise-client-urls=http://127.0.0.1:4001 --data-dir=/var/etcd/data] Config:0xc20824b380 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:772 ExitCode:0 Error: StartedAt:2016-02-02 19:45:35.353220983 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:fbea2d67e6339d5aac386091030eb8c5bd7c82e9f0a3d29d4254dd4ed6f725d5 Node:<nil> NetworkSettings:0xc20866f300 SysInitPath: ResolvConfPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/resolv.conf HostnamePath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hostname HostsPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hosts LogPath:/var/lib/docker/containers/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540-json.log Name:/k8s_etcd.7e452b0b_k8s-etcd-127.0.0.1_default_e171033ec56ced6f29a4417f8b61cbf0_361132f4 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd Destination:/var/etcd Mode: RW:true} {Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/containers/etcd/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208318280 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n[36mmaster_1 | [0mI0202 21:15:55.171926    6562 manager.go:339] Container inspect result: {ID:1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Created:2016-02-02 21:12:56.37330813 +0000 UTC Path:/hyperkube Args:[proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=\"\"] Config:0xc20876e340 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4969 ExitCode:0 Error: StartedAt:2016-02-02 21:12:56.66700108 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc208338800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03-json.log Name:/k8s_kube-proxy.4e393dc3_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_07ffcf9b Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e513e62ea641585218de8b3495fc7a21/containers/kube-proxy/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc2084cd180 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n[36mmaster_1 | [0mI0202 21:15:55.172408    6562 manager.go:339] Container inspect result: {ID:db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Created:2016-02-02 21:14:59.392327351 +0000 UTC Path:/setup-files.sh Args:[] Config:0xc2084bd1e0 State:{Running:false Paused:false Restarting:false OOMKilled:false Pid:0 ExitCode:1 Error: StartedAt:2016-02-02 21:14:59.601273593 +0000 UTC FinishedAt:2016-02-02 21:15:00.138896081 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc20846c800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/resolv.conf HostnamePath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hostname HostsPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hosts LogPath:/var/lib/docker/containers/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f-json.log Name:/k8s_setup.7df70a9a_k8s-master-127.0.0.1_default_eaf8ef5e21f965406a198841c5faa403_f5904769 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data Destination:/data Mode: RW:true} {Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/containers/setup/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208206500 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n[36mmaster_1 | [0mI0202 21:15:55.175011    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n[36mmaster_1 | [0mI0202 21:15:55.231588    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28\"}\n[36mmaster_1 | [0mI0202 21:15:55.232542    6562 manager.go:339] Container inspect result: {ID:212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455 Created:2016-02-02 21:12:55.664721048 +0000 UTC Path:/pause Args:[] Config:0xc208036820 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4952 ExitCode:0 Error: StartedAt:2016-02-02 21:12:55.935180921 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:8950680a606cf7a0b7916dbf4a435b35d28d75c705999847eddb5ed38eb53204 Node:<nil> NetworkSettings:0xc20866f500 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455-json.log Name:/k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_32c906a6 Driver:aufs Mounts:[] Volumes:map[] VolumesRW:map[] HostConfig:0xc208319180 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n[36mmaster_1 | [0mI0202 21:15:55.232693    6562 manager.go:1630] Syncing Pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\": &{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:k8s-proxy-127.0.0.1 GenerateName: Namespace:default SelfLink:/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default UID:e513e62ea641585218de8b3495fc7a21 ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[kubernetes.io/config.source:file kubernetes.io/config.seen:2016-02-02T21:15:54.504500123Z kubernetes.io/config.hash:e513e62ea641585218de8b3495fc7a21]} Spec:{Volumes:[] Containers:[{Name:kube-proxy Image:gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6 Command:[/hyperkube proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=\"\"] Args:[] WorkingDir: Ports:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[] LivenessProbe:<nil> ReadinessProbe:<nil> Lifecycle:<nil> TerminationMessagePath:/dev/termination-log ImagePullPolicy:IfNotPresent SecurityContext:0xc208422600 Stdin:false StdinOnce:false TTY:false}] RestartPolicy:Always TerminationGracePeriodSeconds:0xc20841d548 ActiveDeadlineSeconds:<nil> DNSPolicy:ClusterFirst NodeSelector:map[] ServiceAccountName: NodeName:127.0.0.1 SecurityContext:0xc2083b8080 ImagePullSecrets:[]} Status:{Phase: Conditions:[] Message: Reason: HostIP: PodIP: StartTime:<nil> ContainerStatuses:[]}}\n\n\nwith nsenter modification\ndocker-compose file:\nmaster:\n  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\n  net: host\n  pid: host\n  privileged: true\n  volumes:\n    - /sys:/sys:ro\n    - /dev:/dev\n    - /var/lib/docker/:/var/lib/docker:rw\n    - /var/lib/kubelet/:/var/lib/kubelet:rw\n    - /var/run:/var/run:rw\n  command: ['nsenter', '--target=1', '--mount', '--wd=.', '--', './hyperkube', 'kubelet', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local']\n\nlogs:\nAttaching to aiok8s_master_1\nserver.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.\nserver.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.\nplugins.go:71] No cloud provider specified.\nmanager.go:128] cAdvisor running in container: \"/docker-daemon/docker/307029613f92612249c358cfcb9796055c4d264503e63f3806dac25175a10177\"\nfs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/ major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/mystore major:202 minor:66 fsType: blockSize:0}]\nmachine.go:93] Failed to get system UUID: open /etc/machine-id: no such file or directory\nmanager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID:1463833d523b452349b56f17534ffabe SystemUUID: BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvda1 Capacity:42140499968} {Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968}] DiskMap:map[43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\nmanager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Ubuntu 14.04.3 LTS DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\nserver.go:636] Adding manifest file: etc/kubernetes/manifests\nserver.go:646] Watching apiserver\nmanager.go:191] Setting dockerRoot to /var/lib/docker\nplugins.go:56] Registering credential provider: .dockercfg\nserver.go:608] Started kubelet\nkubelet.go:868] Image garbage collection failed: unable to find data for container /\nserver.go:104] Starting to listen on 0.0.0.0:10250\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nkubelet.go:898] Running in container \"/kubelet\"\nmanager.go:124] Starting to sync pod status with apiserver\nkubelet.go:2246] Starting kubelet main sync loop.\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nfactory.go:245] Registering Docker factory\nfactory.go:94] Registering Raw factory\nmanager.go:1005] Started watching for new ooms in manager\noomparser.go:198] OOM parser using kernel log file: \"/var/log/kern.log\"\nmanager.go:249] Starting recovery of all containers\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:254] Recovery completed\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 1\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=controller-manager pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 3\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available",
      "title": "Running k8s locally via Docker & 1.2.0-alpha.6 does not stand up apiserver"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2669,
    "text": "Cross links dont work in api reference docs when viewed using htmlpreviewFor ex: links in https://htmlpreview.github.io/?https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/api-reference/extensions/v1beta1/definitions.html dont work when they point to docs outside this one.\nLinks work fine when the same doc is viewed on kubernetes.io (ex: http://kubernetes.io/v1.0/docs/api-reference/definitions.html).\nLooks like a bug in htmlpreview. Need to figure out a way to fix this.\ncc @caesarxuchao",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "89",
      "number": "15910",
      "pretext": "For ex: links in https://htmlpreview.github.io/?https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/api-reference/extensions/v1beta1/definitions.html dont work when they point to docs outside this one.\nLinks work fine when the same doc is viewed on kubernetes.io (ex: http://kubernetes.io/v1.0/docs/api-reference/definitions.html).\nLooks like a bug in htmlpreview. Need to figure out a way to fix this.\ncc @caesarxuchao",
      "title": "Cross links dont work in api reference docs when viewed using htmlpreview"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2670,
    "text": "Add Ubernetes Lite e2e test scheduling pods with attached volumes into correct zonePart of #19413",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "90",
      "number": "19416",
      "pretext": "Part of #19413",
      "title": "Add Ubernetes Lite e2e test scheduling pods with attached volumes into correct zone"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2671,
    "text": "DeepHashObject should probably set DisableMethods and DisablePointerMethods for spewFor max consistency.  Some String() methods might hash collide.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "91",
      "number": "6265",
      "pretext": "For max consistency.  Some String() methods might hash collide.",
      "title": "DeepHashObject should probably set DisableMethods and DisablePointerMethods for spew"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2672,
    "text": "Service Name Resolution is not workingHi There\nI have a ubuntu multi-node cluster set up - running. I am able to create pods, controllers and services at will. But I am facing the issue where service name resolution is not working. As part of the guestbook example, I have created redis-slave, redis-master and frontend services. But when containers are trying to talk to each other through this service interface, it is not working.\nE.g\n\nWhen PHP application try and talk to redis-slave:6379 - it fails (This call dont get routed any where)\nWhen redis slave try to synch with master using redis-master , it doesnt work.\nWhen I try and ping redis-master from inside the container, it comes back with unknown host error.\n\nSome logs to show you the problem\n[8] 10 Apr 00:03:57.902 # Unable to connect to MASTER: Connection timed out\n[8] 10 Apr 00:03:58.905 * Connecting to MASTER redis-master:6379\n[8] 10 Apr 00:03:58.909 # Unable to connect to MASTER: Connection timed out\n[8] 10 Apr 00:03:59.913 * Connecting to MASTER redis-master:6379\nTemporary failure in name resolution [tcp://redis-slave:6379]' in /vendor/predis/predis/lib/Predis/Connection/AbstractConnection.php:141\nI do see Kube-Proxy is making the right iptable entries.\nI0409 13:04:47.690367   20353 proxier.go:556] Opened iptables from-containers portal for service \"redis-master\" on TCP 11.1.1.67:6379\nI0409 13:04:47.696276   20353 proxier.go:567] Opened iptables from-host portal for service \"redis-master\" on TCP 11.1.1.67:6379\nI0409 13:11:50.223702   20353 proxier.go:556] Opened iptables from-containers portal for service \"redis-slave\" on TCP 11.1.1.55:6379\nI0409 13:11:50.252093   20353 proxier.go:567] Opened iptables from-host portal for service \"redis-slave\" on TCP 11.1.1.55:6379\nI0409 13:14:17.024773   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 11.1.1.58:8000\nI0409 13:14:17.034296   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 11.1.1.58:8000\nI0409 13:14:17.046845   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 10.64.80.83:8000\nI0409 13:14:17.057679   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 10.64.80.83:8000\nI0409 13:14:17.067562   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 10.64.80.84:8000\nI0409 13:14:17.077085   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 10.64.80.84:8000\nDo you know what is missing in the set up which is causing this problem ? I do not have any DNS (skydns) running here and think that it is desirable but not needed.\nWhat might be going wrong here ?",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "92",
      "number": "6667",
      "pretext": "Hi There\nI have a ubuntu multi-node cluster set up - running. I am able to create pods, controllers and services at will. But I am facing the issue where service name resolution is not working. As part of the guestbook example, I have created redis-slave, redis-master and frontend services. But when containers are trying to talk to each other through this service interface, it is not working.\nE.g\n\nWhen PHP application try and talk to redis-slave:6379 - it fails (This call dont get routed any where)\nWhen redis slave try to synch with master using redis-master , it doesnt work.\nWhen I try and ping redis-master from inside the container, it comes back with unknown host error.\n\nSome logs to show you the problem\n[8] 10 Apr 00:03:57.902 # Unable to connect to MASTER: Connection timed out\n[8] 10 Apr 00:03:58.905 * Connecting to MASTER redis-master:6379\n[8] 10 Apr 00:03:58.909 # Unable to connect to MASTER: Connection timed out\n[8] 10 Apr 00:03:59.913 * Connecting to MASTER redis-master:6379\nTemporary failure in name resolution [tcp://redis-slave:6379]' in /vendor/predis/predis/lib/Predis/Connection/AbstractConnection.php:141\nI do see Kube-Proxy is making the right iptable entries.\nI0409 13:04:47.690367   20353 proxier.go:556] Opened iptables from-containers portal for service \"redis-master\" on TCP 11.1.1.67:6379\nI0409 13:04:47.696276   20353 proxier.go:567] Opened iptables from-host portal for service \"redis-master\" on TCP 11.1.1.67:6379\nI0409 13:11:50.223702   20353 proxier.go:556] Opened iptables from-containers portal for service \"redis-slave\" on TCP 11.1.1.55:6379\nI0409 13:11:50.252093   20353 proxier.go:567] Opened iptables from-host portal for service \"redis-slave\" on TCP 11.1.1.55:6379\nI0409 13:14:17.024773   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 11.1.1.58:8000\nI0409 13:14:17.034296   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 11.1.1.58:8000\nI0409 13:14:17.046845   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 10.64.80.83:8000\nI0409 13:14:17.057679   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 10.64.80.83:8000\nI0409 13:14:17.067562   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 10.64.80.84:8000\nI0409 13:14:17.077085   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 10.64.80.84:8000\nDo you know what is missing in the set up which is causing this problem ? I do not have any DNS (skydns) running here and think that it is desirable but not needed.\nWhat might be going wrong here ?",
      "title": "Service Name Resolution is not working"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2673,
    "text": "vagrant-based cluster is brokenI took the last version from github several days ago, installed without any problems\nhost machine\n[root@linux-c56a cluster]$ ./kubectl.sh version\nClient Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.1\", GitCommit:\"9bae6636d5fcf436a3b14d219ced195e10e21fe8\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.1\", GitCommit:\"9bae6636d5fcf436a3b14d219ced195e10e21fe8\", GitTreeState:\"clean\"}\n\nminion-1, everything is fine\n[root@kubernetes-minion-1 vagrant]# docker ps\nCONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n6603d4e83556        gcr.io/google_containers/heapster_grafana:v0.7    \"/kuisp -p 8080 -c /o\"   8 minutes ago       Up 8 minutes                                                         k8s_grafana.d03a4af7_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_576e5681\nc8fcb0448ffa        gcr.io/google_containers/heapster_influxdb:v0.3   \"/run.sh\"                9 minutes ago       Up 9 minutes                                                         k8s_influxdb.e4e63a11_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_4b7be40b\n097522fb0214        gcr.io/google_containers/kube2sky:1.11            \"/kube2sky -domain=cl\"   10 minutes ago      Up 10 minutes                                                        k8s_kube2sky.9e5dd6c0_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_88c205c6\ncd70ece47d5f        gcr.io/google_containers/etcd:2.0.9               \"/usr/local/bin/etcd \"   10 minutes ago      Up 10 minutes                                                        k8s_etcd.f2082b87_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_2fac6c01\nb2065f47fd00        gcr.io/google_containers/exechealthz:1.0          \"/exechealthz '-cmd=n\"   10 minutes ago      Up 10 minutes                                                        k8s_healthz.75190edc_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a9976982\ne328d461b7c2        gcr.io/google_containers/skydns:2015-03-11-001    \"/skydns -machines=ht\"   10 minutes ago      Up 10 minutes                                                        k8s_skydns.8afa9a39_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_9444aa84\n336b74f063c9        gcr.io/google_containers/heapster:v0.17.0         \"/heapster --source=k\"   10 minutes ago      Up 10 minutes                                                        k8s_heapster.f0daed14_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_b0e636eb\nf9584b644df5        gcr.io/google_containers/kube-ui:v1.1             \"/kube-ui\"               10 minutes ago      Up 10 minutes                                                        k8s_kube-ui.3b8d5a14_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_8855cb76\nebbe49b70250        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes       0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp   k8s_POD.c5371ceb_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_d9581395\n81eae8e8fafe        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.6e934112_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a96650e1\nbc84d79785ae        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.9db2f941_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_249307cc\nec7f57f9ab52        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.7be6d81d_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_bf93650f\n\nminion-2. Completely empty\n[root@kubernetes-minion-2 vagrant]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\nminion-3. Completely empty\n[root@kubernetes-minion-3 vagrant]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\nok, let us run redis from the example (replication factor is 3). Here is what I see after 10 minutes of waiting\n[root@linux-c56a cluster]$ ./kubectl.sh get pods\nNAME                   READY     STATUS    RESTARTS   AGE\nredis-1btiw            0/1       Pending   0          48s\nredis-1lfi0            0/1       Pending   0          4m\nredis-333g6            0/1       Pending   0          3m\nredis-5hpg8            0/1       Pending   0          7m\nredis-5lsy5            0/1       Pending   0          17s\nredis-7be1q            0/1       Pending   0          4m\nredis-9jjwo            0/1       Pending   0          3m\nredis-aospw            0/1       Pending   0          48s\nredis-c96tg            0/1       Pending   0          14m\nredis-eoffa            0/1       Pending   0          7m\nredis-exgmi            0/1       Pending   0          11m\nredis-fkp2a            0/1       Pending   0          48s\nredis-fosf3            0/1       Pending   0          17s\nredis-l483e            0/1       Pending   0          3m\nredis-pc3bl            0/1       Pending   0          8m\nredis-sentinel-4bdy5   0/1       Pending   0          10m\nredis-sentinel-4ch8m   0/1       Pending   0          3m\nredis-sentinel-59x65   0/1       Pending   0          14m\nredis-sentinel-97a10   0/1       Pending   0          3m\nredis-sentinel-abw3f   0/1       Pending   0          11m\nredis-sentinel-bjyth   0/1       Pending   0          7m\nredis-sentinel-c94br   0/1       Pending   0          3m\nredis-sentinel-chkmu   0/1       Pending   0          7m\nredis-sentinel-dtc89   0/1       Pending   0          7m\nredis-sentinel-fxzrq   0/1       Pending   0          17s\nredis-sentinel-h4ixm   0/1       Pending   0          11m\nredis-sentinel-hph84   0/1       Pending   0          11m\nredis-sentinel-j1svl   0/1       Pending   0          7m\nredis-sentinel-kfx09   0/1       Pending   0          17s\nredis-sentinel-mp18j   0/1       Pending   0          7m\nredis-sentinel-t9t0v   0/1       Pending   0          10m\nredis-sentinel-tkyh9   0/1       Pending   0          3m\nredis-sentinel-udu4k   0/1       Pending   0          17s\nredis-sentinel-ukv95   0/1       Pending   0          10m\nredis-sentinel-xrniw   0/1       Pending   0          7m\nredis-sentinel-yhwad   0/1       Pending   0          3m\nredis-sentinel-ysawg   0/1       Pending   0          3m\nredis-slcg7            0/1       Pending   0          7m\nredis-spati            0/1       Pending   0          17s\nredis-taasu            0/1       Pending   0          11m\nredis-v6vif            0/1       Pending   0          8m\nredis-yqlbl            0/1       Pending   0          4m\nredis-yvc2u            0/1       Pending   0          11m\nredis-zkktb            0/1       Pending   0          8m",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "93",
      "number": "13633",
      "pretext": "I took the last version from github several days ago, installed without any problems\nhost machine\n[root@linux-c56a cluster]$ ./kubectl.sh version\nClient Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.1\", GitCommit:\"9bae6636d5fcf436a3b14d219ced195e10e21fe8\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.1\", GitCommit:\"9bae6636d5fcf436a3b14d219ced195e10e21fe8\", GitTreeState:\"clean\"}\n\nminion-1, everything is fine\n[root@kubernetes-minion-1 vagrant]# docker ps\nCONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n6603d4e83556        gcr.io/google_containers/heapster_grafana:v0.7    \"/kuisp -p 8080 -c /o\"   8 minutes ago       Up 8 minutes                                                         k8s_grafana.d03a4af7_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_576e5681\nc8fcb0448ffa        gcr.io/google_containers/heapster_influxdb:v0.3   \"/run.sh\"                9 minutes ago       Up 9 minutes                                                         k8s_influxdb.e4e63a11_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_4b7be40b\n097522fb0214        gcr.io/google_containers/kube2sky:1.11            \"/kube2sky -domain=cl\"   10 minutes ago      Up 10 minutes                                                        k8s_kube2sky.9e5dd6c0_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_88c205c6\ncd70ece47d5f        gcr.io/google_containers/etcd:2.0.9               \"/usr/local/bin/etcd \"   10 minutes ago      Up 10 minutes                                                        k8s_etcd.f2082b87_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_2fac6c01\nb2065f47fd00        gcr.io/google_containers/exechealthz:1.0          \"/exechealthz '-cmd=n\"   10 minutes ago      Up 10 minutes                                                        k8s_healthz.75190edc_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a9976982\ne328d461b7c2        gcr.io/google_containers/skydns:2015-03-11-001    \"/skydns -machines=ht\"   10 minutes ago      Up 10 minutes                                                        k8s_skydns.8afa9a39_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_9444aa84\n336b74f063c9        gcr.io/google_containers/heapster:v0.17.0         \"/heapster --source=k\"   10 minutes ago      Up 10 minutes                                                        k8s_heapster.f0daed14_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_b0e636eb\nf9584b644df5        gcr.io/google_containers/kube-ui:v1.1             \"/kube-ui\"               10 minutes ago      Up 10 minutes                                                        k8s_kube-ui.3b8d5a14_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_8855cb76\nebbe49b70250        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes       0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp   k8s_POD.c5371ceb_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_d9581395\n81eae8e8fafe        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.6e934112_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a96650e1\nbc84d79785ae        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.9db2f941_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_249307cc\nec7f57f9ab52        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.7be6d81d_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_bf93650f\n\nminion-2. Completely empty\n[root@kubernetes-minion-2 vagrant]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\nminion-3. Completely empty\n[root@kubernetes-minion-3 vagrant]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\nok, let us run redis from the example (replication factor is 3). Here is what I see after 10 minutes of waiting\n[root@linux-c56a cluster]$ ./kubectl.sh get pods\nNAME                   READY     STATUS    RESTARTS   AGE\nredis-1btiw            0/1       Pending   0          48s\nredis-1lfi0            0/1       Pending   0          4m\nredis-333g6            0/1       Pending   0          3m\nredis-5hpg8            0/1       Pending   0          7m\nredis-5lsy5            0/1       Pending   0          17s\nredis-7be1q            0/1       Pending   0          4m\nredis-9jjwo            0/1       Pending   0          3m\nredis-aospw            0/1       Pending   0          48s\nredis-c96tg            0/1       Pending   0          14m\nredis-eoffa            0/1       Pending   0          7m\nredis-exgmi            0/1       Pending   0          11m\nredis-fkp2a            0/1       Pending   0          48s\nredis-fosf3            0/1       Pending   0          17s\nredis-l483e            0/1       Pending   0          3m\nredis-pc3bl            0/1       Pending   0          8m\nredis-sentinel-4bdy5   0/1       Pending   0          10m\nredis-sentinel-4ch8m   0/1       Pending   0          3m\nredis-sentinel-59x65   0/1       Pending   0          14m\nredis-sentinel-97a10   0/1       Pending   0          3m\nredis-sentinel-abw3f   0/1       Pending   0          11m\nredis-sentinel-bjyth   0/1       Pending   0          7m\nredis-sentinel-c94br   0/1       Pending   0          3m\nredis-sentinel-chkmu   0/1       Pending   0          7m\nredis-sentinel-dtc89   0/1       Pending   0          7m\nredis-sentinel-fxzrq   0/1       Pending   0          17s\nredis-sentinel-h4ixm   0/1       Pending   0          11m\nredis-sentinel-hph84   0/1       Pending   0          11m\nredis-sentinel-j1svl   0/1       Pending   0          7m\nredis-sentinel-kfx09   0/1       Pending   0          17s\nredis-sentinel-mp18j   0/1       Pending   0          7m\nredis-sentinel-t9t0v   0/1       Pending   0          10m\nredis-sentinel-tkyh9   0/1       Pending   0          3m\nredis-sentinel-udu4k   0/1       Pending   0          17s\nredis-sentinel-ukv95   0/1       Pending   0          10m\nredis-sentinel-xrniw   0/1       Pending   0          7m\nredis-sentinel-yhwad   0/1       Pending   0          3m\nredis-sentinel-ysawg   0/1       Pending   0          3m\nredis-slcg7            0/1       Pending   0          7m\nredis-spati            0/1       Pending   0          17s\nredis-taasu            0/1       Pending   0          11m\nredis-v6vif            0/1       Pending   0          8m\nredis-yqlbl            0/1       Pending   0          4m\nredis-yvc2u            0/1       Pending   0          11m\nredis-zkktb            0/1       Pending   0          8m",
      "title": "vagrant-based cluster is broken"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2674,
    "text": "failed to set up cluster via dockerI am following this tutorial Installing a Kubernetes Master Node via Docker to set up a cluster via Dockar。\nAfter setting up etcd and flannel. I tried to restart docker with --bip & --mtu option.\n\nYou now need to edit the docker configuration to activate new flags. Again, this is system specific.\nThis may be in /etc/default/docker or /etc/systemd/service/docker.service or it may be elsewhere.\nRegardless, you need to add the following to the docker command line:\n--bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}\n\nAnd after I ran\nps aux | grep docker\n\nI found my docker is already started with the right options:\nroot     55351  0.1  0.0 994216 34940 ?        Ssl  14:25   0:07 /usr/bin/docker daemon -H fd:// --bip=10.1.65.1/24 --mtu=8972\n\nBut when I tried to run hello-world procedure with the following command\nsudo docker run hello-world\n\nThe error occurs\ndocker: Error response from daemon: Container command '/hello' not found or does not exist..\n\nBTW, I can run hello-world find with --bip & --mtu removed.\nAny kind of help will be appreciated :)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "94",
      "number": "25792",
      "pretext": "I am following this tutorial Installing a Kubernetes Master Node via Docker to set up a cluster via Dockar。\nAfter setting up etcd and flannel. I tried to restart docker with --bip & --mtu option.\n\nYou now need to edit the docker configuration to activate new flags. Again, this is system specific.\nThis may be in /etc/default/docker or /etc/systemd/service/docker.service or it may be elsewhere.\nRegardless, you need to add the following to the docker command line:\n--bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}\n\nAnd after I ran\nps aux | grep docker\n\nI found my docker is already started with the right options:\nroot     55351  0.1  0.0 994216 34940 ?        Ssl  14:25   0:07 /usr/bin/docker daemon -H fd:// --bip=10.1.65.1/24 --mtu=8972\n\nBut when I tried to run hello-world procedure with the following command\nsudo docker run hello-world\n\nThe error occurs\ndocker: Error response from daemon: Container command '/hello' not found or does not exist..\n\nBTW, I can run hello-world find with --bip & --mtu removed.\nAny kind of help will be appreciated :)",
      "title": "failed to set up cluster via docker"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2675,
    "text": "when using client-go library to communicate with master, client list pods with watch hang foreverProblem: the following codes just hang and never returns\n clientset, err := kubernetes.NewForConfig(config)\n pods, err := clientset.Core().Pods(\"\").List(api.ListOptions{Watch:true})\n\nwhile codes like:\nwatch, err := clientset.Core().Pods(\"\").Watch(api.ListOptions{})\n\nwork as expected.\nIMHO,  the first use is kind of misleading.\n\nIs this a request for help? (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):\nWhat keywords did you search in Kubernetes issues before filing this one? (If you have found any duplicates, you should instead reply there.):\n\nIs this a BUG REPORT or FEATURE REQUEST? (choose one):\n\nKubernetes version (use kubectl version):\nEnvironment:\n\nCloud provider or hardware configuration:\nOS (e.g. from /etc/os-release):\nKernel (e.g. uname -a):\nInstall tools:\nOthers:\n\nWhat happened:\nWhat you expected to happen:\nHow to reproduce it (as minimally and precisely as possible):\nAnything else do we need to know:",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "95",
      "number": "32991",
      "pretext": "Problem: the following codes just hang and never returns\n clientset, err := kubernetes.NewForConfig(config)\n pods, err := clientset.Core().Pods(\"\").List(api.ListOptions{Watch:true})\n\nwhile codes like:\nwatch, err := clientset.Core().Pods(\"\").Watch(api.ListOptions{})\n\nwork as expected.\nIMHO,  the first use is kind of misleading.\n\nIs this a request for help? (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):\nWhat keywords did you search in Kubernetes issues before filing this one? (If you have found any duplicates, you should instead reply there.):\n\nIs this a BUG REPORT or FEATURE REQUEST? (choose one):\n\nKubernetes version (use kubectl version):\nEnvironment:\n\nCloud provider or hardware configuration:\nOS (e.g. from /etc/os-release):\nKernel (e.g. uname -a):\nInstall tools:\nOthers:\n\nWhat happened:\nWhat you expected to happen:\nHow to reproduce it (as minimally and precisely as possible):\nAnything else do we need to know:",
      "title": "when using client-go library to communicate with master, client list pods with watch hang forever"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2676,
    "text": "--cpuset-cpus supported in kubernetes?I want to know if --cpuset-cpus in supported in kubernetes so that we can launch containers with that property. Initiated at #10570  but I am not finding enough resources if it can.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "96",
      "number": "10983",
      "pretext": "I want to know if --cpuset-cpus in supported in kubernetes so that we can launch containers with that property. Initiated at #10570  but I am not finding enough resources if it can.",
      "title": "--cpuset-cpus supported in kubernetes?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2677,
    "text": "Cluster failed to initialize on GCEI tried to setup a trusty cluster on GCE, but it failed with Cluster failed to initialize within 300 seconds.\nHere is my environment setting:\nexport KUBERNETES_PROVIDER=gce\nexport KUBE_GCE_MASTER_PROJECT=ubuntu-os-cloud\nexport KUBE_GCE_MASTER_IMAGE=ubuntu-1404-trusty-v20160304\nexport KUBE_OS_DISTRIBUTION=trusty\n./cluster/kube-up.sh\n\nAm I missing something? CC @dchen1107\nBy the way, this problem has also been confirmed by @Random-Liu .",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "97",
      "number": "22842",
      "pretext": "I tried to setup a trusty cluster on GCE, but it failed with Cluster failed to initialize within 300 seconds.\nHere is my environment setting:\nexport KUBERNETES_PROVIDER=gce\nexport KUBE_GCE_MASTER_PROJECT=ubuntu-os-cloud\nexport KUBE_GCE_MASTER_IMAGE=ubuntu-1404-trusty-v20160304\nexport KUBE_OS_DISTRIBUTION=trusty\n./cluster/kube-up.sh\n\nAm I missing something? CC @dchen1107\nBy the way, this problem has also been confirmed by @Random-Liu .",
      "title": "Cluster failed to initialize on GCE"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2678,
    "text": "Default Job Parallelism from CompletionsCurrently a job's .spec.parallelism defaults to 2.  @bgrant0607 suggested defaulting .spec.parallelism to .spec.completions.  This will allow users to leave it unspecified in most cases, and it will do something intuitive. (Maybe this was how it was in @soltysh original PR, can't recall if it was that or 1).\nThe current default of 2 I liked because it encourages users to think about making their containers concurrency-safe.  But so does setting it from .spec.completions.  And people will ask why it was\nI don't like defaulting to 1 as much because I think people will usually want to override it when they have multiple completions.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "98",
      "number": "14385",
      "pretext": "Currently a job's .spec.parallelism defaults to 2.  @bgrant0607 suggested defaulting .spec.parallelism to .spec.completions.  This will allow users to leave it unspecified in most cases, and it will do something intuitive. (Maybe this was how it was in @soltysh original PR, can't recall if it was that or 1).\nThe current default of 2 I liked because it encourages users to think about making their containers concurrency-safe.  But so does setting it from .spec.completions.  And people will ask why it was\nI don't like defaulting to 1 as much because I think people will usually want to override it when they have multiple completions.",
      "title": "Default Job Parallelism from Completions"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2679,
    "text": "About redis from the main issue, I can not connect master service ip connection pod slaveI am from official instances did a redis master-slave, but I can not connect to the master service allocation in the slave containers such as ip: 10.254.31.52, contrary came in they could not connect to the master slave service assigned ip ask this. what causes the network can not communicate?\nThank you!\nservice ip\n\nslave container in ping master service ip:\n\nmaster container in ping slave service ip:\n\nnew start service",
    "annotations": [{ "label": 141, "user": 3 }],
    "meta": {
      "": "99",
      "number": "28144",
      "pretext": "I am from official instances did a redis master-slave, but I can not connect to the master service allocation in the slave containers such as ip: 10.254.31.52, contrary came in they could not connect to the master slave service assigned ip ask this. what causes the network can not communicate?\nThank you!\nservice ip\n\nslave container in ping master service ip:\n\nmaster container in ping slave service ip:\n\nnew start service",
      "title": "About redis from the main issue, I can not connect master service ip connection pod slave"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2680,
    "text": "Dragging a list that is currently handling an animateTo animation throws exceptionSteps to Reproduce\nThe end goal is to create a scrolling list that automatically \"snaps\" to a given location based on the actual scroll position at which the user stops scrolling. This may not be the best solution (I'm new to Flutter), but here's the process by which I get the crash:\nWrap a ListView within a NotificationListener. Upon receiving a UserScrollNotification with ScrollDirection.idle, call the ScrollController's animateTo() method to scroll to another location (in the example here, I arbitrarily scroll to 10000.0 pixels).\nSample code\nimport 'package:flutter/material.dart';\n\nvoid main() => runApp(new MyApp());\n\nclass MyApp extends StatelessWidget {\n  // This widget is the root of your application.\n  @override\n  Widget build(BuildContext context) {\n    return new MaterialApp(\n      title: 'Scroller issue',\n      home: new MyHomePage(),\n    );\n  }\n}\n\nclass MyHomePage extends StatelessWidget {\n  final _controller = new ScrollController();\n\n  bool _didGetNotification(ScrollNotification notification) {\n    if (notification is UserScrollNotification) {\n      if (notification.direction.toString() == \"ScrollDirection.idle\") {\n        // We've stopped scrolling... now animate automatically to the 10000-pixel spot\n        _controller.animateTo(10000.0, duration: const Duration(seconds: 2), curve: Curves.elasticOut);\n\n        /* Above works great... unless the user tries to interact with the list WHILE it's\n        // animating. In this case, you end up with:\n        **********************************************************************************\n        Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart':\n          Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\n        ********************************************************************************** */\n      }\n    }\n    return true;\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return new Scaffold(\n      body: new NotificationListener(\n        onNotification: _didGetNotification,\n        child: new ListView.builder(\n          padding: new EdgeInsets.all(8.0),\n          controller: _controller,\n          itemExtent: 60.0,\n          itemBuilder: (BuildContext context, int index) {\n            return new Text('Item No. $index');\n          },\n        ),\n      ),\n    );\n  }\n}\nIt works great as long as the user only interacts with the ListView when it is idle, but if the user attempts to scroll/drag/touch the list during the animateTo() process, the following exception is raised within the Flutter Scrollable package:\n'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\n\nAfter this, the ListView becomes unresponsive.\nI'm sure there's a better way to achieve what I'm trying to do, but the platform itself doesn't seem to recover from this. Figured I'd make a note of it.\nThanks!\nLogs\nRun your application with flutter run and attach all the log output.\n    CADisplay.name = LCD;\n    CADisplay.deviceName = PurpleMain;\n    CADisplay.seed = 1;\n    tags = 0;\n    currentMode = <FBSDisplayMode: 0x604000097430; 375x667@2x (750x1334/2) 60Hz sRGB SDR>;\n    safeOverscanRatio = {0.89999997615814209, 0.89999997615814209};\n    nativeCenter = {375, 667};\n    pixelSize = {750, 1334};\n    bounds = {{0, 0}, {375, 667}};\n    CADisplay = <CADisplay:LCD PurpleMain>;\n}\nSyncing files to device iPhone 6s...                  1.9s\n\n🔥  To hot reload your app on the fly, press \"r\". To restart the app entirely, press \"R\".\nAn Observatory debugger and profiler on iPhone 6s is available at: http://127.0.0.1:8100/\nFor a more detailed help message, press \"h\". To quit, press \"q\".\n══╡ EXCEPTION CAUGHT BY GESTURE ╞═══════════════════════════════════════════════════════════════════\nThe following assertion was thrown while handling a gesture:\n'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 467 pos 12: '_hold == null':\nis not true.\n\nEither the assertion indicates an error in the framework itself, or we should provide substantially\nmore information in this error message to help you determine and fix the underlying cause.\nIn either case, please report this assertion by filing a bug on GitHub:\nhttps://github.com/flutter/flutter/issues/new\n\nWhen the exception was thrown, this was the stack:\n#2      ScrollableState._handleDragStart (package:flutter/src/widgets/scrollable.dart:467:12)\n#3      DragGestureRecognizer.acceptGesture.<anonymous closure> (package:flutter/src/gestures/monodrag.dart:169:54)\n#4      GestureRecognizer.invokeCallback (package:flutter/src/gestures/recognizer.dart:102:24)\n#5      DragGestureRecognizer.acceptGesture (package:flutter/src/gestures/monodrag.dart:169:9)\n#6      GestureArenaManager._resolveByDefault (package:flutter/src/gestures/arena.dart:250:25)\n#7      GestureArenaManager._tryToResolveArena.<anonymous closure> (package:flutter/src/gestures/arena.dart:231:31)\n(elided 4 frames from class _AssertionError and package dart:async)\n\nHandler: onStart\nRecognizer:\nVerticalDragGestureRecognizer#6eafd\n════════════════════════════════════════════════════════════════════════════════════════════════════\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 478 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 455 pos 12: '_drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 464 pos 12: '_drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 478 pos 12: '_hold == null || _drag == null': is not true.\n\n\nRun flutter analyze and attach any output of that command also.\nAnalyzing /Users/mfahy/Apps/list_view_attempts...\nNo issues found!\nRan in 6.5s\n\nFlutter Doctor\n[  +21 ms] [/Users/mfahy/flutter/] git rev-parse --abbrev-ref --symbolic @{u}\n[  +36 ms] Exit code 0 from: git rev-parse --abbrev-ref --symbolic @{u}\n[        ] origin/alpha\n[        ] [/Users/mfahy/flutter/] git rev-parse --abbrev-ref HEAD\n[   +7 ms] Exit code 0 from: git rev-parse --abbrev-ref HEAD\n[        ] alpha\n[        ] [/Users/mfahy/flutter/] git ls-remote --get-url origin\n[   +9 ms] Exit code 0 from: git ls-remote --get-url origin\n[        ] https://github.com/flutter/flutter.git\n[        ] [/Users/mfahy/flutter/] git log -n 1 --pretty=format:%H\n[  +28 ms] Exit code 0 from: git log -n 1 --pretty=format:%H\n[        ] 2e449f06f0a3be076e336ad6b30b0e9ec99dbdfe\n[        ] [/Users/mfahy/flutter/] git log -n 1 --pretty=format:%ar\n[  +10 ms] Exit code 0 from: git log -n 1 --pretty=format:%ar\n[        ] 5 days ago\n[        ] [/Users/mfahy/flutter/] git describe --match v*.*.* --first-parent --long --tags\n[  +44 ms] Exit code 0 from: git describe --match v*.*.* --first-parent --long --tags\n[        ] v0.0.21-0-g2e449f06f\n[ +473 ms] /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString\n[+1263 ms] Exit code 0 from: /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString\n[        ] 3.0\n[ +452 ms] [✓] Flutter (on Mac OS X 10.13.3 17D47, locale en-US, channel alpha)\n[   +1 ms]     • Flutter version 0.0.21 at /Users/mfahy/flutter\n[        ]     • Framework revision 2e449f06f0 (5 days ago), 2018-01-29 14:26:51 -0800\n[        ]     • Engine revision 6921873c71\n[        ]     • Tools Dart version 2.0.0-dev.16.0\n[        ]     • Engine Dart version 2.0.0-edge.da1f52592ef73fe3afa485385cb995b9aec0181a\n[ +130 ms] /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString\n[ +218 ms] Exit code 0 from: /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString\n[        ] 3.0\n[  +98 ms] java -version\n[  +87 ms] [✓] Android toolchain - develop for Android devices (Android SDK 27.0.3)\n[        ]     • Android SDK at /Users/mfahy/Library/Android/sdk\n[        ]     • Android NDK location not configured (optional; useful for native profiling support)\n[        ]     • Platform android-27, build-tools 27.0.3\n[        ]     • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n[        ]     • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n[+1189 ms] DevToolsSecurity -status\n[  +49 ms] Developer mode is currently enabled.\n[        ] python -c import six\n[ +126 ms] idevice_id -h\n[  +10 ms] idevice_id -h\n[   +9 ms] idevice_id -l\n[  +28 ms] 5a62f0e8de345b9794b9a61c9bfcd02026c68a86\n[   +1 ms] idevicename\n[  +44 ms] ios-deploy --version\n[  +54 ms] ios-deploy --version\n[  +16 ms] 1.9.2\n[   +1 ms] ios-deploy --version\n[  +33 ms] ios-deploy --version\n[  +21 ms] 1.9.2\n[   +2 ms] pod --version\n[ +905 ms] pod --version\n[ +605 ms] 1.3.1\n[   +2 ms] pod --version\n[ +533 ms] 1.3.1\n[   +1 ms] [-] iOS toolchain - develop for iOS devices (Xcode 9.2)\n[        ]     • Xcode at /Applications/Xcode.app/Contents/Developer\n[        ]     • Xcode 9.2, Build version 9C40b\n[        ]     ✗ Verify that all connected devices have been paired with this computer in Xcode.\n                 If all devices have been paired, libimobiledevice and ideviceinstaller may require updating.\n                 To update, run:\n                   brew uninstall --ignore-dependencies libimobiledevice\n                   brew install --HEAD libimobiledevice\n                   brew install ideviceinstaller\n[        ]     • ios-deploy 1.9.2\n[        ]     • CocoaPods version 1.3.1\n[   +2 ms] [✓] Android Studio (version 3.0)\n[        ]     • Android Studio at /Applications/Android Studio.app/Contents\n[        ]     • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n[   +6 ms] /usr/bin/defaults read /Applications/IntelliJ IDEA.app/Contents/Info CFBundleShortVersionString\n[ +249 ms] Exit code 0 from: /usr/bin/defaults read /Applications/IntelliJ IDEA.app/Contents/Info CFBundleShortVersionString\n[        ] 2017.3.4\n[  +79 ms] [✓] IntelliJ IDEA Ultimate Edition (version 2017.3.4)\n[        ]     • Flutter plugin version 21.2.3\n[        ]     • Dart plugin version 173.4548.30\n[   +4 ms] /Users/mfahy/Library/Android/sdk/platform-tools/adb devices -l\n[  +17 ms] Exit code 0 from: /Users/mfahy/Library/Android/sdk/platform-tools/adb devices -l\n[        ] List of devices attached\n[   +8 ms] idevice_id -h\n[  +82 ms] which ideviceinstaller\n[   +5 ms] Exit code 0 from: which ideviceinstaller\n[        ] /usr/local/bin/ideviceinstaller\n[        ] which iproxy\n[   +4 ms] Exit code 0 from: which iproxy\n[        ] /usr/local/bin/iproxy\n[   +4 ms] /usr/bin/xcrun simctl list --json devices\n[ +218 ms] [✓] Connected devices\n[        ]     • ViPhone 6S • 5a62f0e8de345b9794b9a61c9bfcd02026c68a86 • ios • iOS 11.3\n[        ]     • iPhone 6s  • 6F36E8FD-E570-453E-B943-8243E4FAEDD6     • ios • iOS 11.2 (simulator)\n[  +21 ms] \"flutter doctor\" took 6,972ms.\n[  +42 ms] ensureAnalyticsSent: 38ms\n[   +2 ms] exiting with code 0\n\nHope this helps! Thanks!",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "0",
      "number": "14452",
      "pretext": "Steps to Reproduce\nThe end goal is to create a scrolling list that automatically \"snaps\" to a given location based on the actual scroll position at which the user stops scrolling. This may not be the best solution (I'm new to Flutter), but here's the process by which I get the crash:\nWrap a ListView within a NotificationListener. Upon receiving a UserScrollNotification with ScrollDirection.idle, call the ScrollController's animateTo() method to scroll to another location (in the example here, I arbitrarily scroll to 10000.0 pixels).\nSample code\nimport 'package:flutter/material.dart';\n\nvoid main() => runApp(new MyApp());\n\nclass MyApp extends StatelessWidget {\n  // This widget is the root of your application.\n  @override\n  Widget build(BuildContext context) {\n    return new MaterialApp(\n      title: 'Scroller issue',\n      home: new MyHomePage(),\n    );\n  }\n}\n\nclass MyHomePage extends StatelessWidget {\n  final _controller = new ScrollController();\n\n  bool _didGetNotification(ScrollNotification notification) {\n    if (notification is UserScrollNotification) {\n      if (notification.direction.toString() == \"ScrollDirection.idle\") {\n        // We've stopped scrolling... now animate automatically to the 10000-pixel spot\n        _controller.animateTo(10000.0, duration: const Duration(seconds: 2), curve: Curves.elasticOut);\n\n        /* Above works great... unless the user tries to interact with the list WHILE it's\n        // animating. In this case, you end up with:\n        **********************************************************************************\n        Another exception was thrown: 'package:flutter/src/widgets/scrollable.dart':\n          Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\n        ********************************************************************************** */\n      }\n    }\n    return true;\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return new Scaffold(\n      body: new NotificationListener(\n        onNotification: _didGetNotification,\n        child: new ListView.builder(\n          padding: new EdgeInsets.all(8.0),\n          controller: _controller,\n          itemExtent: 60.0,\n          itemBuilder: (BuildContext context, int index) {\n            return new Text('Item No. $index');\n          },\n        ),\n      ),\n    );\n  }\n}\nIt works great as long as the user only interacts with the ListView when it is idle, but if the user attempts to scroll/drag/touch the list during the animateTo() process, the following exception is raised within the Flutter Scrollable package:\n'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\n\nAfter this, the ListView becomes unresponsive.\nI'm sure there's a better way to achieve what I'm trying to do, but the platform itself doesn't seem to recover from this. Figured I'd make a note of it.\nThanks!\nLogs\nRun your application with flutter run and attach all the log output.\n    CADisplay.name = LCD;\n    CADisplay.deviceName = PurpleMain;\n    CADisplay.seed = 1;\n    tags = 0;\n    currentMode = <FBSDisplayMode: 0x604000097430; 375x667@2x (750x1334/2) 60Hz sRGB SDR>;\n    safeOverscanRatio = {0.89999997615814209, 0.89999997615814209};\n    nativeCenter = {375, 667};\n    pixelSize = {750, 1334};\n    bounds = {{0, 0}, {375, 667}};\n    CADisplay = <CADisplay:LCD PurpleMain>;\n}\nSyncing files to device iPhone 6s...                  1.9s\n\n🔥  To hot reload your app on the fly, press \"r\". To restart the app entirely, press \"R\".\nAn Observatory debugger and profiler on iPhone 6s is available at: http://127.0.0.1:8100/\nFor a more detailed help message, press \"h\". To quit, press \"q\".\n══╡ EXCEPTION CAUGHT BY GESTURE ╞═══════════════════════════════════════════════════════════════════\nThe following assertion was thrown while handling a gesture:\n'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 467 pos 12: '_hold == null':\nis not true.\n\nEither the assertion indicates an error in the framework itself, or we should provide substantially\nmore information in this error message to help you determine and fix the underlying cause.\nIn either case, please report this assertion by filing a bug on GitHub:\nhttps://github.com/flutter/flutter/issues/new\n\nWhen the exception was thrown, this was the stack:\n#2      ScrollableState._handleDragStart (package:flutter/src/widgets/scrollable.dart:467:12)\n#3      DragGestureRecognizer.acceptGesture.<anonymous closure> (package:flutter/src/gestures/monodrag.dart:169:54)\n#4      GestureRecognizer.invokeCallback (package:flutter/src/gestures/recognizer.dart:102:24)\n#5      DragGestureRecognizer.acceptGesture (package:flutter/src/gestures/monodrag.dart:169:9)\n#6      GestureArenaManager._resolveByDefault (package:flutter/src/gestures/arena.dart:250:25)\n#7      GestureArenaManager._tryToResolveArena.<anonymous closure> (package:flutter/src/gestures/arena.dart:231:31)\n(elided 4 frames from class _AssertionError and package dart:async)\n\nHandler: onStart\nRecognizer:\nVerticalDragGestureRecognizer#6eafd\n════════════════════════════════════════════════════════════════════════════════════════════════════\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 472 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 478 pos 12: '_hold == null || _drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 455 pos 12: '_drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 464 pos 12: '_drag == null': is not true.\nAnother exception was thrown: 'package:flutter/src/widgets/scrollable.dart': Failed assertion: line 478 pos 12: '_hold == null || _drag == null': is not true.\n\n\nRun flutter analyze and attach any output of that command also.\nAnalyzing /Users/mfahy/Apps/list_view_attempts...\nNo issues found!\nRan in 6.5s\n\nFlutter Doctor\n[  +21 ms] [/Users/mfahy/flutter/] git rev-parse --abbrev-ref --symbolic @{u}\n[  +36 ms] Exit code 0 from: git rev-parse --abbrev-ref --symbolic @{u}\n[        ] origin/alpha\n[        ] [/Users/mfahy/flutter/] git rev-parse --abbrev-ref HEAD\n[   +7 ms] Exit code 0 from: git rev-parse --abbrev-ref HEAD\n[        ] alpha\n[        ] [/Users/mfahy/flutter/] git ls-remote --get-url origin\n[   +9 ms] Exit code 0 from: git ls-remote --get-url origin\n[        ] https://github.com/flutter/flutter.git\n[        ] [/Users/mfahy/flutter/] git log -n 1 --pretty=format:%H\n[  +28 ms] Exit code 0 from: git log -n 1 --pretty=format:%H\n[        ] 2e449f06f0a3be076e336ad6b30b0e9ec99dbdfe\n[        ] [/Users/mfahy/flutter/] git log -n 1 --pretty=format:%ar\n[  +10 ms] Exit code 0 from: git log -n 1 --pretty=format:%ar\n[        ] 5 days ago\n[        ] [/Users/mfahy/flutter/] git describe --match v*.*.* --first-parent --long --tags\n[  +44 ms] Exit code 0 from: git describe --match v*.*.* --first-parent --long --tags\n[        ] v0.0.21-0-g2e449f06f\n[ +473 ms] /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString\n[+1263 ms] Exit code 0 from: /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString\n[        ] 3.0\n[ +452 ms] [✓] Flutter (on Mac OS X 10.13.3 17D47, locale en-US, channel alpha)\n[   +1 ms]     • Flutter version 0.0.21 at /Users/mfahy/flutter\n[        ]     • Framework revision 2e449f06f0 (5 days ago), 2018-01-29 14:26:51 -0800\n[        ]     • Engine revision 6921873c71\n[        ]     • Tools Dart version 2.0.0-dev.16.0\n[        ]     • Engine Dart version 2.0.0-edge.da1f52592ef73fe3afa485385cb995b9aec0181a\n[ +130 ms] /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString\n[ +218 ms] Exit code 0 from: /usr/bin/defaults read /Applications/Android Studio.app/Contents/Info CFBundleShortVersionString\n[        ] 3.0\n[  +98 ms] java -version\n[  +87 ms] [✓] Android toolchain - develop for Android devices (Android SDK 27.0.3)\n[        ]     • Android SDK at /Users/mfahy/Library/Android/sdk\n[        ]     • Android NDK location not configured (optional; useful for native profiling support)\n[        ]     • Platform android-27, build-tools 27.0.3\n[        ]     • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n[        ]     • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n[+1189 ms] DevToolsSecurity -status\n[  +49 ms] Developer mode is currently enabled.\n[        ] python -c import six\n[ +126 ms] idevice_id -h\n[  +10 ms] idevice_id -h\n[   +9 ms] idevice_id -l\n[  +28 ms] 5a62f0e8de345b9794b9a61c9bfcd02026c68a86\n[   +1 ms] idevicename\n[  +44 ms] ios-deploy --version\n[  +54 ms] ios-deploy --version\n[  +16 ms] 1.9.2\n[   +1 ms] ios-deploy --version\n[  +33 ms] ios-deploy --version\n[  +21 ms] 1.9.2\n[   +2 ms] pod --version\n[ +905 ms] pod --version\n[ +605 ms] 1.3.1\n[   +2 ms] pod --version\n[ +533 ms] 1.3.1\n[   +1 ms] [-] iOS toolchain - develop for iOS devices (Xcode 9.2)\n[        ]     • Xcode at /Applications/Xcode.app/Contents/Developer\n[        ]     • Xcode 9.2, Build version 9C40b\n[        ]     ✗ Verify that all connected devices have been paired with this computer in Xcode.\n                 If all devices have been paired, libimobiledevice and ideviceinstaller may require updating.\n                 To update, run:\n                   brew uninstall --ignore-dependencies libimobiledevice\n                   brew install --HEAD libimobiledevice\n                   brew install ideviceinstaller\n[        ]     • ios-deploy 1.9.2\n[        ]     • CocoaPods version 1.3.1\n[   +2 ms] [✓] Android Studio (version 3.0)\n[        ]     • Android Studio at /Applications/Android Studio.app/Contents\n[        ]     • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n[   +6 ms] /usr/bin/defaults read /Applications/IntelliJ IDEA.app/Contents/Info CFBundleShortVersionString\n[ +249 ms] Exit code 0 from: /usr/bin/defaults read /Applications/IntelliJ IDEA.app/Contents/Info CFBundleShortVersionString\n[        ] 2017.3.4\n[  +79 ms] [✓] IntelliJ IDEA Ultimate Edition (version 2017.3.4)\n[        ]     • Flutter plugin version 21.2.3\n[        ]     • Dart plugin version 173.4548.30\n[   +4 ms] /Users/mfahy/Library/Android/sdk/platform-tools/adb devices -l\n[  +17 ms] Exit code 0 from: /Users/mfahy/Library/Android/sdk/platform-tools/adb devices -l\n[        ] List of devices attached\n[   +8 ms] idevice_id -h\n[  +82 ms] which ideviceinstaller\n[   +5 ms] Exit code 0 from: which ideviceinstaller\n[        ] /usr/local/bin/ideviceinstaller\n[        ] which iproxy\n[   +4 ms] Exit code 0 from: which iproxy\n[        ] /usr/local/bin/iproxy\n[   +4 ms] /usr/bin/xcrun simctl list --json devices\n[ +218 ms] [✓] Connected devices\n[        ]     • ViPhone 6S • 5a62f0e8de345b9794b9a61c9bfcd02026c68a86 • ios • iOS 11.3\n[        ]     • iPhone 6s  • 6F36E8FD-E570-453E-B943-8243E4FAEDD6     • ios • iOS 11.2 (simulator)\n[  +21 ms] \"flutter doctor\" took 6,972ms.\n[  +42 ms] ensureAnalyticsSent: 38ms\n[   +2 ms] exiting with code 0\n\nHope this helps! Thanks!",
      "title": "Dragging a list that is currently handling an animateTo animation throws exception"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2681,
    "text": "Request for a \"Thinking in Flutter\" docWe seem to have a documentation gap, between \"Getting Started\" and \"Tour of the Widget Framework\". In a recent UX study, we noticed that the high-level concepts and the unique bits of Flutter aren't clearly called out, which made it more challenging for a new user to wrap their heads around what Flutter is and how it thinks.\nInitial thoughts on this doc:\n\nAudience is brand new users for Flutter\nIntended for the \"ok, what's this flutter thing? what's different?\"\nIntended to be the next doc you read, after getting started, and before you start diving in\nFairly short, high level\n\nInitial thoughts on table of contents:\n\nWhat is Flutter?\n\nwhat problem(s) does it solve\nwho is it for\nwhat's special?\n\n\nFunctional-reactive framework\n\nwidget lifecycle\nsetState\nOnly rebuilding what needs to be rebuilt (efficient)\nCustomizable/extensible\n\n\nMaterial design\n\nfull set of widgets out of the box\n\n\nDev cycle\n\nHot reload\nFast restart\n\n\nTooling\n\nCLI\nIntelliJ\n\n\nPlugins/interop\n\ncoming soon!\n\n\n\nOnce a new user scans through this doc, they should know\n\nWhat's special about flutter?\nWhat does functional-reactive mean, for Flutter?\nsetState is a thing, and why not to be scared of it\nHow to dev cycle\nWhere to find additional info on widgets, layout, plugins, etc\n\nThanks!\n(this came up in a recent UX study)",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "1",
      "number": "6651",
      "pretext": "We seem to have a documentation gap, between \"Getting Started\" and \"Tour of the Widget Framework\". In a recent UX study, we noticed that the high-level concepts and the unique bits of Flutter aren't clearly called out, which made it more challenging for a new user to wrap their heads around what Flutter is and how it thinks.\nInitial thoughts on this doc:\n\nAudience is brand new users for Flutter\nIntended for the \"ok, what's this flutter thing? what's different?\"\nIntended to be the next doc you read, after getting started, and before you start diving in\nFairly short, high level\n\nInitial thoughts on table of contents:\n\nWhat is Flutter?\n\nwhat problem(s) does it solve\nwho is it for\nwhat's special?\n\n\nFunctional-reactive framework\n\nwidget lifecycle\nsetState\nOnly rebuilding what needs to be rebuilt (efficient)\nCustomizable/extensible\n\n\nMaterial design\n\nfull set of widgets out of the box\n\n\nDev cycle\n\nHot reload\nFast restart\n\n\nTooling\n\nCLI\nIntelliJ\n\n\nPlugins/interop\n\ncoming soon!\n\n\n\nOnce a new user scans through this doc, they should know\n\nWhat's special about flutter?\nWhat does functional-reactive mean, for Flutter?\nsetState is a thing, and why not to be scared of it\nHow to dev cycle\nWhere to find additional info on widgets, layout, plugins, etc\n\nThanks!\n(this came up in a recent UX study)",
      "title": "Request for a \"Thinking in Flutter\" doc"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2682,
    "text": "Tab underline not moving on first swipePlease check out this video for an example: https://dl.dropboxusercontent.com/u/316685/video.mp4\nIn the Buttons page, in the gallery app, swipe left slowly. Notice how the tab underline under FLOATING doesn't animate during swipe.\nThen, complete the swipe, and RAISED will be underlined. Then, swipe left and right, and the underline is animated.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "2",
      "number": "3070",
      "pretext": "Please check out this video for an example: https://dl.dropboxusercontent.com/u/316685/video.mp4\nIn the Buttons page, in the gallery app, swipe left slowly. Notice how the tab underline under FLOATING doesn't animate during swipe.\nThen, complete the swipe, and RAISED will be underlined. Then, swipe left and right, and the underline is animated.",
      "title": "Tab underline not moving on first swipe"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2683,
    "text": "firebase_ml_vision scanning datamatrix is really not accurate.Steps to Reproduce\n\nclone flutter/plugins\ncd packages/firebase_ml_vision/example\nflutter run (on a real device)\nscan some datamatrix\n\nHere is an album with 4 almost exactly same photo but only one work :\nhttps://photos.google.com/share/AF1QipOUTCX6xK77XqMfz4yBki2d3eygi_0GQqNwfiHgDkwl1x_eAodtMVSucjXLgBOIsA?key=Z2lUcnA5bzM5ZWtYOFAwaUNRNzNDOXNGLUlnZUFn\nYou can use it for testing\nWhat is expected\nThe scanning should work and return the data in the datamatrix\nWhat happen\nThe plugin doesn't find the datamatrix in the image most of the time.\nRemark\nOn the emulator the plugins work as expected\nFlutter doctor\nFlutter (Channel stable, v1.2.1, on Linux, locale en_US.UTF-8)\n    • Flutter version 1.2.1 at /home/kevin/Work/github.com/flutter/flutter\n    • Framework revision 8661d8aecd (7 weeks ago), 2019-02-14 19:19:53 -0800\n    • Engine revision 3757390fa4\n    • Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n\n[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at /home/kevin/Android/Sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • Java binary at: /home/kevin/.local/share/JetBrains/Toolbox/apps/AndroidStudio/ch-0/182.5314842/jre/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    • All Android licenses accepted.\n\n[✓] Android Studio (version 3.3)\n    • Android Studio at /home/kevin/.local/share/JetBrains/Toolbox/apps/AndroidStudio/ch-0/182.5314842\n    • Flutter plugin version 33.3.1\n    • Dart plugin version 182.5215\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[✓] VS Code (version 1.33.0)\n    • VS Code at /usr/share/code\n    • Flutter extension version 2.25.0\n\n[✓] Connected device (2 available)\n    • TA 1012                   • NB1GAS3771407100 • android-arm64 • Android 9 (API 28)\n    • Android SDK built for x86 • emulator-5554    • android-x86   • Android 9 (API 28) (emulator)\n\n• No issues found!",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "3",
      "number": "30690",
      "pretext": "Steps to Reproduce\n\nclone flutter/plugins\ncd packages/firebase_ml_vision/example\nflutter run (on a real device)\nscan some datamatrix\n\nHere is an album with 4 almost exactly same photo but only one work :\nhttps://photos.google.com/share/AF1QipOUTCX6xK77XqMfz4yBki2d3eygi_0GQqNwfiHgDkwl1x_eAodtMVSucjXLgBOIsA?key=Z2lUcnA5bzM5ZWtYOFAwaUNRNzNDOXNGLUlnZUFn\nYou can use it for testing\nWhat is expected\nThe scanning should work and return the data in the datamatrix\nWhat happen\nThe plugin doesn't find the datamatrix in the image most of the time.\nRemark\nOn the emulator the plugins work as expected\nFlutter doctor\nFlutter (Channel stable, v1.2.1, on Linux, locale en_US.UTF-8)\n    • Flutter version 1.2.1 at /home/kevin/Work/github.com/flutter/flutter\n    • Framework revision 8661d8aecd (7 weeks ago), 2019-02-14 19:19:53 -0800\n    • Engine revision 3757390fa4\n    • Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n\n[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at /home/kevin/Android/Sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • Java binary at: /home/kevin/.local/share/JetBrains/Toolbox/apps/AndroidStudio/ch-0/182.5314842/jre/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    • All Android licenses accepted.\n\n[✓] Android Studio (version 3.3)\n    • Android Studio at /home/kevin/.local/share/JetBrains/Toolbox/apps/AndroidStudio/ch-0/182.5314842\n    • Flutter plugin version 33.3.1\n    • Dart plugin version 182.5215\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[✓] VS Code (version 1.33.0)\n    • VS Code at /usr/share/code\n    • Flutter extension version 2.25.0\n\n[✓] Connected device (2 available)\n    • TA 1012                   • NB1GAS3771407100 • android-arm64 • Android 9 (API 28)\n    • Android SDK built for x86 • emulator-5554    • android-x86   • Android 9 (API 28) (emulator)\n\n• No issues found!",
      "title": "firebase_ml_vision scanning datamatrix is really not accurate."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2684,
    "text": "App is white, then quits, on iPodI created a brand-new project with flutter create, and then flutter run to my iPod. The app is installed onto the device, but when I run it, it's a pure white screen, and then approx 10s later it quite.\n~/Code/mute_field_6440 $ flutter -d 2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7 run\nStarting lib/main.dart on iPod touch...\n[....] Waiting for iOS device to be connected\n[....] Using N102AP 'iPod touch' (2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7).\n------ Install phase ------\n[  0%] Found N102AP 'iPod touch' (2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7) connected through USB, beginning install\n[  5%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/META-INF/ to device\n[  5%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/META-INF/com.apple.ZipMetadata.plist to device\n[  6%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/_CodeSignature/ to device\n[  6%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/_CodeSignature/CodeResources to device\n[  7%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@2x.png to device\n[  8%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@2x~ipad.png to device\n[  8%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@3x.png to device\n[  9%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29~ipad.png to device\n[ 10%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@2x.png to device\n[ 10%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@2x~ipad.png to device\n[ 11%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@3x.png to device\n[ 12%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40~ipad.png to device\n[ 12%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon60x60@2x.png to device\n[ 13%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon60x60@3x.png to device\n[ 14%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon76x76@2x~ipad.png to device\n[ 14%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon76x76~ipad.png to device\n[ 15%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon83.5x83.5@2x~ipad.png to device\n[ 16%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/embedded.mobileprovision to device\n[ 17%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/ to device\n[ 17%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/ to device\n[ 18%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/_CodeSignature/ to device\n[ 19%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/_CodeSignature/CodeResources to device\n[ 19%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/app.flx to device\n[ 20%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/FlutterApplication to device\n[ 28%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/Info.plist to device\n[ 29%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/icudtl.dat to device\n[ 34%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Info.plist to device\n[ 34%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/ to device\n[ 35%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/01J-lp-oVM-view-Ze5-6b-2t3.nib to device\n[ 36%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/Info.plist to device\n[ 36%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/UIViewController-01J-lp-oVM.nib to device\n[ 37%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/PkgInfo to device\n[ 38%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Runner to device\n[ 49%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/ServiceDefinitions.json to device\n[ 52%] CreatingStagingDirectory\n[ 57%] ExtractingPackage\n[ 60%] InspectingPackage\n[ 60%] TakingInstallLock\n[ 65%] PreflightingApplication\n[ 65%] InstallingEmbeddedProfile\n[ 70%] VerifyingApplication\n[ 75%] CreatingContainer\n[ 80%] InstallingApplication\n[ 85%] PostflightingApplication\n[ 90%] SandboxingApplication\n[ 95%] GeneratingApplicationMap\n[100%] Installed package ios/.generated/build/Release-iphoneos/Runner.app\n\nOutput of the logs:\n~/Code/mute_field_6440 $ flutter -d 2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7 logs\nShowing iPod touch logs:\nMar 10 14:05:05 iPod-touch installd[47] <Notice>: 0x16e12f000 -[MIClientConnection installPath:withOptions:completion:]: Install of \"/var/mobile/Media/PublicStaging/Runner.app\" type Developer (LSInstallType = (null)) requested by mobile_installation_proxy (pid 285)\nMar 10 14:06:12 iPod-touch SpringBoard[54] <Warning>: Forcing crash report of <FBApplicationProcess: 0x14ff58dd0; Runner; pid: 1143> (reason: 1, description: com.yourcompany.muteField6440 failed to scene-create after 19.86s (launch took 0.14s of total time limit 20.00s))\nMar 10 14:06:12 iPod-touch ReportCrash[1144] <Warning>: saved type '109_Runner' report (4 of max 25) as /var/mobile/Library/Logs/CrashReporter/Runner_2016-03-10-140612_iPod-touch.ips\n\n~/flutter/testme3 $ flutter --version\nFlutter from git@github.com:flutter/flutter.git (on master)\nFramework: 3b5fba40227cd0ee0949bbe195edd372f84b0e7e (79 minutes ago)\nEngine:    d1515e676438d79046508ca009044df246564ced\n\n[✓] The iOS toolchain is fully installed.\n[✓] The Android toolchain is fully installed.\n[✓] The Atom development environment is fully installed.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "4",
      "number": "2586",
      "pretext": "I created a brand-new project with flutter create, and then flutter run to my iPod. The app is installed onto the device, but when I run it, it's a pure white screen, and then approx 10s later it quite.\n~/Code/mute_field_6440 $ flutter -d 2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7 run\nStarting lib/main.dart on iPod touch...\n[....] Waiting for iOS device to be connected\n[....] Using N102AP 'iPod touch' (2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7).\n------ Install phase ------\n[  0%] Found N102AP 'iPod touch' (2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7) connected through USB, beginning install\n[  5%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/META-INF/ to device\n[  5%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/META-INF/com.apple.ZipMetadata.plist to device\n[  6%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/_CodeSignature/ to device\n[  6%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/_CodeSignature/CodeResources to device\n[  7%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@2x.png to device\n[  8%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@2x~ipad.png to device\n[  8%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29@3x.png to device\n[  9%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon29x29~ipad.png to device\n[ 10%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@2x.png to device\n[ 10%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@2x~ipad.png to device\n[ 11%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40@3x.png to device\n[ 12%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon40x40~ipad.png to device\n[ 12%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon60x60@2x.png to device\n[ 13%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon60x60@3x.png to device\n[ 14%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon76x76@2x~ipad.png to device\n[ 14%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon76x76~ipad.png to device\n[ 15%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/AppIcon83.5x83.5@2x~ipad.png to device\n[ 16%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/embedded.mobileprovision to device\n[ 17%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/ to device\n[ 17%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/ to device\n[ 18%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/_CodeSignature/ to device\n[ 19%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/_CodeSignature/CodeResources to device\n[ 19%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/app.flx to device\n[ 20%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/FlutterApplication to device\n[ 28%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Frameworks/FlutterApplication.framework/Info.plist to device\n[ 29%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/icudtl.dat to device\n[ 34%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Info.plist to device\n[ 34%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/ to device\n[ 35%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/01J-lp-oVM-view-Ze5-6b-2t3.nib to device\n[ 36%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/Info.plist to device\n[ 36%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/LaunchScreen.storyboardc/UIViewController-01J-lp-oVM.nib to device\n[ 37%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/PkgInfo to device\n[ 38%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/Runner to device\n[ 49%] Copying /Users/sethladd/Code/mute_field_6440/ios/.generated/build/Release-iphoneos/Runner.app/ServiceDefinitions.json to device\n[ 52%] CreatingStagingDirectory\n[ 57%] ExtractingPackage\n[ 60%] InspectingPackage\n[ 60%] TakingInstallLock\n[ 65%] PreflightingApplication\n[ 65%] InstallingEmbeddedProfile\n[ 70%] VerifyingApplication\n[ 75%] CreatingContainer\n[ 80%] InstallingApplication\n[ 85%] PostflightingApplication\n[ 90%] SandboxingApplication\n[ 95%] GeneratingApplicationMap\n[100%] Installed package ios/.generated/build/Release-iphoneos/Runner.app\n\nOutput of the logs:\n~/Code/mute_field_6440 $ flutter -d 2175d44c48c2ba9a5bbad10262cf8e7f04aab8e7 logs\nShowing iPod touch logs:\nMar 10 14:05:05 iPod-touch installd[47] <Notice>: 0x16e12f000 -[MIClientConnection installPath:withOptions:completion:]: Install of \"/var/mobile/Media/PublicStaging/Runner.app\" type Developer (LSInstallType = (null)) requested by mobile_installation_proxy (pid 285)\nMar 10 14:06:12 iPod-touch SpringBoard[54] <Warning>: Forcing crash report of <FBApplicationProcess: 0x14ff58dd0; Runner; pid: 1143> (reason: 1, description: com.yourcompany.muteField6440 failed to scene-create after 19.86s (launch took 0.14s of total time limit 20.00s))\nMar 10 14:06:12 iPod-touch ReportCrash[1144] <Warning>: saved type '109_Runner' report (4 of max 25) as /var/mobile/Library/Logs/CrashReporter/Runner_2016-03-10-140612_iPod-touch.ips\n\n~/flutter/testme3 $ flutter --version\nFlutter from git@github.com:flutter/flutter.git (on master)\nFramework: 3b5fba40227cd0ee0949bbe195edd372f84b0e7e (79 minutes ago)\nEngine:    d1515e676438d79046508ca009044df246564ced\n\n[✓] The iOS toolchain is fully installed.\n[✓] The Android toolchain is fully installed.\n[✓] The Atom development environment is fully installed.",
      "title": "App is white, then quits, on iPod"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2685,
    "text": "Grid View Builder Catch RenderFlex OverflowI am a student who has only recently started to learn Flutter.\nI have a GridView which builds a dynamic number of grids. If the devices screen is too small sometimes the number of grids overflow and this comes through to the console:\nI/flutter ( 6315): ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════\nI/flutter ( 6315): The following message was thrown during layout:\nI/flutter ( 6315): A RenderFlex overflowed by 120 pixels on the bottom.\nI/flutter ( 6315): \nI/flutter ( 6315): The overflowing RenderFlex has an orientation of Axis.vertical.\nI/flutter ( 6315): The edge of the RenderFlex that is overflowing has been marked in the rendering with a yellow and\nI/flutter ( 6315): black striped pattern. This is usually caused by the contents being too big for the RenderFlex.\nI/flutter ( 6315): Consider applying a flex factor (e.g. using an Expanded widget) to force the children of the\nI/flutter ( 6315): RenderFlex to fit within the available space instead of being sized to their natural size.\nI/flutter ( 6315): This is considered an error condition because it indicates that there is content that cannot be\nI/flutter ( 6315): seen. If the content is legitimately bigger than the available space, consider clipping it with a\nI/flutter ( 6315): ClipRect widget before putting it in the flex, or using a scrollable container rather than a Flex,\nI/flutter ( 6315): like a ListView.\nI/flutter ( 6315): The specific RenderFlex in question is:\nI/flutter ( 6315):   RenderFlex#601d5 relayoutBoundary=up8 OVERFLOWING\nI/flutter ( 6315):   creator: Column ← MediaQuery ← Padding ← SafeArea ← Align ← DefaultTextStyle ←\nI/flutter ( 6315):   AnimatedDefaultTextStyle ← _InkFeatures-[GlobalKey#13a7a ink renderer] ←\nI/flutter ( 6315):   NotificationListener<LayoutChangedNotification> ← PhysicalModel ← AnimatedPhysicalModel ← Material\nI/flutter ( 6315):   ← ⋯\nI/flutter ( 6315):   parentData: offset=Offset(0.0, 24.0) (can use size)\nI/flutter ( 6315):   constraints: BoxConstraints(0.0<=w<=320.0, 0.0<=h<=509.3)\nI/flutter ( 6315):   size: Size(320.0, 509.3)\nI/flutter ( 6315):   direction: vertical\nI/flutter ( 6315):   mainAxisAlignment: spaceBetween\nI/flutter ( 6315):   mainAxisSize: max\nI/flutter ( 6315):   crossAxisAlignment: center\nI/flutter ( 6315):   verticalDirection: down\nI/flutter ( 6315): ◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤\nI/flutter ( 6315): ════════════════════════════════════════════════════════════════════════════════════════════════════\n\nreturn new GridView.builder(\n      scrollDirection: Axis.vertical,\n      shrinkWrap: true,\n      gridDelegate: new SliverGridDelegateWithMaxCrossAxisExtent(\n          maxCrossAxisExtent: 150.0, //grid is 500.0 pixels wide, and [maxCrossAxisExtent] is 150.0, this delegate will create a grid with 4 columns that are 125.0 pixels wide\n          mainAxisSpacing: 10.0,\n          crossAxisSpacing: 10,\n          childAspectRatio: gridChildAspecRat,\n      ), //n/3 * 1.5\n      padding: const EdgeInsets.all(5.0),\n      itemCount: buttonsList.length,\n      itemBuilder: (context, i) => new SizedBox(\n            width: 100.0, //100\n            height: 100.0, //100\n            child: new RaisedButton(\n              padding: const EdgeInsets.all(8.0),\n              onPressed: buttonsList[i].enabled?()=>playTicTacToe(buttonsList[i]):null,\n              child: new Text(\n                buttonsList[i].type,\n                style: new TextStyle(color: Colors.white, fontSize: 20.0),\n              ),\n              color: buttonsList[i].bkgrnd,\n              disabledColor: buttonsList[i].bkgrnd,\n            ),\n          ),\n    );\nIs there any way I could add a try / catch block somewhere which could remove cards from the buttonsList  until there aren't enough cards on the screen to  overflow?\ntry {\n     /* GridView Builder */\n    }catch(e){\n      debugPrint('heightTooManyCards');\n        _nCards = _nCards - 1;\n        buttonsList = doInit();\n    }\nI couldn't find anywhere to catch the error message outputted in the terminal during runtime.\nAny ideas on how I could fix this?\nThank you all very much!",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "5",
      "number": "28512",
      "pretext": "I am a student who has only recently started to learn Flutter.\nI have a GridView which builds a dynamic number of grids. If the devices screen is too small sometimes the number of grids overflow and this comes through to the console:\nI/flutter ( 6315): ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════\nI/flutter ( 6315): The following message was thrown during layout:\nI/flutter ( 6315): A RenderFlex overflowed by 120 pixels on the bottom.\nI/flutter ( 6315): \nI/flutter ( 6315): The overflowing RenderFlex has an orientation of Axis.vertical.\nI/flutter ( 6315): The edge of the RenderFlex that is overflowing has been marked in the rendering with a yellow and\nI/flutter ( 6315): black striped pattern. This is usually caused by the contents being too big for the RenderFlex.\nI/flutter ( 6315): Consider applying a flex factor (e.g. using an Expanded widget) to force the children of the\nI/flutter ( 6315): RenderFlex to fit within the available space instead of being sized to their natural size.\nI/flutter ( 6315): This is considered an error condition because it indicates that there is content that cannot be\nI/flutter ( 6315): seen. If the content is legitimately bigger than the available space, consider clipping it with a\nI/flutter ( 6315): ClipRect widget before putting it in the flex, or using a scrollable container rather than a Flex,\nI/flutter ( 6315): like a ListView.\nI/flutter ( 6315): The specific RenderFlex in question is:\nI/flutter ( 6315):   RenderFlex#601d5 relayoutBoundary=up8 OVERFLOWING\nI/flutter ( 6315):   creator: Column ← MediaQuery ← Padding ← SafeArea ← Align ← DefaultTextStyle ←\nI/flutter ( 6315):   AnimatedDefaultTextStyle ← _InkFeatures-[GlobalKey#13a7a ink renderer] ←\nI/flutter ( 6315):   NotificationListener<LayoutChangedNotification> ← PhysicalModel ← AnimatedPhysicalModel ← Material\nI/flutter ( 6315):   ← ⋯\nI/flutter ( 6315):   parentData: offset=Offset(0.0, 24.0) (can use size)\nI/flutter ( 6315):   constraints: BoxConstraints(0.0<=w<=320.0, 0.0<=h<=509.3)\nI/flutter ( 6315):   size: Size(320.0, 509.3)\nI/flutter ( 6315):   direction: vertical\nI/flutter ( 6315):   mainAxisAlignment: spaceBetween\nI/flutter ( 6315):   mainAxisSize: max\nI/flutter ( 6315):   crossAxisAlignment: center\nI/flutter ( 6315):   verticalDirection: down\nI/flutter ( 6315): ◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤◢◤\nI/flutter ( 6315): ════════════════════════════════════════════════════════════════════════════════════════════════════\n\nreturn new GridView.builder(\n      scrollDirection: Axis.vertical,\n      shrinkWrap: true,\n      gridDelegate: new SliverGridDelegateWithMaxCrossAxisExtent(\n          maxCrossAxisExtent: 150.0, //grid is 500.0 pixels wide, and [maxCrossAxisExtent] is 150.0, this delegate will create a grid with 4 columns that are 125.0 pixels wide\n          mainAxisSpacing: 10.0,\n          crossAxisSpacing: 10,\n          childAspectRatio: gridChildAspecRat,\n      ), //n/3 * 1.5\n      padding: const EdgeInsets.all(5.0),\n      itemCount: buttonsList.length,\n      itemBuilder: (context, i) => new SizedBox(\n            width: 100.0, //100\n            height: 100.0, //100\n            child: new RaisedButton(\n              padding: const EdgeInsets.all(8.0),\n              onPressed: buttonsList[i].enabled?()=>playTicTacToe(buttonsList[i]):null,\n              child: new Text(\n                buttonsList[i].type,\n                style: new TextStyle(color: Colors.white, fontSize: 20.0),\n              ),\n              color: buttonsList[i].bkgrnd,\n              disabledColor: buttonsList[i].bkgrnd,\n            ),\n          ),\n    );\nIs there any way I could add a try / catch block somewhere which could remove cards from the buttonsList  until there aren't enough cards on the screen to  overflow?\ntry {\n     /* GridView Builder */\n    }catch(e){\n      debugPrint('heightTooManyCards');\n        _nCards = _nCards - 1;\n        buttonsList = doInit();\n    }\nI couldn't find anywhere to catch the error message outputted in the terminal during runtime.\nAny ideas on how I could fix this?\nThank you all very much!",
      "title": "Grid View Builder Catch RenderFlex Overflow"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2686,
    "text": "is flutter have concept like android “addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK)”here is flow thats i want to manage back\n1 - on click home page's button start store list page.\n2 - on click store list page's button start filter page.\n3 - on click store filter page's button store list page.\n4 - on press back go to home page.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "6",
      "number": "18792",
      "pretext": "here is flow thats i want to manage back\n1 - on click home page's button start store list page.\n2 - on click store list page's button start filter page.\n3 - on click store filter page's button store list page.\n4 - on press back go to home page.",
      "title": "is flutter have concept like android “addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK)”"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2687,
    "text": "firebase_storage Content-Type autodetection not working on AndroidThe integration test passes on iOS and fails on Android.\nI/flutter (21641): 00:02 +0: FirebaseStorage putFile, getDownloadURL, writeToFile [E]\nI/flutter (21641):   Expected: 'text/plain'\nI/flutter (21641):     Actual: 'application/octet-stream'\nI/flutter (21641):      Which: is different.\nI/flutter (21641):             Expected: text/plain ...\nI/flutter (21641):               Actual: applicatio ...\nI/flutter (21641):                       ^\nI/flutter (21641):              Differ at offset 0\nI/flutter (21641):   \nI/flutter (21641):   package:test_api/src/frontend/expect.dart 152:30                                                       fail\nI/flutter (21641):   package:test_api/src/frontend/expect.dart 146:3                                                        _expect\nI/flutter (21641):   package:test_api/src/frontend/expect.dart 59:3                                                         expect\nI/flutter (21641):   package:flutter_test/src/widget_tester.dart 196:3                                                      expect\nI/flutter (21641):   Users/jackson/git/plugins_io/packages/firebase_storage/example/test_driver/firebase_storage.dart 47:7  main.<fn>.<fn>\nI/flutter (21641):   ===== asynchronous gap ===========================\nI/flutter (21641):   dart:async/future_impl.dart 22:43                                                                      _Completer.completeError\nI/flutter (21641):   dart:async-patch/async_patch.dart 40:18                                                                _AsyncAwaitCompleter.completeError\nI/flutter (21641):   Users/jackson/git/plugins_io/packages/firebase_storage/example/test_driver/firebase_storage.dart       main.<fn>.<fn>\nI/flutter (21641):   ===== async\nI/flutter (21641): 00:02 +0 -1: (tearDownAll)\nI/flutter (21641): 00:02 +1 -1: Some tests failed.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "7",
      "number": "32181",
      "pretext": "The integration test passes on iOS and fails on Android.\nI/flutter (21641): 00:02 +0: FirebaseStorage putFile, getDownloadURL, writeToFile [E]\nI/flutter (21641):   Expected: 'text/plain'\nI/flutter (21641):     Actual: 'application/octet-stream'\nI/flutter (21641):      Which: is different.\nI/flutter (21641):             Expected: text/plain ...\nI/flutter (21641):               Actual: applicatio ...\nI/flutter (21641):                       ^\nI/flutter (21641):              Differ at offset 0\nI/flutter (21641):   \nI/flutter (21641):   package:test_api/src/frontend/expect.dart 152:30                                                       fail\nI/flutter (21641):   package:test_api/src/frontend/expect.dart 146:3                                                        _expect\nI/flutter (21641):   package:test_api/src/frontend/expect.dart 59:3                                                         expect\nI/flutter (21641):   package:flutter_test/src/widget_tester.dart 196:3                                                      expect\nI/flutter (21641):   Users/jackson/git/plugins_io/packages/firebase_storage/example/test_driver/firebase_storage.dart 47:7  main.<fn>.<fn>\nI/flutter (21641):   ===== asynchronous gap ===========================\nI/flutter (21641):   dart:async/future_impl.dart 22:43                                                                      _Completer.completeError\nI/flutter (21641):   dart:async-patch/async_patch.dart 40:18                                                                _AsyncAwaitCompleter.completeError\nI/flutter (21641):   Users/jackson/git/plugins_io/packages/firebase_storage/example/test_driver/firebase_storage.dart       main.<fn>.<fn>\nI/flutter (21641):   ===== async\nI/flutter (21641): 00:02 +0 -1: (tearDownAll)\nI/flutter (21641): 00:02 +1 -1: Some tests failed.",
      "title": "firebase_storage Content-Type autodetection not working on Android"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2688,
    "text": "`flutter run --release` stuck when building for iOS: \"library not found for -lPods-Runner\"My flutter run --release for my iPhone is stuck at:\n~/Code/hello_cupertino[master*] $ flutter run --release\nLaunching lib/main.dart on flutter 6s in release mode...\nTraceback (most recent call last):\n  File \"/tmp/fruitstrap_d83d5bc53967baa0ee18626ba87b6254b2ab5418.py\", line 35, in run_command\n    lldb.target.Launch(lldb.SBLaunchInfo(shlex.split(args[1] and args[1] or '--enable-dart-profiling')), error)\nIndexError: list index out of range\n\nAny ideas?\n~/Code/hello_cupertino[master*] $ flutter --version\nFlutter • channel master • git@github.com:flutter/flutter.git\nFramework • revision a0f0c42fe3 (64 minutes ago) • 2017-01-31 14:48:48\nEngine • revision 2d54edf0f9\nTools • Dart 1.22.0-dev.9.1\n\nand\n~/Code/hello_cupertino[master*] $ flutter doctor\n[✓] Flutter (on Mac OS, channel master)\n    • Flutter at /Users/sethladd/Code/flutter\n    • Framework revision a0f0c42fe3 (64 minutes ago), 2017-01-31 14:48:48\n    • Engine revision 2d54edf0f9\n    • Tools Dart version 1.22.0-dev.9.1\n\n[✓] Android toolchain - develop for Android devices (Android SDK 25.0.0)\n    • Android SDK at /Users/sethladd/Library/Android/sdk\n    • Platform android-25, build-tools 25.0.0\n    • Java(TM) SE Runtime Environment (build 1.8.0_91-b14)\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 8.2.1)\n    • XCode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 8.2.1, Build version 8C1002\n\n[✓] IntelliJ IDEA Community Edition (version 2016.3.1)\n    • Dart plugin version 163.9166.22\n    • Flutter plugin version 0.1.4\n\n[✓] Connected devices\n    • flutter 6s • d83d5bc53967baa0ee18626ba87b6254b2ab5418 • ios • iOS 10.1.1 (14B100)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "8",
      "number": "7766",
      "pretext": "My flutter run --release for my iPhone is stuck at:\n~/Code/hello_cupertino[master*] $ flutter run --release\nLaunching lib/main.dart on flutter 6s in release mode...\nTraceback (most recent call last):\n  File \"/tmp/fruitstrap_d83d5bc53967baa0ee18626ba87b6254b2ab5418.py\", line 35, in run_command\n    lldb.target.Launch(lldb.SBLaunchInfo(shlex.split(args[1] and args[1] or '--enable-dart-profiling')), error)\nIndexError: list index out of range\n\nAny ideas?\n~/Code/hello_cupertino[master*] $ flutter --version\nFlutter • channel master • git@github.com:flutter/flutter.git\nFramework • revision a0f0c42fe3 (64 minutes ago) • 2017-01-31 14:48:48\nEngine • revision 2d54edf0f9\nTools • Dart 1.22.0-dev.9.1\n\nand\n~/Code/hello_cupertino[master*] $ flutter doctor\n[✓] Flutter (on Mac OS, channel master)\n    • Flutter at /Users/sethladd/Code/flutter\n    • Framework revision a0f0c42fe3 (64 minutes ago), 2017-01-31 14:48:48\n    • Engine revision 2d54edf0f9\n    • Tools Dart version 1.22.0-dev.9.1\n\n[✓] Android toolchain - develop for Android devices (Android SDK 25.0.0)\n    • Android SDK at /Users/sethladd/Library/Android/sdk\n    • Platform android-25, build-tools 25.0.0\n    • Java(TM) SE Runtime Environment (build 1.8.0_91-b14)\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 8.2.1)\n    • XCode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 8.2.1, Build version 8C1002\n\n[✓] IntelliJ IDEA Community Edition (version 2016.3.1)\n    • Dart plugin version 163.9166.22\n    • Flutter plugin version 0.1.4\n\n[✓] Connected devices\n    • flutter 6s • d83d5bc53967baa0ee18626ba87b6254b2ab5418 • ios • iOS 10.1.1 (14B100)",
      "title": "`flutter run --release` stuck when building for iOS: \"library not found for -lPods-Runner\""
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2689,
    "text": "Rename BlockBody and fix references to RenderBlockBaseI thought we had decided to rename BlockBody, but I can't find a bug on it anymore.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "9",
      "number": "8888",
      "pretext": "I thought we had decided to rename BlockBody, but I can't find a bug on it anymore.",
      "title": "Rename BlockBody and fix references to RenderBlockBase"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2690,
    "text": "FormatException: Could not parse \"EAP AI-162.3715353\"?log:\nFlutter crash report; please file at https://github.com/flutter/flutter/issues.\ncommand\nflutter doctor\nexception\nFormatException: Could not parse \"EAP AI-162.3715353\".\npackage:flutter_tools/src/base/version.dart 48                      Version.Version.parse\npackage:flutter_tools/src/android/android_studio.dart 64            AndroidStudio.AndroidStudio.fromMacOSBundle\npackage:flutter_tools/src/android/android_studio.dart 161           AndroidStudio._allMacOS.<fn>\ndart:core                                                           Iterable.toList\npackage:flutter_tools/src/android/android_studio.dart 163           AndroidStudio._allMacOS\npackage:flutter_tools/src/android/android_studio.dart 124           AndroidStudio.allInstalled\npackage:flutter_tools/src/android/android_studio_validator.dart 23  AndroidStudioValidator.allValidators\npackage:flutter_tools/src/doctor.dart 63                            Doctor.validators\npackage:flutter_tools/src/doctor.dart 121                           Doctor.diagnose\npackage:flutter_tools/src/commands/doctor.dart 19                   DoctorCommand.runCommand\npackage:flutter_tools/src/runner/flutter_command.dart 148           FlutterCommand.verifyThenRunCommand\npackage:flutter_tools/src/runner/flutter_command.dart 119           FlutterCommand.run\npackage:args/command_runner.dart 194                                CommandRunner.runCommand\npackage:flutter_tools/src/runner/flutter_command_runner.dart 221    FlutterCommandRunner.runCommand\npackage:args/command_runner.dart 109                                CommandRunner.run.<fn>\ndart:async                                                          Future.Future.sync\npackage:args/command_runner.dart 109                                CommandRunner.run\npackage:flutter_tools/src/runner/flutter_command_runner.dart 150    FlutterCommandRunner.run\npackage:flutter_tools/executable.dart 128                           main.<fn>.<fn>\npackage:stack_trace                                                 Chain.capture\npackage:flutter_tools/executable.dart 127                           main.<fn>\npackage:flutter_tools/src/base/context.dart 76                      AppContext._run\npackage:flutter_tools/src/base/context.dart 66                      AppContext.runInZone.<fn>\ndart:async                                                          runZoned\npackage:flutter_tools/src/base/context.dart 65                      AppContext.runInZone\npackage:flutter_tools/executable.dart 98                            main\n../packages/flutter_tools/bin/flutter_tools.dart 8                  main\n===== asynchronous gap ===========================\ndart:async                                                          _Completer.completeError\npackage:flutter_tools/src/doctor.dart 149                           Doctor.diagnose\n===== asynchronous gap ===========================\ndart:async                                                          Future.Future.microtask\npackage:flutter_tools/src/doctor.dart                               Doctor.diagnose\npackage:flutter_tools/src/commands/doctor.dart 19                   DoctorCommand.runCommand\n===== asynchronous gap ===========================\ndart:async                                                          Future.Future.microtask\npackage:flutter_tools/src/commands/doctor.dart                      DoctorCommand.runCommand\npackage:flutter_tools/src/runner/flutter_command.dart 148           FlutterCommand.verifyThenRunCommand\n===== asynchronous gap ===========================\ndart:async                                                          _asyncThenWrapperHelper\npackage:flutter_tools/src/runner/flutter_command.dart               FlutterCommand.verifyThenRunCommand\npackage:flutter_tools/src/runner/flutter_command.dart 119           FlutterCommand.run\npackage:args/command_runner.dart 194                                CommandRunner.runCommand\n===== asynchronous gap ===========================\ndart:async                                                          Future.Future.microtask\npackage:args/command_runner.dart                                    CommandRunner.runCommand\npackage:flutter_tools/src/runner/flutter_command_runner.dart 221    FlutterCommandRunner.runCommand\n===== asynchronous gap ===========================\ndart:async                                                          _asyncThenWrapperHelper\npackage:flutter_tools/src/runner/flutter_command_runner.dart        FlutterCommandRunner.runCommand\npackage:args/command_runner.dart 109                                CommandRunner.run.<fn>\ndart:async                                                          Future.Future.sync\npackage:args/command_runner.dart 109                                CommandRunner.run\npackage:flutter_tools/src/runner/flutter_command_runner.dart 150    FlutterCommandRunner.run\npackage:flutter_tools/executable.dart 128                           main.<fn>.<fn>\n===== asynchronous gap ===========================\ndart:async                                                          Future.Future.microtask\npackage:flutter_tools/executable.dart                               main.<fn>.<fn>\npackage:stack_trace                                                 Chain.capture\npackage:flutter_tools/executable.dart 127                           main.<fn>\n\nflutter doctor\n[✓] Flutter (on Mac OS, channel master)\n    • Flutter at /Users/zhai/Documents/Flutter/flutter\n    • Framework revision 2a05cfdf07 (2 hours ago), 2017-02-23 16:27:30\n    • Engine revision ab09530927\n    • Tools Dart version 1.23.0-dev.0.0\n\n[✓] Android toolchain - develop for Android devices (Android SDK 25.0.2)\n    • Android SDK at /Users/zhai/Documents/WorkSpace/adt-bundle-mac-x86_64-20130917/sdk\n    • Platform android-25, build-tools 25.0.2\n    • Java Development Kit (JDK) found: javac 1.8.0_92\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 8.2.1)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 8.2.1, Build version 8C1002\n    • ios-deploy 1.9.0",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "10",
      "number": "8373",
      "pretext": "log:\nFlutter crash report; please file at https://github.com/flutter/flutter/issues.\ncommand\nflutter doctor\nexception\nFormatException: Could not parse \"EAP AI-162.3715353\".\npackage:flutter_tools/src/base/version.dart 48                      Version.Version.parse\npackage:flutter_tools/src/android/android_studio.dart 64            AndroidStudio.AndroidStudio.fromMacOSBundle\npackage:flutter_tools/src/android/android_studio.dart 161           AndroidStudio._allMacOS.<fn>\ndart:core                                                           Iterable.toList\npackage:flutter_tools/src/android/android_studio.dart 163           AndroidStudio._allMacOS\npackage:flutter_tools/src/android/android_studio.dart 124           AndroidStudio.allInstalled\npackage:flutter_tools/src/android/android_studio_validator.dart 23  AndroidStudioValidator.allValidators\npackage:flutter_tools/src/doctor.dart 63                            Doctor.validators\npackage:flutter_tools/src/doctor.dart 121                           Doctor.diagnose\npackage:flutter_tools/src/commands/doctor.dart 19                   DoctorCommand.runCommand\npackage:flutter_tools/src/runner/flutter_command.dart 148           FlutterCommand.verifyThenRunCommand\npackage:flutter_tools/src/runner/flutter_command.dart 119           FlutterCommand.run\npackage:args/command_runner.dart 194                                CommandRunner.runCommand\npackage:flutter_tools/src/runner/flutter_command_runner.dart 221    FlutterCommandRunner.runCommand\npackage:args/command_runner.dart 109                                CommandRunner.run.<fn>\ndart:async                                                          Future.Future.sync\npackage:args/command_runner.dart 109                                CommandRunner.run\npackage:flutter_tools/src/runner/flutter_command_runner.dart 150    FlutterCommandRunner.run\npackage:flutter_tools/executable.dart 128                           main.<fn>.<fn>\npackage:stack_trace                                                 Chain.capture\npackage:flutter_tools/executable.dart 127                           main.<fn>\npackage:flutter_tools/src/base/context.dart 76                      AppContext._run\npackage:flutter_tools/src/base/context.dart 66                      AppContext.runInZone.<fn>\ndart:async                                                          runZoned\npackage:flutter_tools/src/base/context.dart 65                      AppContext.runInZone\npackage:flutter_tools/executable.dart 98                            main\n../packages/flutter_tools/bin/flutter_tools.dart 8                  main\n===== asynchronous gap ===========================\ndart:async                                                          _Completer.completeError\npackage:flutter_tools/src/doctor.dart 149                           Doctor.diagnose\n===== asynchronous gap ===========================\ndart:async                                                          Future.Future.microtask\npackage:flutter_tools/src/doctor.dart                               Doctor.diagnose\npackage:flutter_tools/src/commands/doctor.dart 19                   DoctorCommand.runCommand\n===== asynchronous gap ===========================\ndart:async                                                          Future.Future.microtask\npackage:flutter_tools/src/commands/doctor.dart                      DoctorCommand.runCommand\npackage:flutter_tools/src/runner/flutter_command.dart 148           FlutterCommand.verifyThenRunCommand\n===== asynchronous gap ===========================\ndart:async                                                          _asyncThenWrapperHelper\npackage:flutter_tools/src/runner/flutter_command.dart               FlutterCommand.verifyThenRunCommand\npackage:flutter_tools/src/runner/flutter_command.dart 119           FlutterCommand.run\npackage:args/command_runner.dart 194                                CommandRunner.runCommand\n===== asynchronous gap ===========================\ndart:async                                                          Future.Future.microtask\npackage:args/command_runner.dart                                    CommandRunner.runCommand\npackage:flutter_tools/src/runner/flutter_command_runner.dart 221    FlutterCommandRunner.runCommand\n===== asynchronous gap ===========================\ndart:async                                                          _asyncThenWrapperHelper\npackage:flutter_tools/src/runner/flutter_command_runner.dart        FlutterCommandRunner.runCommand\npackage:args/command_runner.dart 109                                CommandRunner.run.<fn>\ndart:async                                                          Future.Future.sync\npackage:args/command_runner.dart 109                                CommandRunner.run\npackage:flutter_tools/src/runner/flutter_command_runner.dart 150    FlutterCommandRunner.run\npackage:flutter_tools/executable.dart 128                           main.<fn>.<fn>\n===== asynchronous gap ===========================\ndart:async                                                          Future.Future.microtask\npackage:flutter_tools/executable.dart                               main.<fn>.<fn>\npackage:stack_trace                                                 Chain.capture\npackage:flutter_tools/executable.dart 127                           main.<fn>\n\nflutter doctor\n[✓] Flutter (on Mac OS, channel master)\n    • Flutter at /Users/zhai/Documents/Flutter/flutter\n    • Framework revision 2a05cfdf07 (2 hours ago), 2017-02-23 16:27:30\n    • Engine revision ab09530927\n    • Tools Dart version 1.23.0-dev.0.0\n\n[✓] Android toolchain - develop for Android devices (Android SDK 25.0.2)\n    • Android SDK at /Users/zhai/Documents/WorkSpace/adt-bundle-mac-x86_64-20130917/sdk\n    • Platform android-25, build-tools 25.0.2\n    • Java Development Kit (JDK) found: javac 1.8.0_92\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 8.2.1)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 8.2.1, Build version 8C1002\n    • ios-deploy 1.9.0",
      "title": "FormatException: Could not parse \"EAP AI-162.3715353\"?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2691,
    "text": "The parameter 'options' isnt defined and the method GoogleMapOptions isnt define for classI have problem with the \"options: GoogleMapOptions\" part, where the options is not define. Am i missing something? Im new with flutter.\nThis is my main.dart\n**import 'package:flutter/material.dart';\nimport 'package:google_maps_flutter/google_maps_flutter.dart';\nimport 'dart:async';\nvoid main() => runApp(MyApp());\nclass MyApp extends StatelessWidget {\n// This widget is the root of your application.\n@override\nWidget build(BuildContext context) {\nreturn MaterialApp(\ntitle: 'Google Map',\ntheme: ThemeData(\nprimarySwatch: Colors.blue,\n),\nhome: MyHomePage(title: 'Google Map'),\n);\n}\n}\nclass MyHomePage extends StatefulWidget {\nMyHomePage({Key key, this.title}) : super(key: key);\nfinal String title;\n@override\n_MyHomePageState createState() => _MyHomePageState();\n}\nclass _MyHomePageState extends State {\n@override\nWidget build(BuildContext context) {\nreturn Scaffold(\n  appBar: AppBar(\n    title: Text(widget.title),\n  ),\n  body: Center(\n    child: Column(\n      children: <Widget>[\n      GoogleMap(\n        onMapCreated: (GoogleMapController controller) {},\n        options: GoogleMapOptions(\n          cameraPosition: CameraPosition(\n            target: LatLng(37.4219999, -122.0862462),\n          ),\n        ), initialCameraPosition: null,\n      ),\n      ],\n    ),\n  ),\n  floatingActionButton: FloatingActionButton(\n  ), // This trailing comma makes auto-formatting nicer for build methods.\n);\n\n}\n}**\nwhere the pubspec i had include\ngoogle_maps_flutter:\ngit:\nurl: git://github.com/flutter/plugins\npath: packages/google_maps_flutter",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "11",
      "number": "29031",
      "pretext": "I have problem with the \"options: GoogleMapOptions\" part, where the options is not define. Am i missing something? Im new with flutter.\nThis is my main.dart\n**import 'package:flutter/material.dart';\nimport 'package:google_maps_flutter/google_maps_flutter.dart';\nimport 'dart:async';\nvoid main() => runApp(MyApp());\nclass MyApp extends StatelessWidget {\n// This widget is the root of your application.\n@override\nWidget build(BuildContext context) {\nreturn MaterialApp(\ntitle: 'Google Map',\ntheme: ThemeData(\nprimarySwatch: Colors.blue,\n),\nhome: MyHomePage(title: 'Google Map'),\n);\n}\n}\nclass MyHomePage extends StatefulWidget {\nMyHomePage({Key key, this.title}) : super(key: key);\nfinal String title;\n@override\n_MyHomePageState createState() => _MyHomePageState();\n}\nclass _MyHomePageState extends State {\n@override\nWidget build(BuildContext context) {\nreturn Scaffold(\n  appBar: AppBar(\n    title: Text(widget.title),\n  ),\n  body: Center(\n    child: Column(\n      children: <Widget>[\n      GoogleMap(\n        onMapCreated: (GoogleMapController controller) {},\n        options: GoogleMapOptions(\n          cameraPosition: CameraPosition(\n            target: LatLng(37.4219999, -122.0862462),\n          ),\n        ), initialCameraPosition: null,\n      ),\n      ],\n    ),\n  ),\n  floatingActionButton: FloatingActionButton(\n  ), // This trailing comma makes auto-formatting nicer for build methods.\n);\n\n}\n}**\nwhere the pubspec i had include\ngoogle_maps_flutter:\ngit:\nurl: git://github.com/flutter/plugins\npath: packages/google_maps_flutter",
      "title": "The parameter 'options' isnt defined and the method GoogleMapOptions isnt define for class"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2692,
    "text": "Tabview errorhave a bottom navigation bar, and the 2 tab is have appbar + 2 tabs.\nget error when clicking bottomNavigationBar 3 to 1 without clicking at 2\nhttps://github.com/jhionan/flutterError\nError:\n`I/flutter (11406): ══╡ EXCEPTION CAUGHT BY WIDGETS LIBRARY ╞═══════════════════════════════════════════════════════════\nI/flutter (11406): The following assertion was thrown while finalizing the widget tree:\nI/flutter (11406): 'package:flutter/src/widgets/scroll_position.dart': Failed assertion: line 657 pos 12: 'pixels !=\nI/flutter (11406): null': is not true.\nI/flutter (11406): \nI/flutter (11406): Either the assertion indicates an error in the framework itself, or we should provide substantially\nI/flutter (11406): more information in this error message to help you determine and fix the underlying cause.\nI/flutter (11406): In either case, please report this assertion by filing a bug on GitHub:\nI/flutter (11406):   https://github.com/flutter/flutter/issues/new\nI/flutter (11406): \nI/flutter (11406): When the exception was thrown, this was the stack:\nI/flutter (11406): #2      ScrollPosition.dispose (package:flutter/src/widgets/scroll_position.dart)\nI/flutter (11406): #3      ScrollPositionWithSingleContext.dispose (package:flutter/src/widgets/scroll_position_with_single_context.dart:255:11)\nI/flutter (11406): #4      ScrollableState.dispose (package:flutter/src/widgets/scrollable.dart:324:14)\nI/flutter (11406): #5      StatefulElement.unmount (package:flutter/src/widgets/framework.dart:3821:12)\nI/flutter (11406): #6      _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1697:13)\nI/flutter (11406): #7      _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #8      ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #9      _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #10     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #11     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #12     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #13     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #14     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #15     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #16     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #17     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #18     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #19     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #20     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #21     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #22     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #23     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #24     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #25     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #26     MultiChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4742:16)\nI/flutter (11406): #27     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #28     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #29     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #30     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #31     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #32     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #33     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #34     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #35     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #36     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #37     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #38     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)\nI/flutter (11406): #39     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #40     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #41     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #42     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #43     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #44     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)\nI/flutter (11406): #45     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #46     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #47     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #48     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #49     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #50     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #51     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #52     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #53     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #54     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #55     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #56     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #57     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #58     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #59     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #60     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #61     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #62     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #63     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #64     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #65     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)\nI/flutter (11406): #66     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #67     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #68     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #69     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #70     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #71     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #72     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #73     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #74     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #75     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #76     ListIterable.forEach (dart:_internal/iterable.dart:39:13)\nI/flutter (11406): #77     _InactiveElements._unmountAll (package:flutter/src/widgets/framework.dart:1706:25)\nI/flutter (11406): #78     BuildOwner.finalizeTree.<anonymous closure> (package:flutter/src/widgets/framework.dart:2328:27)\nI/flutter (11406): #79     BuildOwner.lockState (package:flutter/src/widgets/framework.dart:2160:15)\nI/flutter (11406): #80     BuildOwner.finalizeTree (package:flutter/src/widgets/framework.dart:2327:7)\nI/flutter (11406): #81     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&RendererBinding&WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:628:18)\nI/flutter (11406): #82     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:208:5)\nI/flutter (11406): #83     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:990:15)\nI/flutter (11406): #84     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:930:9)\nI/flutter (11406): #85     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:842:5)\nI/flutter (11406): #86     _invoke (dart:ui/hooks.dart:120:13)\nI/flutter (11406): #87     _drawFrame (dart:ui/hooks.dart:109:3)\nI/flutter (11406): (elided 2 frames from class _AssertionError)\nI/flutter (11406): ════════════════════════════════════════════════════════════════════════════════════════════════════`",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "12",
      "number": "18763",
      "pretext": "have a bottom navigation bar, and the 2 tab is have appbar + 2 tabs.\nget error when clicking bottomNavigationBar 3 to 1 without clicking at 2\nhttps://github.com/jhionan/flutterError\nError:\n`I/flutter (11406): ══╡ EXCEPTION CAUGHT BY WIDGETS LIBRARY ╞═══════════════════════════════════════════════════════════\nI/flutter (11406): The following assertion was thrown while finalizing the widget tree:\nI/flutter (11406): 'package:flutter/src/widgets/scroll_position.dart': Failed assertion: line 657 pos 12: 'pixels !=\nI/flutter (11406): null': is not true.\nI/flutter (11406): \nI/flutter (11406): Either the assertion indicates an error in the framework itself, or we should provide substantially\nI/flutter (11406): more information in this error message to help you determine and fix the underlying cause.\nI/flutter (11406): In either case, please report this assertion by filing a bug on GitHub:\nI/flutter (11406):   https://github.com/flutter/flutter/issues/new\nI/flutter (11406): \nI/flutter (11406): When the exception was thrown, this was the stack:\nI/flutter (11406): #2      ScrollPosition.dispose (package:flutter/src/widgets/scroll_position.dart)\nI/flutter (11406): #3      ScrollPositionWithSingleContext.dispose (package:flutter/src/widgets/scroll_position_with_single_context.dart:255:11)\nI/flutter (11406): #4      ScrollableState.dispose (package:flutter/src/widgets/scrollable.dart:324:14)\nI/flutter (11406): #5      StatefulElement.unmount (package:flutter/src/widgets/framework.dart:3821:12)\nI/flutter (11406): #6      _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1697:13)\nI/flutter (11406): #7      _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #8      ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #9      _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #10     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #11     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #12     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #13     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #14     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #15     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #16     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #17     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #18     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #19     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #20     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #21     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #22     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #23     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #24     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #25     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #26     MultiChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4742:16)\nI/flutter (11406): #27     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #28     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #29     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #30     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #31     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #32     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #33     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #34     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #35     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #36     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #37     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #38     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)\nI/flutter (11406): #39     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #40     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #41     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #42     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #43     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #44     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)\nI/flutter (11406): #45     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #46     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #47     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #48     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #49     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #50     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #51     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #52     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #53     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #54     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #55     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #56     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #57     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #58     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #59     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #60     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #61     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #62     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #63     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #64     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #65     SingleChildRenderObjectElement.visitChildren (package:flutter/src/widgets/framework.dart:4642:14)\nI/flutter (11406): #66     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #67     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #68     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #69     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #70     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #71     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #72     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #73     _InactiveElements._unmount.<anonymous closure> (package:flutter/src/widgets/framework.dart:1695:7)\nI/flutter (11406): #74     ComponentElement.visitChildren (package:flutter/src/widgets/framework.dart:3676:14)\nI/flutter (11406): #75     _InactiveElements._unmount (package:flutter/src/widgets/framework.dart:1693:13)\nI/flutter (11406): #76     ListIterable.forEach (dart:_internal/iterable.dart:39:13)\nI/flutter (11406): #77     _InactiveElements._unmountAll (package:flutter/src/widgets/framework.dart:1706:25)\nI/flutter (11406): #78     BuildOwner.finalizeTree.<anonymous closure> (package:flutter/src/widgets/framework.dart:2328:27)\nI/flutter (11406): #79     BuildOwner.lockState (package:flutter/src/widgets/framework.dart:2160:15)\nI/flutter (11406): #80     BuildOwner.finalizeTree (package:flutter/src/widgets/framework.dart:2327:7)\nI/flutter (11406): #81     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&RendererBinding&WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:628:18)\nI/flutter (11406): #82     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:208:5)\nI/flutter (11406): #83     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:990:15)\nI/flutter (11406): #84     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:930:9)\nI/flutter (11406): #85     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:842:5)\nI/flutter (11406): #86     _invoke (dart:ui/hooks.dart:120:13)\nI/flutter (11406): #87     _drawFrame (dart:ui/hooks.dart:109:3)\nI/flutter (11406): (elided 2 frames from class _AssertionError)\nI/flutter (11406): ════════════════════════════════════════════════════════════════════════════════════════════════════`",
      "title": "Tabview error"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2693,
    "text": "Calling popAndPushNamed throws exceptionIf there's more than one route in the current stack and when popAndPushNamed is called(or calling pop and then push), the following exception will be thrown:\nI/flutter ( 6710): NoSuchMethodError: The getter 'subtreeContext' was called on null.\nI/flutter ( 6710): Receiver: null\nI/flutter ( 6710): Tried calling: subtreeContext\nI/flutter ( 6710): When the exception was thrown, this was the stack:\nI/flutter ( 6710): #0      Object._noSuchMethod (dart:core-patch/object_patch.dart:44)\nI/flutter ( 6710): #1      Object.noSuchMethod (dart:core-patch/object_patch.dart:47)\nI/flutter ( 6710): #2      HeroController._updateQuest (package:flutter/src/widgets/heroes.dart:579)\nI/flutter ( 6710): #3      BindingBase&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:653)\nI/flutter ( 6710): #4      BindingBase&SchedulerBinding.handleBeginFrame (package:flutter/src/scheduler/binding.dart:596)\nI/flutter ( 6710): #5      _beginFrame (file:///b/build/slave/Linux_Engine/build/src/flutter/lib/ui/hooks.dart:83)",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "13",
      "number": "7203",
      "pretext": "If there's more than one route in the current stack and when popAndPushNamed is called(or calling pop and then push), the following exception will be thrown:\nI/flutter ( 6710): NoSuchMethodError: The getter 'subtreeContext' was called on null.\nI/flutter ( 6710): Receiver: null\nI/flutter ( 6710): Tried calling: subtreeContext\nI/flutter ( 6710): When the exception was thrown, this was the stack:\nI/flutter ( 6710): #0      Object._noSuchMethod (dart:core-patch/object_patch.dart:44)\nI/flutter ( 6710): #1      Object.noSuchMethod (dart:core-patch/object_patch.dart:47)\nI/flutter ( 6710): #2      HeroController._updateQuest (package:flutter/src/widgets/heroes.dart:579)\nI/flutter ( 6710): #3      BindingBase&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:653)\nI/flutter ( 6710): #4      BindingBase&SchedulerBinding.handleBeginFrame (package:flutter/src/scheduler/binding.dart:596)\nI/flutter ( 6710): #5      _beginFrame (file:///b/build/slave/Linux_Engine/build/src/flutter/lib/ui/hooks.dart:83)",
      "title": "Calling popAndPushNamed throws exception"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2694,
    "text": "Make `flutter test` verify that target package depends on `flutter_test`Example test failures visible at https://travis-ci.org/flutter/plugins/jobs/242412801\nRUNNING battery tests...\nRunning \"flutter packages get\" in battery...          4.5s\n00:01 +0 -1: loading /home/travis/build/flutter/plugins/packages/battery/test/battery_test.dart [E]\n  Failed to load \"/home/travis/build/flutter/plugins/packages/battery/test/battery_test.dart\": Failed assertion: boolean expression must not be null\n  package:test            test\n  battery_test.dart 24:3  main\n  ===== asynchronous gap ===========================\n  package:test            serializeSuite\n  listener.dart 17:27     main\n  \n00:01 +0 -1: Some tests failed.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "14",
      "number": "10656",
      "pretext": "Example test failures visible at https://travis-ci.org/flutter/plugins/jobs/242412801\nRUNNING battery tests...\nRunning \"flutter packages get\" in battery...          4.5s\n00:01 +0 -1: loading /home/travis/build/flutter/plugins/packages/battery/test/battery_test.dart [E]\n  Failed to load \"/home/travis/build/flutter/plugins/packages/battery/test/battery_test.dart\": Failed assertion: boolean expression must not be null\n  package:test            test\n  battery_test.dart 24:3  main\n  ===== asynchronous gap ===========================\n  package:test            serializeSuite\n  listener.dart 17:27     main\n  \n00:01 +0 -1: Some tests failed.",
      "title": "Make `flutter test` verify that target package depends on `flutter_test`"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2695,
    "text": "a11y: Semantics Tree doesn't update fast enough for scrollingOn the Gallery App homepage in Android, select an item in the list of demos and start swiping right multiple times really fast. Continue swiping right when the end of the list is reached. Android will scroll and attempt to focus the next item in the list. However, the Semantics Tree has not been updated yet, focusing the next item fails (because from Android's a11y perspective there is no element) and the boing sound is played. After a couple of swipes, the semantic tree has been updated and everything works as expected.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "15",
      "number": "11204",
      "pretext": "On the Gallery App homepage in Android, select an item in the list of demos and start swiping right multiple times really fast. Continue swiping right when the end of the list is reached. Android will scroll and attempt to focus the next item in the list. However, the Semantics Tree has not been updated yet, focusing the next item fails (because from Android's a11y perspective there is no element) and the boing sound is played. After a couple of swipes, the semantic tree has been updated and everything works as expected.",
      "title": "a11y: Semantics Tree doesn't update fast enough for scrolling"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2696,
    "text": "Offer options that balances embeddable Flutter load time with resource consumptionWe should identify the right balance point between loading the embedding / vm / isolate / framework / userland element tree / resources in the tree based on each part’s load time vs its memory use.\nThis involves #32945 to audit cold init time and also auditing memory consumption.\nOur recommended embedding strategy + default behaviors from APIs should be reasonably balanced between fast loading and memory usage.\nDocument alterations users can make given those APIs such as prewarming/keeping the vm only, prewarming/keeping the isolate. Flutter side APIs to flush parts of the tree when window is not visible.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "16",
      "number": "32946",
      "pretext": "We should identify the right balance point between loading the embedding / vm / isolate / framework / userland element tree / resources in the tree based on each part’s load time vs its memory use.\nThis involves #32945 to audit cold init time and also auditing memory consumption.\nOur recommended embedding strategy + default behaviors from APIs should be reasonably balanced between fast loading and memory usage.\nDocument alterations users can make given those APIs such as prewarming/keeping the vm only, prewarming/keeping the isolate. Flutter side APIs to flush parts of the tree when window is not visible.",
      "title": "Offer options that balances embeddable Flutter load time with resource consumption"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2697,
    "text": "app run crash after plugin image picker installedmy device is Redmi note 5",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "17",
      "number": "26063",
      "pretext": "my device is Redmi note 5",
      "title": "app run crash after plugin image picker installed"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2698,
    "text": "Empty application after publishing it in iTunes Connect (Firebase related)I'm pretty sorry for the lack of information, and I'm sure this is something I'm doing wrong, but I don't know where else I can ask. I have a pretty small application that shows a list of values from a static file. I uploaded it to iTunes Connect and downloaded it using TestFlight. Everything works just fine in my iPhone X.\nI have modified the application to add a Firebase connection with a Firestore DDBB. If I run the application from Android Studio directly in my phone, it works, and it shows the data from the database. If I archive the app and upload it to iTunes Connect, and install it through TestFlight, when opening the application, it just appears as a white screen. The application works but shows nothing, but an empty white screen.\nDo you know what is happening or how I can get information that can help to solve my problem?\nRegards",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "18",
      "number": "25167",
      "pretext": "I'm pretty sorry for the lack of information, and I'm sure this is something I'm doing wrong, but I don't know where else I can ask. I have a pretty small application that shows a list of values from a static file. I uploaded it to iTunes Connect and downloaded it using TestFlight. Everything works just fine in my iPhone X.\nI have modified the application to add a Firebase connection with a Firestore DDBB. If I run the application from Android Studio directly in my phone, it works, and it shows the data from the database. If I archive the app and upload it to iTunes Connect, and install it through TestFlight, when opening the application, it just appears as a white screen. The application works but shows nothing, but an empty white screen.\nDo you know what is happening or how I can get information that can help to solve my problem?\nRegards",
      "title": "Empty application after publishing it in iTunes Connect (Firebase related)"
    },
    "annotation_approver": null
  },
  {
    "id": 2699,
    "text": "Extract modal bottom action sheet concept from Material to WidgetsThere are multiple Cupertino concepts of a modal dismissable action sheet at the bottom of the screen too such as\nhttps://developer.apple.com/ios/human-interface-guidelines/views/action-sheets/ and https://developer.apple.com/ios/human-interface-guidelines/controls/pickers/. The current showModalBottomSheet helper mechanism is in bottom_sheet.dart and has a lot of Material concepts baked in. Extract into common utility.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "19",
      "number": "14076",
      "pretext": "There are multiple Cupertino concepts of a modal dismissable action sheet at the bottom of the screen too such as\nhttps://developer.apple.com/ios/human-interface-guidelines/views/action-sheets/ and https://developer.apple.com/ios/human-interface-guidelines/controls/pickers/. The current showModalBottomSheet helper mechanism is in bottom_sheet.dart and has a lot of Material concepts baked in. Extract into common utility.",
      "title": "Extract modal bottom action sheet concept from Material to Widgets"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2700,
    "text": "Should the modal barrier of an Alert also change the color of the status bar?Steps to Reproduce\n\nCreate a custom Status Bar color with\n\nSystemChrome.setSystemUIOverlayStyle(\n  SystemUiOverlayStyle.dark.copyWith(\nstatusBarColor: Colors.red,\n));\n\nThen create an AlertDialog that shows up on an event\n\nnew AlertDialog(\n                    title: new Text(\"Something went wrong!\"),\n                    content:\n                        new Text(\"Be sure you are connected to the Device\"),\n                    actions: [\n                      new FlatButton(\n                          child: const Text(\"Ok\"),\n                          onPressed: () {\n                            Navigator.pop(context);\n                            _alertShown = 0;\n                          }),\n                    ])\nWhen the AlertDialog is fired up the custom status bar color stays bright and its opacity is not affected",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "20",
      "number": "24969",
      "pretext": "Steps to Reproduce\n\nCreate a custom Status Bar color with\n\nSystemChrome.setSystemUIOverlayStyle(\n  SystemUiOverlayStyle.dark.copyWith(\nstatusBarColor: Colors.red,\n));\n\nThen create an AlertDialog that shows up on an event\n\nnew AlertDialog(\n                    title: new Text(\"Something went wrong!\"),\n                    content:\n                        new Text(\"Be sure you are connected to the Device\"),\n                    actions: [\n                      new FlatButton(\n                          child: const Text(\"Ok\"),\n                          onPressed: () {\n                            Navigator.pop(context);\n                            _alertShown = 0;\n                          }),\n                    ])\nWhen the AlertDialog is fired up the custom status bar color stays bright and its opacity is not affected",
      "title": "Should the modal barrier of an Alert also change the color of the status bar?"
    },
    "annotation_approver": null
  },
  {
    "id": 2701,
    "text": "webview_flutter: showDialog displays the dialog behind any webviewsSteps to Reproduce\n\nCreate an app with a widget that contains a WebView.\nIn that widget, add a WillPopScope widget.\nIn the WillPopScope.onWillPop event handler, call showDialog() to show a dialog.\n\nFlutter doctor output\n[✓] Flutter (Channel beta, v0.11.9, on Mac OS X 10.14.4 18E227, locale en-US)\n• Flutter version 0.11.9 at /Users/cvolzke/development/flutter\n• Framework revision d48e6e4 (6 months ago), 2018-11-20 22:05:23 -0500\n• Engine revision 5c81474\n• Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)\n[!] Android toolchain - develop for Android devices (Android SDK 28.0.3)\n• Android SDK at /Users/cvolzke/Library/Android/sdk\n• Android NDK location not configured (optional; useful for native profiling support)\n• Platform android-stable, build-tools 28.0.3\n• Java binary at: /Library/Java/JavaVirtualMachines/jdk-10-latest/Contents/Home/bin/java\n• Java version Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)\n✗ Android license status unknown.\n[!] iOS toolchain - develop for iOS devices (Xcode 10.0)\n• Xcode at /Applications/Xcode.app/Contents/Developer\n• Xcode 10.0, Build version 10A255\n✗ Verify that all connected devices have been paired with this computer in Xcode.\nIf all devices have been paired, libimobiledevice and ideviceinstaller may require updating.\nTo update with Brew, run:\nbrew update\nbrew uninstall --ignore-dependencies libimobiledevice\nbrew uninstall --ignore-dependencies usbmuxd\nbrew install --HEAD usbmuxd\nbrew unlink usbmuxd\nbrew link usbmuxd\nbrew install --HEAD libimobiledevice\nbrew install ideviceinstaller\n• ios-deploy 1.9.4\n• CocoaPods version 1.5.3\n[!] Android Studio (not installed)\n• Android Studio not found; download from https://developer.android.com/studio/index.html\n(or visit https://flutter.io/setup/#android-setup for detailed instructions).\n[!] IntelliJ IDEA Ultimate Edition (version 2018.1.4)\n• IntelliJ at /Applications/IntelliJ IDEA.app\n✗ Flutter plugin not installed; this adds Flutter specific functionality.\n✗ Dart plugin not installed; this adds Dart specific functionality.\n• For information about installing plugins, see\nhttps://flutter.io/intellij-setup/#installing-the-plugins\n[!] Connected device\n! No devices available",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "21",
      "number": "32445",
      "pretext": "Steps to Reproduce\n\nCreate an app with a widget that contains a WebView.\nIn that widget, add a WillPopScope widget.\nIn the WillPopScope.onWillPop event handler, call showDialog() to show a dialog.\n\nFlutter doctor output\n[✓] Flutter (Channel beta, v0.11.9, on Mac OS X 10.14.4 18E227, locale en-US)\n• Flutter version 0.11.9 at /Users/cvolzke/development/flutter\n• Framework revision d48e6e4 (6 months ago), 2018-11-20 22:05:23 -0500\n• Engine revision 5c81474\n• Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)\n[!] Android toolchain - develop for Android devices (Android SDK 28.0.3)\n• Android SDK at /Users/cvolzke/Library/Android/sdk\n• Android NDK location not configured (optional; useful for native profiling support)\n• Platform android-stable, build-tools 28.0.3\n• Java binary at: /Library/Java/JavaVirtualMachines/jdk-10-latest/Contents/Home/bin/java\n• Java version Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)\n✗ Android license status unknown.\n[!] iOS toolchain - develop for iOS devices (Xcode 10.0)\n• Xcode at /Applications/Xcode.app/Contents/Developer\n• Xcode 10.0, Build version 10A255\n✗ Verify that all connected devices have been paired with this computer in Xcode.\nIf all devices have been paired, libimobiledevice and ideviceinstaller may require updating.\nTo update with Brew, run:\nbrew update\nbrew uninstall --ignore-dependencies libimobiledevice\nbrew uninstall --ignore-dependencies usbmuxd\nbrew install --HEAD usbmuxd\nbrew unlink usbmuxd\nbrew link usbmuxd\nbrew install --HEAD libimobiledevice\nbrew install ideviceinstaller\n• ios-deploy 1.9.4\n• CocoaPods version 1.5.3\n[!] Android Studio (not installed)\n• Android Studio not found; download from https://developer.android.com/studio/index.html\n(or visit https://flutter.io/setup/#android-setup for detailed instructions).\n[!] IntelliJ IDEA Ultimate Edition (version 2018.1.4)\n• IntelliJ at /Applications/IntelliJ IDEA.app\n✗ Flutter plugin not installed; this adds Flutter specific functionality.\n✗ Dart plugin not installed; this adds Dart specific functionality.\n• For information about installing plugins, see\nhttps://flutter.io/intellij-setup/#installing-the-plugins\n[!] Connected device\n! No devices available",
      "title": "webview_flutter: showDialog displays the dialog behind any webviews"
    },
    "annotation_approver": null
  },
  {
    "id": 2702,
    "text": "assertion in material/input_decorator.dart seems not correctvoid removeChildRenderObject(RenderObject child) {\n    assert(child is RenderBox);\n    assert(renderObject.childToSlot.keys.contains(child));\n    _updateRenderObject(null, renderObject.childToSlot[child]);\n    assert(!renderObject.childToSlot.keys.contains(child));\n    assert(!renderObject.slotToChild.keys.contains(slot));\n  }\n\nThe last line  assert(!renderObject.slotToChild.keys.contains(slot)) seems incorrect, since slot is for _RenderDecorationElement. It should assert !renderObject.slotToChild.keys.contains(renderObject.childToSlot[child]));",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "22",
      "number": "30434",
      "pretext": "void removeChildRenderObject(RenderObject child) {\n    assert(child is RenderBox);\n    assert(renderObject.childToSlot.keys.contains(child));\n    _updateRenderObject(null, renderObject.childToSlot[child]);\n    assert(!renderObject.childToSlot.keys.contains(child));\n    assert(!renderObject.slotToChild.keys.contains(slot));\n  }\n\nThe last line  assert(!renderObject.slotToChild.keys.contains(slot)) seems incorrect, since slot is for _RenderDecorationElement. It should assert !renderObject.slotToChild.keys.contains(renderObject.childToSlot[child]));",
      "title": "assertion in material/input_decorator.dart seems not correct"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2703,
    "text": "iOS simulator builds have clear toolbarRan flutter init and launched on an iOS simulator using https://github.com/flutter/engine/wiki/Flutter-Apps-on-iOS\nGot an invisible toolbar background (you can faintly see the white text overlaid on it):\n\nRunning on an iOS device works fine.\nOther sample apps are also having issues with the simulator, but this is probably the simplest reduced test case.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "23",
      "number": "839",
      "pretext": "Ran flutter init and launched on an iOS simulator using https://github.com/flutter/engine/wiki/Flutter-Apps-on-iOS\nGot an invisible toolbar background (you can faintly see the white text overlaid on it):\n\nRunning on an iOS device works fine.\nOther sample apps are also having issues with the simulator, but this is probably the simplest reduced test case.",
      "title": "iOS simulator builds have clear toolbar"
    },
    "annotation_approver": null
  },
  {
    "id": 2704,
    "text": "MaterialApp should have a debugShowKeylines modeWe should have a mode where we show the keylines from this picture, exactly the way they're shown in this picture:\nhttps://www.google.com/design/spec/layout/metrics-keylines.html#metrics-keylines-ratio-keylines",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "24",
      "number": "1147",
      "pretext": "We should have a mode where we show the keylines from this picture, exactly the way they're shown in this picture:\nhttps://www.google.com/design/spec/layout/metrics-keylines.html#metrics-keylines-ratio-keylines",
      "title": "MaterialApp should have a debugShowKeylines mode"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2705,
    "text": "firebase_database: FirebaseAnimatedList should expose index to itemBuilderThis is regarding firebase_database 0.0.14\nI noticed that the itemBuilder for FirebaseAnimatedList has the following typedef:\ntypedef Widget FirebaseAnimatedListItemBuilder(\n  BuildContext context,\n  DataSnapshot snapshot,\n  Animation<double> animation,\n);\n\nThe itemBuilder of AnimatedList has the following typedef:\ntypedef Widget AnimatedListItemBuilder(\n  BuildContext context,\n  int index, \n  Animation<double> animation,\n);\n\nThe index of the current element being rendered is essentially being hidden by FirebaseAnimatedList. It would be very useful for creating numbered lists / striped lists etc.\nIs this omission by design? Can we add index to FirebaseAnimatedListItemBuilder?\nI am aware that this is a breaking change so I'm open to feedback.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "25",
      "number": "11760",
      "pretext": "This is regarding firebase_database 0.0.14\nI noticed that the itemBuilder for FirebaseAnimatedList has the following typedef:\ntypedef Widget FirebaseAnimatedListItemBuilder(\n  BuildContext context,\n  DataSnapshot snapshot,\n  Animation<double> animation,\n);\n\nThe itemBuilder of AnimatedList has the following typedef:\ntypedef Widget AnimatedListItemBuilder(\n  BuildContext context,\n  int index, \n  Animation<double> animation,\n);\n\nThe index of the current element being rendered is essentially being hidden by FirebaseAnimatedList. It would be very useful for creating numbered lists / striped lists etc.\nIs this omission by design? Can we add index to FirebaseAnimatedListItemBuilder?\nI am aware that this is a breaking change so I'm open to feedback.",
      "title": "firebase_database: FirebaseAnimatedList should expose index to itemBuilder"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2706,
    "text": "Improve Flutter's memory consumptionThis is a tracking bug for the collection of sub-issues around improving Flutter's memory consumption, and offering developers more flexibility in choosing where to set the dial on the time performance vs memory consumption spectrum.\nRelated issues:\n\n #13493: Flutter should provide more control over image caching\n  #26187: Flutter should be smarter about memory limits for images\n #16995: Free resources after a Flutter view is disposed\n #19358: Unmount everything and dispose states when host activity dies\n #23231: Memory leaks on iOS\n #20690: More deterministic measurement of memory consumption.\n #26081: Disable in-memory decoded frame cache by default\n #26443: Consider mipmapping ui.Images generated by the engine\n #15479: OnMemoryPressure is unreliable on Android\n #25155: Raster cache images may be much bigger than the visible/clipped area\n #19558: IO thread GrContext memory needs to be cleaned up\n #44013: Persist the memory profile timeline",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "26",
      "number": "32156",
      "pretext": "This is a tracking bug for the collection of sub-issues around improving Flutter's memory consumption, and offering developers more flexibility in choosing where to set the dial on the time performance vs memory consumption spectrum.\nRelated issues:\n\n #13493: Flutter should provide more control over image caching\n  #26187: Flutter should be smarter about memory limits for images\n #16995: Free resources after a Flutter view is disposed\n #19358: Unmount everything and dispose states when host activity dies\n #23231: Memory leaks on iOS\n #20690: More deterministic measurement of memory consumption.\n #26081: Disable in-memory decoded frame cache by default\n #26443: Consider mipmapping ui.Images generated by the engine\n #15479: OnMemoryPressure is unreliable on Android\n #25155: Raster cache images may be much bigger than the visible/clipped area\n #19558: IO thread GrContext memory needs to be cleaned up\n #44013: Persist the memory profile timeline",
      "title": "Improve Flutter's memory consumption"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2707,
    "text": "Support progress bar \"query indeterminate\" modehttps://www.google.com/design/spec/components/progress-activity.html#progress-activity-types-of-indicators",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "27",
      "number": "3112",
      "pretext": "https://www.google.com/design/spec/components/progress-activity.html#progress-activity-types-of-indicators",
      "title": "Support progress bar \"query indeterminate\" mode"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2708,
    "text": "[firebase_admob] - app compilation crashed after the plugin has been addedSteps to Reproduce\n\n\nPlugin added to pubspec.yaml. Below is the list of dependencies section:\n\ndependencies:\n  flutter:\n    sdk: flutter\n  flutter_colorpicker: any\n  logging: ^0.11.3+2\n  date_calendar: ^0.2.0\n  shared_preferences: ^0.4.3\n  intl: ^0.15.7\n  flutter_localizations:\n    sdk: flutter\n  fluro: ^1.4.0\n  rxdart: ^0.20.0\n  sqflite: ^1.1.0\n  json_serializable: ^2.0.2\n  charts_flutter: ^0.5.0\n  flutter_local_notifications: 0.5.1+2\n  flutter_calendar_carousel: 1.3.13\n  sprintf: ^4.0.2\n  firebase_admob: 0.7.0\n\nUsed version 0.7.0 according to AndroidX compatibility list\n\nAdded to AndroidManifest.xml\n\n<meta-data\n            android:name=\"com.google.android.gms.ads.APP_ID\"\n            android:value=\"[ADMOB_APP_ID]\"/>\n\nwhere:\n\nAPP_ID is my app id e.g. com.program.myapp and\nADMOB_APP_ID is the AdMob id (I have registered AdMob account)\n\nNote: My app is NOT yet published on Google play\n\nAdded to android level build.gradle\n\nsubprojects {\n    project.configurations.all {\n        resolutionStrategy.eachDependency { details ->\n            if (details.requested.group == 'androidx.core' &&\n                    !details.requested.name.contains('androidx')) {\n                details.useVersion \"1.0.1\"\n            }\n        }\n    }\n}\n\n\nAdded to app level build.gradle\n\ndependencies {\n    ....\n    implementation 'com.google.android.gms:play-services-ads:17.1.1'\n}\n\n\nStart building. Below is the output\n\nD8: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zzb`.\nD8: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.\ncom.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)\n\tat java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)\n\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)\n\tat com.android.ide.common.internal.WaitableExecutor.waitForTasksWithQuickFail(WaitableExecutor.java:146)\n\tat com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.transform(DexArchiveBuilderTransform.java:405)\n\tat com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:239)\n\tat com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:235)\n\tat com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)\n\tat com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:230)\n\tat sun.reflect.GeneratedMethodAccessor825.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)\n\tat org.gradle.api.internal.project.taskfactory.IncrementalTaskAction.doExecute(IncrementalTaskAction.java:50)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:131)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:120)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:99)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:77)\n\tat org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)\n\tat org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:59)\n\tat org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)\n\tat org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:59)\n\tat org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:101)\n\tat org.gradle.api.internal.tasks.execution.FinalizeInputFilePropertiesTaskExecuter.execute(FinalizeInputFilePropertiesTaskExecuter.java:44)\n\tat org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:91)\n\tat org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:62)\n\tat org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:59)\n\tat org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)\n\tat org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)\n\tat org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.run(EventFiringTaskExecuter.java:51)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:46)\n\tat org.gradle.execution.taskgraph.LocalTaskInfoExecutor.execute(LocalTaskInfoExecutor.java:42)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:277)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:262)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:135)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:130)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.execute(DefaultTaskPlanExecutor.java:200)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.executeWithWork(DefaultTaskPlanExecutor.java:191)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.run(DefaultTaskPlanExecutor.java:130)\n\tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\n\tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n\tat com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:900)\n\tat com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.lambda$convertToDexArchive$6(DexArchiveBuilderTransform.java:825)\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\nCaused by: com.android.builder.dexing.DexArchiveBuilderException: Error while dexing.\n\tat com.android.builder.dexing.D8DexArchiveBuilder.getExceptionToRethrow(D8DexArchiveBuilder.java:124)\n\tat com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:101)\n\tat com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:895)\n\t... 6 more\nCaused by: com.android.tools.r8.CompilationFailedException: Compilation failed to complete\n\tat com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:65)\n\tat com.android.tools.r8.utils.ExceptionUtils.withD8CompilationHandler(ExceptionUtils.java:43)\n\tat com.android.tools.r8.D8.run(D8.java:90)\n\tat com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:99)\n\t... 7 more\nCaused by: com.android.tools.r8.utils.AbortException: Error: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.\n\tat com.android.tools.r8.utils.Reporter.failIfPendingErrors(Reporter.java:116)\n\tat com.android.tools.r8.utils.Reporter.fatalError(Reporter.java:74)\n\tat com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:59)\n\t... 10 more\n\n\nFAILURE: Build failed with an exception.\n\n* What went wrong:\nExecution failed for task ':app:transformClassesWithDexBuilderForDevelopmentDebug'.\n> com.android.build.api.transform.TransformException: com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n\n* Try:\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\n\n* Get more help at https://help.gradle.org\n\nBUILD FAILED in 10s\nFinished with error: Gradle task assembleDevelopmentDebug failed with exit code 1\n\n\nLogs\n\nflutter run --verbose log\n[+2400 ms] D8: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zzb`.\n[  +76 ms] D8: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.\n[        ] com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\c\naches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n[   +5 ms]      at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n[        ]      at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n[        ]      at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n[        ]      at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n[   +6 ms]      at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)\n[   +1 ms]      at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)\n[  +15 ms]      at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)\n[   +1 ms]      at com.android.ide.common.internal.WaitableExecutor.waitForTasksWithQuickFail(WaitableExecutor.java:146)\n[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.transform(DexArchiveBuilderTransform.java:405)\n[        ]      at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:239)\n[        ]      at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:235)\n[        ]      at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)\n[        ]      at com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:230)\n[        ]      at sun.reflect.GeneratedMethodAccessor825.invoke(Unknown Source)\n[        ]      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[        ]      at java.lang.reflect.Method.invoke(Method.java:498)\n[        ]      at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)\n[        ]      at org.gradle.api.internal.project.taskfactory.IncrementalTaskAction.doExecute(IncrementalTaskAction.java:50)\n[        ]      at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39)\n[        ]      at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:131)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n[        ]      at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:120)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:99)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:77)\n[        ]      at org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)\n[        ]      at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:59)\n[        ]      at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)\n[        ]      at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:59)\n[        ]      at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:101)\n[        ]      at org.gradle.api.internal.tasks.execution.FinalizeInputFilePropertiesTaskExecuter.execute(FinalizeInputFilePropertiesTaskExecuter.java:44)\n[        ]      at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:91)\n[        ]      at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:62)\n[        ]      at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:59)\n[        ]      at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)\n[        ]      at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)\n[        ]      at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.run(EventFiringTaskExecuter.java:51)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n[        ]      at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n[        ]      at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:46)\n[        ]      at org.gradle.execution.taskgraph.LocalTaskInfoExecutor.execute(LocalTaskInfoExecutor.java:42)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:277)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:262)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:135)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:130)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.execute(DefaultTaskPlanExecutor.java:200)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.executeWithWork(DefaultTaskPlanExecutor.java:191)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.run(DefaultTaskPlanExecutor.java:130)\n[        ]      at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\n[        ]      at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\n[        ]      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n[        ]      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n[        ]      at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n[        ]      at java.lang.Thread.run(Thread.java:745)\n[        ] Caused by: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-fireba\nse-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:900)\n[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.lambda$convertToDexArchive$6(DexArchiveBuilderTransform.java:825)\n[        ]      at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)\n[        ]      at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n[        ]      at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n[        ]      at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n[   +1 ms]      at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\n[  +12 ms] Caused by: com.android.builder.dexing.DexArchiveBuilderException: Error while dexing.\n[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.getExceptionToRethrow(D8DexArchiveBuilder.java:124)\n[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:101)\n[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:895)\n[        ]      ... 6 more\n[        ] Caused by: com.android.tools.r8.CompilationFailedException: Compilation failed to complete\n[        ]      at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:65)\n[        ]      at com.android.tools.r8.utils.ExceptionUtils.withD8CompilationHandler(ExceptionUtils.java:43)\n[        ]      at com.android.tools.r8.D8.run(D8.java:90)\n[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:99)\n[        ]      ... 7 more\n[        ] Caused by: com.android.tools.r8.utils.AbortException: Error: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `c\nom.google.android.gms.internal.measurement.zzfq$zzb$zzb`.\n[        ]      at com.android.tools.r8.utils.Reporter.failIfPendingErrors(Reporter.java:116)\n[   +1 ms]      at com.android.tools.r8.utils.Reporter.fatalError(Reporter.java:74)\n[        ]      at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:59)\n[        ]      ... 10 more\n[  +73 ms] FAILURE: Build failed with an exception.\n[        ] * What went wrong:\n[        ] Execution failed for task ':app:transformClassesWithDexBuilderForDevelopmentDebug'.\n[        ] > com.android.build.api.transform.TransformException: com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilde\nrException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\cla\nsses.jar\n[        ] * Try:\n[        ] Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\n[        ] * Get more help at https://help.gradle.org\n[        ] BUILD FAILED in 9s\n[        ] *******************************************************************************************\n[   +1 ms] The Gradle failure may have been because of AndroidX incompatibilities in this Flutter app.\n[  +13 ms] See https://goo.gl/CP92wY for more information on the problem and how to fix it.\n[        ] *******************************************************************************************\nGradle task assembleDevelopmentDebug failed with exit code 1\n\n#0      throwToolExit (package:flutter_tools/src/base/common.dart:24:3)\n#1      _buildGradleProjectV2 (package:flutter_tools/src/android/gradle.dart:462:5)\n<asynchronous suspension>\n#2      buildGradleProject (package:flutter_tools/src/android/gradle.dart:331:14)\n<asynchronous suspension>\n#3      buildApk (package:flutter_tools/src/android/apk.dart:43:10)\n<asynchronous suspension>\n#4      AndroidDevice.startApp (package:flutter_tools/src/android/android_device.dart:378:13)\n<asynchronous suspension>\n#5      FlutterDevice.runHot (package:flutter_tools/src/resident_runner.dart:308:54)\n<asynchronous suspension>\n#6      HotRunner.run (package:flutter_tools/src/run_hot.dart:295:39)\n<asynchronous suspension>\n#7      RunCommand.runCommand (package:flutter_tools/src/commands/run.dart:405:37)\n<asynchronous suspension>\n#8      FlutterCommand.verifyThenRunCommand (package:flutter_tools/src/runner/flutter_command.dart:545:18)\n#9      _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)\n#10     _rootRunUnary (dart:async/zone.dart:1132:38)\n#11     _CustomZone.runUnary (dart:async/zone.dart:1029:19)\n#12     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)\n#13     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)\n#14     Future._propagateToListeners (dart:async/future_impl.dart:668:32)\n#15     Future._complete (dart:async/future_impl.dart:473:7)\n#16     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)\n#17     _AsyncAwaitCompleter.complete (dart:async/runtime/libasync_patch.dart:28:18)\n#18     _completeOnAsyncReturn (dart:async/runtime/libasync_patch.dart:294:13)\n#19     RunCommand.usageValues (package:flutter_tools/src/commands/run.dart)\n#20     _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)\n#21     _rootRunUnary (dart:async/zone.dart:1132:38)\n#22     _CustomZone.runUnary (dart:async/zone.dart:1029:19)\n#23     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)\n#24     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)\n#25     Future._propagateToListeners (dart:async/future_impl.dart:668:32)\n#26     Future._complete (dart:async/future_impl.dart:473:7)\n#27     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)\n#28     _AsyncAwaitCompleter.complete (dart:async/runtime/libasync_patch.dart:28:18)\n#29     _completeOnAsyncReturn (dart:async/runtime/libasync_patch.dart:294:13)\n#30     AndroidDevice.targetPlatform (package:flutter_tools/src/android/android_device.dart)\n#31     _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)\n#32     _rootRunUnary (dart:async/zone.dart:1132:38)\n#33     _CustomZone.runUnary (dart:async/zone.dart:1029:19)\n#34     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)\n#35     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)\n#36     Future._propagateToListeners (dart:async/future_impl.dart:668:32)\n#37     Future._complete (dart:async/future_impl.dart:473:7)\n#38     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)\n#39     _AsyncAwaitCompleter.complete.<anonymous closure> (dart:async/runtime/libasync_patch.dart:33:20)\n#40     _rootRun (dart:async/zone.dart:1124:13)\n#41     _CustomZone.run (dart:async/zone.dart:1021:19)\n#42     _CustomZone.bindCallback.<anonymous closure> (dart:async/zone.dart:947:23)\n#43     _microtaskLoop (dart:async/schedule_microtask.dart:41:21)\n#44     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50:5)\n#45     _runPendingImmediateCallback (dart:isolate/runtime/libisolate_patch.dart:115:13)\n#46     _RawReceivePortImpl._handleMessage (dart:isolate/runtime/libisolate_patch.dart:172:5)\n\n\n\n\n\nflutter doctor -v log\n[√] Flutter (Channel stable, v1.2.1, on Microsoft Windows [Version 10.0.17134.590], locale en-US)\n    • Flutter version 1.2.1 at E:\\DevTools\\flutter\n    • Framework revision 8661d8aecd (13 days ago), 2019-02-14 19:19:53 -0800\n    • Engine revision 3757390fa4\n    • Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n\n[√] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at E:\\DevTools\\Android\\sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • ANDROID_HOME = E:\\DevTools\\Android\\sdk\n    • Java binary at: E:\\DevTools\\android-studio\\jre\\bin\\java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    • All Android licenses accepted.\n\n[√] Android Studio (version 3.3)\n    • Android Studio at E:\\DevTools\\android-studio\n    • Flutter plugin version 33.3.1\n    • Dart plugin version 182.5215\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[√] Connected device (1 available)\n    • Android SDK built for x86 64 • emulator-5554 • android-x64 • Android 8.0.0 (API 26) (emulator)\n\n• No issues found!\n\nSide notes:\n\nIn case step 4 is omitted, I can compile the app, but  then I get The Google Mobile Ads SDK was initialized incorrectly. although I believe all other settings are just fine.\nI also tried the most recent plugin version and it has the identical behavior.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "28",
      "number": "28647",
      "pretext": "Steps to Reproduce\n\n\nPlugin added to pubspec.yaml. Below is the list of dependencies section:\n\ndependencies:\n  flutter:\n    sdk: flutter\n  flutter_colorpicker: any\n  logging: ^0.11.3+2\n  date_calendar: ^0.2.0\n  shared_preferences: ^0.4.3\n  intl: ^0.15.7\n  flutter_localizations:\n    sdk: flutter\n  fluro: ^1.4.0\n  rxdart: ^0.20.0\n  sqflite: ^1.1.0\n  json_serializable: ^2.0.2\n  charts_flutter: ^0.5.0\n  flutter_local_notifications: 0.5.1+2\n  flutter_calendar_carousel: 1.3.13\n  sprintf: ^4.0.2\n  firebase_admob: 0.7.0\n\nUsed version 0.7.0 according to AndroidX compatibility list\n\nAdded to AndroidManifest.xml\n\n<meta-data\n            android:name=\"com.google.android.gms.ads.APP_ID\"\n            android:value=\"[ADMOB_APP_ID]\"/>\n\nwhere:\n\nAPP_ID is my app id e.g. com.program.myapp and\nADMOB_APP_ID is the AdMob id (I have registered AdMob account)\n\nNote: My app is NOT yet published on Google play\n\nAdded to android level build.gradle\n\nsubprojects {\n    project.configurations.all {\n        resolutionStrategy.eachDependency { details ->\n            if (details.requested.group == 'androidx.core' &&\n                    !details.requested.name.contains('androidx')) {\n                details.useVersion \"1.0.1\"\n            }\n        }\n    }\n}\n\n\nAdded to app level build.gradle\n\ndependencies {\n    ....\n    implementation 'com.google.android.gms:play-services-ads:17.1.1'\n}\n\n\nStart building. Below is the output\n\nD8: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zzb`.\nD8: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.\ncom.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)\n\tat java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)\n\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)\n\tat com.android.ide.common.internal.WaitableExecutor.waitForTasksWithQuickFail(WaitableExecutor.java:146)\n\tat com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.transform(DexArchiveBuilderTransform.java:405)\n\tat com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:239)\n\tat com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:235)\n\tat com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)\n\tat com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:230)\n\tat sun.reflect.GeneratedMethodAccessor825.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)\n\tat org.gradle.api.internal.project.taskfactory.IncrementalTaskAction.doExecute(IncrementalTaskAction.java:50)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:131)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:120)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:99)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:77)\n\tat org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)\n\tat org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:59)\n\tat org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)\n\tat org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:59)\n\tat org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:101)\n\tat org.gradle.api.internal.tasks.execution.FinalizeInputFilePropertiesTaskExecuter.execute(FinalizeInputFilePropertiesTaskExecuter.java:44)\n\tat org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:91)\n\tat org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:62)\n\tat org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:59)\n\tat org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)\n\tat org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)\n\tat org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.run(EventFiringTaskExecuter.java:51)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:46)\n\tat org.gradle.execution.taskgraph.LocalTaskInfoExecutor.execute(LocalTaskInfoExecutor.java:42)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:277)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:262)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:135)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:130)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.execute(DefaultTaskPlanExecutor.java:200)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.executeWithWork(DefaultTaskPlanExecutor.java:191)\n\tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.run(DefaultTaskPlanExecutor.java:130)\n\tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\n\tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n\tat com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:900)\n\tat com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.lambda$convertToDexArchive$6(DexArchiveBuilderTransform.java:825)\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\nCaused by: com.android.builder.dexing.DexArchiveBuilderException: Error while dexing.\n\tat com.android.builder.dexing.D8DexArchiveBuilder.getExceptionToRethrow(D8DexArchiveBuilder.java:124)\n\tat com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:101)\n\tat com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:895)\n\t... 6 more\nCaused by: com.android.tools.r8.CompilationFailedException: Compilation failed to complete\n\tat com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:65)\n\tat com.android.tools.r8.utils.ExceptionUtils.withD8CompilationHandler(ExceptionUtils.java:43)\n\tat com.android.tools.r8.D8.run(D8.java:90)\n\tat com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:99)\n\t... 7 more\nCaused by: com.android.tools.r8.utils.AbortException: Error: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.\n\tat com.android.tools.r8.utils.Reporter.failIfPendingErrors(Reporter.java:116)\n\tat com.android.tools.r8.utils.Reporter.fatalError(Reporter.java:74)\n\tat com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:59)\n\t... 10 more\n\n\nFAILURE: Build failed with an exception.\n\n* What went wrong:\nExecution failed for task ':app:transformClassesWithDexBuilderForDevelopmentDebug'.\n> com.android.build.api.transform.TransformException: com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n\n* Try:\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\n\n* Get more help at https://help.gradle.org\n\nBUILD FAILED in 10s\nFinished with error: Gradle task assembleDevelopmentDebug failed with exit code 1\n\n\nLogs\n\nflutter run --verbose log\n[+2400 ms] D8: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zzb`.\n[  +76 ms] D8: Type com.google.android.gms.internal.measurement.zzwv is referenced as an interface from `com.google.android.gms.internal.measurement.zzfq$zzb$zza`.\n[        ] com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\c\naches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n[   +5 ms]      at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n[        ]      at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n[        ]      at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n[        ]      at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n[   +6 ms]      at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)\n[   +1 ms]      at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)\n[  +15 ms]      at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)\n[   +1 ms]      at com.android.ide.common.internal.WaitableExecutor.waitForTasksWithQuickFail(WaitableExecutor.java:146)\n[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.transform(DexArchiveBuilderTransform.java:405)\n[        ]      at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:239)\n[        ]      at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:235)\n[        ]      at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)\n[        ]      at com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:230)\n[        ]      at sun.reflect.GeneratedMethodAccessor825.invoke(Unknown Source)\n[        ]      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[        ]      at java.lang.reflect.Method.invoke(Method.java:498)\n[        ]      at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)\n[        ]      at org.gradle.api.internal.project.taskfactory.IncrementalTaskAction.doExecute(IncrementalTaskAction.java:50)\n[        ]      at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39)\n[        ]      at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:131)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n[        ]      at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:120)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:99)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:77)\n[        ]      at org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)\n[        ]      at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:59)\n[        ]      at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)\n[        ]      at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:59)\n[        ]      at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:101)\n[        ]      at org.gradle.api.internal.tasks.execution.FinalizeInputFilePropertiesTaskExecuter.execute(FinalizeInputFilePropertiesTaskExecuter.java:44)\n[        ]      at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:91)\n[        ]      at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:62)\n[        ]      at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:59)\n[        ]      at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)\n[        ]      at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)\n[        ]      at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)\n[        ]      at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.run(EventFiringTaskExecuter.java:51)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:300)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:292)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:174)\n[        ]      at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n[        ]      at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n[        ]      at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:46)\n[        ]      at org.gradle.execution.taskgraph.LocalTaskInfoExecutor.execute(LocalTaskInfoExecutor.java:42)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:277)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareWorkItemExecutor.execute(DefaultTaskExecutionGraph.java:262)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:135)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:130)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.execute(DefaultTaskPlanExecutor.java:200)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.executeWithWork(DefaultTaskPlanExecutor.java:191)\n[        ]      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$ExecutorWorker.run(DefaultTaskPlanExecutor.java:130)\n[        ]      at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\n[        ]      at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\n[        ]      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n[        ]      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n[        ]      at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n[        ]      at java.lang.Thread.run(Thread.java:745)\n[        ] Caused by: com.android.builder.dexing.DexArchiveBuilderException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-fireba\nse-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\classes.jar\n[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:900)\n[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.lambda$convertToDexArchive$6(DexArchiveBuilderTransform.java:825)\n[        ]      at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)\n[        ]      at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n[        ]      at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n[        ]      at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n[   +1 ms]      at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\n[  +12 ms] Caused by: com.android.builder.dexing.DexArchiveBuilderException: Error while dexing.\n[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.getExceptionToRethrow(D8DexArchiveBuilder.java:124)\n[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:101)\n[        ]      at com.android.build.gradle.internal.transforms.DexArchiveBuilderTransform.launchProcessing(DexArchiveBuilderTransform.java:895)\n[        ]      ... 6 more\n[        ] Caused by: com.android.tools.r8.CompilationFailedException: Compilation failed to complete\n[        ]      at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:65)\n[        ]      at com.android.tools.r8.utils.ExceptionUtils.withD8CompilationHandler(ExceptionUtils.java:43)\n[        ]      at com.android.tools.r8.D8.run(D8.java:90)\n[        ]      at com.android.builder.dexing.D8DexArchiveBuilder.convert(D8DexArchiveBuilder.java:99)\n[        ]      ... 7 more\n[        ] Caused by: com.android.tools.r8.utils.AbortException: Error: Type com.google.android.gms.internal.measurement.zzvp is referenced as an interface from `c\nom.google.android.gms.internal.measurement.zzfq$zzb$zzb`.\n[        ]      at com.android.tools.r8.utils.Reporter.failIfPendingErrors(Reporter.java:116)\n[   +1 ms]      at com.android.tools.r8.utils.Reporter.fatalError(Reporter.java:74)\n[        ]      at com.android.tools.r8.utils.ExceptionUtils.withCompilationHandler(ExceptionUtils.java:59)\n[        ]      ... 10 more\n[  +73 ms] FAILURE: Build failed with an exception.\n[        ] * What went wrong:\n[        ] Execution failed for task ':app:transformClassesWithDexBuilderForDevelopmentDebug'.\n[        ] > com.android.build.api.transform.TransformException: com.android.builder.dexing.DexArchiveBuilderException: com.android.builder.dexing.DexArchiveBuilde\nrException: Failed to process C:\\Users\\angel\\.gradle\\caches\\transforms-1\\files-1.1\\jetified-firebase-analytics-16.0.4.aar\\cfb42b1b1914d3415eb1bdec7778a942\\jars\\cla\nsses.jar\n[        ] * Try:\n[        ] Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\n[        ] * Get more help at https://help.gradle.org\n[        ] BUILD FAILED in 9s\n[        ] *******************************************************************************************\n[   +1 ms] The Gradle failure may have been because of AndroidX incompatibilities in this Flutter app.\n[  +13 ms] See https://goo.gl/CP92wY for more information on the problem and how to fix it.\n[        ] *******************************************************************************************\nGradle task assembleDevelopmentDebug failed with exit code 1\n\n#0      throwToolExit (package:flutter_tools/src/base/common.dart:24:3)\n#1      _buildGradleProjectV2 (package:flutter_tools/src/android/gradle.dart:462:5)\n<asynchronous suspension>\n#2      buildGradleProject (package:flutter_tools/src/android/gradle.dart:331:14)\n<asynchronous suspension>\n#3      buildApk (package:flutter_tools/src/android/apk.dart:43:10)\n<asynchronous suspension>\n#4      AndroidDevice.startApp (package:flutter_tools/src/android/android_device.dart:378:13)\n<asynchronous suspension>\n#5      FlutterDevice.runHot (package:flutter_tools/src/resident_runner.dart:308:54)\n<asynchronous suspension>\n#6      HotRunner.run (package:flutter_tools/src/run_hot.dart:295:39)\n<asynchronous suspension>\n#7      RunCommand.runCommand (package:flutter_tools/src/commands/run.dart:405:37)\n<asynchronous suspension>\n#8      FlutterCommand.verifyThenRunCommand (package:flutter_tools/src/runner/flutter_command.dart:545:18)\n#9      _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)\n#10     _rootRunUnary (dart:async/zone.dart:1132:38)\n#11     _CustomZone.runUnary (dart:async/zone.dart:1029:19)\n#12     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)\n#13     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)\n#14     Future._propagateToListeners (dart:async/future_impl.dart:668:32)\n#15     Future._complete (dart:async/future_impl.dart:473:7)\n#16     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)\n#17     _AsyncAwaitCompleter.complete (dart:async/runtime/libasync_patch.dart:28:18)\n#18     _completeOnAsyncReturn (dart:async/runtime/libasync_patch.dart:294:13)\n#19     RunCommand.usageValues (package:flutter_tools/src/commands/run.dart)\n#20     _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)\n#21     _rootRunUnary (dart:async/zone.dart:1132:38)\n#22     _CustomZone.runUnary (dart:async/zone.dart:1029:19)\n#23     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)\n#24     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)\n#25     Future._propagateToListeners (dart:async/future_impl.dart:668:32)\n#26     Future._complete (dart:async/future_impl.dart:473:7)\n#27     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)\n#28     _AsyncAwaitCompleter.complete (dart:async/runtime/libasync_patch.dart:28:18)\n#29     _completeOnAsyncReturn (dart:async/runtime/libasync_patch.dart:294:13)\n#30     AndroidDevice.targetPlatform (package:flutter_tools/src/android/android_device.dart)\n#31     _asyncThenWrapperHelper.<anonymous closure> (dart:async/runtime/libasync_patch.dart:77:64)\n#32     _rootRunUnary (dart:async/zone.dart:1132:38)\n#33     _CustomZone.runUnary (dart:async/zone.dart:1029:19)\n#34     _FutureListener.handleValue (dart:async/future_impl.dart:126:18)\n#35     Future._propagateToListeners.handleValueCallback (dart:async/future_impl.dart:639:45)\n#36     Future._propagateToListeners (dart:async/future_impl.dart:668:32)\n#37     Future._complete (dart:async/future_impl.dart:473:7)\n#38     _SyncCompleter.complete (dart:async/future_impl.dart:51:12)\n#39     _AsyncAwaitCompleter.complete.<anonymous closure> (dart:async/runtime/libasync_patch.dart:33:20)\n#40     _rootRun (dart:async/zone.dart:1124:13)\n#41     _CustomZone.run (dart:async/zone.dart:1021:19)\n#42     _CustomZone.bindCallback.<anonymous closure> (dart:async/zone.dart:947:23)\n#43     _microtaskLoop (dart:async/schedule_microtask.dart:41:21)\n#44     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50:5)\n#45     _runPendingImmediateCallback (dart:isolate/runtime/libisolate_patch.dart:115:13)\n#46     _RawReceivePortImpl._handleMessage (dart:isolate/runtime/libisolate_patch.dart:172:5)\n\n\n\n\n\nflutter doctor -v log\n[√] Flutter (Channel stable, v1.2.1, on Microsoft Windows [Version 10.0.17134.590], locale en-US)\n    • Flutter version 1.2.1 at E:\\DevTools\\flutter\n    • Framework revision 8661d8aecd (13 days ago), 2019-02-14 19:19:53 -0800\n    • Engine revision 3757390fa4\n    • Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n\n[√] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at E:\\DevTools\\Android\\sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • ANDROID_HOME = E:\\DevTools\\Android\\sdk\n    • Java binary at: E:\\DevTools\\android-studio\\jre\\bin\\java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    • All Android licenses accepted.\n\n[√] Android Studio (version 3.3)\n    • Android Studio at E:\\DevTools\\android-studio\n    • Flutter plugin version 33.3.1\n    • Dart plugin version 182.5215\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[√] Connected device (1 available)\n    • Android SDK built for x86 64 • emulator-5554 • android-x64 • Android 8.0.0 (API 26) (emulator)\n\n• No issues found!\n\nSide notes:\n\nIn case step 4 is omitted, I can compile the app, but  then I get The Google Mobile Ads SDK was initialized incorrectly. although I believe all other settings are just fine.\nI also tried the most recent plugin version and it has the identical behavior.",
      "title": "[firebase_admob] - app compilation crashed after the plugin has been added"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2709,
    "text": "firebase_messaging notification doesn't show if app is killed#Hi! I copy the example from this link:\nhttps://github.com/flutter/plugins/tree/master/packages/firebase_messaging/example\nBut it works only with app in foreground or in background.\nIf I send a notification with the app killed from task manager, the notification doesn't show.\nThis is what I send on body :\n{\n  \"notification\": {\"body\": \"message\",\"title\": \"this is a title\"}, \n  \"priority\": \"high\",\n  \"data\": {\"click_action\": \"FLUTTER_NOTIFICATION_CLICK\", \"id\": \"1\", \"status\": \"done\"}, \n  \"to\": \"myToken\"\n}\nSomeone can help me?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "29",
      "number": "18675",
      "pretext": "#Hi! I copy the example from this link:\nhttps://github.com/flutter/plugins/tree/master/packages/firebase_messaging/example\nBut it works only with app in foreground or in background.\nIf I send a notification with the app killed from task manager, the notification doesn't show.\nThis is what I send on body :\n{\n  \"notification\": {\"body\": \"message\",\"title\": \"this is a title\"}, \n  \"priority\": \"high\",\n  \"data\": {\"click_action\": \"FLUTTER_NOTIFICATION_CLICK\", \"id\": \"1\", \"status\": \"done\"}, \n  \"to\": \"myToken\"\n}\nSomeone can help me?",
      "title": "firebase_messaging notification doesn't show if app is killed"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2710,
    "text": "Add iOS dependency to Flutter Plugin So far I've the plugin (standalone) and the app created. Now I need to (somehow) use and consume a static framework (obj-c) in iOS plugin.\nI've the file plugin.h and plugin.m and I added the mylib.framework to the root of iOS folder, next to podspec file. Also tried to add the s.dependency 'mylib' to it. Still, it doesn't seem to work at all. Everytime I try to flutter build ios on my main app (using the plugin) I get the  #import <myplugin/myplugin.h> not found\nPS: The library works if I add to the iOS project on the Flutter app, but I MUST use it on a plugin.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "30",
      "number": "17978",
      "pretext": "So far I've the plugin (standalone) and the app created. Now I need to (somehow) use and consume a static framework (obj-c) in iOS plugin.\nI've the file plugin.h and plugin.m and I added the mylib.framework to the root of iOS folder, next to podspec file. Also tried to add the s.dependency 'mylib' to it. Still, it doesn't seem to work at all. Everytime I try to flutter build ios on my main app (using the plugin) I get the  #import <myplugin/myplugin.h> not found\nPS: The library works if I add to the iOS project on the Flutter app, but I MUST use it on a plugin.",
      "title": "Add iOS dependency to Flutter Plugin "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2711,
    "text": "The site don't work fineHi guys, it's normal for the mobile version of the site to show up like that in the widget catalog and the Widget index",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "31",
      "number": "26292",
      "pretext": "Hi guys, it's normal for the mobile version of the site to show up like that in the widget catalog and the Widget index",
      "title": "The site don't work fine"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2712,
    "text": "Can not make project when open for editing in android studiol am writing an application with flutter and it works well at my android device ,but it builds failed  when l click \"open for editing in android studio\" ,hope someone can help me,\nflutter doctor:\n[✓] Flutter (Channel stable, v1.5.4, on Mac OS X 10.14.3 18D109, locale zh-Hans-CN)\n\n[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n[✓] iOS toolchain - develop for iOS devices (Xcode 10.2.1)\n[✓] Android Studio (version 3.4)\n[✓] IntelliJ IDEA Ultimate Edition (version 2019.1.1)\n[✓] Connected device (1 available)\n\nhere are the error logs:\norg.gradle.process.internal.ExecException: Process 'command '/Users/lierdong/development/flutter/bin/flutter'' finished with non-zero exit value 1\n\tat org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:396)\n\tat org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:37)\n\tat org.gradle.api.internal.file.DefaultFileOperations.exec(DefaultFileOperations.java:234)\n\tat org.gradle.api.internal.project.DefaultProject.exec(DefaultProject.java:1113)\n\tat org.gradle.api.internal.project.DefaultProject.exec(DefaultProject.java:1108)\n\tat org.gradle.api.Project$exec$7.call(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:115)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:127)\n\tat BaseFlutterTask.buildBundle(/Users/lierdong/development/flutter/packages/flutter_tools/gradle/flutter.gradle:529)\n\tat BaseFlutterTask$buildBundle.callCurrent(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:156)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:160)\n\tat FlutterTask.build(/Users/lierdong/development/flutter/packages/flutter_tools/gradle/flutter.gradle:667)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:48)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:41)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:28)\n\tat org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:704)\n\tat org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:671)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$2.run(ExecuteActionsTaskExecuter.java:284)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:301)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:293)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:273)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:258)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.access$200(ExecuteActionsTaskExecuter.java:67)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:145)\n\tat org.gradle.internal.execution.impl.steps.ExecuteStep.execute(ExecuteStep.java:49)\n\tat org.gradle.internal.execution.impl.steps.CancelExecutionStep.execute(CancelExecutionStep.java:34)\n\tat org.gradle.internal.execution.impl.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:69)\n\tat org.gradle.internal.execution.impl.steps.TimeoutStep.execute(TimeoutStep.java:49)\n\tat org.gradle.internal.execution.impl.steps.CatchExceptionStep.execute(CatchExceptionStep.java:33)\n\tat org.gradle.internal.execution.impl.steps.CreateOutputsStep.execute(CreateOutputsStep.java:50)\n\tat org.gradle.internal.execution.impl.steps.SnapshotOutputStep.execute(SnapshotOutputStep.java:43)\n\tat org.gradle.internal.execution.impl.steps.SnapshotOutputStep.execute(SnapshotOutputStep.java:29)\n\tat org.gradle.internal.execution.impl.steps.CacheStep.executeWithoutCache(CacheStep.java:134)\n\tat org.gradle.internal.execution.impl.steps.CacheStep.lambda$execute$3(CacheStep.java:83)\n\tat java.util.Optional.orElseGet(Optional.java:267)\n\tat org.gradle.internal.execution.impl.steps.CacheStep.execute(CacheStep.java:82)\n\tat org.gradle.internal.execution.impl.steps.CacheStep.execute(CacheStep.java:36)\n\tat org.gradle.internal.execution.impl.steps.PrepareCachingStep.execute(PrepareCachingStep.java:33)\n\tat org.gradle.internal.execution.impl.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:38)\n\tat org.gradle.internal.execution.impl.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:23)\n\tat org.gradle.internal.execution.impl.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96)\n\tat org.gradle.internal.execution.impl.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89)\n\tat java.util.Optional.map(Optional.java:215)\n\tat org.gradle.internal.execution.impl.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:52)\n\tat org.gradle.internal.execution.impl.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:36)\n\tat org.gradle.internal.execution.impl.DefaultWorkExecutor.execute(DefaultWorkExecutor.java:34)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:91)\n\tat org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:91)\n\tat org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:57)\n\tat org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:119)\n\tat org.gradle.api.internal.tasks.execution.ResolvePreviousStateExecuter.execute(ResolvePreviousStateExecuter.java:43)\n\tat org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:93)\n\tat org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:45)\n\tat org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:94)\n\tat org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:56)\n\tat org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:55)\n\tat org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:36)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.executeTask(EventFiringTaskExecuter.java:67)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:52)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:49)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:315)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:305)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:101)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:49)\n\tat org.gradle.execution.plan.LocalTaskNodeExecutor.execute(LocalTaskNodeExecutor.java:43)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:355)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129)\n\tat org.gradle.execution.plan.DefaultPlanExecutor.process(DefaultPlanExecutor.java:74)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph.executeWithServices(DefaultTaskExecutionGraph.java:178)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph.execute(DefaultTaskExecutionGraph.java:154)\n\tat org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:41)\n\tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:40)\n\tat org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:24)\n\tat org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:46)\n\tat org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:49)\n\tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:40)\n\tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:33)\n\tat org.gradle.initialization.DefaultGradleLauncher$ExecuteTasks.run(DefaultGradleLauncher.java:383)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:301)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:293)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n\tat org.gradle.initialization.DefaultGradleLauncher.runTasks(DefaultGradleLauncher.java:247)\n\tat org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:159)\n\tat org.gradle.initialization.DefaultGradleLauncher.executeTasks(DefaultGradleLauncher.java:134)\n\tat org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:58)\n\tat org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:55)\n\tat org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:82)\n\tat org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:75)\n\tat org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:183)\n\tat org.gradle.internal.work.StopShieldingWorkerLeaseService.withLocks(StopShieldingWorkerLeaseService.java:40)\n\tat org.gradle.internal.invocation.GradleBuildController.doBuild(GradleBuildController.java:75)\n\tat org.gradle.internal.invocation.GradleBuildController.run(GradleBuildController.java:55)\n\tat org.gradle.tooling.internal.provider.runner.BuildModelActionRunner.run(BuildModelActionRunner.java:54)\n\tat org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)\n\tat org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)\n\tat org.gradle.launcher.exec.BuildOutcomeReportingBuildActionRunner.run(BuildOutcomeReportingBuildActionRunner.java:58)\n\tat org.gradle.tooling.internal.provider.ValidatingBuildActionRunner.run(ValidatingBuildActionRunner.java:32)\n\tat org.gradle.launcher.exec.BuildCompletionNotifyingBuildActionRunner.run(BuildCompletionNotifyingBuildActionRunner.java:39)\n\tat org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:49)\n\tat org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:44)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:315)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:305)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:101)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)\n\tat org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner.run(RunAsBuildOperationBuildActionRunner.java:44)\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:49)\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:46)\n\tat org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:78)\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46)\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:31)\n\tat org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:42)\n\tat org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:28)\n\tat org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:78)\n\tat org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:52)\n\tat org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:59)\n\tat org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:36)\n\tat org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:68)\n\tat org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:38)\n\tat org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:37)\n\tat org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:26)\n\tat org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:43)\n\tat org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:29)\n\tat org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:60)\n\tat org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:32)\n\tat org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:55)\n\tat org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:41)\n\tat org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:48)\n\tat org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:32)\n\tat org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67)\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)\n\tat org.gradle.util.Swapper.swap(Swapper.java:38)\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:62)\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:81)\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)\n\tat org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:295)\n\tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\n\tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n\tat java.lang.Thread.run(Thread.java:745)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "32",
      "number": "32472",
      "pretext": "l am writing an application with flutter and it works well at my android device ,but it builds failed  when l click \"open for editing in android studio\" ,hope someone can help me,\nflutter doctor:\n[✓] Flutter (Channel stable, v1.5.4, on Mac OS X 10.14.3 18D109, locale zh-Hans-CN)\n\n[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n[✓] iOS toolchain - develop for iOS devices (Xcode 10.2.1)\n[✓] Android Studio (version 3.4)\n[✓] IntelliJ IDEA Ultimate Edition (version 2019.1.1)\n[✓] Connected device (1 available)\n\nhere are the error logs:\norg.gradle.process.internal.ExecException: Process 'command '/Users/lierdong/development/flutter/bin/flutter'' finished with non-zero exit value 1\n\tat org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:396)\n\tat org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:37)\n\tat org.gradle.api.internal.file.DefaultFileOperations.exec(DefaultFileOperations.java:234)\n\tat org.gradle.api.internal.project.DefaultProject.exec(DefaultProject.java:1113)\n\tat org.gradle.api.internal.project.DefaultProject.exec(DefaultProject.java:1108)\n\tat org.gradle.api.Project$exec$7.call(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:115)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:127)\n\tat BaseFlutterTask.buildBundle(/Users/lierdong/development/flutter/packages/flutter_tools/gradle/flutter.gradle:529)\n\tat BaseFlutterTask$buildBundle.callCurrent(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:156)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:160)\n\tat FlutterTask.build(/Users/lierdong/development/flutter/packages/flutter_tools/gradle/flutter.gradle:667)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:48)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:41)\n\tat org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:28)\n\tat org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:704)\n\tat org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:671)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$2.run(ExecuteActionsTaskExecuter.java:284)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:301)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:293)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:273)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:258)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.access$200(ExecuteActionsTaskExecuter.java:67)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:145)\n\tat org.gradle.internal.execution.impl.steps.ExecuteStep.execute(ExecuteStep.java:49)\n\tat org.gradle.internal.execution.impl.steps.CancelExecutionStep.execute(CancelExecutionStep.java:34)\n\tat org.gradle.internal.execution.impl.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:69)\n\tat org.gradle.internal.execution.impl.steps.TimeoutStep.execute(TimeoutStep.java:49)\n\tat org.gradle.internal.execution.impl.steps.CatchExceptionStep.execute(CatchExceptionStep.java:33)\n\tat org.gradle.internal.execution.impl.steps.CreateOutputsStep.execute(CreateOutputsStep.java:50)\n\tat org.gradle.internal.execution.impl.steps.SnapshotOutputStep.execute(SnapshotOutputStep.java:43)\n\tat org.gradle.internal.execution.impl.steps.SnapshotOutputStep.execute(SnapshotOutputStep.java:29)\n\tat org.gradle.internal.execution.impl.steps.CacheStep.executeWithoutCache(CacheStep.java:134)\n\tat org.gradle.internal.execution.impl.steps.CacheStep.lambda$execute$3(CacheStep.java:83)\n\tat java.util.Optional.orElseGet(Optional.java:267)\n\tat org.gradle.internal.execution.impl.steps.CacheStep.execute(CacheStep.java:82)\n\tat org.gradle.internal.execution.impl.steps.CacheStep.execute(CacheStep.java:36)\n\tat org.gradle.internal.execution.impl.steps.PrepareCachingStep.execute(PrepareCachingStep.java:33)\n\tat org.gradle.internal.execution.impl.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:38)\n\tat org.gradle.internal.execution.impl.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:23)\n\tat org.gradle.internal.execution.impl.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96)\n\tat org.gradle.internal.execution.impl.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89)\n\tat java.util.Optional.map(Optional.java:215)\n\tat org.gradle.internal.execution.impl.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:52)\n\tat org.gradle.internal.execution.impl.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:36)\n\tat org.gradle.internal.execution.impl.DefaultWorkExecutor.execute(DefaultWorkExecutor.java:34)\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:91)\n\tat org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:91)\n\tat org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:57)\n\tat org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:119)\n\tat org.gradle.api.internal.tasks.execution.ResolvePreviousStateExecuter.execute(ResolvePreviousStateExecuter.java:43)\n\tat org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:93)\n\tat org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:45)\n\tat org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:94)\n\tat org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:56)\n\tat org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:55)\n\tat org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:36)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.executeTask(EventFiringTaskExecuter.java:67)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:52)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:49)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:315)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:305)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:101)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)\n\tat org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:49)\n\tat org.gradle.execution.plan.LocalTaskNodeExecutor.execute(LocalTaskNodeExecutor.java:43)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:355)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193)\n\tat org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129)\n\tat org.gradle.execution.plan.DefaultPlanExecutor.process(DefaultPlanExecutor.java:74)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph.executeWithServices(DefaultTaskExecutionGraph.java:178)\n\tat org.gradle.execution.taskgraph.DefaultTaskExecutionGraph.execute(DefaultTaskExecutionGraph.java:154)\n\tat org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:41)\n\tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:40)\n\tat org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:24)\n\tat org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:46)\n\tat org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:49)\n\tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:40)\n\tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:33)\n\tat org.gradle.initialization.DefaultGradleLauncher$ExecuteTasks.run(DefaultGradleLauncher.java:383)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:301)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:293)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n\tat org.gradle.initialization.DefaultGradleLauncher.runTasks(DefaultGradleLauncher.java:247)\n\tat org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:159)\n\tat org.gradle.initialization.DefaultGradleLauncher.executeTasks(DefaultGradleLauncher.java:134)\n\tat org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:58)\n\tat org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:55)\n\tat org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:82)\n\tat org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:75)\n\tat org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:183)\n\tat org.gradle.internal.work.StopShieldingWorkerLeaseService.withLocks(StopShieldingWorkerLeaseService.java:40)\n\tat org.gradle.internal.invocation.GradleBuildController.doBuild(GradleBuildController.java:75)\n\tat org.gradle.internal.invocation.GradleBuildController.run(GradleBuildController.java:55)\n\tat org.gradle.tooling.internal.provider.runner.BuildModelActionRunner.run(BuildModelActionRunner.java:54)\n\tat org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)\n\tat org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)\n\tat org.gradle.launcher.exec.BuildOutcomeReportingBuildActionRunner.run(BuildOutcomeReportingBuildActionRunner.java:58)\n\tat org.gradle.tooling.internal.provider.ValidatingBuildActionRunner.run(ValidatingBuildActionRunner.java:32)\n\tat org.gradle.launcher.exec.BuildCompletionNotifyingBuildActionRunner.run(BuildCompletionNotifyingBuildActionRunner.java:39)\n\tat org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:49)\n\tat org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:44)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:315)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:305)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:175)\n\tat org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:101)\n\tat org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)\n\tat org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner.run(RunAsBuildOperationBuildActionRunner.java:44)\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:49)\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:46)\n\tat org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:78)\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46)\n\tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:31)\n\tat org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:42)\n\tat org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:28)\n\tat org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:78)\n\tat org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:52)\n\tat org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:59)\n\tat org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:36)\n\tat org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:68)\n\tat org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:38)\n\tat org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:37)\n\tat org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:26)\n\tat org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:43)\n\tat org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:29)\n\tat org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:60)\n\tat org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:32)\n\tat org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:55)\n\tat org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:41)\n\tat org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:48)\n\tat org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:32)\n\tat org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67)\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)\n\tat org.gradle.util.Swapper.swap(Swapper.java:38)\n\tat org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:62)\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:81)\n\tat org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)\n\tat org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)\n\tat org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)\n\tat org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:295)\n\tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\n\tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n\tat java.lang.Thread.run(Thread.java:745)",
      "title": "Can not make project when open for editing in android studio"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2713,
    "text": "Running flutter create myapp by VSCode always have problemsWhen I try to create a flutter project by visual studio code, lib and test file always get a red signal and it stay in creating Running \"flutter packages get\" in myapp...   status",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "33",
      "number": "18746",
      "pretext": "When I try to create a flutter project by visual studio code, lib and test file always get a red signal and it stay in creating Running \"flutter packages get\" in myapp...   status",
      "title": "Running flutter create myapp by VSCode always have problems"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2714,
    "text": "Remove ignoreTransform and highQuality options from MaskFilterThe ignoreTransform doesn't make much sense in a composited system because the local transform (which you're attempting to ignore) will depend on the compositing strategy, which means the ouput will depend on the compositing strategy.\nApparently the quality is ignored in Skia's GPU backend.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "34",
      "number": "7204",
      "pretext": "The ignoreTransform doesn't make much sense in a composited system because the local transform (which you're attempting to ignore) will depend on the compositing strategy, which means the ouput will depend on the compositing strategy.\nApparently the quality is ignored in Skia's GPU backend.",
      "title": "Remove ignoreTransform and highQuality options from MaskFilter"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2715,
    "text": "InputEventReceiver exception dispatching input event with Flutter 1.2.1 and Android JellyBean 4.1.2Steps to Reproduce\nUsing Android Studio. I have two different applications (one is a game, and the other is a set of widgets to better manage the space in tablets).\n\n\nThey were running smoothly in flutter 1.1.2 beta for devices from API 16 up to API 27, and emulators from API 21 up to API 28.\n\n\nAfter flutter upgrade to flutter 1.2.1 beta, both applications in my device with API 16 fails at the moment of touching anything inside the application with the same error.\n\n\nThen the application dies and a dialog that reads Unfortunately, <application_name> has stopped and an exception is thrown through the log file.\n\n\nBoth applications works fine in a Samsung Galaxy Nexus API 17 Jelly Bean MR1 and up to a Moto G5 Plus API 27, so this is a problem only in Jelly Bean API 16.\n\n\nLogs\nBoth applications start fine. But touching anything in the screen we get the following log error:\n[+445864 ms] E/InputEventReceiver( 3225): Exception dispatching input event.\n[   +4 ms] E/MessageQueue-JNI( 3225): Exception in MessageQueue callback: handleReceiveCallback\n[  +22 ms] E/MessageQueue-JNI( 3225): java.lang.NoSuchMethodError: android.view.MotionEvent.isFromSource\n[        ] E/MessageQueue-JNI( 3225):   at io.flutter.view.FlutterView.onGenericMotionEvent(FlutterView.java:590)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEventInternal(View.java:7238)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7219)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)\n[        ] E/MessageQueue-JNI( 3225):   at\ncom.android.internal.policy.impl.PhoneWindow$DecorView.superDispatchGenericMotionEvent(PhoneWindow.java:1950)\n[   +1 ms] E/MessageQueue-JNI( 3225):   at\ncom.android.internal.policy.impl.PhoneWindow.superDispatchGenericMotionEvent(PhoneWindow.java:1406)\n[        ] E/MessageQueue-JNI( 3225):   at android.app.Activity.dispatchGenericMotionEvent(Activity.java:2446)\n[        ] E/MessageQueue-JNI( 3225):   at\ncom.android.internal.policy.impl.PhoneWindow$DecorView.dispatchGenericMotionEvent(PhoneWindow.java:1904)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchPointerEvent(View.java:7325)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.deliverPointerEvent(ViewRootImpl.java:3350)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.deliverInputEvent(ViewRootImpl.java:3295)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.doProcessInputEvents(ViewRootImpl.java:4331)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.enqueueInputEvent(ViewRootImpl.java:4310)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl$WindowInputEventReceiver.onInputEvent(ViewRootImpl.java:4402)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.InputEventReceiver.dispatchInputEvent(InputEventReceiver.java:171)\n[        ] E/MessageQueue-JNI( 3225):   at android.os.MessageQueue.nativePollOnce(Native Method)\n[        ] E/MessageQueue-JNI( 3225):   at android.os.MessageQueue.next(MessageQueue.java:125)\n[        ] E/MessageQueue-JNI( 3225):   at android.os.Looper.loop(Looper.java:124)\n[        ] E/MessageQueue-JNI( 3225):   at android.app.ActivityThread.main(ActivityThread.java:4842)\n[        ] E/MessageQueue-JNI( 3225):   at java.lang.reflect.Method.invokeNative(Native Method)\n[        ] E/MessageQueue-JNI( 3225):   at java.lang.reflect.Method.invoke(Method.java:511)\n[        ] E/MessageQueue-JNI( 3225):   at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:890)\n[        ] E/MessageQueue-JNI( 3225):   at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:657)\n[        ] E/MessageQueue-JNI( 3225):   at dalvik.system.NativeStart.main(Native Method)\n[        ] W/dalvikvm( 3225): threadid=1: thread exiting with uncaught exception (group=0x427e2438)\n\nAfter touching the OK in the termination dialog the log is:\n\n[+88378 ms] Service protocol connection closed.\n[   +1 ms] Lost connection to device.\n[   +3 ms] DevFS: Deleting filesystem on the device\n(file:///data/data/games.appsu.minessweeper/cache/mines_sweeperSWJIMS/mines_sweeper/)\n[   +1 ms] Sending to VM service: _deleteDevFS({fsName: mines_sweeper})\n[ +258 ms] Ignored error while cleaning up DevFS: TimeoutException after 0:00:00.250000: Future not completed\n[   +6 ms] \"flutter run\" took 593,477ms.\n[        ] \"flutter run\" took 593,477ms.\n\nflutter analyze results:\nAnalyzing mines_sweeper...                                              \nNo issues found! (ran in 13.5s)\n\nflutter doctor -v:\n[✓] Flutter (Channel beta, v1.2.1, on Linux, locale en_US.UTF-8)\n    • Flutter version 1.2.1 at /home/sarah/flutter\n    • Framework revision 8661d8aecd (3 weeks ago), 2019-02-14 19:19:53 -0800\n    • Engine revision 3757390fa4\n    • Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n\n[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at /home/sarah/Android/Sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • Java binary at: /opt/android-studio/jre/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    • All Android licenses accepted.\n\n[✓] Android Studio (version 3.4)\n    • Android Studio at /opt/android-studio\n    • Flutter plugin version 33.3.2\n    • Dart plugin version 183.5901\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[✓] IntelliJ IDEA Community Edition (version 2018.3)\n    • IntelliJ at /opt/idea-IC-183.4588.61\n    • Flutter plugin version 31.1.4\n    • Dart plugin version 183.4588.61\n\n[✓] Connected device (1 available)\n    • d07ab5034db7 • d07ab5034db7 • android-arm • Android 4.1.2 (API 16)\n\n• No issues found!",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "35",
      "number": "28984",
      "pretext": "Steps to Reproduce\nUsing Android Studio. I have two different applications (one is a game, and the other is a set of widgets to better manage the space in tablets).\n\n\nThey were running smoothly in flutter 1.1.2 beta for devices from API 16 up to API 27, and emulators from API 21 up to API 28.\n\n\nAfter flutter upgrade to flutter 1.2.1 beta, both applications in my device with API 16 fails at the moment of touching anything inside the application with the same error.\n\n\nThen the application dies and a dialog that reads Unfortunately, <application_name> has stopped and an exception is thrown through the log file.\n\n\nBoth applications works fine in a Samsung Galaxy Nexus API 17 Jelly Bean MR1 and up to a Moto G5 Plus API 27, so this is a problem only in Jelly Bean API 16.\n\n\nLogs\nBoth applications start fine. But touching anything in the screen we get the following log error:\n[+445864 ms] E/InputEventReceiver( 3225): Exception dispatching input event.\n[   +4 ms] E/MessageQueue-JNI( 3225): Exception in MessageQueue callback: handleReceiveCallback\n[  +22 ms] E/MessageQueue-JNI( 3225): java.lang.NoSuchMethodError: android.view.MotionEvent.isFromSource\n[        ] E/MessageQueue-JNI( 3225):   at io.flutter.view.FlutterView.onGenericMotionEvent(FlutterView.java:590)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEventInternal(View.java:7238)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7219)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchTransformedGenericPointerEvent(ViewGroup.java:1787)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewGroup.dispatchHoverEvent(ViewGroup.java:1478)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchGenericMotionEvent(View.java:7209)\n[        ] E/MessageQueue-JNI( 3225):   at\ncom.android.internal.policy.impl.PhoneWindow$DecorView.superDispatchGenericMotionEvent(PhoneWindow.java:1950)\n[   +1 ms] E/MessageQueue-JNI( 3225):   at\ncom.android.internal.policy.impl.PhoneWindow.superDispatchGenericMotionEvent(PhoneWindow.java:1406)\n[        ] E/MessageQueue-JNI( 3225):   at android.app.Activity.dispatchGenericMotionEvent(Activity.java:2446)\n[        ] E/MessageQueue-JNI( 3225):   at\ncom.android.internal.policy.impl.PhoneWindow$DecorView.dispatchGenericMotionEvent(PhoneWindow.java:1904)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.View.dispatchPointerEvent(View.java:7325)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.deliverPointerEvent(ViewRootImpl.java:3350)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.deliverInputEvent(ViewRootImpl.java:3295)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.doProcessInputEvents(ViewRootImpl.java:4331)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl.enqueueInputEvent(ViewRootImpl.java:4310)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.ViewRootImpl$WindowInputEventReceiver.onInputEvent(ViewRootImpl.java:4402)\n[        ] E/MessageQueue-JNI( 3225):   at android.view.InputEventReceiver.dispatchInputEvent(InputEventReceiver.java:171)\n[        ] E/MessageQueue-JNI( 3225):   at android.os.MessageQueue.nativePollOnce(Native Method)\n[        ] E/MessageQueue-JNI( 3225):   at android.os.MessageQueue.next(MessageQueue.java:125)\n[        ] E/MessageQueue-JNI( 3225):   at android.os.Looper.loop(Looper.java:124)\n[        ] E/MessageQueue-JNI( 3225):   at android.app.ActivityThread.main(ActivityThread.java:4842)\n[        ] E/MessageQueue-JNI( 3225):   at java.lang.reflect.Method.invokeNative(Native Method)\n[        ] E/MessageQueue-JNI( 3225):   at java.lang.reflect.Method.invoke(Method.java:511)\n[        ] E/MessageQueue-JNI( 3225):   at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:890)\n[        ] E/MessageQueue-JNI( 3225):   at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:657)\n[        ] E/MessageQueue-JNI( 3225):   at dalvik.system.NativeStart.main(Native Method)\n[        ] W/dalvikvm( 3225): threadid=1: thread exiting with uncaught exception (group=0x427e2438)\n\nAfter touching the OK in the termination dialog the log is:\n\n[+88378 ms] Service protocol connection closed.\n[   +1 ms] Lost connection to device.\n[   +3 ms] DevFS: Deleting filesystem on the device\n(file:///data/data/games.appsu.minessweeper/cache/mines_sweeperSWJIMS/mines_sweeper/)\n[   +1 ms] Sending to VM service: _deleteDevFS({fsName: mines_sweeper})\n[ +258 ms] Ignored error while cleaning up DevFS: TimeoutException after 0:00:00.250000: Future not completed\n[   +6 ms] \"flutter run\" took 593,477ms.\n[        ] \"flutter run\" took 593,477ms.\n\nflutter analyze results:\nAnalyzing mines_sweeper...                                              \nNo issues found! (ran in 13.5s)\n\nflutter doctor -v:\n[✓] Flutter (Channel beta, v1.2.1, on Linux, locale en_US.UTF-8)\n    • Flutter version 1.2.1 at /home/sarah/flutter\n    • Framework revision 8661d8aecd (3 weeks ago), 2019-02-14 19:19:53 -0800\n    • Engine revision 3757390fa4\n    • Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n\n[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at /home/sarah/Android/Sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • Java binary at: /opt/android-studio/jre/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    • All Android licenses accepted.\n\n[✓] Android Studio (version 3.4)\n    • Android Studio at /opt/android-studio\n    • Flutter plugin version 33.3.2\n    • Dart plugin version 183.5901\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[✓] IntelliJ IDEA Community Edition (version 2018.3)\n    • IntelliJ at /opt/idea-IC-183.4588.61\n    • Flutter plugin version 31.1.4\n    • Dart plugin version 183.4588.61\n\n[✓] Connected device (1 available)\n    • d07ab5034db7 • d07ab5034db7 • android-arm • Android 4.1.2 (API 16)\n\n• No issues found!",
      "title": "InputEventReceiver exception dispatching input event with Flutter 1.2.1 and Android JellyBean 4.1.2"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2716,
    "text": "missing localization data != nullSteps to Reproduce\n\n\n...\n...\n...\n\nLogs",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "36",
      "number": "24676",
      "pretext": "Steps to Reproduce\n\n\n...\n...\n...\n\nLogs",
      "title": "missing localization data != null"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2717,
    "text": "Request to hot reload fails (on Windows)[   +2 ms] _flutter.setAssetBundlePath: {viewId: _flutterView/0xeb872ee0, assetDirectory: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/build/flutter_assets}\n[  +16 ms] Error -32601 received from application: Method not found\n[        ] {request: {method: _flutter.setAssetBundlePath, params: {viewId: _flutterView/0xeb872ee0, assetDirectory: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/build/flutter_assets}}}\n[   +2 ms] _reloadSources: {pause: false, rootLibUri: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/lib/main.dart, packagesUri: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/.pac\nkages, isolateId: isolates/646758416}\n[+5918 ms] F/libc    (29804): Fatal signal 11 (SIGSEGV), code 1, fault addr 0x10c in tid 29826 (ui_thread), pid 29804 (xamples.gallery)\n[ +259 ms] *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\n[   +2 ms] Build fingerprint: 'google/sailfish/sailfish:8.1.0/OPP5.170921.005/4373449:userdebug/dev-keys'\n[        ] Revision: '0'\n[        ] ABI: 'arm'\n[        ] pid: 29804, tid: 29826, name: ui_thread  >>> io.flutter.examples.gallery <<<\n[        ] signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x10c\n[        ] Cause: null pointer dereference\n[        ]     r0 d1058500  r1 c3825819  r2 00000001  r3 80000000\n[        ]     r4 00000001  r5 00000000  r6 c47cc478  r7 d1058500\n[        ]     r8 00000000  r9 00000000  sl ffffffff  fp d105868c\n[        ]     ip d215bb04  sp d12d1400  lr d1844ef9  pc d1844ef8  cpsr 600f0030\n[   +5 ms] backtrace:\n[   +4 ms]     #00 pc 00474ef8  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #01 pc 0057752b  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #02 pc 00413eab  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #03 pc 00416e19  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #04 pc 004140ed  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #05 pc 004ed44d  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #06 pc 004ed36d  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #07 pc 004657fb  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #08 pc 00464bef  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #09 pc 006a2ee3  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #10 pc 00387b51  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #11 pc 003874af  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #12 pc 0046353b  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #13 pc 0045cf15  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #14 pc 0059452f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #15 pc 0058d8ed  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #16 pc 0058de9f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #17 pc 00459dc3  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #18 pc 00475ebf  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[   +5 ms]     #19 pc 00475f4f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #20 pc 0068b1ef  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #21 pc 00385db5  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #22 pc 0008d3d9  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #23 pc 0008fc75  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #24 pc 000101bd  /system/lib/libutils.so (android::Looper::pollInner(int)+576)\n[        ]     #25 pc 0000fee5  /system/lib/libutils.so (android::Looper::pollOnce(int, int*, int*, void**)+32)\n[        ]     #26 pc 0000c443  /system/lib/libandroid.so (ALooper_pollOnce+50)\n[        ]     #27 pc 0008fc2f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #28 pc 0008d469  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #29 pc 0008e829  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #30 pc 00047e6b  /system/lib/libc.so (__pthread_start(void*)+22)\n[        ]     #31 pc 0001b1d9  /system/lib/libc.so (__start_thread+32)\n[+1282 ms] Service protocol connection closed.\n[   +5 ms] Lost connection to device.\n[   +4 ms] DevFS: Deleting filesystem on the device (file:///data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/)\n[   +3 ms] _deleteDevFS: {fsName: flutter_gallery}\n[ +249 ms] TimeoutException after 0:00:00.250000: Future not completed\n[  +39 ms] \"flutter run\" took 93,465ms.\n[ +231 ms] ensureAnalyticsSent: 225ms\n[   +5 ms] exiting with code 0\nC:\\src\\flutter\\flutter\\examples\\flutter_gallery [master ≡ +4 ~3 -0 !]>\n\nThis is on be0c488 running on Pixel.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "37",
      "number": "14497",
      "pretext": "[   +2 ms] _flutter.setAssetBundlePath: {viewId: _flutterView/0xeb872ee0, assetDirectory: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/build/flutter_assets}\n[  +16 ms] Error -32601 received from application: Method not found\n[        ] {request: {method: _flutter.setAssetBundlePath, params: {viewId: _flutterView/0xeb872ee0, assetDirectory: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/build/flutter_assets}}}\n[   +2 ms] _reloadSources: {pause: false, rootLibUri: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/lib/main.dart, packagesUri: /data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/.pac\nkages, isolateId: isolates/646758416}\n[+5918 ms] F/libc    (29804): Fatal signal 11 (SIGSEGV), code 1, fault addr 0x10c in tid 29826 (ui_thread), pid 29804 (xamples.gallery)\n[ +259 ms] *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\n[   +2 ms] Build fingerprint: 'google/sailfish/sailfish:8.1.0/OPP5.170921.005/4373449:userdebug/dev-keys'\n[        ] Revision: '0'\n[        ] ABI: 'arm'\n[        ] pid: 29804, tid: 29826, name: ui_thread  >>> io.flutter.examples.gallery <<<\n[        ] signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x10c\n[        ] Cause: null pointer dereference\n[        ]     r0 d1058500  r1 c3825819  r2 00000001  r3 80000000\n[        ]     r4 00000001  r5 00000000  r6 c47cc478  r7 d1058500\n[        ]     r8 00000000  r9 00000000  sl ffffffff  fp d105868c\n[        ]     ip d215bb04  sp d12d1400  lr d1844ef9  pc d1844ef8  cpsr 600f0030\n[   +5 ms] backtrace:\n[   +4 ms]     #00 pc 00474ef8  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #01 pc 0057752b  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #02 pc 00413eab  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #03 pc 00416e19  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #04 pc 004140ed  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #05 pc 004ed44d  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #06 pc 004ed36d  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #07 pc 004657fb  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #08 pc 00464bef  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #09 pc 006a2ee3  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #10 pc 00387b51  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #11 pc 003874af  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #12 pc 0046353b  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #13 pc 0045cf15  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #14 pc 0059452f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #15 pc 0058d8ed  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #16 pc 0058de9f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #17 pc 00459dc3  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #18 pc 00475ebf  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[   +5 ms]     #19 pc 00475f4f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #20 pc 0068b1ef  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #21 pc 00385db5  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #22 pc 0008d3d9  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #23 pc 0008fc75  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #24 pc 000101bd  /system/lib/libutils.so (android::Looper::pollInner(int)+576)\n[        ]     #25 pc 0000fee5  /system/lib/libutils.so (android::Looper::pollOnce(int, int*, int*, void**)+32)\n[        ]     #26 pc 0000c443  /system/lib/libandroid.so (ALooper_pollOnce+50)\n[        ]     #27 pc 0008fc2f  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #28 pc 0008d469  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #29 pc 0008e829  /data/app/io.flutter.examples.gallery-Vg1_0RwBCqQaxD72wo_qEA==/lib/arm/libflutter.so\n[        ]     #30 pc 00047e6b  /system/lib/libc.so (__pthread_start(void*)+22)\n[        ]     #31 pc 0001b1d9  /system/lib/libc.so (__start_thread+32)\n[+1282 ms] Service protocol connection closed.\n[   +5 ms] Lost connection to device.\n[   +4 ms] DevFS: Deleting filesystem on the device (file:///data/user/0/io.flutter.examples.gallery/cache/flutter_galleryZPFBGN/flutter_gallery/)\n[   +3 ms] _deleteDevFS: {fsName: flutter_gallery}\n[ +249 ms] TimeoutException after 0:00:00.250000: Future not completed\n[  +39 ms] \"flutter run\" took 93,465ms.\n[ +231 ms] ensureAnalyticsSent: 225ms\n[   +5 ms] exiting with code 0\nC:\\src\\flutter\\flutter\\examples\\flutter_gallery [master ≡ +4 ~3 -0 !]>\n\nThis is on be0c488 running on Pixel.",
      "title": "Request to hot reload fails (on Windows)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2718,
    "text": "google_sign_in: auto-publish API docsWe should probably update http://flutter.github.io/google_sign_in/ manually. It would be super cool to automate the publishing of that site.\nOur users are finding these sites and asking questions about them :)",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "38",
      "number": "5385",
      "pretext": "We should probably update http://flutter.github.io/google_sign_in/ manually. It would be super cool to automate the publishing of that site.\nOur users are finding these sites and asking questions about them :)",
      "title": "google_sign_in: auto-publish API docs"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2719,
    "text": "DataTable checkbox tap target is too smallThe checkbox is only given the minimum possible width, currently 18. it should be at least 48 by 48.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "39",
      "number": "19963",
      "pretext": "The checkbox is only given the minimum possible width, currently 18. it should be at least 48 by 48.",
      "title": "DataTable checkbox tap target is too small"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2720,
    "text": "Documentation for keytoolOn https://flutter.io/android-release/ we should add some documentation that explains what keytool does and especially say where it leaves the key.jks file. It wasn't in the current directory, nor in the app directory. I had to search and finally found it in my home directory (ick!).",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "40",
      "number": "10159",
      "pretext": "On https://flutter.io/android-release/ we should add some documentation that explains what keytool does and especially say where it leaves the key.jks file. It wasn't in the current directory, nor in the app directory. I had to search and finally found it in my home directory (ick!).",
      "title": "Documentation for keytool"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2721,
    "text": "remove \"flutter start\"~/flutter/testme2 $ flutter start --help\nRun your Flutter app on an attached device.\n\nUsage: flutter run [arguments]\n\nWhich is the real one? If start is deprecated in favor of run, maybe we can print out \"start is replaced by run. Please update your scripts, as we will soon remove start.\"",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "41",
      "number": "2208",
      "pretext": "~/flutter/testme2 $ flutter start --help\nRun your Flutter app on an attached device.\n\nUsage: flutter run [arguments]\n\nWhich is the real one? If start is deprecated in favor of run, maybe we can print out \"start is replaced by run. Please update your scripts, as we will soon remove start.\"",
      "title": "remove \"flutter start\""
    },
    "annotation_approver": null
  },
  {
    "id": 2722,
    "text": "Host OS not supported on WindowsHi! I'm trying Flutter on Windows platform and i have installed everything with no problem. I created the app from command line but when i try to launch it, (from command line or even from IntellijIDEA plugin) i get this error.\nUnimplementedError: Host OS not supported.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "42",
      "number": "8388",
      "pretext": "Hi! I'm trying Flutter on Windows platform and i have installed everything with no problem. I created the app from command line but when i try to launch it, (from command line or even from IntellijIDEA plugin) i get this error.\nUnimplementedError: Host OS not supported.",
      "title": "Host OS not supported on Windows"
    },
    "annotation_approver": "Manolomon"
  },
  {
    "id": 2723,
    "text": "_googleSignIn.signIn() deadlock?_googleSignIn.signIn() does not seem to work correctly?\nThis may be related to\n\nflutter/plugins#94\n#10552\n\nSteps to Reproduce\nYou can reproduce it doing the flutter-firebase codelab (https://codelabs.developers.google.com/codelabs/flutter-firebase/#5), using Android 8 (SDK 26). There seems to be some kind of deadlock when calling this method.\nIt also does not seem to work using the following versions:\ndependencies:\n  flutter:\n    sdk: flutter\n  image_picker: 0.1.1\n  google_sign_in: 0.3.1\n  firebase_analytics: 0.1.0\n  firebase_auth: 0.2.3\n  firebase_database: 0.1.0\n  firebase_storage: 0.0.6\n  firebase_messaging: 0.0.5\nWe also found out that you can sign in like this:\n\ncall _googleSignIn.signIn()\nselect the user\nwe noticed a very short flicker at the top of the screen, but that's it\nif you use await _googleSignIn.signIn(), the code below is never executed\nthere is also no exception thrown\n(deadlock in async call?)\nclose the app, so that it stops running in the background\nopen the app again\nsign in using _googleSignIn.signInSilently(): this works now because the account has been selected already",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "43",
      "number": "12137",
      "pretext": "_googleSignIn.signIn() does not seem to work correctly?\nThis may be related to\n\nflutter/plugins#94\n#10552\n\nSteps to Reproduce\nYou can reproduce it doing the flutter-firebase codelab (https://codelabs.developers.google.com/codelabs/flutter-firebase/#5), using Android 8 (SDK 26). There seems to be some kind of deadlock when calling this method.\nIt also does not seem to work using the following versions:\ndependencies:\n  flutter:\n    sdk: flutter\n  image_picker: 0.1.1\n  google_sign_in: 0.3.1\n  firebase_analytics: 0.1.0\n  firebase_auth: 0.2.3\n  firebase_database: 0.1.0\n  firebase_storage: 0.0.6\n  firebase_messaging: 0.0.5\nWe also found out that you can sign in like this:\n\ncall _googleSignIn.signIn()\nselect the user\nwe noticed a very short flicker at the top of the screen, but that's it\nif you use await _googleSignIn.signIn(), the code below is never executed\nthere is also no exception thrown\n(deadlock in async call?)\nclose the app, so that it stops running in the background\nopen the app again\nsign in using _googleSignIn.signInSilently(): this works now because the account has been selected already",
      "title": "_googleSignIn.signIn() deadlock?"
    },
    "annotation_approver": null
  },
  {
    "id": 2724,
    "text": "AppBar Hero Transition could crossfade https://youtu.be/KbyuPMZ-9_A\n@abarth says we should fix this, but to do so would require re-writing the Hero system (which has to happen for other reasons).  Just noting down one example of where being able to fade/animate-color would be nice.\n[✓] Flutter (on Linux, channel master)\n    • Flutter at /src/flutter\n    • Framework revision 92d0445a8f (43 minutes ago), engine revision 603b0701a6\n\n[✓] Android toolchain - develop for Android devices (Android SDK 23.0.1)\n    • Android SDK at /usr/local/google/home/eseidel/Android/Sdk\n    • Platform android-23, build-tools 23.0.1\n    • OpenJDK Runtime Environment (IcedTea 2.6.6) (7u101-2.6.6-0ubuntu0.14.04.1)\n\n[✓] Atom - a lightweight development environment for Flutter\n    • flutter plugin version 0.2.2\n    • dartlang plugin version 0.6.22",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "44",
      "number": "5232",
      "pretext": "https://youtu.be/KbyuPMZ-9_A\n@abarth says we should fix this, but to do so would require re-writing the Hero system (which has to happen for other reasons).  Just noting down one example of where being able to fade/animate-color would be nice.\n[✓] Flutter (on Linux, channel master)\n    • Flutter at /src/flutter\n    • Framework revision 92d0445a8f (43 minutes ago), engine revision 603b0701a6\n\n[✓] Android toolchain - develop for Android devices (Android SDK 23.0.1)\n    • Android SDK at /usr/local/google/home/eseidel/Android/Sdk\n    • Platform android-23, build-tools 23.0.1\n    • OpenJDK Runtime Environment (IcedTea 2.6.6) (7u101-2.6.6-0ubuntu0.14.04.1)\n\n[✓] Atom - a lightweight development environment for Flutter\n    • flutter plugin version 0.2.2\n    • dartlang plugin version 0.6.22",
      "title": "AppBar Hero Transition could crossfade "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2725,
    "text": "New Feature Request - TabController disable Slide TransitionNew Feature - TabController disable Slide Transition\nAbility to Disable or change the Horizontal Slide Transition when changing Tabs. I would like to switch pages by Fading instead of sliding left or right.\nThank you.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "45",
      "number": "16474",
      "pretext": "New Feature - TabController disable Slide Transition\nAbility to Disable or change the Horizontal Slide Transition when changing Tabs. I would like to switch pages by Fading instead of sliding left or right.\nThank you.",
      "title": "New Feature Request - TabController disable Slide Transition"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2726,
    "text": "Flutter_web initialization errorHello ! Please, I'm trying to create a new project flutter_web but I have an error in pub get :\nResolving dependencies...\nGit error. Command: git fetch\nfatal: not a git repository (or any of the parent directories): .git\n\nI reinstalled git but the problem persists , I also launched the pub cache repair but to no avail\nFlutter doctor output :\n[√] Flutter (Channel stable, v1.5.4-hotfix.2, on Microsoft Windows [version 10.0.10240], locale fr-FR)\n    • Flutter version 1.5.4-hotfix.2 at D:\\Dev\\FLUTTER\\flutter\n    • Framework revision 7a4c33425d (12 days ago), 2019-04-29 11:05:24 -0700\n    • Engine revision 52c7a1e849\n    • Dart version 2.3.0 (build 2.3.0-dev.0.5 a1668566e5)\n\n[!] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at D:\\Dev\\SDK1\\SDK1\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • Java binary at: C:\\Program Files\\Android\\Android Studio\\jre\\bin\\java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    X Android license status unknown.\n      Try re-installing or updating your Android SDK Manager.\n      See https://developer.android.com/studio/#downloads or visit https://flutter.dev/setup/#android-setup for detailed instructions.\n\n[√] Android Studio (version 3.3)\n    • Android Studio at C:\\Program Files\\Android\\Android Studio\n    • Flutter plugin version 34.0.1\n    • Dart plugin version 182.5215\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[!] Connected device\n    ! No devices available",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "46",
      "number": "32556",
      "pretext": "Hello ! Please, I'm trying to create a new project flutter_web but I have an error in pub get :\nResolving dependencies...\nGit error. Command: git fetch\nfatal: not a git repository (or any of the parent directories): .git\n\nI reinstalled git but the problem persists , I also launched the pub cache repair but to no avail\nFlutter doctor output :\n[√] Flutter (Channel stable, v1.5.4-hotfix.2, on Microsoft Windows [version 10.0.10240], locale fr-FR)\n    • Flutter version 1.5.4-hotfix.2 at D:\\Dev\\FLUTTER\\flutter\n    • Framework revision 7a4c33425d (12 days ago), 2019-04-29 11:05:24 -0700\n    • Engine revision 52c7a1e849\n    • Dart version 2.3.0 (build 2.3.0-dev.0.5 a1668566e5)\n\n[!] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at D:\\Dev\\SDK1\\SDK1\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • Java binary at: C:\\Program Files\\Android\\Android Studio\\jre\\bin\\java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    X Android license status unknown.\n      Try re-installing or updating your Android SDK Manager.\n      See https://developer.android.com/studio/#downloads or visit https://flutter.dev/setup/#android-setup for detailed instructions.\n\n[√] Android Studio (version 3.3)\n    • Android Studio at C:\\Program Files\\Android\\Android Studio\n    • Flutter plugin version 34.0.1\n    • Dart plugin version 182.5215\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[!] Connected device\n    ! No devices available",
      "title": "Flutter_web initialization error"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2727,
    "text": "Building Beautiful UIs Tutorial - Silly Link DisplaySteps to Reproduce\nOn step six under Place the message list, there is a chopped up link on, \"Naming the argument...\"",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "47",
      "number": "10731",
      "pretext": "Steps to Reproduce\nOn step six under Place the message list, there is a chopped up link on, \"Naming the argument...\"",
      "title": "Building Beautiful UIs Tutorial - Silly Link Display"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2728,
    "text": "Text cursor doesn't blink when the focus is requested programmatically within onSubmitted callbackI wanted to retain keyboard focus in a TextField even after the text is submitted, so that I can type in multiple messages in a row in a chat app. The default behavior is to unfocus when the text is submitted, so I added a FocusScope.of(context).requestFocus() call in my handleSubmit callback.\nWith this change, after submitting the text, the text field goes into a weird state where it has focus (i.e. I can type text) but the cursor isn't blinking.\nBelow is the minimal example code that reproduces this issue.\nimport 'package:flutter/material.dart';\n\nvoid main() {\n  runApp(new MyApp());\n}\n\nclass MyApp extends StatelessWidget {\n  @override\n  Widget build(BuildContext context) {\n    return new MaterialApp(\n      home: new MyHomePage(),\n    );\n  }\n}\n\nclass MyHomePage extends StatefulWidget {\n  MyHomePage({Key key}) : super(key: key);\n\n  @override\n  _MyHomePageState createState() => new _MyHomePageState();\n}\n\nclass _MyHomePageState extends State<MyHomePage> {\n  final TextEditingController _controller = new TextEditingController();\n  final FocusNode _focusNode = new FocusNode();\n\n  @override\n  Widget build(BuildContext context) {\n    return new Scaffold(\n      body: new Center(\n        child: new Container(\n          width: 300.0,\n          child: new TextField(\n            controller: _controller,\n            focusNode: _focusNode,\n            onSubmitted: (String text) {\n              print(text);\n              _controller.clear();\n              FocusScope.of(context).requestFocus(_focusNode);\n            },\n          ),\n        ),\n      ),\n    );\n  }\n}",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "48",
      "number": "11183",
      "pretext": "I wanted to retain keyboard focus in a TextField even after the text is submitted, so that I can type in multiple messages in a row in a chat app. The default behavior is to unfocus when the text is submitted, so I added a FocusScope.of(context).requestFocus() call in my handleSubmit callback.\nWith this change, after submitting the text, the text field goes into a weird state where it has focus (i.e. I can type text) but the cursor isn't blinking.\nBelow is the minimal example code that reproduces this issue.\nimport 'package:flutter/material.dart';\n\nvoid main() {\n  runApp(new MyApp());\n}\n\nclass MyApp extends StatelessWidget {\n  @override\n  Widget build(BuildContext context) {\n    return new MaterialApp(\n      home: new MyHomePage(),\n    );\n  }\n}\n\nclass MyHomePage extends StatefulWidget {\n  MyHomePage({Key key}) : super(key: key);\n\n  @override\n  _MyHomePageState createState() => new _MyHomePageState();\n}\n\nclass _MyHomePageState extends State<MyHomePage> {\n  final TextEditingController _controller = new TextEditingController();\n  final FocusNode _focusNode = new FocusNode();\n\n  @override\n  Widget build(BuildContext context) {\n    return new Scaffold(\n      body: new Center(\n        child: new Container(\n          width: 300.0,\n          child: new TextField(\n            controller: _controller,\n            focusNode: _focusNode,\n            onSubmitted: (String text) {\n              print(text);\n              _controller.clear();\n              FocusScope.of(context).requestFocus(_focusNode);\n            },\n          ),\n        ),\n      ),\n    );\n  }\n}",
      "title": "Text cursor doesn't blink when the focus is requested programmatically within onSubmitted callback"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2729,
    "text": "Flutter Gallery app crashes when switching between app bar options in the \"Contact profile\" studySwitching between the different app bar display options in the \"Contact profile\" study can cause the Flutter Gallery app to crash. I tested this on a OnePlus3T using the production version on the PlayStore and locally using the version in v0.5.1.\n@HansMuller I think this is behind the error in #17598\nSteps to Reproduce\n\n\nOpen the \"Contact profile\" study\n\n\nScroll the bottom of the page\n\n\nSelect the drop down menu in top right of the app bar and choose the \"App bar snaps\" option\n\n\nScroll up until the full app bar is displayed (screenshot attached)\n\n\n\nSelect the \"App bar floats\" option\n\n\nThe app will not immediately crash but it is in a bad state and further actions will cause various exceptions and crashes. Logs attached below.\nLogs\nError when scrolling:\n[+34049 ms] I/flutter (29797): ══╡ EXCEPTION CAUGHT BY FOUNDATION LIBRARY ╞════════════════════════════════════════════════════════\n[  +19 ms] I/flutter (29797): The following NoSuchMethodError was thrown while dispatching notifications for ValueNotifier<bool>:\n[        ] I/flutter (29797): The method 'stop' was called on null.\n[        ] I/flutter (29797): Receiver: null\n[   +1 ms] I/flutter (29797): Tried calling: stop(canceled: true)\n[        ] I/flutter (29797):\n[        ] I/flutter (29797): When the exception was thrown, this was the stack:\n[  +12 ms] I/flutter (29797): #0      Object.noSuchMethod (dart:core/runtime/libobject_patch.dart:46:5)\n[        ] I/flutter (29797): #1      AnimationController.stop (package:flutter/src/animation/animation_controller.dart:499:13)\n[        ] I/flutter (29797): #2      RenderSliverFloatingPersistentHeader.maybeStopSnapAnimation (package:flutter/src/rendering/sliver_persistent_header.dart:457:18)\n[        ] I/flutter (29797): #3      _FloatingAppBarState._isScrollingListener (package:flutter/src/material/app_bar.dart:547:15)\n[        ] I/flutter (29797): #4      ChangeNotifier.notifyListeners (package:flutter/src/foundation/change_notifier.dart:161:21)\n[        ] I/flutter (29797): #5      ValueNotifier.value= (package:flutter/src/foundation/change_notifier.dart:217:5)\n[        ] I/flutter (29797): #6      ScrollPosition.beginActivity (package:flutter/src/widgets/scroll_position.dart:613:25)\n[        ] I/flutter (29797): #7      ScrollPositionWithSingleContext.beginActivity (package:flutter/src/widgets/scroll_position_with_single_context.dart:117:11)\n[        ] I/flutter (29797): #8      ScrollPositionWithSingleContext.drag (package:flutter/src/widgets/scroll_position_with_single_context.dart:245:5)\n[        ] I/flutter (29797): #9      ScrollableState._handleDragStart (package:flutter/src/widgets/scrollable.dart:443:22)\n[   +1 ms] I/flutter (29797): #10     DragGestureRecognizer.acceptGesture.<anonymous closure> (package:flutter/src/gestures/monodrag.dart:169:47)\n[        ] I/flutter (29797): #11     GestureRecognizer.invokeCallback (package:flutter/src/gestures/recognizer.dart:102:24)\n[        ] I/flutter (29797): #12     DragGestureRecognizer.acceptGesture (package:flutter/src/gestures/monodrag.dart:169:9)\n[        ] I/flutter (29797): #13     GestureArenaManager._resolveByDefault (package:flutter/src/gestures/arena.dart:250:25)\n[        ] I/flutter (29797): #14     GestureArenaManager._tryToResolveArena.<anonymous closure> (package:flutter/src/gestures/arena.dart:231:31)\n[        ] I/flutter (29797): (elided 2 frames from package dart:async)\n[        ] I/flutter (29797):\n[        ] I/flutter (29797): The ValueNotifier<bool> sending notification was:\n[        ] I/flutter (29797):   ValueNotifier<bool>#0a1d4(true)\n[        ] I/flutter (29797): ════════════════════════════════════════════════════════════════════════════════════════════════════\n\nError when trying to switch to the 'App bar snaps' option:\n[+40490 ms] I/flutter (29797): Another exception was thrown: AnimationController.dispose() called more than once.\n[   +8 ms] I/flutter (29797): Another exception was thrown: A RenderViewport expected a child of type RenderSliver but received a child of type RenderErrorBox.\n[   +9 ms] I/chatty  (29797): uid=10084(io.flutter.demo.gallery) 1.ui identical 1 line\n[        ] I/flutter (29797): Another exception was thrown: A RenderViewport expected a child of type RenderSliver but received a child of type RenderErrorBox.\n[        ] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3497 pos 14: 'owner._debugCurrentBuildTarget == this': is not true.\n[+2095 ms] I/chatty  (29797): uid=10084(io.flutter.demo.gallery) 1.ui identical 20 lines\n[        ] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3497 pos 14: 'owner._debugCurrentBuildTarget == this': is not true.\n\nError when clicking the back button\n[+242379 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 4062 pos 14: '() {\n[ +279 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 2240 pos 16: '!_dirtyElements[index]._active || _dirtyElements[index]._debugIsInScope(context)': is not true.\n\nError when trying to re-open the Contact profile study after the last crash\n[+457845 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3681 pos 12: 'child == _child': is not true.\n[   +6 ms] I/flutter (29797): Another exception was thrown: Duplicate GlobalKey detected in widget tree.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "49",
      "number": "19582",
      "pretext": "Switching between the different app bar display options in the \"Contact profile\" study can cause the Flutter Gallery app to crash. I tested this on a OnePlus3T using the production version on the PlayStore and locally using the version in v0.5.1.\n@HansMuller I think this is behind the error in #17598\nSteps to Reproduce\n\n\nOpen the \"Contact profile\" study\n\n\nScroll the bottom of the page\n\n\nSelect the drop down menu in top right of the app bar and choose the \"App bar snaps\" option\n\n\nScroll up until the full app bar is displayed (screenshot attached)\n\n\n\nSelect the \"App bar floats\" option\n\n\nThe app will not immediately crash but it is in a bad state and further actions will cause various exceptions and crashes. Logs attached below.\nLogs\nError when scrolling:\n[+34049 ms] I/flutter (29797): ══╡ EXCEPTION CAUGHT BY FOUNDATION LIBRARY ╞════════════════════════════════════════════════════════\n[  +19 ms] I/flutter (29797): The following NoSuchMethodError was thrown while dispatching notifications for ValueNotifier<bool>:\n[        ] I/flutter (29797): The method 'stop' was called on null.\n[        ] I/flutter (29797): Receiver: null\n[   +1 ms] I/flutter (29797): Tried calling: stop(canceled: true)\n[        ] I/flutter (29797):\n[        ] I/flutter (29797): When the exception was thrown, this was the stack:\n[  +12 ms] I/flutter (29797): #0      Object.noSuchMethod (dart:core/runtime/libobject_patch.dart:46:5)\n[        ] I/flutter (29797): #1      AnimationController.stop (package:flutter/src/animation/animation_controller.dart:499:13)\n[        ] I/flutter (29797): #2      RenderSliverFloatingPersistentHeader.maybeStopSnapAnimation (package:flutter/src/rendering/sliver_persistent_header.dart:457:18)\n[        ] I/flutter (29797): #3      _FloatingAppBarState._isScrollingListener (package:flutter/src/material/app_bar.dart:547:15)\n[        ] I/flutter (29797): #4      ChangeNotifier.notifyListeners (package:flutter/src/foundation/change_notifier.dart:161:21)\n[        ] I/flutter (29797): #5      ValueNotifier.value= (package:flutter/src/foundation/change_notifier.dart:217:5)\n[        ] I/flutter (29797): #6      ScrollPosition.beginActivity (package:flutter/src/widgets/scroll_position.dart:613:25)\n[        ] I/flutter (29797): #7      ScrollPositionWithSingleContext.beginActivity (package:flutter/src/widgets/scroll_position_with_single_context.dart:117:11)\n[        ] I/flutter (29797): #8      ScrollPositionWithSingleContext.drag (package:flutter/src/widgets/scroll_position_with_single_context.dart:245:5)\n[        ] I/flutter (29797): #9      ScrollableState._handleDragStart (package:flutter/src/widgets/scrollable.dart:443:22)\n[   +1 ms] I/flutter (29797): #10     DragGestureRecognizer.acceptGesture.<anonymous closure> (package:flutter/src/gestures/monodrag.dart:169:47)\n[        ] I/flutter (29797): #11     GestureRecognizer.invokeCallback (package:flutter/src/gestures/recognizer.dart:102:24)\n[        ] I/flutter (29797): #12     DragGestureRecognizer.acceptGesture (package:flutter/src/gestures/monodrag.dart:169:9)\n[        ] I/flutter (29797): #13     GestureArenaManager._resolveByDefault (package:flutter/src/gestures/arena.dart:250:25)\n[        ] I/flutter (29797): #14     GestureArenaManager._tryToResolveArena.<anonymous closure> (package:flutter/src/gestures/arena.dart:231:31)\n[        ] I/flutter (29797): (elided 2 frames from package dart:async)\n[        ] I/flutter (29797):\n[        ] I/flutter (29797): The ValueNotifier<bool> sending notification was:\n[        ] I/flutter (29797):   ValueNotifier<bool>#0a1d4(true)\n[        ] I/flutter (29797): ════════════════════════════════════════════════════════════════════════════════════════════════════\n\nError when trying to switch to the 'App bar snaps' option:\n[+40490 ms] I/flutter (29797): Another exception was thrown: AnimationController.dispose() called more than once.\n[   +8 ms] I/flutter (29797): Another exception was thrown: A RenderViewport expected a child of type RenderSliver but received a child of type RenderErrorBox.\n[   +9 ms] I/chatty  (29797): uid=10084(io.flutter.demo.gallery) 1.ui identical 1 line\n[        ] I/flutter (29797): Another exception was thrown: A RenderViewport expected a child of type RenderSliver but received a child of type RenderErrorBox.\n[        ] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3497 pos 14: 'owner._debugCurrentBuildTarget == this': is not true.\n[+2095 ms] I/chatty  (29797): uid=10084(io.flutter.demo.gallery) 1.ui identical 20 lines\n[        ] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3497 pos 14: 'owner._debugCurrentBuildTarget == this': is not true.\n\nError when clicking the back button\n[+242379 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 4062 pos 14: '() {\n[ +279 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 2240 pos 16: '!_dirtyElements[index]._active || _dirtyElements[index]._debugIsInScope(context)': is not true.\n\nError when trying to re-open the Contact profile study after the last crash\n[+457845 ms] I/flutter (29797): Another exception was thrown: 'package:flutter/src/widgets/framework.dart': Failed assertion: line 3681 pos 12: 'child == _child': is not true.\n[   +6 ms] I/flutter (29797): Another exception was thrown: Duplicate GlobalKey detected in widget tree.",
      "title": "Flutter Gallery app crashes when switching between app bar options in the \"Contact profile\" study"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2730,
    "text": "Command “flutter create –t module xxx” is not workI want to Add Flutter to existing apps , but command \"flutter create –t module xxx\" is not work,\nexec command \"flutter create –t module xxx\" , and output \"Multiple output directories specified.\"\nDoctor summary (to see all details, run flutter doctor -v):\n[✓] Flutter (Channel master, v0.5.8-pre.241, on Mac OS X 10.12.6 16G1510, locale zh-Hans-CN)\n[✓] Android toolchain - develop for Android devices (Android SDK 27.0.3)\n[!] iOS toolchain - develop for iOS devices\n✗ Xcode installation is incomplete; a full installation is necessary for iOS development.\nDownload at: https://developer.apple.com/xcode/download/\nOr install Xcode via the App Store.\nOnce installed, run:\nsudo xcode-select --switch /Applications/Xcode.app/Contents/Developer\n✗ libimobiledevice and ideviceinstaller are not installed. To install, run:\nbrew install --HEAD libimobiledevice\nbrew install ideviceinstaller\n✗ ios-deploy not installed. To install:\nbrew install ios-deploy\n✗ CocoaPods not installed.\nCocoaPods is used to retrieve the iOS platform side's plugin code that responds to your plugin usage on the Dart side.\nWithout resolving iOS dependencies with CocoaPods, plugins will not work on iOS.\nFor more info, see https://flutter.io/platform-plugins\nTo install:\nbrew install cocoapods\npod setup\n[✓] Android Studio (version 3.1)\n[!] IntelliJ IDEA Community Edition (version 2018.2)\n✗ Flutter plugin not installed; this adds Flutter specific functionality.\n✗ Dart plugin not installed; this adds Dart specific functionality.\n[✓] Connected devices (1 available)\n! Doctor found issues in 2 categories.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "50",
      "number": "20249",
      "pretext": "I want to Add Flutter to existing apps , but command \"flutter create –t module xxx\" is not work,\nexec command \"flutter create –t module xxx\" , and output \"Multiple output directories specified.\"\nDoctor summary (to see all details, run flutter doctor -v):\n[✓] Flutter (Channel master, v0.5.8-pre.241, on Mac OS X 10.12.6 16G1510, locale zh-Hans-CN)\n[✓] Android toolchain - develop for Android devices (Android SDK 27.0.3)\n[!] iOS toolchain - develop for iOS devices\n✗ Xcode installation is incomplete; a full installation is necessary for iOS development.\nDownload at: https://developer.apple.com/xcode/download/\nOr install Xcode via the App Store.\nOnce installed, run:\nsudo xcode-select --switch /Applications/Xcode.app/Contents/Developer\n✗ libimobiledevice and ideviceinstaller are not installed. To install, run:\nbrew install --HEAD libimobiledevice\nbrew install ideviceinstaller\n✗ ios-deploy not installed. To install:\nbrew install ios-deploy\n✗ CocoaPods not installed.\nCocoaPods is used to retrieve the iOS platform side's plugin code that responds to your plugin usage on the Dart side.\nWithout resolving iOS dependencies with CocoaPods, plugins will not work on iOS.\nFor more info, see https://flutter.io/platform-plugins\nTo install:\nbrew install cocoapods\npod setup\n[✓] Android Studio (version 3.1)\n[!] IntelliJ IDEA Community Edition (version 2018.2)\n✗ Flutter plugin not installed; this adds Flutter specific functionality.\n✗ Dart plugin not installed; this adds Dart specific functionality.\n[✓] Connected devices (1 available)\n! Doctor found issues in 2 categories.",
      "title": "Command “flutter create –t module xxx” is not work"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2731,
    "text": "NestedScrollView cannot scroll inner list viewI have a NestedScrollView with a SliverAppBar with an expandedHeight and a SliverFixedExtentList inside. Basically this example.\nNow I want to jumpTo position 5000.0 in the list.\nSo I add a ScrollController to the NestedScrollView. But the range that I can jumpTo seems to be limited to the height of the header.\nI am not able to programatically scroll the SliverFixedExtentList to any position.\nI believe this is because the NestedScrollView doesn't allow access to the innerController, but that is just a guess.\nIs there a way to achieve this any other way?\nWhy, because I try to have a TabBar and each tab scrolls the ListView underneath to a specific section - basically like in the UeberEats app's menu page.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "51",
      "number": "18099",
      "pretext": "I have a NestedScrollView with a SliverAppBar with an expandedHeight and a SliverFixedExtentList inside. Basically this example.\nNow I want to jumpTo position 5000.0 in the list.\nSo I add a ScrollController to the NestedScrollView. But the range that I can jumpTo seems to be limited to the height of the header.\nI am not able to programatically scroll the SliverFixedExtentList to any position.\nI believe this is because the NestedScrollView doesn't allow access to the innerController, but that is just a guess.\nIs there a way to achieve this any other way?\nWhy, because I try to have a TabBar and each tab scrolls the ListView underneath to a specific section - basically like in the UeberEats app's menu page.",
      "title": "NestedScrollView cannot scroll inner list view"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2732,
    "text": "broken link to android studio(reported by user)\non https://flutter.io/setup/\nbroken link\nhttps://flutter.io/setup/https//developer.android.com/sdk/index.html\ncorrect link\nhttps://developer.android.com/sdk/index.html",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "52",
      "number": "2130",
      "pretext": "(reported by user)\non https://flutter.io/setup/\nbroken link\nhttps://flutter.io/setup/https//developer.android.com/sdk/index.html\ncorrect link\nhttps://developer.android.com/sdk/index.html",
      "title": "broken link to android studio"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2733,
    "text": "Non-actionable assert: '_placeholderSize == null' is not true.The assertion '_placeholderSize == null' is not true. actually means that liftToOverlay was called twice. The text in the assertion should include this helpful hint.\nSteps to Reproduce\nUse a Mimicable\nCall liftToOverlay a second time before the first has finished.\nFlutter Version\nFlutter from https://github.com/flutter/flutter.git (on master)\nFramework: 9ce6bff (2 days ago)\nEngine:    006a702\nLogs\n03-02 10:20:56.726  8213  8230 I flutter : -- EXCEPTION CAUGHT BY GESTURE LIBRARY ---------------------------------\n03-02 10:20:56.728  8213  8230 I flutter : The following exception was raised while routing a pointer event:\n03-02 10:20:56.729  8213  8230 I flutter : 'packages/flutter/src/widgets/mimic.dart': Failed assertion: line 176: '_placeholderSize == null' is not true.\n03-02 10:20:56.729  8213  8230 I flutter : Event:\n03-02 10:20:56.732  8213  8230 I flutter : PointerUpEvent(Point(115.5, 102.2))\n03-02 10:20:56.732  8213  8230 I flutter : Stack trace:\n03-02 10:20:56.735  8213  8230 I flutter : #0      _AssertionError._throwNew (dart:core-patch/errors_patch.dart:27)\n03-02 10:20:56.735  8213  8230 I flutter : #1      MimicableState.startMimic (packages/flutter/src/widgets/mimic.dart:176)\n03-02 10:20:56.735  8213  8230 I flutter : #2      MimicableState.liftToOverlay (packages/flutter/src/widgets/mimic.dart:198)\n03-02 10:20:56.735  8213  8230 I flutter : #3      RouteBarState._onTap (/media/jimbe/mojossd/sysui/src/hub/lib/route_bar.dart:146)\n03-02 10:20:56.735  8213  8230 I flutter : #4      RouteBarState.build.<anonymous closure>.<anonymous closure> (/media/jimbe/mojossd/sysui/src/hub/lib/route_bar.dart:167)\n03-02 10:20:56.736  8213  8230 I flutter : #5      _InkResponseState._handleTap (packages/flutter/src/material/ink_well.dart:101)\n03-02 10:20:56.736  8213  8230 I flutter : #6      TapGestureRecognizer._checkUp (packages/flutter/src/gestures/tap.dart:83)\n03-02 10:20:56.736  8213  8230 I flutter : #7      TapGestureRecognizer.handlePrimaryPointer (packages/flutter/src/gestures/tap.dart:33)\n03-02 10:20:56.737  8213  8230 I flutter : #8      PrimaryPointerGestureRecognizer.handleEvent (packages/flutter/src/gestures/recognizer.dart:138)\n03-02 10:20:56.737  8213  8230 I flutter : #9      PointerRouter.route (packages/flutter/src/gestures/pointer_router.dart:65)\n03-02 10:20:56.737  8213  8230 I flutter : #10     BindingBase&Scheduler&Gesturer.handleEvent (packages/flutter/src/gestures/binding.dart:117)\n03-02 10:20:56.737  8213  8230 I flutter : #11     BindingBase&Scheduler&Gesturer.dispatchEvent (packages/flutter/src/gestures/binding.dart:96)\n03-02 10:20:56.737  8213  8230 I flutter : #12     BindingBase&Scheduler&Gesturer._handlePointerEvent (packages/flutter/src/gestures/binding.dart:71)\n03-02 10:20:56.737  8213  8230 I flutter : #13     BindingBase&Scheduler&Gesturer._handlePointerPacket (packages/flutter/src/gestures/binding.dart:43)\n03-02 10:20:56.737  8213  8230 I flutter : #14     _dispatchPointerPacket (file:///b/build/slave/Linux_Engine/build/src/out/android_Release/gen/sky/bindings/hooks.dart:43)\n03-02 10:20:56.737  8213  8230 I flutter : ------------------------------------------------------------------------",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "53",
      "number": "2326",
      "pretext": "The assertion '_placeholderSize == null' is not true. actually means that liftToOverlay was called twice. The text in the assertion should include this helpful hint.\nSteps to Reproduce\nUse a Mimicable\nCall liftToOverlay a second time before the first has finished.\nFlutter Version\nFlutter from https://github.com/flutter/flutter.git (on master)\nFramework: 9ce6bff (2 days ago)\nEngine:    006a702\nLogs\n03-02 10:20:56.726  8213  8230 I flutter : -- EXCEPTION CAUGHT BY GESTURE LIBRARY ---------------------------------\n03-02 10:20:56.728  8213  8230 I flutter : The following exception was raised while routing a pointer event:\n03-02 10:20:56.729  8213  8230 I flutter : 'packages/flutter/src/widgets/mimic.dart': Failed assertion: line 176: '_placeholderSize == null' is not true.\n03-02 10:20:56.729  8213  8230 I flutter : Event:\n03-02 10:20:56.732  8213  8230 I flutter : PointerUpEvent(Point(115.5, 102.2))\n03-02 10:20:56.732  8213  8230 I flutter : Stack trace:\n03-02 10:20:56.735  8213  8230 I flutter : #0      _AssertionError._throwNew (dart:core-patch/errors_patch.dart:27)\n03-02 10:20:56.735  8213  8230 I flutter : #1      MimicableState.startMimic (packages/flutter/src/widgets/mimic.dart:176)\n03-02 10:20:56.735  8213  8230 I flutter : #2      MimicableState.liftToOverlay (packages/flutter/src/widgets/mimic.dart:198)\n03-02 10:20:56.735  8213  8230 I flutter : #3      RouteBarState._onTap (/media/jimbe/mojossd/sysui/src/hub/lib/route_bar.dart:146)\n03-02 10:20:56.735  8213  8230 I flutter : #4      RouteBarState.build.<anonymous closure>.<anonymous closure> (/media/jimbe/mojossd/sysui/src/hub/lib/route_bar.dart:167)\n03-02 10:20:56.736  8213  8230 I flutter : #5      _InkResponseState._handleTap (packages/flutter/src/material/ink_well.dart:101)\n03-02 10:20:56.736  8213  8230 I flutter : #6      TapGestureRecognizer._checkUp (packages/flutter/src/gestures/tap.dart:83)\n03-02 10:20:56.736  8213  8230 I flutter : #7      TapGestureRecognizer.handlePrimaryPointer (packages/flutter/src/gestures/tap.dart:33)\n03-02 10:20:56.737  8213  8230 I flutter : #8      PrimaryPointerGestureRecognizer.handleEvent (packages/flutter/src/gestures/recognizer.dart:138)\n03-02 10:20:56.737  8213  8230 I flutter : #9      PointerRouter.route (packages/flutter/src/gestures/pointer_router.dart:65)\n03-02 10:20:56.737  8213  8230 I flutter : #10     BindingBase&Scheduler&Gesturer.handleEvent (packages/flutter/src/gestures/binding.dart:117)\n03-02 10:20:56.737  8213  8230 I flutter : #11     BindingBase&Scheduler&Gesturer.dispatchEvent (packages/flutter/src/gestures/binding.dart:96)\n03-02 10:20:56.737  8213  8230 I flutter : #12     BindingBase&Scheduler&Gesturer._handlePointerEvent (packages/flutter/src/gestures/binding.dart:71)\n03-02 10:20:56.737  8213  8230 I flutter : #13     BindingBase&Scheduler&Gesturer._handlePointerPacket (packages/flutter/src/gestures/binding.dart:43)\n03-02 10:20:56.737  8213  8230 I flutter : #14     _dispatchPointerPacket (file:///b/build/slave/Linux_Engine/build/src/out/android_Release/gen/sky/bindings/hooks.dart:43)\n03-02 10:20:56.737  8213  8230 I flutter : ------------------------------------------------------------------------",
      "title": "Non-actionable assert: '_placeholderSize == null' is not true."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2734,
    "text": "Querying for a non-existent location waits foreverSteps to Reproduce\nawait FirebaseDatabase.instance.reference().child('foo/bar').once();\nAs far as I know this should return immediately in any case, not only when a value is found,\nbut in Flutter it only returns after the first event which may never come.\nThis makes it impossible to check if a location exists.\nfirebase_database 0.1.2\nLogs\nFlutter Doctor\n✓] Flutter (on Mac OS X 10.13.1 17B1003, locale en-AT, channel alpha)\n    • Flutter at /Users/zoechi/flutter/flutter\n    • Framework revision fd7853faad (7 days ago), 2017-12-05 16:12:55 -0800\n    • Engine revision b57fca02b5\n    • Tools Dart version 1.25.0-dev.11.0\n    • Engine Dart version 2.0.0-edge.d4cfecb1065d322d3670df7e9ec9a0cc2d4b90f0\n\n[✓] Android toolchain - develop for Android devices (Android SDK 27.0.1)\n    • Android SDK at /usr/local/opt/android-sdk\n    • Android NDK at /usr/local/opt/android-sdk/ndk-bundle\n    • Platform android-27, build-tools 27.0.1\n    • ANDROID_HOME = /usr/local/opt/android-sdk\n    • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 9.2)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 9.2, Build version 9C40b\n    • ios-deploy 1.9.2\n    • CocoaPods version 1.3.1\n\n[✓] Android Studio (version 3.0)\n    • Android Studio at /Applications/Android Studio.app/Contents\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n\n[✓] IntelliJ IDEA Ultimate Edition (version 2017.3)\n    • Flutter plugin version 20.0.3\n    • Dart plugin version 173.3727.108\n\n[✓] Connected devices\n    • Pixel XL                  • HT69V0203649  • android-arm • Android 8.1.0 (API 27)\n    • Android SDK built for x86 • emulator-5554 • android-x86 • Android 8.0.0 (API 26) (emulator)\n\n\n\nFor more information about diagnosing and reporting Flutter bugs, please see https://flutter.io/bug-reports/.\n\nSee also https://stackoverflow.com/questions/37910008/check-if-value-exists-in-firebase-db",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "54",
      "number": "13510",
      "pretext": "Steps to Reproduce\nawait FirebaseDatabase.instance.reference().child('foo/bar').once();\nAs far as I know this should return immediately in any case, not only when a value is found,\nbut in Flutter it only returns after the first event which may never come.\nThis makes it impossible to check if a location exists.\nfirebase_database 0.1.2\nLogs\nFlutter Doctor\n✓] Flutter (on Mac OS X 10.13.1 17B1003, locale en-AT, channel alpha)\n    • Flutter at /Users/zoechi/flutter/flutter\n    • Framework revision fd7853faad (7 days ago), 2017-12-05 16:12:55 -0800\n    • Engine revision b57fca02b5\n    • Tools Dart version 1.25.0-dev.11.0\n    • Engine Dart version 2.0.0-edge.d4cfecb1065d322d3670df7e9ec9a0cc2d4b90f0\n\n[✓] Android toolchain - develop for Android devices (Android SDK 27.0.1)\n    • Android SDK at /usr/local/opt/android-sdk\n    • Android NDK at /usr/local/opt/android-sdk/ndk-bundle\n    • Platform android-27, build-tools 27.0.1\n    • ANDROID_HOME = /usr/local/opt/android-sdk\n    • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 9.2)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 9.2, Build version 9C40b\n    • ios-deploy 1.9.2\n    • CocoaPods version 1.3.1\n\n[✓] Android Studio (version 3.0)\n    • Android Studio at /Applications/Android Studio.app/Contents\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n\n[✓] IntelliJ IDEA Ultimate Edition (version 2017.3)\n    • Flutter plugin version 20.0.3\n    • Dart plugin version 173.3727.108\n\n[✓] Connected devices\n    • Pixel XL                  • HT69V0203649  • android-arm • Android 8.1.0 (API 27)\n    • Android SDK built for x86 • emulator-5554 • android-x86 • Android 8.0.0 (API 26) (emulator)\n\n\n\nFor more information about diagnosing and reporting Flutter bugs, please see https://flutter.io/bug-reports/.\n\nSee also https://stackoverflow.com/questions/37910008/check-if-value-exists-in-firebase-db",
      "title": "Querying for a non-existent location waits forever"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2735,
    "text": "Issue from website page Building Layouts in FlutterFrom URL: https://flutter.io/tutorials/layout/\nIn the second figure, the caption to the right that reads \"Column\" should read \"Rows\"",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "55",
      "number": "19956",
      "pretext": "From URL: https://flutter.io/tutorials/layout/\nIn the second figure, the caption to the right that reads \"Column\" should read \"Rows\"",
      "title": "Issue from website page Building Layouts in Flutter"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2736,
    "text": "When generating golden files, catch the case of orphan filesSometimes when writing a test you change your mind about what file names to use, but you do so after having run it with the old names, and then check in the unused files. We should be able to catch that case since when generating goldens we know every test we run and know every file it generated.\nThis might require that we put files from tests in a directory specific to the test.\ncc @tvolkert",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "56",
      "number": "17360",
      "pretext": "Sometimes when writing a test you change your mind about what file names to use, but you do so after having run it with the old names, and then check in the unused files. We should be able to catch that case since when generating goldens we know every test we run and know every file it generated.\nThis might require that we put files from tests in a directory specific to the test.\ncc @tvolkert",
      "title": "When generating golden files, catch the case of orphan files"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2737,
    "text": "canvas.drawImageRect can't output the imageWhen i use the  drawRect, The out byte can display on the image widget,\nBut  when  i use the  drawImageRect, I found the out byte  always be the same code , and it can't display on the image widget.   But I can draw it on the  CustomPainter\nIf  I change the image2 to the source image, It will be ok.\nHow can i do  to  get the cut image\n               child: Text(\"image\"),\n               onPressed: (){\n               var source = Rect.fromLTWH(0, 0, this.image.width.toDouble(), this.image.height.toDouble());\n               var dest = Rect.fromLTWH(0,0,200,200);\n               PictureRecorder recorder =  PictureRecorder();\n               Canvas canvas2 = Canvas(recorder);\n               Paint paint2 = new Paint();\n               paint2.color =Colors.red;\n               canvas2.drawRect(Rect.fromLTWH(0, 0, 150, 150), paint2);\n               canvas2.drawImageRect(image, source, dest, paint2);\n               var image2 = recorder.endRecording().toImage(dest.width.toInt(), dest.height.toInt());\n               image2.toByteData(format:ui.ImageByteFormat.png).then((byte){\n                 imageDes=   Uint8List.view(byte.buffer);\n               var mByte=byte.buffer.asUint8List();\n               print(\"aaa ${mByte}\");\n               getApplicationDocumentsDirectory().then((dir){\n               String path = dir.path +\"/test.png\";\n               new File(path).writeAsBytesSync(mByte);\n               print(path);\n               });\n               setState(() {\n                 imageDes =mByte;\n               });\n               });\n              print(imageDes);\n           Navigator.push(context,new MaterialPageRoute(builder: (context) =>  ImageTestPage(title: \"image\",image: byte.buffer.asUint8List())));\n        },\n      );\n           }),` ` \n` \n\n\n`class SignaturePainter extends CustomPainter {\n SignaturePainter(this.points,this.isStart,this.width,this.height,this.cWidth,this.cHeight,this.image);\n bool  isStart = false;\n double startX=0;\n double startY=0;\n double cWidth =200;\n double cHeight =200;\n double width;\n double height;\n flutterui.Image image;\n\n final List<Offset> points; \n\n void paint(Canvas canvas, Size size) {\n   Paint paint = new Paint()\n     ..color = Colors.blue[200]\n     ..strokeCap = StrokeCap.round\n     ..isAntiAlias = true\n     ..strokeWidth = 2.0\n\n     ..strokeJoin = StrokeJoin.bevel;\n//    print(\"change ${this.points}\");\n   if(image!=null){\n\n     double dwidth =0;\n     double dheight =0;\n     if(image.width.toDouble()/width>image.height.toDouble()/height){\n       dwidth = width;\n       dheight = image.height.toDouble()*dwidth/image.width.toDouble();\n     }\n     else{\n       dheight = height;\n       dwidth = image.width.toDouble() * dheight/image.height.toDouble();\n     }\n     canvas.drawImageRect(image, Rect.fromLTWH(0, 0, image.width.toDouble(), image.height.toDouble()), Rect.fromLTWH((width-dwidth)/2,\n         (height-dheight)/2, dwidth,dheight), paint);\n\n//      var source = Rect.fromLTWH(0, 0, this.image.width.toDouble(), this.image.height.toDouble());\n//      var dest = Rect.fromLTWH(0,0,400,400);\n//      PictureRecorder recorder =  PictureRecorder();\n//      Canvas canvas2 = Canvas(recorder);\n//      Paint paint2 = new Paint();\n//      canvas2.drawImageRect(image, source, dest, paint2);\n//      var image2 = recorder.endRecording().toImage(dest.width.toInt(), dest.height.toInt());\n////    canvas.drawImageRect(image2, Rect.fromLTWH(0, 0, image.width.toDouble(), image.height.toDouble()), Rect.fromLTWH(0,0,400,400), paint);\n//      image2.toByteData(format:ui.ImageByteFormat.png).then((byte){\n//        var mByte=byte.buffer.asUint8List();\n//        print(\"aaa ${mByte}\");\n//        getApplicationDocumentsDirectory().then((dir){\n//          String path = dir.path +\"/test.png\";\n//          new File(path).writeAsBytesSync(mByte);\n//          print(path);\n//        });\n//      });\n   }\n\n   if(this.points.length>1){\n     double startX = points[1].dx - points[0].dx+points[2].dx;\n     double startY = points[1].dy - points[0].dy+points[2].dy;\n     if(startX<0)\n       startX = 0;\n     else if(startX+cWidth>width){\n       startX = width-cWidth;\n     }\n     if(startY<0)\n       startY=0;\n     else if(startY + cHeight>height){\n       startY = height-cHeight;\n     }\n//      canvas.drawRect(Rect.fromLTRB(startX,startY,startX+200,startY+200), paint);\n     List<Offset> points2 =[\n       Offset(startX,startY),\n       Offset(startX+cWidth,startY),\n       Offset(startX+cWidth,startY+cHeight),\n       Offset(startX,startY+cHeight),\n       Offset(startX,startY),\n     ];\n     canvas.drawPoints(PointMode.polygon, points2, paint);\n   }\n   else{\n     List<Offset> points2 =[\n       Offset(startX,startY),\n       Offset(startX+cWidth,startY),\n       Offset(startX+cWidth,startY+cHeight),\n       Offset(startX,startY+cHeight),\n       Offset(startX,startY),\n     ];\n     canvas.drawPoints(PointMode.polygon, points2, paint);\n   }\n\n }`",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "57",
      "number": "25875",
      "pretext": "When i use the  drawRect, The out byte can display on the image widget,\nBut  when  i use the  drawImageRect, I found the out byte  always be the same code , and it can't display on the image widget.   But I can draw it on the  CustomPainter\nIf  I change the image2 to the source image, It will be ok.\nHow can i do  to  get the cut image\n               child: Text(\"image\"),\n               onPressed: (){\n               var source = Rect.fromLTWH(0, 0, this.image.width.toDouble(), this.image.height.toDouble());\n               var dest = Rect.fromLTWH(0,0,200,200);\n               PictureRecorder recorder =  PictureRecorder();\n               Canvas canvas2 = Canvas(recorder);\n               Paint paint2 = new Paint();\n               paint2.color =Colors.red;\n               canvas2.drawRect(Rect.fromLTWH(0, 0, 150, 150), paint2);\n               canvas2.drawImageRect(image, source, dest, paint2);\n               var image2 = recorder.endRecording().toImage(dest.width.toInt(), dest.height.toInt());\n               image2.toByteData(format:ui.ImageByteFormat.png).then((byte){\n                 imageDes=   Uint8List.view(byte.buffer);\n               var mByte=byte.buffer.asUint8List();\n               print(\"aaa ${mByte}\");\n               getApplicationDocumentsDirectory().then((dir){\n               String path = dir.path +\"/test.png\";\n               new File(path).writeAsBytesSync(mByte);\n               print(path);\n               });\n               setState(() {\n                 imageDes =mByte;\n               });\n               });\n              print(imageDes);\n           Navigator.push(context,new MaterialPageRoute(builder: (context) =>  ImageTestPage(title: \"image\",image: byte.buffer.asUint8List())));\n        },\n      );\n           }),` ` \n` \n\n\n`class SignaturePainter extends CustomPainter {\n SignaturePainter(this.points,this.isStart,this.width,this.height,this.cWidth,this.cHeight,this.image);\n bool  isStart = false;\n double startX=0;\n double startY=0;\n double cWidth =200;\n double cHeight =200;\n double width;\n double height;\n flutterui.Image image;\n\n final List<Offset> points; \n\n void paint(Canvas canvas, Size size) {\n   Paint paint = new Paint()\n     ..color = Colors.blue[200]\n     ..strokeCap = StrokeCap.round\n     ..isAntiAlias = true\n     ..strokeWidth = 2.0\n\n     ..strokeJoin = StrokeJoin.bevel;\n//    print(\"change ${this.points}\");\n   if(image!=null){\n\n     double dwidth =0;\n     double dheight =0;\n     if(image.width.toDouble()/width>image.height.toDouble()/height){\n       dwidth = width;\n       dheight = image.height.toDouble()*dwidth/image.width.toDouble();\n     }\n     else{\n       dheight = height;\n       dwidth = image.width.toDouble() * dheight/image.height.toDouble();\n     }\n     canvas.drawImageRect(image, Rect.fromLTWH(0, 0, image.width.toDouble(), image.height.toDouble()), Rect.fromLTWH((width-dwidth)/2,\n         (height-dheight)/2, dwidth,dheight), paint);\n\n//      var source = Rect.fromLTWH(0, 0, this.image.width.toDouble(), this.image.height.toDouble());\n//      var dest = Rect.fromLTWH(0,0,400,400);\n//      PictureRecorder recorder =  PictureRecorder();\n//      Canvas canvas2 = Canvas(recorder);\n//      Paint paint2 = new Paint();\n//      canvas2.drawImageRect(image, source, dest, paint2);\n//      var image2 = recorder.endRecording().toImage(dest.width.toInt(), dest.height.toInt());\n////    canvas.drawImageRect(image2, Rect.fromLTWH(0, 0, image.width.toDouble(), image.height.toDouble()), Rect.fromLTWH(0,0,400,400), paint);\n//      image2.toByteData(format:ui.ImageByteFormat.png).then((byte){\n//        var mByte=byte.buffer.asUint8List();\n//        print(\"aaa ${mByte}\");\n//        getApplicationDocumentsDirectory().then((dir){\n//          String path = dir.path +\"/test.png\";\n//          new File(path).writeAsBytesSync(mByte);\n//          print(path);\n//        });\n//      });\n   }\n\n   if(this.points.length>1){\n     double startX = points[1].dx - points[0].dx+points[2].dx;\n     double startY = points[1].dy - points[0].dy+points[2].dy;\n     if(startX<0)\n       startX = 0;\n     else if(startX+cWidth>width){\n       startX = width-cWidth;\n     }\n     if(startY<0)\n       startY=0;\n     else if(startY + cHeight>height){\n       startY = height-cHeight;\n     }\n//      canvas.drawRect(Rect.fromLTRB(startX,startY,startX+200,startY+200), paint);\n     List<Offset> points2 =[\n       Offset(startX,startY),\n       Offset(startX+cWidth,startY),\n       Offset(startX+cWidth,startY+cHeight),\n       Offset(startX,startY+cHeight),\n       Offset(startX,startY),\n     ];\n     canvas.drawPoints(PointMode.polygon, points2, paint);\n   }\n   else{\n     List<Offset> points2 =[\n       Offset(startX,startY),\n       Offset(startX+cWidth,startY),\n       Offset(startX+cWidth,startY+cHeight),\n       Offset(startX,startY+cHeight),\n       Offset(startX,startY),\n     ];\n     canvas.drawPoints(PointMode.polygon, points2, paint);\n   }\n\n }`",
      "title": "canvas.drawImageRect can't output the image"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2738,
    "text": "Unable to debug / hit breakpoints in flutter app in the visual studio codeI am using vs code and trying to debug the application, but when i hit 'Start Debugging' button in vs code, app simply launches and never hits breakpoint. anyone has a clue asto what could be wrong ?",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "58",
      "number": "29992",
      "pretext": "I am using vs code and trying to debug the application, but when i hit 'Start Debugging' button in vs code, app simply launches and never hits breakpoint. anyone has a clue asto what could be wrong ?",
      "title": "Unable to debug / hit breakpoints in flutter app in the visual studio code"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2739,
    "text": "Could not find a file named \"pubspec.yaml\" in \"...pub.dartlang.org/archive-2.0.4\".Steps to Reproduce\n\n\nNavigate to a flutter project\nrun flutter doctor (or any flutter command)\nExperience this output:\n\nBuilding flutter tool...\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (9 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (8 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (7 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (6 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (5 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (4 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (3 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (2 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (1 tries left)\nCommand 'pub upgrade' still failed after 10 tries, giving up.\n\nLogs\nCan't run anything with flutter to get some logs. It just gives me the above response.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "59",
      "number": "22175",
      "pretext": "Steps to Reproduce\n\n\nNavigate to a flutter project\nrun flutter doctor (or any flutter command)\nExperience this output:\n\nBuilding flutter tool...\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (9 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (8 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (7 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (6 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (5 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (4 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (3 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (2 tries left)\nCould not find a file named \"pubspec.yaml\" in \"/Users/mlgyshan/.pub-cache/hosted/pub.dartlang.org/archive-2.0.4\".\nError: Unable to 'pub upgrade' flutter tool. Retrying in five seconds... (1 tries left)\nCommand 'pub upgrade' still failed after 10 tries, giving up.\n\nLogs\nCan't run anything with flutter to get some logs. It just gives me the above response.",
      "title": "Could not find a file named \"pubspec.yaml\" in \"...pub.dartlang.org/archive-2.0.4\"."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2740,
    "text": "Orientation(Rotation) Issue of Camera PluginEven with the provided example, I'm experiencing an issue with camera plugin's orientation when device is rotated. I confirmed this error across different android versions. When the orientation is portrait, everything is fine, working so well. But when we turn around the device, the camera view is 90 degrees off the correct view in clockwise direction, that it should provide. I will attach few screenshots which would help to describe the issue more precisely.\nPortrait View\n\nLandscape View",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "60",
      "number": "21545",
      "pretext": "Even with the provided example, I'm experiencing an issue with camera plugin's orientation when device is rotated. I confirmed this error across different android versions. When the orientation is portrait, everything is fine, working so well. But when we turn around the device, the camera view is 90 degrees off the correct view in clockwise direction, that it should provide. I will attach few screenshots which would help to describe the issue more precisely.\nPortrait View\n\nLandscape View",
      "title": "Orientation(Rotation) Issue of Camera Plugin"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2741,
    "text": "Engine roll caused regressionWith engine roll 6a8a73c the following benchmark regressed:\n\nfutter_gallery_ios32__transition_perf/99th_percentile_frame_rasterizer_time_millis\n\n/cc @bkonyi",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "61",
      "number": "21632",
      "pretext": "With engine roll 6a8a73c the following benchmark regressed:\n\nfutter_gallery_ios32__transition_perf/99th_percentile_frame_rasterizer_time_millis\n\n/cc @bkonyi",
      "title": "Engine roll caused regression"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2742,
    "text": "Request: firebase_storage to support metadataI could use support reading/writing metadata, especially custom metadata field.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "62",
      "number": "16421",
      "pretext": "I could use support reading/writing metadata, especially custom metadata field.",
      "title": "Request: firebase_storage to support metadata"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2743,
    "text": "Is this a typo? assert(transform.determinant != 0.0);I found this line of code in flutter/lib/src/painting/matrix_utils.dart, line 166, commit: 27b058a\nassert(transform.determinant != 0.0);\n\nI guess it is expected to be:\nassert(transform.determinant() != 0.0);\n\nHowever, if my guess is correct, there is another issue which causes this assertion to fail. Followed are the steps to reproduce\n\nReplace determinant with determinant() as above;\nOpen project flutter_gallery in Android Studio;\nStart running main.dart in Android simulator;\nAs the application is launched, tap 'Material -> Bottom app bar' to open the bottom app bar demo;\nTap 'None' radio in FAB shape, the Floating Action Button should disappear;\nTap 'Circular' or 'Diamond' radio in  FAB shape.\n\nThe console output looks as follows\nLaunching lib/main.dart on Android SDK built for x86 in debug mode...\nInitializing gradle...\nResolving dependencies...\nRunning Gradle task 'assembleDebug'...\nBuilt build/app/outputs/apk/debug/app-debug.apk.\nInstalling build/app/outputs/apk/app.apk...\nSyncing files to device Android SDK built for x86...\nD/EGL_emulation( 7685): eglMakeCurrent: 0xe74852a0: ver 3 0 (tinfo 0xe74832e0)\nI/flutter ( 7685): ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════\nI/flutter ( 7685): The following NoSuchMethodError was thrown during paint():\nI/flutter ( 7685): The getter 'left' was called on null.\nI/flutter ( 7685): Receiver: null\nI/flutter ( 7685): Tried calling: left\nI/flutter ( 7685): \nI/flutter ( 7685): When the exception was thrown, this was the stack:\nI/flutter ( 7685): #0      Object.noSuchMethod (dart:core-patch/object_patch.dart:50:5)\nI/flutter ( 7685): #1      Rect.overlaps (dart:ui/geometry.dart:771:24)\nI/flutter ( 7685): #2      _DiamondNotchedRectangle.getOuterPath (package:flutter_gallery/demo/material/bottom_app_bar_demo.dart:459:15)\nI/flutter ( 7685): #3      _BottomAppBarClipper.getClip (package:flutter/src/material/bottom_app_bar.dart:166:18)\nI/flutter ( 7685): #4      _RenderCustomClip._updateClip (package:flutter/src/rendering/proxy_box.dart:1212:25)\nI/flutter ( 7685): #5      RenderPhysicalShape.paint (package:flutter/src/rendering/proxy_box.dart:1780:7)\nI/flutter ( 7685): #6      RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #7      PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #8      _RenderCustomMultiChildLayoutBox&RenderBox&ContainerRenderObjectMixin&RenderBoxContainerDefaultsMixin.defaultPaint (package:flutter/src/rendering/box.dart:2273:15)\nI/flutter ( 7685): #9      RenderCustomMultiChildLayoutBox.paint (package:flutter/src/rendering/custom_layout.dart:361:5)\nI/flutter ( 7685): #10     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #11     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #12     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)\nI/flutter ( 7685): #13     _RenderInkFeatures.paint (package:flutter/src/material/material.dart:510:11)\nI/flutter ( 7685): #14     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #15     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #16     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)\nI/flutter ( 7685): #17     PaintingContext.pushLayer (package:flutter/src/rendering/object.dart:370:12)\nI/flutter ( 7685): #18     RenderPhysicalModel.paint (package:flutter/src/rendering/proxy_box.dart:1716:15)\nI/flutter ( 7685): #19     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #20     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #21     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)\nI/flutter ( 7685): #22     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #23     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #24     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)\nI/flutter ( 7685): #25     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #26     PaintingContext._repaintCompositedChild (package:flutter/src/rendering/object.dart:128:11)\nI/flutter ( 7685): #27     PaintingContext.repaintCompositedChild (package:flutter/src/rendering/object.dart:96:5)\nI/flutter ( 7685): #28     PipelineOwner.flushPaint (package:flutter/src/rendering/object.dart:859:29)\nI/flutter ( 7685): #29     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding.drawFrame (package:flutter/src/rendering/binding.dart:349:19)\nI/flutter ( 7685): #30     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding&WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:701:13)\nI/flutter ( 7685): #31     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:286:5)\nI/flutter ( 7685): #32     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:1012:15)\nI/flutter ( 7685): #33     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:952:9)\nI/flutter ( 7685): #34     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:864:5)\nI/flutter ( 7685): #38     _invoke (dart:ui/hooks.dart:219:10)\nI/flutter ( 7685): #39     _drawFrame (dart:ui/hooks.dart:178:3)\nI/flutter ( 7685): (elided 3 frames from package dart:async)\nI/flutter ( 7685): \nI/flutter ( 7685): The following RenderObject was being processed when the exception was fired:\nI/flutter ( 7685):   RenderPhysicalShape#9225c relayoutBoundary=up1\nI/flutter ( 7685):   creator: PhysicalShape ← BottomAppBar ← _DemoBottomAppBar ← MediaQuery ←\nI/flutter ( 7685):   LayoutId-[<_ScaffoldSlot.bottomNavigationBar>] ← CustomMultiChildLayout ← AnimatedBuilder ←\nI/flutter ( 7685):   DefaultTextStyle ← AnimatedDefaultTextStyle ← _InkFeatures-[GlobalKey#52319 ink renderer] ←\nI/flutter ( 7685):   NotificationListener<LayoutChangedNotification> ← PhysicalModel ← ⋯\nI/flutter ( 7685):   parentData: offset=Offset(0.0, 635.4); id=_ScaffoldSlot.bottomNavigationBar (can use size)\nI/flutter ( 7685):   constraints: BoxConstraints(w=411.4, 0.0<=h<=683.4)\nI/flutter ( 7685):   size: Size(411.4, 48.0)\nI/flutter ( 7685):   elevation: 8.0\nI/flutter ( 7685):   color: Color(0xffffffff)\nI/flutter ( 7685):   shadowColor: Color(0xffffffff)\nI/flutter ( 7685):   clipper: _BottomAppBarClipper\nI/flutter ( 7685): This RenderObject had the following descendants (showing up to depth 5):\nI/flutter ( 7685):   RenderCustomPaint#22f12 relayoutBoundary=up2\nI/flutter ( 7685):     _RenderInkFeatures#b82a0 relayoutBoundary=up3\nI/flutter ( 7685):       RenderPadding#39bef relayoutBoundary=up4\nI/flutter ( 7685):         RenderFlex#40c20 relayoutBoundary=up5\nI/flutter ( 7685):           RenderSemanticsGestureHandler#24e16 relayoutBoundary=up6\nI/flutter ( 7685):           RenderSemanticsGestureHandler#cdda6 relayoutBoundary=up6\nI/flutter ( 7685):           RenderSemanticsGestureHandler#26a78 relayoutBoundary=up6\nI/flutter ( 7685): ════════════════════════════════════════════════════════════════════════════════════════════════════\nI/flutter ( 7685): Another exception was thrown: 'package:flutter/src/painting/matrix_utils.dart': Failed assertion: line 166 pos 12: 'transform.determinant() != 0.0': is not true.\nI/flutter ( 7685): Another exception was thrown: NoSuchMethodError: The getter 'left' was called on null.\nI/flutter ( 7685): Another exception was thrown: 'package:flutter/src/painting/matrix_utils.dart': Failed assertion: line 166 pos 12: 'transform.determinant() != 0.0': is not true.\nApplication finished.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "63",
      "number": "31045",
      "pretext": "I found this line of code in flutter/lib/src/painting/matrix_utils.dart, line 166, commit: 27b058a\nassert(transform.determinant != 0.0);\n\nI guess it is expected to be:\nassert(transform.determinant() != 0.0);\n\nHowever, if my guess is correct, there is another issue which causes this assertion to fail. Followed are the steps to reproduce\n\nReplace determinant with determinant() as above;\nOpen project flutter_gallery in Android Studio;\nStart running main.dart in Android simulator;\nAs the application is launched, tap 'Material -> Bottom app bar' to open the bottom app bar demo;\nTap 'None' radio in FAB shape, the Floating Action Button should disappear;\nTap 'Circular' or 'Diamond' radio in  FAB shape.\n\nThe console output looks as follows\nLaunching lib/main.dart on Android SDK built for x86 in debug mode...\nInitializing gradle...\nResolving dependencies...\nRunning Gradle task 'assembleDebug'...\nBuilt build/app/outputs/apk/debug/app-debug.apk.\nInstalling build/app/outputs/apk/app.apk...\nSyncing files to device Android SDK built for x86...\nD/EGL_emulation( 7685): eglMakeCurrent: 0xe74852a0: ver 3 0 (tinfo 0xe74832e0)\nI/flutter ( 7685): ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════\nI/flutter ( 7685): The following NoSuchMethodError was thrown during paint():\nI/flutter ( 7685): The getter 'left' was called on null.\nI/flutter ( 7685): Receiver: null\nI/flutter ( 7685): Tried calling: left\nI/flutter ( 7685): \nI/flutter ( 7685): When the exception was thrown, this was the stack:\nI/flutter ( 7685): #0      Object.noSuchMethod (dart:core-patch/object_patch.dart:50:5)\nI/flutter ( 7685): #1      Rect.overlaps (dart:ui/geometry.dart:771:24)\nI/flutter ( 7685): #2      _DiamondNotchedRectangle.getOuterPath (package:flutter_gallery/demo/material/bottom_app_bar_demo.dart:459:15)\nI/flutter ( 7685): #3      _BottomAppBarClipper.getClip (package:flutter/src/material/bottom_app_bar.dart:166:18)\nI/flutter ( 7685): #4      _RenderCustomClip._updateClip (package:flutter/src/rendering/proxy_box.dart:1212:25)\nI/flutter ( 7685): #5      RenderPhysicalShape.paint (package:flutter/src/rendering/proxy_box.dart:1780:7)\nI/flutter ( 7685): #6      RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #7      PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #8      _RenderCustomMultiChildLayoutBox&RenderBox&ContainerRenderObjectMixin&RenderBoxContainerDefaultsMixin.defaultPaint (package:flutter/src/rendering/box.dart:2273:15)\nI/flutter ( 7685): #9      RenderCustomMultiChildLayoutBox.paint (package:flutter/src/rendering/custom_layout.dart:361:5)\nI/flutter ( 7685): #10     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #11     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #12     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)\nI/flutter ( 7685): #13     _RenderInkFeatures.paint (package:flutter/src/material/material.dart:510:11)\nI/flutter ( 7685): #14     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #15     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #16     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)\nI/flutter ( 7685): #17     PaintingContext.pushLayer (package:flutter/src/rendering/object.dart:370:12)\nI/flutter ( 7685): #18     RenderPhysicalModel.paint (package:flutter/src/rendering/proxy_box.dart:1716:15)\nI/flutter ( 7685): #19     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #20     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #21     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)\nI/flutter ( 7685): #22     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #23     PaintingContext.paintChild (package:flutter/src/rendering/object.dart:173:13)\nI/flutter ( 7685): #24     _RenderProxyBox&RenderBox&RenderObjectWithChildMixin&RenderProxyBoxMixin.paint (package:flutter/src/rendering/proxy_box.dart:123:15)\nI/flutter ( 7685): #25     RenderObject._paintWithContext (package:flutter/src/rendering/object.dart:2104:7)\nI/flutter ( 7685): #26     PaintingContext._repaintCompositedChild (package:flutter/src/rendering/object.dart:128:11)\nI/flutter ( 7685): #27     PaintingContext.repaintCompositedChild (package:flutter/src/rendering/object.dart:96:5)\nI/flutter ( 7685): #28     PipelineOwner.flushPaint (package:flutter/src/rendering/object.dart:859:29)\nI/flutter ( 7685): #29     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding.drawFrame (package:flutter/src/rendering/binding.dart:349:19)\nI/flutter ( 7685): #30     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding&WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:701:13)\nI/flutter ( 7685): #31     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding&PaintingBinding&SemanticsBinding&RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:286:5)\nI/flutter ( 7685): #32     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:1012:15)\nI/flutter ( 7685): #33     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:952:9)\nI/flutter ( 7685): #34     _WidgetsFlutterBinding&BindingBase&GestureBinding&ServicesBinding&SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:864:5)\nI/flutter ( 7685): #38     _invoke (dart:ui/hooks.dart:219:10)\nI/flutter ( 7685): #39     _drawFrame (dart:ui/hooks.dart:178:3)\nI/flutter ( 7685): (elided 3 frames from package dart:async)\nI/flutter ( 7685): \nI/flutter ( 7685): The following RenderObject was being processed when the exception was fired:\nI/flutter ( 7685):   RenderPhysicalShape#9225c relayoutBoundary=up1\nI/flutter ( 7685):   creator: PhysicalShape ← BottomAppBar ← _DemoBottomAppBar ← MediaQuery ←\nI/flutter ( 7685):   LayoutId-[<_ScaffoldSlot.bottomNavigationBar>] ← CustomMultiChildLayout ← AnimatedBuilder ←\nI/flutter ( 7685):   DefaultTextStyle ← AnimatedDefaultTextStyle ← _InkFeatures-[GlobalKey#52319 ink renderer] ←\nI/flutter ( 7685):   NotificationListener<LayoutChangedNotification> ← PhysicalModel ← ⋯\nI/flutter ( 7685):   parentData: offset=Offset(0.0, 635.4); id=_ScaffoldSlot.bottomNavigationBar (can use size)\nI/flutter ( 7685):   constraints: BoxConstraints(w=411.4, 0.0<=h<=683.4)\nI/flutter ( 7685):   size: Size(411.4, 48.0)\nI/flutter ( 7685):   elevation: 8.0\nI/flutter ( 7685):   color: Color(0xffffffff)\nI/flutter ( 7685):   shadowColor: Color(0xffffffff)\nI/flutter ( 7685):   clipper: _BottomAppBarClipper\nI/flutter ( 7685): This RenderObject had the following descendants (showing up to depth 5):\nI/flutter ( 7685):   RenderCustomPaint#22f12 relayoutBoundary=up2\nI/flutter ( 7685):     _RenderInkFeatures#b82a0 relayoutBoundary=up3\nI/flutter ( 7685):       RenderPadding#39bef relayoutBoundary=up4\nI/flutter ( 7685):         RenderFlex#40c20 relayoutBoundary=up5\nI/flutter ( 7685):           RenderSemanticsGestureHandler#24e16 relayoutBoundary=up6\nI/flutter ( 7685):           RenderSemanticsGestureHandler#cdda6 relayoutBoundary=up6\nI/flutter ( 7685):           RenderSemanticsGestureHandler#26a78 relayoutBoundary=up6\nI/flutter ( 7685): ════════════════════════════════════════════════════════════════════════════════════════════════════\nI/flutter ( 7685): Another exception was thrown: 'package:flutter/src/painting/matrix_utils.dart': Failed assertion: line 166 pos 12: 'transform.determinant() != 0.0': is not true.\nI/flutter ( 7685): Another exception was thrown: NoSuchMethodError: The getter 'left' was called on null.\nI/flutter ( 7685): Another exception was thrown: 'package:flutter/src/painting/matrix_utils.dart': Failed assertion: line 166 pos 12: 'transform.determinant() != 0.0': is not true.\nApplication finished.",
      "title": "Is this a typo? assert(transform.determinant != 0.0);"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2744,
    "text": "Unclear how to add/edit platform view code to an existing Flutter appMore or less the reverse of #8945.\nGiven a new Flutter project, if some code needs to be done in the platform side:\n\n How to open Xcode, how is the iOS code structured\n How to open Android Studio, how is the Android code structured\n How to bring in existing local iOS/Android source code\n How to refer to existing iOS/Android packages using gradle / CocoaPods\n\ncc @sethladd, @Sfshaza",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "64",
      "number": "14373",
      "pretext": "More or less the reverse of #8945.\nGiven a new Flutter project, if some code needs to be done in the platform side:\n\n How to open Xcode, how is the iOS code structured\n How to open Android Studio, how is the Android code structured\n How to bring in existing local iOS/Android source code\n How to refer to existing iOS/Android packages using gradle / CocoaPods\n\ncc @sethladd, @Sfshaza",
      "title": "Unclear how to add/edit platform view code to an existing Flutter app"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2745,
    "text": "firebase_admob pauses video when app goes to backgroundWhen user switches app during rewarding video playback and brings it back to active state, video is paused. Yes, if user clicks on \"close\" button he/she is prompted to resume the video, but it would be nice to have an API to automatically continue playing video",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "65",
      "number": "26829",
      "pretext": "When user switches app during rewarding video playback and brings it back to active state, video is paused. Yes, if user clicks on \"close\" button he/she is prompted to resume the video, but it would be nice to have an API to automatically continue playing video",
      "title": "firebase_admob pauses video when app goes to background"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2746,
    "text": "Double shadow flash when moving component list underneath app barScrolling the component list up under the app bar flashes the shadow at the border between the app bar and the component list.\nMovie:  https://dl.dropboxusercontent.com/u/316685/RECORDING.mp4",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "66",
      "number": "2832",
      "pretext": "Scrolling the component list up under the app bar flashes the shadow at the border between the app bar and the component list.\nMovie:  https://dl.dropboxusercontent.com/u/316685/RECORDING.mp4",
      "title": "Double shadow flash when moving component list underneath app bar"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2747,
    "text": "VScode autoformat intending spacesI tried to auto-format my code in vscode with a flutter extension. And this gave me this result\n\nSometimes it uses 2 spaces, and sometimes it uses 4 spaces. Is this working correctly?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "67",
      "number": "21476",
      "pretext": "I tried to auto-format my code in vscode with a flutter extension. And this gave me this result\n\nSometimes it uses 2 spaces, and sometimes it uses 4 spaces. Is this working correctly?",
      "title": "VScode autoformat intending spaces"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2748,
    "text": "Testing buttons using tester.tap to show a dialog gives Framework ErrorSteps to Reproduce\n\nHave a method open an AlertDialog\nHave a button execute the method\nIn a test, use await tester.tap(find.byKey(const Key('LoginBtnKey'))); to tap the button\nThen verify that the dialog is shown using expect(find.byType(AlertDialog), findsOneWidget);\nThis gives a framework error.\n\n\nLogs\n\n══╡ EXCEPTION CAUGHT BY GESTURE ╞═══════════════════════════════════════════════════════════════════\nThe following assertion was thrown while handling a gesture:\n'package:flutter/src/widgets/localizations.dart': Failed assertion: line 446 pos 12: 'context !=\nnull': is not true.\n\nEither the assertion indicates an error in the framework itself, or we should provide substantially\nmore information in this error message to help you determine and fix the underlying cause.\nIn either case, please report this assertion by filing a bug on GitHub:\n  https://github.com/flutter/flutter/issues/new?template=BUG.md\n\nWhen the exception was thrown, this was the stack:\n#2      Localizations.of (package:flutter/src/widgets/localizations.dart:446:12)\n#3      debugCheckHasMaterialLocalizations.<anonymous closure> (package:flutter/src/material/debug.dart:88:23)\n#4      debugCheckHasMaterialLocalizations (package:flutter/src/material/debug.dart:127:4)\n#5      showDialog (package:flutter/src/material/dialog.dart:701:10)\n\n\nAnalyzing weekplanner...                                                \nNo issues found! (ran in 6.1s; 157 public members lack documentation)\n\n\n[✓] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale da-DK)\n    • Flutter version 1.2.1 at /Users/Tricky/development/flutter\n    • Framework revision 8661d8aecd (8 weeks ago), 2019-02-14 19:19:53 -0800\n    • Engine revision 3757390fa4\n    • Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n\n[!] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at /Users/Tricky/Library/Android/sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    ✗ Android licenses not accepted.  To resolve this, run: flutter doctor --android-licenses\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 10.2)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 10.2, Build version 10E125\n    • ios-deploy 1.9.4\n    • CocoaPods version 1.6.1\n\n[✓] Android Studio (version 3.3)\n    • Android Studio at /Applications/Android Studio.app/Contents\n    • Flutter plugin version 33.3.1\n    • Dart plugin version 182.5215\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[✓] IntelliJ IDEA Community Edition (version 2018.3.5)\n    • IntelliJ at /Applications/IntelliJ IDEA CE.app\n    • Flutter plugin version 33.3.2\n    • Dart plugin version 183.5912.23\n\n[!] VS Code (version 1.29.1)\n    • VS Code at /Applications/Visual Studio Code.app/Contents\n    ✗ Flutter extension not installed; install from\n      https://marketplace.visualstudio.com/items?itemName=Dart-Code.flutter\n\n[!] Connected device\n    ! No devices available\n\n! Doctor found issues in 3 categories.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "68",
      "number": "31049",
      "pretext": "Steps to Reproduce\n\nHave a method open an AlertDialog\nHave a button execute the method\nIn a test, use await tester.tap(find.byKey(const Key('LoginBtnKey'))); to tap the button\nThen verify that the dialog is shown using expect(find.byType(AlertDialog), findsOneWidget);\nThis gives a framework error.\n\n\nLogs\n\n══╡ EXCEPTION CAUGHT BY GESTURE ╞═══════════════════════════════════════════════════════════════════\nThe following assertion was thrown while handling a gesture:\n'package:flutter/src/widgets/localizations.dart': Failed assertion: line 446 pos 12: 'context !=\nnull': is not true.\n\nEither the assertion indicates an error in the framework itself, or we should provide substantially\nmore information in this error message to help you determine and fix the underlying cause.\nIn either case, please report this assertion by filing a bug on GitHub:\n  https://github.com/flutter/flutter/issues/new?template=BUG.md\n\nWhen the exception was thrown, this was the stack:\n#2      Localizations.of (package:flutter/src/widgets/localizations.dart:446:12)\n#3      debugCheckHasMaterialLocalizations.<anonymous closure> (package:flutter/src/material/debug.dart:88:23)\n#4      debugCheckHasMaterialLocalizations (package:flutter/src/material/debug.dart:127:4)\n#5      showDialog (package:flutter/src/material/dialog.dart:701:10)\n\n\nAnalyzing weekplanner...                                                \nNo issues found! (ran in 6.1s; 157 public members lack documentation)\n\n\n[✓] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale da-DK)\n    • Flutter version 1.2.1 at /Users/Tricky/development/flutter\n    • Framework revision 8661d8aecd (8 weeks ago), 2019-02-14 19:19:53 -0800\n    • Engine revision 3757390fa4\n    • Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n\n[!] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n    • Android SDK at /Users/Tricky/Library/Android/sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n    ✗ Android licenses not accepted.  To resolve this, run: flutter doctor --android-licenses\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 10.2)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 10.2, Build version 10E125\n    • ios-deploy 1.9.4\n    • CocoaPods version 1.6.1\n\n[✓] Android Studio (version 3.3)\n    • Android Studio at /Applications/Android Studio.app/Contents\n    • Flutter plugin version 33.3.1\n    • Dart plugin version 182.5215\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1248-b01)\n\n[✓] IntelliJ IDEA Community Edition (version 2018.3.5)\n    • IntelliJ at /Applications/IntelliJ IDEA CE.app\n    • Flutter plugin version 33.3.2\n    • Dart plugin version 183.5912.23\n\n[!] VS Code (version 1.29.1)\n    • VS Code at /Applications/Visual Studio Code.app/Contents\n    ✗ Flutter extension not installed; install from\n      https://marketplace.visualstudio.com/items?itemName=Dart-Code.flutter\n\n[!] Connected device\n    ! No devices available\n\n! Doctor found issues in 3 categories.",
      "title": "Testing buttons using tester.tap to show a dialog gives Framework Error"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2749,
    "text": "firebase_admob ^0.8.0+4 Android crashes on startup (Release version only)Steps to Reproduce\n\nSo I'm having issues getting the flutter firebase_admob plugin to work on Android (Release). It initializes and loads ads perfectly for the debug build on Android and the debug/release builds on iOS. I've already made the necessary changes to the AndroidManifest.xml file and included my admob app id (See screenshot). I'm really lost at this point. I've looked at all of the available solutions for the issue I'm having and none have worked. Please help!\nLogs\n\n\n\n\n\n\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:30:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:35:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:41:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:47:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:58:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:65:3 • sdk_version_async_exported_from_core\ninfo • This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final:\nDeleteProgressDialog.progress, DeleteProgressDialog.timeRemaining • lib/ui/DeleteProgressDialog.dart:5:7 • must_be_immutable\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:22:1 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:76:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:130:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:138:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:146:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:245:3 • sdk_version_async_exported_from_core\ninfo • This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final: UserInfoHeader.user •\nlib/ui/UserInfoHeader.dart:5:7 • must_be_immutable\n\n\n[✓] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale en-US)\n• Flutter version 1.2.1 at /Users/xorrior/flutter\n• Framework revision 8661d8a (2 months ago), 2019-02-14 19:19:53 -0800\n• Engine revision 3757390\n• Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n• Android SDK at /Users/xorrior/Library/Android/sdk\n• Android NDK location not configured (optional; useful for native profiling support)\n• Platform android-28, build-tools 28.0.3\n• Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n• Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)\n• All Android licenses accepted.\n[✓] iOS toolchain - develop for iOS devices (Xcode 10.2.1)\n• Xcode at /Applications/Xcode.app/Contents/Developer\n• Xcode 10.2.1, Build version 10E1001\n• ios-deploy 1.9.4\n• CocoaPods version 1.5.3\n[✓] Android Studio (version 3.4)\n• Android Studio at /Applications/Android Studio.app/Contents\n• Flutter plugin version 34.0.2\n• Dart plugin version 183.5901\n• Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)\n[✓] IntelliJ IDEA Ultimate Edition (version 2019.1.1)\n• IntelliJ at /Applications/IntelliJ IDEA.app\n• Flutter plugin version 34.0.4\n• Dart plugin version 191.7019\n[✓] VS Code (version 1.33.1)\n• VS Code at /Applications/Visual Studio Code.app/Contents\n• Flutter extension version 2.25.1\n[✓] Connected device (1 available)\n• Pixel 3 XL • 93VY18Y3R • android-arm64 • Android 9 (API 28)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "69",
      "number": "31617",
      "pretext": "Steps to Reproduce\n\nSo I'm having issues getting the flutter firebase_admob plugin to work on Android (Release). It initializes and loads ads perfectly for the debug build on Android and the debug/release builds on iOS. I've already made the necessary changes to the AndroidManifest.xml file and included my admob app id (See screenshot). I'm really lost at this point. I've looked at all of the available solutions for the issue I'm having and none have worked. Please help!\nLogs\n\n\n\n\n\n\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:30:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:35:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:41:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:47:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:58:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/TwitterApi.dart:65:3 • sdk_version_async_exported_from_core\ninfo • This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final:\nDeleteProgressDialog.progress, DeleteProgressDialog.timeRemaining • lib/ui/DeleteProgressDialog.dart:5:7 • must_be_immutable\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:22:1 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:76:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:130:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:138:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:146:3 • sdk_version_async_exported_from_core\ninfo • The class 'Future' was not exported from 'dart:core' until version 2.1, but this code is required to be able to run on earlier versions •\nlib/ui/Login.dart:245:3 • sdk_version_async_exported_from_core\ninfo • This class (or a class which this class inherits from) is marked as '@immutable', but one or more of its instance fields are not final: UserInfoHeader.user •\nlib/ui/UserInfoHeader.dart:5:7 • must_be_immutable\n\n\n[✓] Flutter (Channel stable, v1.2.1, on Mac OS X 10.14.4 18E226, locale en-US)\n• Flutter version 1.2.1 at /Users/xorrior/flutter\n• Framework revision 8661d8a (2 months ago), 2019-02-14 19:19:53 -0800\n• Engine revision 3757390\n• Dart version 2.1.2 (build 2.1.2-dev.0.0 0a7dcf17eb)\n[✓] Android toolchain - develop for Android devices (Android SDK version 28.0.3)\n• Android SDK at /Users/xorrior/Library/Android/sdk\n• Android NDK location not configured (optional; useful for native profiling support)\n• Platform android-28, build-tools 28.0.3\n• Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n• Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)\n• All Android licenses accepted.\n[✓] iOS toolchain - develop for iOS devices (Xcode 10.2.1)\n• Xcode at /Applications/Xcode.app/Contents/Developer\n• Xcode 10.2.1, Build version 10E1001\n• ios-deploy 1.9.4\n• CocoaPods version 1.5.3\n[✓] Android Studio (version 3.4)\n• Android Studio at /Applications/Android Studio.app/Contents\n• Flutter plugin version 34.0.2\n• Dart plugin version 183.5901\n• Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1343-b01)\n[✓] IntelliJ IDEA Ultimate Edition (version 2019.1.1)\n• IntelliJ at /Applications/IntelliJ IDEA.app\n• Flutter plugin version 34.0.4\n• Dart plugin version 191.7019\n[✓] VS Code (version 1.33.1)\n• VS Code at /Applications/Visual Studio Code.app/Contents\n• Flutter extension version 2.25.1\n[✓] Connected device (1 available)\n• Pixel 3 XL • 93VY18Y3R • android-arm64 • Android 9 (API 28)",
      "title": "firebase_admob ^0.8.0+4 Android crashes on startup (Release version only)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2750,
    "text": "Hot reloading a page with streambuilder bugs outI have a Streambuilder listening to a behaviourSubject broadcastStream, but whenever I Hot Reload or Hot Restart the code for any small change, The streambuilder stops seing what's on the tip of the stream, forcing me to re-do what I originally did to fill the stream",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "70",
      "number": "21068",
      "pretext": "I have a Streambuilder listening to a behaviourSubject broadcastStream, but whenever I Hot Reload or Hot Restart the code for any small change, The streambuilder stops seing what's on the tip of the stream, forcing me to re-do what I originally did to fill the stream",
      "title": "Hot reloading a page with streambuilder bugs out"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2751,
    "text": "`flutter start` doesn't give good error messages when it can't find main.dartWhen I try to flutter start any of the example programs widgets, fitness or rendering I get an error like the following:\nDownloading Sky Snapshot from the cloud, one moment please...\nDownloading Sky Shell from the cloud, one moment please...\n[1108/213053:FATAL:loader.cc(21)] Check failed: base::ReadFileToString(path, &source). /home/dmta/flutter/flutter/examples/rendering/./lib/main.dart\nThe other examples I have tried worked fine.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "71",
      "number": "53",
      "pretext": "When I try to flutter start any of the example programs widgets, fitness or rendering I get an error like the following:\nDownloading Sky Snapshot from the cloud, one moment please...\nDownloading Sky Shell from the cloud, one moment please...\n[1108/213053:FATAL:loader.cc(21)] Check failed: base::ReadFileToString(path, &source). /home/dmta/flutter/flutter/examples/rendering/./lib/main.dart\nThe other examples I have tried worked fine.",
      "title": "`flutter start` doesn't give good error messages when it can't find main.dart"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2752,
    "text": "ARM SupportNot exactly an issue but a question.\nIm working for a small dev company and we're talking about using flutter in production.\nThere is no deadline in the next 6 months.\nWill it be an arm version by then?\nAnd as a secondary question, will it be stable enough?\nThank you",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "72",
      "number": "2844",
      "pretext": "Not exactly an issue but a question.\nIm working for a small dev company and we're talking about using flutter in production.\nThere is no deadline in the next 6 months.\nWill it be an arm version by then?\nAnd as a secondary question, will it be stable enough?\nThank you",
      "title": "ARM Support"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2753,
    "text": "Issue on new instructions for integrating Flutter into existing iOS project. Steps to Reproduce\n\nThe previous version worked fine. However after  I have followed the new instructions(updated yesterday) for integrating Flutter into existing iOS project. This error came up when I push a FlutterViewController\nLogs\n\n[VERBOSE-2:dart_error.cc(16)] Unhandled exception:\nMissingPluginException(No implementation found for method getAll on channel plugins.flutter.io/package_info)\n#0      MethodChannel.invokeMethod (package:flutter/src/services/platform_channel.dart:278:7)\n<asynchronous suspension>\n#1      PackageInfo.fromPlatform (package:package_info/package_info.dart:35:17)\n<asynchronous suspension>\n#2      main (file:///Users/jlin/Workspace/iOS-Univadis/Univadis/Flutter/flunivadis/lib/main.dart:8:53)\n<asynchronous suspension>\n#3      _startIsolate.<anonymous closure> (dart:isolate/runtime/libisolate_patch.dart:279:19)\n#4      _RawReceivePortImpl._handleMessage (dart:isolate/runtime/libisolate_patch.dart:165:12)\n\n\n➜  Flutter git:(poc/integrate_flutter) ✗ flutter doctor -v\n[✓] Flutter (Channel dev, v0.5.7, on Mac OS X 10.13.6 17G65, locale en-FR)\n    • Flutter version 0.5.7 at /Users/jlin/lib/flutter\n    • Framework revision 66091f9696 (6 weeks ago), 2018-07-09 12:52:41 -0700\n    • Engine revision 6fe748490d\n    • Dart version 2.0.0-dev.63.0.flutter-4c9689c1d2\n\n[✓] Android toolchain - develop for Android devices (Android SDK 28.0.2)\n    • Android SDK at /Users/jlin/Library/Android/sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.2\n    • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1024-b01)\n    • All Android licenses accepted.\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 9.4.1)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 9.4.1, Build version 9F2000\n    • ios-deploy 1.9.2\n    • CocoaPods version 1.5.3\n\n[✓] Android Studio (version 3.1)\n    • Android Studio at /Applications/Android Studio.app/Contents\n    • Flutter plugin version 27.1.1\n    • Dart plugin version 173.4700\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1024-b01)\n\n[✓] Connected devices (2 available)\n    • iPhone 8   • 5BC11302-634E-4D3A-ABE5-C52EA137DEBC • ios • iOS 11.4 (simulator)\n    • iPad Air 2 • 2E079443-DD9E-4803-894D-596BB7F7B597 • ios • iOS 11.4 (simulator)\n\n• No issues found!",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "73",
      "number": "20912",
      "pretext": "Steps to Reproduce\n\nThe previous version worked fine. However after  I have followed the new instructions(updated yesterday) for integrating Flutter into existing iOS project. This error came up when I push a FlutterViewController\nLogs\n\n[VERBOSE-2:dart_error.cc(16)] Unhandled exception:\nMissingPluginException(No implementation found for method getAll on channel plugins.flutter.io/package_info)\n#0      MethodChannel.invokeMethod (package:flutter/src/services/platform_channel.dart:278:7)\n<asynchronous suspension>\n#1      PackageInfo.fromPlatform (package:package_info/package_info.dart:35:17)\n<asynchronous suspension>\n#2      main (file:///Users/jlin/Workspace/iOS-Univadis/Univadis/Flutter/flunivadis/lib/main.dart:8:53)\n<asynchronous suspension>\n#3      _startIsolate.<anonymous closure> (dart:isolate/runtime/libisolate_patch.dart:279:19)\n#4      _RawReceivePortImpl._handleMessage (dart:isolate/runtime/libisolate_patch.dart:165:12)\n\n\n➜  Flutter git:(poc/integrate_flutter) ✗ flutter doctor -v\n[✓] Flutter (Channel dev, v0.5.7, on Mac OS X 10.13.6 17G65, locale en-FR)\n    • Flutter version 0.5.7 at /Users/jlin/lib/flutter\n    • Framework revision 66091f9696 (6 weeks ago), 2018-07-09 12:52:41 -0700\n    • Engine revision 6fe748490d\n    • Dart version 2.0.0-dev.63.0.flutter-4c9689c1d2\n\n[✓] Android toolchain - develop for Android devices (Android SDK 28.0.2)\n    • Android SDK at /Users/jlin/Library/Android/sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.2\n    • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1024-b01)\n    • All Android licenses accepted.\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 9.4.1)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 9.4.1, Build version 9F2000\n    • ios-deploy 1.9.2\n    • CocoaPods version 1.5.3\n\n[✓] Android Studio (version 3.1)\n    • Android Studio at /Applications/Android Studio.app/Contents\n    • Flutter plugin version 27.1.1\n    • Dart plugin version 173.4700\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1024-b01)\n\n[✓] Connected devices (2 available)\n    • iPhone 8   • 5BC11302-634E-4D3A-ABE5-C52EA137DEBC • ios • iOS 11.4 (simulator)\n    • iPad Air 2 • 2E079443-DD9E-4803-894D-596BB7F7B597 • ios • iOS 11.4 (simulator)\n\n• No issues found!",
      "title": "Issue on new instructions for integrating Flutter into existing iOS project. "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2754,
    "text": "Update DevFS to use a WatcherInstead of scanning the project directory on hot reload, setup a Directory watcher to report updated dart files. When hot reloading, grab the current set and pass them to the resident compiler.\nSince we no longer need to sync source files to the device, the extra work we're doing to setup the devfs for each potential entry is wasted. We only need to sync the dill file and assets.\nAdditionally, including build_runner will drastically increase the number of files that will be scanned with the current approach - while there is no issue with a watcher based approach.",
    "annotations": [{ "label": 143, "user": 1 }],
    "meta": {
      "": "74",
      "number": "27446",
      "pretext": "Instead of scanning the project directory on hot reload, setup a Directory watcher to report updated dart files. When hot reloading, grab the current set and pass them to the resident compiler.\nSince we no longer need to sync source files to the device, the extra work we're doing to setup the devfs for each potential entry is wasted. We only need to sync the dill file and assets.\nAdditionally, including build_runner will drastically increase the number of files that will be scanned with the current approach - while there is no issue with a watcher based approach.",
      "title": "Update DevFS to use a Watcher"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2755,
    "text": "flutter tool provide hint when/how to install iOS simulatorSteps to Reproduce\n\nClean install (no iOS simulator)\nflutter create test-ios\nflutter run\n\nAt this point it responds with No connected devices. which is correct but unhelpful. I would like it to hint that I could install the iOS simulator and point to docs explaining how to do so. Maybe flutter doctor should indicate that it is not installed / configured as well.\nFlutter Doctor\n$ flutter doctor\n[✓] Flutter (on Mac OS, channel master)\n    • Flutter at /Users/danrubel/work/git/flutter/flutter\n    • Framework revision 83bf5d10c0 (21 hours ago), 2016-08-30 14:11:54\n    • Engine revision c4022b61fa\n    • Tools Dart version 1.19.0-dev.5.0\n\n[✓] Android toolchain - develop for Android devices (Android SDK 22.0.1)\n    • Android SDK at /Users/danrubel/Library/Android/sdk\n    • Platform android-23, build-tools 22.0.1\n    • Java(TM) SE Runtime Environment (build 1.8.0_91-b14)\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 7.3.1)\n    • XCode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 7.3.1, Build version 7D1014\n\n[✓] Atom - a lightweight development environment for Flutter\n    • flutter plugin version 0.2.4\n    • dartlang plugin version 0.6.35",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "75",
      "number": "5674",
      "pretext": "Steps to Reproduce\n\nClean install (no iOS simulator)\nflutter create test-ios\nflutter run\n\nAt this point it responds with No connected devices. which is correct but unhelpful. I would like it to hint that I could install the iOS simulator and point to docs explaining how to do so. Maybe flutter doctor should indicate that it is not installed / configured as well.\nFlutter Doctor\n$ flutter doctor\n[✓] Flutter (on Mac OS, channel master)\n    • Flutter at /Users/danrubel/work/git/flutter/flutter\n    • Framework revision 83bf5d10c0 (21 hours ago), 2016-08-30 14:11:54\n    • Engine revision c4022b61fa\n    • Tools Dart version 1.19.0-dev.5.0\n\n[✓] Android toolchain - develop for Android devices (Android SDK 22.0.1)\n    • Android SDK at /Users/danrubel/Library/Android/sdk\n    • Platform android-23, build-tools 22.0.1\n    • Java(TM) SE Runtime Environment (build 1.8.0_91-b14)\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 7.3.1)\n    • XCode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 7.3.1, Build version 7D1014\n\n[✓] Atom - a lightweight development environment for Flutter\n    • flutter plugin version 0.2.4\n    • dartlang plugin version 0.6.35",
      "title": "flutter tool provide hint when/how to install iOS simulator"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2756,
    "text": "RenderObject exceptions in performLayout should also include information about what the child(ren) areWould help debug bugs like #1210 .",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "76",
      "number": "1211",
      "pretext": "Would help debug bugs like #1210 .",
      "title": "RenderObject exceptions in performLayout should also include information about what the child(ren) are"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2757,
    "text": "TextAlign RTLWe probably need to create an RTL-aware version of TextAlign that is used by TextPainter, then use that in RenderParagraph, RichText, and Text so that you can align start/end as well as left/right. Also \"justify\" needs to define the justification of the last line.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "77",
      "number": "11380",
      "pretext": "We probably need to create an RTL-aware version of TextAlign that is used by TextPainter, then use that in RenderParagraph, RichText, and Text so that you can align start/end as well as left/right. Also \"justify\" needs to define the justification of the last line.",
      "title": "TextAlign RTL"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2758,
    "text": "a11y for password text fieldsLook at what the behavior on native is (e.g. is content read out?) and ensure that flutter does the same.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "78",
      "number": "15274",
      "pretext": "Look at what the behavior on native is (e.g. is content read out?) and ensure that flutter does the same.",
      "title": "a11y for password text fields"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2759,
    "text": "Android overscroll indicator is causing a reload/repaintSteps to Reproduce\n\nUse Block to render a scrollable page.\nThe first time top or bottom is reached, observe that the content is reloaded.\nSecond and subsequent times the overscroll indicator is shown correctly.\n\nProblem does not occur if we manually switch off the overscroll indicator (or switch to the iOS bounce indicator).\nFlutter Doctor\n[✓] Flutter (on Linux, channel alpha)\n• Framework revision 9c0c022 (3 weeks ago), engine revision bb98655\n[✓] Android toolchain - develop for Android devices (Android SDK 23.0.2)\n• Platform android-23, build-tools 23.0.2\n• OpenJDK Runtime Environment (build 1.8.0-google-v7-123992248-123972143)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "79",
      "number": "4944",
      "pretext": "Steps to Reproduce\n\nUse Block to render a scrollable page.\nThe first time top or bottom is reached, observe that the content is reloaded.\nSecond and subsequent times the overscroll indicator is shown correctly.\n\nProblem does not occur if we manually switch off the overscroll indicator (or switch to the iOS bounce indicator).\nFlutter Doctor\n[✓] Flutter (on Linux, channel alpha)\n• Framework revision 9c0c022 (3 weeks ago), engine revision bb98655\n[✓] Android toolchain - develop for Android devices (Android SDK 23.0.2)\n• Platform android-23, build-tools 23.0.2\n• OpenJDK Runtime Environment (build 1.8.0-google-v7-123992248-123972143)",
      "title": "Android overscroll indicator is causing a reload/repaint"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2760,
    "text": "Support keyboard type on multiline text inputs (Android)There's keyboard types for single-line inputs only.\nMultiline text inputs need return key.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "80",
      "number": "6979",
      "pretext": "There's keyboard types for single-line inputs only.\nMultiline text inputs need return key.",
      "title": "Support keyboard type on multiline text inputs (Android)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2761,
    "text": "App does not work after flutter upgrade.I just tried to upgrade flutter to 0.2.4\nThe engine upgrade worked but post upgrade flutter doctor reports errors and my app also does not work anymore.\n# App execution error::\n\nLaunching lib/main.dart on iPhone X in debug mode...\n[VERBOSE-2:dart_error.cc(16)] error: Unsupported tag at this point: 0.\n[VERBOSE-2:dart_error.cc(16)] Dart_GetClosure expects argument 'library' to be non-null.\n\n\n# flutter doctor error ::\nRunning flutter doctor...\nDoctor summary (to see all details, run flutter doctor -v):\n[✓] Flutter (Channel dev, v0.2.4, on Mac OS X 10.13.3 17D102, locale en-US)\n\nOops; flutter has exited unexpectedly.\nSending crash report to Google.\nCrash report sent (report ID: 2f792ac750627b94)\nUnhandled exception:\nNoSuchMethodError: The method 'run' was called on null.\nReceiver: null\nTried calling: run(Instance(length:2) of '_GrowableList', environment: null, workingDirectory: null)\n#0      Object.noSuchMethod (dart:core-patch/dart:core/object_patch.dart:46)\n#1      runAsync (package:flutter_tools/src/base/process.dart:227)\n<asynchronous suspension>\n#2      IOSWorkflow.macDevMode (package:flutter_tools/src/ios/ios_workflow.dart:45)\n<asynchronous suspension>\n#3      IOSWorkflow.validate (package:flutter_tools/src/ios/ios_workflow.dart:97)\n<asynchronous suspension>\n#4      Doctor.startValidatorTasks (package:flutter_tools/src/doctor.dart:71)\n#5      Doctor.diagnose (package:flutter_tools/src/doctor.dart:128)\n<asynchronous suspension>\n#6      _doctorText.<anonymous closure> (package:flutter_tools/runner.dart:237)\n#7      AppContext._run (package:flutter_tools/src/base/context.dart:76)\n<asynchronous suspension>\n#8      AppContext.runInZone.<anonymous closure> (package:flutter_tools/src/base/context.dart:66)\n#9      _rootRun (dart:async/zone.dart:1126)\n#10     _CustomZone.run (dart:async/zone.dart:1023)\n#11     runZoned (dart:async/zone.dart:1501)\n#12     AppContext.runInZone (package:flutter_tools/src/base/context.dart:65)\n#13     _doctorText (package:flutter_tools/runner.dart:237)\n<asynchronous suspension>\n#14     _createLocalCrashReport (package:flutter_tools/runner.dart:212)\n<asynchronous suspension>\n#15     _handleToolError (package:flutter_tools/runner.dart:167)\n<asynchronous suspension>\n#16     run.<anonymous closure> (package:flutter_tools/runner.dart:94)\n<asynchronous suspension>\n#17     AppContext._run (package:flutter_tools/src/base/context.dart:76)\n<asynchronous suspension>\n#18     AppContext.runInZone.<anonymous closure> (package:flutter_tools/src/base/context.dart:66)\n#19     _rootRun (dart:async/zone.dart:1126)\n#20     _CustomZone.run (dart:async/zone.dart:1023)\n#21     runZoned (dart:async/zone.dart:1501)\n#22     AppContext.runInZone (package:flutter_tools/src/base/context.dart:65)\n#23     run (package:flutter_tools/runner.dart:61)\n<asynchronous suspension>\n#24     main (package:flutter_tools/executable.dart:48)\n<asynchronous suspension>\n#25     main (file:///Users/MBahl/flutter/flutter/packages/flutter_tools/bin/flutter_tools.dart:16)\n#26     _startIsolate.<anonymous closure> (dart:isolate-patch/dart:isolate/isolate_patch.dart:277)\n#27     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:165)",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "81",
      "number": "15943",
      "pretext": "I just tried to upgrade flutter to 0.2.4\nThe engine upgrade worked but post upgrade flutter doctor reports errors and my app also does not work anymore.\n# App execution error::\n\nLaunching lib/main.dart on iPhone X in debug mode...\n[VERBOSE-2:dart_error.cc(16)] error: Unsupported tag at this point: 0.\n[VERBOSE-2:dart_error.cc(16)] Dart_GetClosure expects argument 'library' to be non-null.\n\n\n# flutter doctor error ::\nRunning flutter doctor...\nDoctor summary (to see all details, run flutter doctor -v):\n[✓] Flutter (Channel dev, v0.2.4, on Mac OS X 10.13.3 17D102, locale en-US)\n\nOops; flutter has exited unexpectedly.\nSending crash report to Google.\nCrash report sent (report ID: 2f792ac750627b94)\nUnhandled exception:\nNoSuchMethodError: The method 'run' was called on null.\nReceiver: null\nTried calling: run(Instance(length:2) of '_GrowableList', environment: null, workingDirectory: null)\n#0      Object.noSuchMethod (dart:core-patch/dart:core/object_patch.dart:46)\n#1      runAsync (package:flutter_tools/src/base/process.dart:227)\n<asynchronous suspension>\n#2      IOSWorkflow.macDevMode (package:flutter_tools/src/ios/ios_workflow.dart:45)\n<asynchronous suspension>\n#3      IOSWorkflow.validate (package:flutter_tools/src/ios/ios_workflow.dart:97)\n<asynchronous suspension>\n#4      Doctor.startValidatorTasks (package:flutter_tools/src/doctor.dart:71)\n#5      Doctor.diagnose (package:flutter_tools/src/doctor.dart:128)\n<asynchronous suspension>\n#6      _doctorText.<anonymous closure> (package:flutter_tools/runner.dart:237)\n#7      AppContext._run (package:flutter_tools/src/base/context.dart:76)\n<asynchronous suspension>\n#8      AppContext.runInZone.<anonymous closure> (package:flutter_tools/src/base/context.dart:66)\n#9      _rootRun (dart:async/zone.dart:1126)\n#10     _CustomZone.run (dart:async/zone.dart:1023)\n#11     runZoned (dart:async/zone.dart:1501)\n#12     AppContext.runInZone (package:flutter_tools/src/base/context.dart:65)\n#13     _doctorText (package:flutter_tools/runner.dart:237)\n<asynchronous suspension>\n#14     _createLocalCrashReport (package:flutter_tools/runner.dart:212)\n<asynchronous suspension>\n#15     _handleToolError (package:flutter_tools/runner.dart:167)\n<asynchronous suspension>\n#16     run.<anonymous closure> (package:flutter_tools/runner.dart:94)\n<asynchronous suspension>\n#17     AppContext._run (package:flutter_tools/src/base/context.dart:76)\n<asynchronous suspension>\n#18     AppContext.runInZone.<anonymous closure> (package:flutter_tools/src/base/context.dart:66)\n#19     _rootRun (dart:async/zone.dart:1126)\n#20     _CustomZone.run (dart:async/zone.dart:1023)\n#21     runZoned (dart:async/zone.dart:1501)\n#22     AppContext.runInZone (package:flutter_tools/src/base/context.dart:65)\n#23     run (package:flutter_tools/runner.dart:61)\n<asynchronous suspension>\n#24     main (package:flutter_tools/executable.dart:48)\n<asynchronous suspension>\n#25     main (file:///Users/MBahl/flutter/flutter/packages/flutter_tools/bin/flutter_tools.dart:16)\n#26     _startIsolate.<anonymous closure> (dart:isolate-patch/dart:isolate/isolate_patch.dart:277)\n#27     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:165)",
      "title": "App does not work after flutter upgrade."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2762,
    "text": "`--debug-port 0` on iOS is not \"magical\"Assuming this means the default port.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "82",
      "number": "1799",
      "pretext": "Assuming this means the default port.",
      "title": "`--debug-port 0` on iOS is not \"magical\""
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2763,
    "text": "plugins/video_player: java.lang.NullPointerException in gradleThere is NullPointerException during executing gradle tasks from the android directory of the project which has dependency to video_player plugin:\njava.lang.NullPointerException: (No message provided)Close stacktrace\nat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:782)\nat com.google.common.base.Splitter.split(Splitter.java:376)\nat com.android.utils.PathUtils.getClassPathItems(PathUtils.java:84)\n\nGradle version: 5.1.1\nFull gradle scan: https://scans.gradle.com/s/4uiq56gz6bv7u\n$ flutter doctor\nDoctor summary (to see all details, run flutter doctor -v):\n[✓] Flutter (Channel beta, v1.0.0, on Mac OS X 10.14.2 18C54, locale ru-RU)\n[✓] Android toolchain - develop for Android devices (Android SDK 28.0.3)\n[✓] iOS toolchain - develop for iOS devices (Xcode 10.1)\n[✓] Android Studio (version 3.3)\n[✓] IntelliJ IDEA Ultimate Edition (version 2018.3)\n\nUpdate:\nSeems that this happens when two dependencies are together:\n  flutter_udid: ^0.0.3\n  video_player: ^0.10.0\nSteps to reproduce\n\nflutter create test_project\nadd next dependencies to pubspec.yaml:\n\nflutter_udid: ^0.0.3\nvideo_player: ^0.10.0\n\ncd android\n./gradlew tasks",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "83",
      "number": "27124",
      "pretext": "There is NullPointerException during executing gradle tasks from the android directory of the project which has dependency to video_player plugin:\njava.lang.NullPointerException: (No message provided)Close stacktrace\nat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:782)\nat com.google.common.base.Splitter.split(Splitter.java:376)\nat com.android.utils.PathUtils.getClassPathItems(PathUtils.java:84)\n\nGradle version: 5.1.1\nFull gradle scan: https://scans.gradle.com/s/4uiq56gz6bv7u\n$ flutter doctor\nDoctor summary (to see all details, run flutter doctor -v):\n[✓] Flutter (Channel beta, v1.0.0, on Mac OS X 10.14.2 18C54, locale ru-RU)\n[✓] Android toolchain - develop for Android devices (Android SDK 28.0.3)\n[✓] iOS toolchain - develop for iOS devices (Xcode 10.1)\n[✓] Android Studio (version 3.3)\n[✓] IntelliJ IDEA Ultimate Edition (version 2018.3)\n\nUpdate:\nSeems that this happens when two dependencies are together:\n  flutter_udid: ^0.0.3\n  video_player: ^0.10.0\nSteps to reproduce\n\nflutter create test_project\nadd next dependencies to pubspec.yaml:\n\nflutter_udid: ^0.0.3\nvideo_player: ^0.10.0\n\ncd android\n./gradlew tasks",
      "title": "plugins/video_player: java.lang.NullPointerException in gradle"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2764,
    "text": "[image_picker] Camera permission not neededHi,\nsince the plugin is not using the camera, just requesting the intent, the CAMERA permission is not needed. Here the tutorial on how to use the camera intent from the official docs: https://developer.android.com/training/camera/photobasics\nThanks!",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "84",
      "number": "19806",
      "pretext": "Hi,\nsince the plugin is not using the camera, just requesting the intent, the CAMERA permission is not needed. Here the tutorial on how to use the camera intent from the official docs: https://developer.android.com/training/camera/photobasics\nThanks!",
      "title": "[image_picker] Camera permission not needed"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2765,
    "text": "Teach flutter test to build_runnerExperiment with build_runner and modular kernel as a pre-compilation step (swappable with the current frontend_server approach).\nOpen questions:\n\n\nHow do compilation strategies compare on the current benchmarks for small apps? For large ones? Namely: 1st run, repeated run, repeated run with changes, et cetera. See the current test benchmarks.\n\n\nDoes build_runner give us a better path forward for test integration into other tools?\n\n\nDoes this allow us to better unify web and native tests?\n\n\nCan we ship the build_rules in the engine with the current build_runner integration strategy?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "85",
      "number": "31645",
      "pretext": "Experiment with build_runner and modular kernel as a pre-compilation step (swappable with the current frontend_server approach).\nOpen questions:\n\n\nHow do compilation strategies compare on the current benchmarks for small apps? For large ones? Namely: 1st run, repeated run, repeated run with changes, et cetera. See the current test benchmarks.\n\n\nDoes build_runner give us a better path forward for test integration into other tools?\n\n\nDoes this allow us to better unify web and native tests?\n\n\nCan we ship the build_rules in the engine with the current build_runner integration strategy?",
      "title": "Teach flutter test to build_runner"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2766,
    "text": "Android dependency 'com.android.support:recyclerview-v7' has different version for the compile (23.0.1) and runtime (27.1.1) classpath. You should manually set the same version via DependencyResolutionapply plugin: \"com.android.application\"\n\nimport com.android.build.OutputFile\n\ndef enableProguardInReleaseBuilds = false\n\nandroid {\n    compileSdkVersion 27\n    buildToolsVersion '27.0.3'\n\n    defaultConfig {\n        applicationId \"com.destressambulance\"\n        minSdkVersion 16\n        targetSdkVersion 27\n        versionCode 1\n        versionName \"1.0\"\n        ndk {\n            abiFilters \"armeabi-v7a\", \"x86\"\n        }\n    }\n    splits {\n        abi {\n            reset()\n            enable enableSeparateBuildPerCPUArchitecture\n            universalApk false  // If true, also generate a universal APK\n            include \"armeabi-v7a\", \"x86\"\n        }\n    }\n    buildTypes {\n        release {\n            minifyEnabled enableProguardInReleaseBuilds\n            proguardFiles getDefaultProguardFile(\"proguard-android.txt\"), \"proguard-rules.pro\"\n        }\n    }\n    // applicationVariants are e.g. debug, release\n    applicationVariants.all { variant ->\n        variant.outputs.each { output ->\n            // For each separate APK per architecture, set a unique version code as described here:\n            // http://tools.android.com/tech-docs/new-build-system/user-guide/apk-splits\n            def versionCodes = [\"armeabi-v7a\":1, \"x86\":2]\n            def abi = output.getFilter(OutputFile.ABI)\n            if (abi != null) {  // null for the universal-debug, universal-release variants\n                output.versionCodeOverride =\n                        versionCodes.get(abi) * 1048576 + defaultConfig.versionCode\n            }\n        }\n    }\n}\n\ndependencies {\n    implementation project(':react-native-fcm')\n    implementation project(':react-native-customized-image-picker')\n    implementation project(':react-native-google-places')\n    implementation project(':react-native-image-picker')\n    implementation fileTree(dir: \"libs\", include: [\"*.jar\"])\n    //noinspection GradleCompatible\n    implementation \"com.android.support:appcompat-v7:27.1.1\"\n    implementation \"com.facebook.react:react-native:0.20.1\"  // From node_modules\n}\n\n// Run this once to be able to run the application with BUCK\n// puts all compile dependencies into folder libs for BUCK to use\ntask copyDownloadableDepsToLibs(type: Copy) {\n    from configurations.compile\n    into 'libs'\n}",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "86",
      "number": "17902",
      "pretext": "apply plugin: \"com.android.application\"\n\nimport com.android.build.OutputFile\n\ndef enableProguardInReleaseBuilds = false\n\nandroid {\n    compileSdkVersion 27\n    buildToolsVersion '27.0.3'\n\n    defaultConfig {\n        applicationId \"com.destressambulance\"\n        minSdkVersion 16\n        targetSdkVersion 27\n        versionCode 1\n        versionName \"1.0\"\n        ndk {\n            abiFilters \"armeabi-v7a\", \"x86\"\n        }\n    }\n    splits {\n        abi {\n            reset()\n            enable enableSeparateBuildPerCPUArchitecture\n            universalApk false  // If true, also generate a universal APK\n            include \"armeabi-v7a\", \"x86\"\n        }\n    }\n    buildTypes {\n        release {\n            minifyEnabled enableProguardInReleaseBuilds\n            proguardFiles getDefaultProguardFile(\"proguard-android.txt\"), \"proguard-rules.pro\"\n        }\n    }\n    // applicationVariants are e.g. debug, release\n    applicationVariants.all { variant ->\n        variant.outputs.each { output ->\n            // For each separate APK per architecture, set a unique version code as described here:\n            // http://tools.android.com/tech-docs/new-build-system/user-guide/apk-splits\n            def versionCodes = [\"armeabi-v7a\":1, \"x86\":2]\n            def abi = output.getFilter(OutputFile.ABI)\n            if (abi != null) {  // null for the universal-debug, universal-release variants\n                output.versionCodeOverride =\n                        versionCodes.get(abi) * 1048576 + defaultConfig.versionCode\n            }\n        }\n    }\n}\n\ndependencies {\n    implementation project(':react-native-fcm')\n    implementation project(':react-native-customized-image-picker')\n    implementation project(':react-native-google-places')\n    implementation project(':react-native-image-picker')\n    implementation fileTree(dir: \"libs\", include: [\"*.jar\"])\n    //noinspection GradleCompatible\n    implementation \"com.android.support:appcompat-v7:27.1.1\"\n    implementation \"com.facebook.react:react-native:0.20.1\"  // From node_modules\n}\n\n// Run this once to be able to run the application with BUCK\n// puts all compile dependencies into folder libs for BUCK to use\ntask copyDownloadableDepsToLibs(type: Copy) {\n    from configurations.compile\n    into 'libs'\n}",
      "title": "Android dependency 'com.android.support:recyclerview-v7' has different version for the compile (23.0.1) and runtime (27.1.1) classpath. You should manually set the same version via DependencyResolution"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2767,
    "text": "Attach widget creation source in build/paint profile timeline eventsWould be nice in places like https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/widgets/framework.dart#L3627 and https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/rendering/object.dart#L122 to also point out where the widget was created.\ncc @jacob314 I think we chatted about this but I forgot where.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "87",
      "number": "17161",
      "pretext": "Would be nice in places like https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/widgets/framework.dart#L3627 and https://github.com/flutter/flutter/blob/master/packages/flutter/lib/src/rendering/object.dart#L122 to also point out where the widget was created.\ncc @jacob314 I think we chatted about this but I forgot where.",
      "title": "Attach widget creation source in build/paint profile timeline events"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2768,
    "text": "Plugin: BatteryTracks a simple battery plugin. We will write this as a first-party plugin to serve as a sample of how to write a plugin.\nIt will integrate with to BatteryManager on Android, and UIDevice on iOS",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "88",
      "number": "7817",
      "pretext": "Tracks a simple battery plugin. We will write this as a first-party plugin to serve as a sample of how to write a plugin.\nIt will integrate with to BatteryManager on Android, and UIDevice on iOS",
      "title": "Plugin: Battery"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2769,
    "text": "Document references must have an even number of segmentsI'm suffering horrible error of cloud firestore in whole day.... I don't know what is the problem, I asked for stackoverflow, but there was no answer, how to deal with???\nFuture<String> test() async {\n  CollectionReference col = Firestore.instance.collection('messages');\n  QuerySnapshot querySnapshot = await col.where('begin',isEqualTo: false).getDocuments();\n}\nWhat is wrong with my code??\nError\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): Failed to handle method call\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): java.lang.IllegalArgumentException: Invalid document reference. Document references must have an even number of segments, but messages has 1\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat com.google.firebase.firestore.DocumentReference.zza(com.google.firebase:firebase-firestore@@17.1.1:81)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat com.google.firebase.firestore.FirebaseFirestore.document(com.google.firebase:firebase-firestore@@17.1.1:259)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.plugins.firebase.cloudfirestore.CloudFirestorePlugin.getDocumentReference(CloudFirestorePlugin.java:92)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.plugins.firebase.cloudfirestore.CloudFirestorePlugin.onMethodCall(CloudFirestorePlugin.java:474)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage(MethodChannel.java:200)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.view.FlutterNativeView$PlatformMessageHandlerImpl.handlePlatformMessage(FlutterNativeView.java:188)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage(FlutterJNI.java:202)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat android.os.MessageQueue.nativePollOnce(Native Method)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat android.os.MessageQueue.next(MessageQueue.java:325)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat android.os.Looper.loop(Looper.java:142)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat android.app.ActivityThread.main(ActivityThread.java:6938)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat java.lang.reflect.Method.invoke(Native Method)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:327)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1374)",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "89",
      "number": "27931",
      "pretext": "I'm suffering horrible error of cloud firestore in whole day.... I don't know what is the problem, I asked for stackoverflow, but there was no answer, how to deal with???\nFuture<String> test() async {\n  CollectionReference col = Firestore.instance.collection('messages');\n  QuerySnapshot querySnapshot = await col.where('begin',isEqualTo: false).getDocuments();\n}\nWhat is wrong with my code??\nError\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): Failed to handle method call\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): java.lang.IllegalArgumentException: Invalid document reference. Document references must have an even number of segments, but messages has 1\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat com.google.firebase.firestore.DocumentReference.zza(com.google.firebase:firebase-firestore@@17.1.1:81)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat com.google.firebase.firestore.FirebaseFirestore.document(com.google.firebase:firebase-firestore@@17.1.1:259)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.plugins.firebase.cloudfirestore.CloudFirestorePlugin.getDocumentReference(CloudFirestorePlugin.java:92)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.plugins.firebase.cloudfirestore.CloudFirestorePlugin.onMethodCall(CloudFirestorePlugin.java:474)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage(MethodChannel.java:200)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.view.FlutterNativeView$PlatformMessageHandlerImpl.handlePlatformMessage(FlutterNativeView.java:188)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage(FlutterJNI.java:202)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat android.os.MessageQueue.nativePollOnce(Native Method)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat android.os.MessageQueue.next(MessageQueue.java:325)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat android.os.Looper.loop(Looper.java:142)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat android.app.ActivityThread.main(ActivityThread.java:6938)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat java.lang.reflect.Method.invoke(Native Method)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:327)\nE/MethodChannel#plugins.flutter.io/cloud_firestore(12549): \tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1374)",
      "title": "Document references must have an even number of segments"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2770,
    "text": "setState might not update the widgets in showDrawerSorry if this is sparse on details. I mainly want to determine if there is a bug with showDrawer or with the way I attempted to use it.\nYesterday, I attempted to use showDrawer with the 7be58b1 alpha branch of Flutter. The drawer appeared/dismissed correctly, and the widgets I put in the drawer were correct.\nUnfortunately, I noticed that no matter how I attempted to change the state of what was drawn in the drawer, it would not re-render. In my case, I tried to put a Switch inside a DrawerItem to toggle debug mode for my app. I did confirm that the state did actually change, which led me to think that Flutter has a bug.\nI was following the Stocks demo app quite closely, which uses a Drawer to toggle the user's market sentiment (Optimistic vs Pessimistic). However, I was assuming that the app's behavior would match the Sky Demo app in the Google Play Store. I only just realized that this app is actually 3 months old (last update August 25), so it is very possible that in that time, the Stocks example does not function as it did before.\nI am currently unable to try running these examples (though I have found the instructions), so if someone else can run the Stocks app and determine whether the Optimistic/Pessimistic radio buttons update the drawer menu or not, that would be very helpful.\nOtherwise, I will look into this more after the break.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "90",
      "number": "604",
      "pretext": "Sorry if this is sparse on details. I mainly want to determine if there is a bug with showDrawer or with the way I attempted to use it.\nYesterday, I attempted to use showDrawer with the 7be58b1 alpha branch of Flutter. The drawer appeared/dismissed correctly, and the widgets I put in the drawer were correct.\nUnfortunately, I noticed that no matter how I attempted to change the state of what was drawn in the drawer, it would not re-render. In my case, I tried to put a Switch inside a DrawerItem to toggle debug mode for my app. I did confirm that the state did actually change, which led me to think that Flutter has a bug.\nI was following the Stocks demo app quite closely, which uses a Drawer to toggle the user's market sentiment (Optimistic vs Pessimistic). However, I was assuming that the app's behavior would match the Sky Demo app in the Google Play Store. I only just realized that this app is actually 3 months old (last update August 25), so it is very possible that in that time, the Stocks example does not function as it did before.\nI am currently unable to try running these examples (though I have found the instructions), so if someone else can run the Stocks app and determine whether the Optimistic/Pessimistic radio buttons update the drawer menu or not, that would be very helpful.\nOtherwise, I will look into this more after the break.",
      "title": "setState might not update the widgets in showDrawer"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2771,
    "text": "Installation fails on Android emulators without Google APIs without a clear indication of the causeTried setting up a flutter app on my personal linux laptop, flutter run failed to start the app on this Android 24 emulator:\n\nChanging the architecture to use Google APIs resolved this issue, but it was unclear from the errors I got that that was an appropriate resolution.  Flutter run provided no indication of the error, and I get the following cryptic output from ADB logcat:\n11-06 20:45:43.000  1246  1246 W art     : Unexpected CPU variant for X86 using defaults: x86\nFull ADB output:\nhttp://pastebin.com/CEghLXgj\nNote: this is unrelated to leafy, I'm just trying out the third party workflow.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "91",
      "number": "6728",
      "pretext": "Tried setting up a flutter app on my personal linux laptop, flutter run failed to start the app on this Android 24 emulator:\n\nChanging the architecture to use Google APIs resolved this issue, but it was unclear from the errors I got that that was an appropriate resolution.  Flutter run provided no indication of the error, and I get the following cryptic output from ADB logcat:\n11-06 20:45:43.000  1246  1246 W art     : Unexpected CPU variant for X86 using defaults: x86\nFull ADB output:\nhttp://pastebin.com/CEghLXgj\nNote: this is unrelated to leafy, I'm just trying out the third party workflow.",
      "title": "Installation fails on Android emulators without Google APIs without a clear indication of the cause"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2772,
    "text": "Compiler crash when \"new\" keyword removed from default appSteps to Reproduce\nI accidentally left the \"new\" keyword out of a line of code while working on an app, and when I rebuilt it, the compiler crashed. I then created a brand new project in IntelliJ, and verified the issue still occurred.\nSteps to reproduce:\n\n\nCreate a new IntelliJ Flutter project\n\n\nIntelliJ spits out the \"increment counter\" app.\n\n\nRemove the new keyword from line 67:\n 66: // than having to individually change instances of widgets.\n 67: return new Scaffold(\n 68:   appBar: new AppBar(\n\n\n\nTrigger a hot reload.\n\n\nThe compiler crashes with the message included below.\n\n\nInterestingly, the compiler does not crash if you leave line 67 alone and instead remove the \"new\" keyword from line 68. Line 67 is creating the instance that's used as the return value of the method, if that matters.\nLogs\nPerforming hot reload...\ncompiler message: Unhandled exception:\ncompiler message: Crash when compiling file:///Users/redbrogdon/source/crashtest/lib/main.dart,\ncompiler message: at character offset 2160:\ncompiler message: lib/main.dart: Internal problem: Unhandled this in defaultTreeNode.\ncompiler message: #0      internalProblem (package:front_end/src/fasta/problems.dart:30)\ncompiler message: #1      unhandled (package:front_end/src/fasta/problems.dart:43)\ncompiler message: #2      ConstnessEvaluator.defaultTreeNode (package:front_end/src/fasta/kernel/constness_evaluator.dart:112)\ncompiler message: #3      TreeVisitor.defaultExpression (package:kernel/visitor.dart:140)\ncompiler message: #4      TreeVisitor.visitThisExpression (package:kernel/visitor.dart:171)\ncompiler message: #5      ThisExpression.accept (package:kernel/ast.dart:3341)\ncompiler message: #6      ConstnessEvaluator.visitPropertyGet (package:front_end/src/fasta/kernel/constness_evaluator.dart:287)\ncompiler message: #7      PropertyGet.accept (package:kernel/ast.dart:2242)\ncompiler message: #8      ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)\ncompiler message: #9      ConstructorInvocation.accept (package:kernel/ast.dart:2983)\ncompiler message: #10     ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)\ncompiler message: #11     ConstructorInvocation.accept (package:kernel/ast.dart:2983)\ncompiler message: #12     ConstnessEvaluator.evaluate (package:front_end/src/fasta/kernel/constness_evaluator.dart:99)\ncompiler message: #13     evaluateConstness (package:front_end/src/fasta/kernel/constness_evaluator.dart:466)\ncompiler message: #14     BodyBuilder.inferConstness (package:front_end/src/fasta/kernel/body_builder.dart:710)\ncompiler message: #15     BodyBuilder.finishFunction (package:front_end/src/fasta/kernel/body_builder.dart:697)\ncompiler message: #16     DietListener.listenerFinishFunction (package:front_end/src/fasta/source/diet_listener.dart:684)\ncompiler message: #17     DietListener.parseFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:718)\ncompiler message: #18     DietListener.buildFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:565)\ncompiler message: #19     DietListener.endMethod (package:front_end/src/fasta/source/diet_listener.dart:530)\ncompiler message: #20     Parser.parseMethod (package:front_end/src/fasta/parser/parser.dart:3796)\ncompiler message: #21     Parser.parseClassMemberImpl (package:front_end/src/fasta/parser/parser.dart:3670)\ncompiler message: #22     Parser.parseClassBody (package:front_end/src/fasta/parser/parser.dart:3467)\ncompiler message: #23     Parser.parseClass (package:front_end/src/fasta/parser/parser.dart:1699)\ncompiler message: #24     Parser.parseClassOrNamedMixinApplication (package:front_end/src/fasta/parser/parser.dart:1659)\ncompiler message: #25     Parser.parseTopLevelKeywordDeclaration (package:front_end/src/fasta/parser/parser.dart:535)\ncompiler message: #26     Parser.parseTopLevelDeclarationImpl (package:front_end/src/fasta/parser/parser.dart:451)\ncompiler message: #27     Parser.parseUnit (package:front_end/src/fasta/parser/parser.dart:335)\ncompiler message: #28     SourceLoader.buildBody (package:front_end/src/fasta/source/source_loader.dart:198)\ncompiler message: <asynchronous suspension>\ncompiler message: #29     Loader.buildBodies (package:front_end/src/fasta/loader.dart:157)\ncompiler message: <asynchronous suspension>\ncompiler message: #30     KernelTarget.buildComponent (package:front_end/src/fasta/kernel/kernel_target.dart:292)\ncompiler message: <asynchronous suspension>\ncompiler message: #31     IncrementalCompiler.computeDelta.<anonymous closure> (package:front_end/src/fasta/incremental_compiler.dart:140)\ncompiler message: <asynchronous suspension>\ncompiler message: #32     CompilerContext.runInContext.<anonymous closure> (package:front_end/src/fasta/compiler_context.dart:105)\ncompiler message: #33     _rootRun (dart:async/zone.dart:1126)\ncompiler message: #34     _CustomZone.run (dart:async/zone.dart:1023)\ncompiler message: #35     runZoned (dart:async/zone.dart:1501)\ncompiler message: #36     CompilerContext.runInContext (package:front_end/src/fasta/compiler_context.dart:105)\ncompiler message: #37     IncrementalCompiler.computeDelta (package:front_end/src/fasta/incremental_compiler.dart:61)\ncompiler message: <asynchronous suspension>\ncompiler message: #38     IncrementalCompiler.compile (package:vm/incremental_compiler.dart:33)\ncompiler message: <asynchronous suspension>\ncompiler message: #39     FrontendCompiler.recompileDelta (package:vm/frontend_server.dart:338)\ncompiler message: <asynchronous suspension>\ncompiler message: #40     _FlutterFrontendCompiler.recompileDelta (package:frontend_server/server.dart:34)\ncompiler message: <asynchronous suspension>\ncompiler message: #41     listenAndCompile.<anonymous closure> (package:vm/frontend_server.dart:468)\ncompiler message: <asynchronous suspension>\ncompiler message: #42     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #43     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #44     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #45     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)\ncompiler message: #46     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)\ncompiler message: #47     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)\ncompiler message: #48     _LineSplitterSink._addLines (dart:convert/line_splitter.dart:154)\ncompiler message: #49     _LineSplitterSink.addSlice (dart:convert/line_splitter.dart:129)\ncompiler message: #50     StringConversionSinkMixin.add (dart:convert/string_conversion.dart:189)\ncompiler message: #51     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)\ncompiler message: #52     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #53     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #54     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #55     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)\ncompiler message: #56     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)\ncompiler message: #57     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)\ncompiler message: #58     _StringAdapterSink.addSlice (dart:convert/string_conversion.dart:273)\ncompiler message: #59     _Utf8ConversionSink.addSlice (dart:convert/string_conversion.dart:348)\ncompiler message: #60     _Utf8ConversionSink.add (dart:convert/string_conversion.dart:341)\ncompiler message: #61     _ConverterStreamEventSink.add (dart:convert/chunked_conversion.dart:86)\ncompiler message: #62     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)\ncompiler message: #63     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #64     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #65     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #66     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)\ncompiler message: #67     _StreamController._add (dart:async/stream_controller.dart:639)\ncompiler message: #68     _StreamController.add (dart:async/stream_controller.dart:585)\ncompiler message: #69     _Socket._onData (dart:io-patch/socket_patch.dart:1674)\ncompiler message: #70     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #71     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #72     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #73     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)\ncompiler message: #74     _StreamController._add (dart:async/stream_controller.dart:639)\ncompiler message: #75     _StreamController.add (dart:async/stream_controller.dart:585)\ncompiler message: #76     new _RawSocket.<anonymous closure> (dart:io-patch/socket_patch.dart:1247)\ncompiler message: #77     _NativeSocket.issueReadEvent.issue (dart:io-patch/socket_patch.dart:799)\ncompiler message: #78     _microtaskLoop (dart:async/schedule_microtask.dart:41)\ncompiler message: #79     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50)\ncompiler message: #80     _runPendingImmediateCallback (dart:isolate-patch/dart:isolate/isolate_patch.dart:113)\ncompiler message: #81     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:166)\ncompiler message: \ncompiler message: \ncompiler message: #0      internalProblem (package:front_end/src/fasta/problems.dart:30)\ncompiler message: #1      unhandled (package:front_end/src/fasta/problems.dart:43)\ncompiler message: #2      ConstnessEvaluator.defaultTreeNode (package:front_end/src/fasta/kernel/constness_evaluator.dart:112)\ncompiler message: #3      TreeVisitor.defaultExpression (package:kernel/visitor.dart:140)\ncompiler message: #4      TreeVisitor.visitThisExpression (package:kernel/visitor.dart:171)\ncompiler message: #5      ThisExpression.accept (package:kernel/ast.dart:3341)\ncompiler message: #6      ConstnessEvaluator.visitPropertyGet (package:front_end/src/fasta/kernel/constness_evaluator.dart:287)\ncompiler message: #7      PropertyGet.accept (package:kernel/ast.dart:2242)\ncompiler message: #8      ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)\ncompiler message: #9      ConstructorInvocation.accept (package:kernel/ast.dart:2983)\ncompiler message: #10     ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)\ncompiler message: #11     ConstructorInvocation.accept (package:kernel/ast.dart:2983)\ncompiler message: #12     ConstnessEvaluator.evaluate (package:front_end/src/fasta/kernel/constness_evaluator.dart:99)\ncompiler message: #13     evaluateConstness (package:front_end/src/fasta/kernel/constness_evaluator.dart:466)\ncompiler message: #14     BodyBuilder.inferConstness (package:front_end/src/fasta/kernel/body_builder.dart:710)\ncompiler message: #15     BodyBuilder.finishFunction (package:front_end/src/fasta/kernel/body_builder.dart:697)\ncompiler message: #16     DietListener.listenerFinishFunction (package:front_end/src/fasta/source/diet_listener.dart:684)\ncompiler message: #17     DietListener.parseFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:718)\ncompiler message: #18     DietListener.buildFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:565)\ncompiler message: #19     DietListener.endMethod (package:front_end/src/fasta/source/diet_listener.dart:530)\ncompiler message: #20     Parser.parseMethod (package:front_end/src/fasta/parser/parser.dart:3796)\ncompiler message: #21     Parser.parseClassMemberImpl (package:front_end/src/fasta/parser/parser.dart:3670)\ncompiler message: #22     Parser.parseClassBody (package:front_end/src/fasta/parser/parser.dart:3467)\ncompiler message: #23     Parser.parseClass (package:front_end/src/fasta/parser/parser.dart:1699)\ncompiler message: #24     Parser.parseClassOrNamedMixinApplication (package:front_end/src/fasta/parser/parser.dart:1659)\ncompiler message: #25     Parser.parseTopLevelKeywordDeclaration (package:front_end/src/fasta/parser/parser.dart:535)\ncompiler message: #26     Parser.parseTopLevelDeclarationImpl (package:front_end/src/fasta/parser/parser.dart:451)\ncompiler message: #27     Parser.parseUnit (package:front_end/src/fasta/parser/parser.dart:335)\ncompiler message: #28     SourceLoader.buildBody (package:front_end/src/fasta/source/source_loader.dart:198)\ncompiler message: <asynchronous suspension>\ncompiler message: #29     Loader.buildBodies (package:front_end/src/fasta/loader.dart:157)\ncompiler message: <asynchronous suspension>\ncompiler message: #30     KernelTarget.buildComponent (package:front_end/src/fasta/kernel/kernel_target.dart:292)\ncompiler message: <asynchronous suspension>\ncompiler message: #31     IncrementalCompiler.computeDelta.<anonymous closure> (package:front_end/src/fasta/incremental_compiler.dart:140)\ncompiler message: <asynchronous suspension>\ncompiler message: #32     CompilerContext.runInContext.<anonymous closure> (package:front_end/src/fasta/compiler_context.dart:105)\ncompiler message: #33     _rootRun (dart:async/zone.dart:1126)\ncompiler message: #34     _CustomZone.run (dart:async/zone.dart:1023)\ncompiler message: #35     runZoned (dart:async/zone.dart:1501)\ncompiler message: #36     CompilerContext.runInContext (package:front_end/src/fasta/compiler_context.dart:105)\ncompiler message: #37     IncrementalCompiler.computeDelta (package:front_end/src/fasta/incremental_compiler.dart:61)\ncompiler message: <asynchronous suspension>\ncompiler message: #38     IncrementalCompiler.compile (package:vm/incremental_compiler.dart:33)\ncompiler message: <asynchronous suspension>\ncompiler message: #39     FrontendCompiler.recompileDelta (package:vm/frontend_server.dart:338)\ncompiler message: <asynchronous suspension>\ncompiler message: #40     _FlutterFrontendCompiler.recompileDelta (package:frontend_server/server.dart:34)\ncompiler message: <asynchronous suspension>\ncompiler message: #41     listenAndCompile.<anonymous closure> (package:vm/frontend_server.dart:468)\ncompiler message: <asynchronous suspension>\ncompiler message: #42     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #43     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #44     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #45     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)\ncompiler message: #46     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)\ncompiler message: #47     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)\ncompiler message: #48     _LineSplitterSink._addLines (dart:convert/line_splitter.dart:154)\ncompiler message: #49     _LineSplitterSink.addSlice (dart:convert/line_splitter.dart:129)\ncompiler message: #50     StringConversionSinkMixin.add (dart:convert/string_conversion.dart:189)\ncompiler message: #51     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)\ncompiler message: #52     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #53     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #54     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #55     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)\ncompiler message: #56     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)\ncompiler message: #57     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)\ncompiler message: #58     _StringAdapterSink.addSlice (dart:convert/string_conversion.dart:273)\ncompiler message: #59     _Utf8ConversionSink.addSlice (dart:convert/string_conversion.dart:348)\ncompiler message: #60     _Utf8ConversionSink.add (dart:convert/string_conversion.dart:341)\ncompiler message: #61     _ConverterStreamEventSink.add (dart:convert/chunked_conversion.dart:86)\ncompiler message: #62     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)\ncompiler message: #63     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #64     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #65     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #66     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)\ncompiler message: #67     _StreamController._add (dart:async/stream_controller.dart:639)\ncompiler message: #68     _StreamController.add (dart:async/stream_controller.dart:585)\ncompiler message: #69     _Socket._onData (dart:io-patch/socket_patch.dart:1674)\ncompiler message: #70     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #71     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #72     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #73     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)\ncompiler message: #74     _StreamController._add (dart:async/stream_controller.dart:639)\ncompiler message: #75     _StreamController.add (dart:async/stream_controller.dart:585)\ncompiler message: #76     new _RawSocket.<anonymous closure> (dart:io-patch/socket_patch.dart:1247)\ncompiler message: #77     _NativeSocket.issueReadEvent.issue (dart:io-patch/socket_patch.dart:799)\ncompiler message: #78     _microtaskLoop (dart:async/schedule_microtask.dart:41)\ncompiler message: #79     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50)\ncompiler message: #80     _runPendingImmediateCallback (dart:isolate-patch/dart:isolate/isolate_patch.dart:113)\ncompiler message: #81     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:166)\nReloaded 1 of 385 libraries in 902ms.\n\nFlutter Doctor\n$ flutter doctor\nDoctor summary (to see all details, run flutter doctor -v):\n[✓] Flutter (Channel beta, v0.2.8, on Mac OS X 10.13.3 17D102, locale en-US)\n[✓] Android toolchain - develop for Android devices (Android SDK 26.0.3)\n[✓] iOS toolchain - develop for iOS devices (Xcode 9.2)\n[✓] Android Studio (version 3.0)\n[✓] IntelliJ IDEA Community Edition (version 2018.1)\n[✓] VS Code (version 1.20.1)\n[✓] Connected devices (1 available)\n\n• No issues found!",
    "annotations": [{ "label": 144, "user": 1 }],
    "meta": {
      "": "92",
      "number": "16440",
      "pretext": "Steps to Reproduce\nI accidentally left the \"new\" keyword out of a line of code while working on an app, and when I rebuilt it, the compiler crashed. I then created a brand new project in IntelliJ, and verified the issue still occurred.\nSteps to reproduce:\n\n\nCreate a new IntelliJ Flutter project\n\n\nIntelliJ spits out the \"increment counter\" app.\n\n\nRemove the new keyword from line 67:\n 66: // than having to individually change instances of widgets.\n 67: return new Scaffold(\n 68:   appBar: new AppBar(\n\n\n\nTrigger a hot reload.\n\n\nThe compiler crashes with the message included below.\n\n\nInterestingly, the compiler does not crash if you leave line 67 alone and instead remove the \"new\" keyword from line 68. Line 67 is creating the instance that's used as the return value of the method, if that matters.\nLogs\nPerforming hot reload...\ncompiler message: Unhandled exception:\ncompiler message: Crash when compiling file:///Users/redbrogdon/source/crashtest/lib/main.dart,\ncompiler message: at character offset 2160:\ncompiler message: lib/main.dart: Internal problem: Unhandled this in defaultTreeNode.\ncompiler message: #0      internalProblem (package:front_end/src/fasta/problems.dart:30)\ncompiler message: #1      unhandled (package:front_end/src/fasta/problems.dart:43)\ncompiler message: #2      ConstnessEvaluator.defaultTreeNode (package:front_end/src/fasta/kernel/constness_evaluator.dart:112)\ncompiler message: #3      TreeVisitor.defaultExpression (package:kernel/visitor.dart:140)\ncompiler message: #4      TreeVisitor.visitThisExpression (package:kernel/visitor.dart:171)\ncompiler message: #5      ThisExpression.accept (package:kernel/ast.dart:3341)\ncompiler message: #6      ConstnessEvaluator.visitPropertyGet (package:front_end/src/fasta/kernel/constness_evaluator.dart:287)\ncompiler message: #7      PropertyGet.accept (package:kernel/ast.dart:2242)\ncompiler message: #8      ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)\ncompiler message: #9      ConstructorInvocation.accept (package:kernel/ast.dart:2983)\ncompiler message: #10     ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)\ncompiler message: #11     ConstructorInvocation.accept (package:kernel/ast.dart:2983)\ncompiler message: #12     ConstnessEvaluator.evaluate (package:front_end/src/fasta/kernel/constness_evaluator.dart:99)\ncompiler message: #13     evaluateConstness (package:front_end/src/fasta/kernel/constness_evaluator.dart:466)\ncompiler message: #14     BodyBuilder.inferConstness (package:front_end/src/fasta/kernel/body_builder.dart:710)\ncompiler message: #15     BodyBuilder.finishFunction (package:front_end/src/fasta/kernel/body_builder.dart:697)\ncompiler message: #16     DietListener.listenerFinishFunction (package:front_end/src/fasta/source/diet_listener.dart:684)\ncompiler message: #17     DietListener.parseFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:718)\ncompiler message: #18     DietListener.buildFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:565)\ncompiler message: #19     DietListener.endMethod (package:front_end/src/fasta/source/diet_listener.dart:530)\ncompiler message: #20     Parser.parseMethod (package:front_end/src/fasta/parser/parser.dart:3796)\ncompiler message: #21     Parser.parseClassMemberImpl (package:front_end/src/fasta/parser/parser.dart:3670)\ncompiler message: #22     Parser.parseClassBody (package:front_end/src/fasta/parser/parser.dart:3467)\ncompiler message: #23     Parser.parseClass (package:front_end/src/fasta/parser/parser.dart:1699)\ncompiler message: #24     Parser.parseClassOrNamedMixinApplication (package:front_end/src/fasta/parser/parser.dart:1659)\ncompiler message: #25     Parser.parseTopLevelKeywordDeclaration (package:front_end/src/fasta/parser/parser.dart:535)\ncompiler message: #26     Parser.parseTopLevelDeclarationImpl (package:front_end/src/fasta/parser/parser.dart:451)\ncompiler message: #27     Parser.parseUnit (package:front_end/src/fasta/parser/parser.dart:335)\ncompiler message: #28     SourceLoader.buildBody (package:front_end/src/fasta/source/source_loader.dart:198)\ncompiler message: <asynchronous suspension>\ncompiler message: #29     Loader.buildBodies (package:front_end/src/fasta/loader.dart:157)\ncompiler message: <asynchronous suspension>\ncompiler message: #30     KernelTarget.buildComponent (package:front_end/src/fasta/kernel/kernel_target.dart:292)\ncompiler message: <asynchronous suspension>\ncompiler message: #31     IncrementalCompiler.computeDelta.<anonymous closure> (package:front_end/src/fasta/incremental_compiler.dart:140)\ncompiler message: <asynchronous suspension>\ncompiler message: #32     CompilerContext.runInContext.<anonymous closure> (package:front_end/src/fasta/compiler_context.dart:105)\ncompiler message: #33     _rootRun (dart:async/zone.dart:1126)\ncompiler message: #34     _CustomZone.run (dart:async/zone.dart:1023)\ncompiler message: #35     runZoned (dart:async/zone.dart:1501)\ncompiler message: #36     CompilerContext.runInContext (package:front_end/src/fasta/compiler_context.dart:105)\ncompiler message: #37     IncrementalCompiler.computeDelta (package:front_end/src/fasta/incremental_compiler.dart:61)\ncompiler message: <asynchronous suspension>\ncompiler message: #38     IncrementalCompiler.compile (package:vm/incremental_compiler.dart:33)\ncompiler message: <asynchronous suspension>\ncompiler message: #39     FrontendCompiler.recompileDelta (package:vm/frontend_server.dart:338)\ncompiler message: <asynchronous suspension>\ncompiler message: #40     _FlutterFrontendCompiler.recompileDelta (package:frontend_server/server.dart:34)\ncompiler message: <asynchronous suspension>\ncompiler message: #41     listenAndCompile.<anonymous closure> (package:vm/frontend_server.dart:468)\ncompiler message: <asynchronous suspension>\ncompiler message: #42     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #43     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #44     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #45     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)\ncompiler message: #46     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)\ncompiler message: #47     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)\ncompiler message: #48     _LineSplitterSink._addLines (dart:convert/line_splitter.dart:154)\ncompiler message: #49     _LineSplitterSink.addSlice (dart:convert/line_splitter.dart:129)\ncompiler message: #50     StringConversionSinkMixin.add (dart:convert/string_conversion.dart:189)\ncompiler message: #51     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)\ncompiler message: #52     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #53     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #54     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #55     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)\ncompiler message: #56     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)\ncompiler message: #57     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)\ncompiler message: #58     _StringAdapterSink.addSlice (dart:convert/string_conversion.dart:273)\ncompiler message: #59     _Utf8ConversionSink.addSlice (dart:convert/string_conversion.dart:348)\ncompiler message: #60     _Utf8ConversionSink.add (dart:convert/string_conversion.dart:341)\ncompiler message: #61     _ConverterStreamEventSink.add (dart:convert/chunked_conversion.dart:86)\ncompiler message: #62     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)\ncompiler message: #63     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #64     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #65     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #66     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)\ncompiler message: #67     _StreamController._add (dart:async/stream_controller.dart:639)\ncompiler message: #68     _StreamController.add (dart:async/stream_controller.dart:585)\ncompiler message: #69     _Socket._onData (dart:io-patch/socket_patch.dart:1674)\ncompiler message: #70     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #71     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #72     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #73     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)\ncompiler message: #74     _StreamController._add (dart:async/stream_controller.dart:639)\ncompiler message: #75     _StreamController.add (dart:async/stream_controller.dart:585)\ncompiler message: #76     new _RawSocket.<anonymous closure> (dart:io-patch/socket_patch.dart:1247)\ncompiler message: #77     _NativeSocket.issueReadEvent.issue (dart:io-patch/socket_patch.dart:799)\ncompiler message: #78     _microtaskLoop (dart:async/schedule_microtask.dart:41)\ncompiler message: #79     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50)\ncompiler message: #80     _runPendingImmediateCallback (dart:isolate-patch/dart:isolate/isolate_patch.dart:113)\ncompiler message: #81     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:166)\ncompiler message: \ncompiler message: \ncompiler message: #0      internalProblem (package:front_end/src/fasta/problems.dart:30)\ncompiler message: #1      unhandled (package:front_end/src/fasta/problems.dart:43)\ncompiler message: #2      ConstnessEvaluator.defaultTreeNode (package:front_end/src/fasta/kernel/constness_evaluator.dart:112)\ncompiler message: #3      TreeVisitor.defaultExpression (package:kernel/visitor.dart:140)\ncompiler message: #4      TreeVisitor.visitThisExpression (package:kernel/visitor.dart:171)\ncompiler message: #5      ThisExpression.accept (package:kernel/ast.dart:3341)\ncompiler message: #6      ConstnessEvaluator.visitPropertyGet (package:front_end/src/fasta/kernel/constness_evaluator.dart:287)\ncompiler message: #7      PropertyGet.accept (package:kernel/ast.dart:2242)\ncompiler message: #8      ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)\ncompiler message: #9      ConstructorInvocation.accept (package:kernel/ast.dart:2983)\ncompiler message: #10     ConstnessEvaluator.visitConstructorInvocation (package:front_end/src/fasta/kernel/constness_evaluator.dart:175)\ncompiler message: #11     ConstructorInvocation.accept (package:kernel/ast.dart:2983)\ncompiler message: #12     ConstnessEvaluator.evaluate (package:front_end/src/fasta/kernel/constness_evaluator.dart:99)\ncompiler message: #13     evaluateConstness (package:front_end/src/fasta/kernel/constness_evaluator.dart:466)\ncompiler message: #14     BodyBuilder.inferConstness (package:front_end/src/fasta/kernel/body_builder.dart:710)\ncompiler message: #15     BodyBuilder.finishFunction (package:front_end/src/fasta/kernel/body_builder.dart:697)\ncompiler message: #16     DietListener.listenerFinishFunction (package:front_end/src/fasta/source/diet_listener.dart:684)\ncompiler message: #17     DietListener.parseFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:718)\ncompiler message: #18     DietListener.buildFunctionBody (package:front_end/src/fasta/source/diet_listener.dart:565)\ncompiler message: #19     DietListener.endMethod (package:front_end/src/fasta/source/diet_listener.dart:530)\ncompiler message: #20     Parser.parseMethod (package:front_end/src/fasta/parser/parser.dart:3796)\ncompiler message: #21     Parser.parseClassMemberImpl (package:front_end/src/fasta/parser/parser.dart:3670)\ncompiler message: #22     Parser.parseClassBody (package:front_end/src/fasta/parser/parser.dart:3467)\ncompiler message: #23     Parser.parseClass (package:front_end/src/fasta/parser/parser.dart:1699)\ncompiler message: #24     Parser.parseClassOrNamedMixinApplication (package:front_end/src/fasta/parser/parser.dart:1659)\ncompiler message: #25     Parser.parseTopLevelKeywordDeclaration (package:front_end/src/fasta/parser/parser.dart:535)\ncompiler message: #26     Parser.parseTopLevelDeclarationImpl (package:front_end/src/fasta/parser/parser.dart:451)\ncompiler message: #27     Parser.parseUnit (package:front_end/src/fasta/parser/parser.dart:335)\ncompiler message: #28     SourceLoader.buildBody (package:front_end/src/fasta/source/source_loader.dart:198)\ncompiler message: <asynchronous suspension>\ncompiler message: #29     Loader.buildBodies (package:front_end/src/fasta/loader.dart:157)\ncompiler message: <asynchronous suspension>\ncompiler message: #30     KernelTarget.buildComponent (package:front_end/src/fasta/kernel/kernel_target.dart:292)\ncompiler message: <asynchronous suspension>\ncompiler message: #31     IncrementalCompiler.computeDelta.<anonymous closure> (package:front_end/src/fasta/incremental_compiler.dart:140)\ncompiler message: <asynchronous suspension>\ncompiler message: #32     CompilerContext.runInContext.<anonymous closure> (package:front_end/src/fasta/compiler_context.dart:105)\ncompiler message: #33     _rootRun (dart:async/zone.dart:1126)\ncompiler message: #34     _CustomZone.run (dart:async/zone.dart:1023)\ncompiler message: #35     runZoned (dart:async/zone.dart:1501)\ncompiler message: #36     CompilerContext.runInContext (package:front_end/src/fasta/compiler_context.dart:105)\ncompiler message: #37     IncrementalCompiler.computeDelta (package:front_end/src/fasta/incremental_compiler.dart:61)\ncompiler message: <asynchronous suspension>\ncompiler message: #38     IncrementalCompiler.compile (package:vm/incremental_compiler.dart:33)\ncompiler message: <asynchronous suspension>\ncompiler message: #39     FrontendCompiler.recompileDelta (package:vm/frontend_server.dart:338)\ncompiler message: <asynchronous suspension>\ncompiler message: #40     _FlutterFrontendCompiler.recompileDelta (package:frontend_server/server.dart:34)\ncompiler message: <asynchronous suspension>\ncompiler message: #41     listenAndCompile.<anonymous closure> (package:vm/frontend_server.dart:468)\ncompiler message: <asynchronous suspension>\ncompiler message: #42     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #43     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #44     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #45     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)\ncompiler message: #46     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)\ncompiler message: #47     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)\ncompiler message: #48     _LineSplitterSink._addLines (dart:convert/line_splitter.dart:154)\ncompiler message: #49     _LineSplitterSink.addSlice (dart:convert/line_splitter.dart:129)\ncompiler message: #50     StringConversionSinkMixin.add (dart:convert/string_conversion.dart:189)\ncompiler message: #51     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)\ncompiler message: #52     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #53     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #54     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #55     _SinkTransformerStreamSubscription._add (dart:async/stream_transformers.dart:68)\ncompiler message: #56     _EventSinkWrapper.add (dart:async/stream_transformers.dart:15)\ncompiler message: #57     _StringAdapterSink.add (dart:convert/string_conversion.dart:268)\ncompiler message: #58     _StringAdapterSink.addSlice (dart:convert/string_conversion.dart:273)\ncompiler message: #59     _Utf8ConversionSink.addSlice (dart:convert/string_conversion.dart:348)\ncompiler message: #60     _Utf8ConversionSink.add (dart:convert/string_conversion.dart:341)\ncompiler message: #61     _ConverterStreamEventSink.add (dart:convert/chunked_conversion.dart:86)\ncompiler message: #62     _SinkTransformerStreamSubscription._handleData (dart:async/stream_transformers.dart:120)\ncompiler message: #63     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #64     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #65     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #66     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)\ncompiler message: #67     _StreamController._add (dart:async/stream_controller.dart:639)\ncompiler message: #68     _StreamController.add (dart:async/stream_controller.dart:585)\ncompiler message: #69     _Socket._onData (dart:io-patch/socket_patch.dart:1674)\ncompiler message: #70     _RootZone.runUnaryGuarded (dart:async/zone.dart:1316)\ncompiler message: #71     _BufferingStreamSubscription._sendData (dart:async/stream_impl.dart:330)\ncompiler message: #72     _BufferingStreamSubscription._add (dart:async/stream_impl.dart:257)\ncompiler message: #73     _StreamController&&_SyncStreamControllerDispatch._sendData (dart:async/stream_controller.dart:763)\ncompiler message: #74     _StreamController._add (dart:async/stream_controller.dart:639)\ncompiler message: #75     _StreamController.add (dart:async/stream_controller.dart:585)\ncompiler message: #76     new _RawSocket.<anonymous closure> (dart:io-patch/socket_patch.dart:1247)\ncompiler message: #77     _NativeSocket.issueReadEvent.issue (dart:io-patch/socket_patch.dart:799)\ncompiler message: #78     _microtaskLoop (dart:async/schedule_microtask.dart:41)\ncompiler message: #79     _startMicrotaskLoop (dart:async/schedule_microtask.dart:50)\ncompiler message: #80     _runPendingImmediateCallback (dart:isolate-patch/dart:isolate/isolate_patch.dart:113)\ncompiler message: #81     _RawReceivePortImpl._handleMessage (dart:isolate-patch/dart:isolate/isolate_patch.dart:166)\nReloaded 1 of 385 libraries in 902ms.\n\nFlutter Doctor\n$ flutter doctor\nDoctor summary (to see all details, run flutter doctor -v):\n[✓] Flutter (Channel beta, v0.2.8, on Mac OS X 10.13.3 17D102, locale en-US)\n[✓] Android toolchain - develop for Android devices (Android SDK 26.0.3)\n[✓] iOS toolchain - develop for iOS devices (Xcode 9.2)\n[✓] Android Studio (version 3.0)\n[✓] IntelliJ IDEA Community Edition (version 2018.1)\n[✓] VS Code (version 1.20.1)\n[✓] Connected devices (1 available)\n\n• No issues found!",
      "title": "Compiler crash when \"new\" keyword removed from default app"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2773,
    "text": "Let add-to-app FlutterViewControllers be back swipable in a UINavigationController",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "93",
      "number": "32866",
      "pretext": "",
      "title": "Let add-to-app FlutterViewControllers be back swipable in a UINavigationController"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2774,
    "text": "Broken links on websiteLinks to FlutterViewController.h are broken for example.\nA more sustainable fix is probably some sort of automated way to check these as part of publishing. :(",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "94",
      "number": "7306",
      "pretext": "Links to FlutterViewController.h are broken for example.\nA more sustainable fix is probably some sort of automated way to check these as part of publishing. :(",
      "title": "Broken links on website"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2775,
    "text": "Regression in Android license status detection.#16035 introduced a bug with Android sdkmanager version 26.1.1 because the --add-modules option isn't supported:\n$ SDKMANAGER_OPTS=\"--add-modules java.se.ee\" /path/to/android/sdk/tools/bin/sdkmanager --licenses\nUnrecognized option: --add-modules\nError: Could not create the Java Virtual Machine.\nError: A fatal exception has occurred. Program will exit\n\nThe manifestation was that when the user ran flutter doctor -v, they'd see the following:\n[!] Android toolchain - develop for Android devices (Android SDK 27.0.3)\n    ...\n    ✗ Android license status unknown.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "95",
      "number": "16228",
      "pretext": "#16035 introduced a bug with Android sdkmanager version 26.1.1 because the --add-modules option isn't supported:\n$ SDKMANAGER_OPTS=\"--add-modules java.se.ee\" /path/to/android/sdk/tools/bin/sdkmanager --licenses\nUnrecognized option: --add-modules\nError: Could not create the Java Virtual Machine.\nError: A fatal exception has occurred. Program will exit\n\nThe manifestation was that when the user ran flutter doctor -v, they'd see the following:\n[!] Android toolchain - develop for Android devices (Android SDK 27.0.3)\n    ...\n    ✗ Android license status unknown.",
      "title": "Regression in Android license status detection."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2776,
    "text": "java.lang.NoClassDefFoundError: com.google.firebase.FirebaseAppSteps to Reproduce\nI have an application with Flutter. It works really fine, but the app crashes on API level 18 and 19.\nWhen I try to run test the apk in Test Lab, it show me error on API level 18 and 19\nLogs\n Fatal exception\n\tat com.google.firebase.provider.FirebaseInitProvider.onCreate(com.google.firebase:firebase-common@@16.0.2:53)\nFATAL EXCEPTION: main\njava.lang.NoClassDefFoundError: com.google.firebase.FirebaseApp\n\tat com.google.firebase.provider.FirebaseInitProvider.onCreate(com.google.firebase:firebase-common@@16.0.2:53)\n\tat android.content.ContentProvider.attachInfo(ContentProvider.java:1214)\n\tat android.content.ContentProvider.attachInfo(ContentProvider.java:1189)\n\tat com.google.firebase.provider.FirebaseInitProvider.attachInfo(com.google.firebase:firebase-common@@16.0.2:47)\n\tat android.app.ActivityThread.installProvider(ActivityThread.java:5119)\n\tat android.app.ActivityThread.installContentProviders(ActivityThread.java:4725)\n\tat android.app.ActivityThread.handleBindApplication(ActivityThread.java:4665)\n\tat android.app.ActivityThread.access$1400(ActivityThread.java:159)\n\tat android.app.ActivityThread$H.handleMessage(ActivityThread.java:1376)\n\tat android.os.Handler.dispatchMessage(Handler.java:99)\n\tat android.os.Looper.loop(Looper.java:176)\n\tat android.app.ActivityThread.main(ActivityThread.java:5419)\n\tat java.lang.reflect.Method.invokeNative(Native Method)\n\tat java.lang.reflect.Method.invoke(Method.java:525)\n\tat com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1046)\n\tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:862)\n\tat dalvik.system.NativeStart.main(Native Method)\n\ngradle.build\ndef localProperties = new Properties()\ndef localPropertiesFile = rootProject.file('local.properties')\nif (localPropertiesFile.exists()) {\n    localPropertiesFile.withReader('UTF-8') { reader ->\n        localProperties.load(reader)\n    }\n}\n\ndef flutterRoot = localProperties.getProperty('flutter.sdk')\nif (flutterRoot == null) {\n    throw new GradleException(\"Flutter SDK not found. Define location with flutter.sdk in the local.properties file.\")\n}\n\ndef flutterVersionCode = localProperties.getProperty('flutter.versionCode')\nif (flutterVersionCode == null) {\n    flutterVersionCode = '1'\n}\n\ndef flutterVersionName = localProperties.getProperty('flutter.versionName')\nif (flutterVersionName == null) {\n    flutterVersionName = '1.0'\n}\n\napply plugin: 'com.android.application'\napply from: \"$flutterRoot/packages/flutter_tools/gradle/flutter.gradle\"\n\nrepositories {\n    mavenCentral()\n    maven { url 'https://mapbox.bintray.com/mapbox' }\n}\n\ndef keystoreProperties = new Properties()\ndef keystorePropertiesFile = rootProject.file('key.properties')\nif (keystorePropertiesFile.exists()) {\n    keystoreProperties.load(new FileInputStream(keystorePropertiesFile))\n}\n\nandroid {\n    compileSdkVersion 28\n    lintOptions {\n        disable 'InvalidPackage'\n    }\n    defaultConfig {\n        // TODO: Specify your own unique Application ID (https://developer.android.com/studio/build/application-id.html).\n        applicationId \"com.tripmate.myapp\"\n        minSdkVersion 16\n        targetSdkVersion 27\n        versionCode flutterVersionCode.toInteger()\n        versionName flutterVersionName\n        testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\"\n        multiDexEnabled true\n        ndk {\n        abiFilters 'armeabi-v7a'\n        }\n\n    }\n    signingConfigs {\n        release {\n            keyAlias keystoreProperties['keyAlias']\n            keyPassword keystoreProperties['keyPassword']\n            storeFile file(keystoreProperties['storeFile'])\n            storePassword keystoreProperties['storePassword']\n        }\n    }\n    buildTypes {\n        release {\n            signingConfig signingConfigs.release\n        }\n    }\n    buildToolsVersion '28.0.3'\n    compileOptions {\n        sourceCompatibility JavaVersion.VERSION_1_8\n        targetCompatibility JavaVersion.VERSION_1_8\n    }\n}\n\nflutter {\n    source '../..'\n}\n\ndependencies {\n    implementation 'android.arch.core:runtime:1.1.1'\n    implementation 'com.android.support:multidex:1.0.0'\n    implementation 'com.android.support.constraint:constraint-layout:1.1.3'\n    testImplementation 'junit:junit:4.12'\n    androidTestImplementation 'com.android.support.test:runner:1.0.2'\n    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'\n    implementation 'com.google.firebase:firebase-core:16.0.7'\n    //Maps Box Maps SDK\n    implementation 'com.mapbox.mapboxsdk:mapbox-android-sdk:7.1.2'\n    //Permission management API\n    implementation 'com.google.android.gms:play-services-maps:16.1.0'\n    //Google play services\n    implementation 'com.google.android.gms:play-services-location:16.0.0'\n    //Bubble Layout\n    implementation 'com.daasuu:BubbleLayout:1.2.0'\n    //Mapbox navigation\n    implementation 'com.mapbox.mapboxsdk:mapbox-android-navigation-ui:0.30.0'\n    //Mapbox Annotation Plugin\n    implementation 'com.mapbox.mapboxsdk:mapbox-android-plugin-annotation-v7:0.5.0'\n    implementation 'com.android.support:design:1.0.0'\n\n}\n\nconfigurations.all {\n    resolutionStrategy {\n        force 'com.android.support:support-v4:27.1.0'\n    }\n}\n\napply plugin: 'com.google.gms.google-services'\napply plugin: 'io.fabric'\n\n\nCan anyone help me with this?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "96",
      "number": "31123",
      "pretext": "Steps to Reproduce\nI have an application with Flutter. It works really fine, but the app crashes on API level 18 and 19.\nWhen I try to run test the apk in Test Lab, it show me error on API level 18 and 19\nLogs\n Fatal exception\n\tat com.google.firebase.provider.FirebaseInitProvider.onCreate(com.google.firebase:firebase-common@@16.0.2:53)\nFATAL EXCEPTION: main\njava.lang.NoClassDefFoundError: com.google.firebase.FirebaseApp\n\tat com.google.firebase.provider.FirebaseInitProvider.onCreate(com.google.firebase:firebase-common@@16.0.2:53)\n\tat android.content.ContentProvider.attachInfo(ContentProvider.java:1214)\n\tat android.content.ContentProvider.attachInfo(ContentProvider.java:1189)\n\tat com.google.firebase.provider.FirebaseInitProvider.attachInfo(com.google.firebase:firebase-common@@16.0.2:47)\n\tat android.app.ActivityThread.installProvider(ActivityThread.java:5119)\n\tat android.app.ActivityThread.installContentProviders(ActivityThread.java:4725)\n\tat android.app.ActivityThread.handleBindApplication(ActivityThread.java:4665)\n\tat android.app.ActivityThread.access$1400(ActivityThread.java:159)\n\tat android.app.ActivityThread$H.handleMessage(ActivityThread.java:1376)\n\tat android.os.Handler.dispatchMessage(Handler.java:99)\n\tat android.os.Looper.loop(Looper.java:176)\n\tat android.app.ActivityThread.main(ActivityThread.java:5419)\n\tat java.lang.reflect.Method.invokeNative(Native Method)\n\tat java.lang.reflect.Method.invoke(Method.java:525)\n\tat com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1046)\n\tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:862)\n\tat dalvik.system.NativeStart.main(Native Method)\n\ngradle.build\ndef localProperties = new Properties()\ndef localPropertiesFile = rootProject.file('local.properties')\nif (localPropertiesFile.exists()) {\n    localPropertiesFile.withReader('UTF-8') { reader ->\n        localProperties.load(reader)\n    }\n}\n\ndef flutterRoot = localProperties.getProperty('flutter.sdk')\nif (flutterRoot == null) {\n    throw new GradleException(\"Flutter SDK not found. Define location with flutter.sdk in the local.properties file.\")\n}\n\ndef flutterVersionCode = localProperties.getProperty('flutter.versionCode')\nif (flutterVersionCode == null) {\n    flutterVersionCode = '1'\n}\n\ndef flutterVersionName = localProperties.getProperty('flutter.versionName')\nif (flutterVersionName == null) {\n    flutterVersionName = '1.0'\n}\n\napply plugin: 'com.android.application'\napply from: \"$flutterRoot/packages/flutter_tools/gradle/flutter.gradle\"\n\nrepositories {\n    mavenCentral()\n    maven { url 'https://mapbox.bintray.com/mapbox' }\n}\n\ndef keystoreProperties = new Properties()\ndef keystorePropertiesFile = rootProject.file('key.properties')\nif (keystorePropertiesFile.exists()) {\n    keystoreProperties.load(new FileInputStream(keystorePropertiesFile))\n}\n\nandroid {\n    compileSdkVersion 28\n    lintOptions {\n        disable 'InvalidPackage'\n    }\n    defaultConfig {\n        // TODO: Specify your own unique Application ID (https://developer.android.com/studio/build/application-id.html).\n        applicationId \"com.tripmate.myapp\"\n        minSdkVersion 16\n        targetSdkVersion 27\n        versionCode flutterVersionCode.toInteger()\n        versionName flutterVersionName\n        testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\"\n        multiDexEnabled true\n        ndk {\n        abiFilters 'armeabi-v7a'\n        }\n\n    }\n    signingConfigs {\n        release {\n            keyAlias keystoreProperties['keyAlias']\n            keyPassword keystoreProperties['keyPassword']\n            storeFile file(keystoreProperties['storeFile'])\n            storePassword keystoreProperties['storePassword']\n        }\n    }\n    buildTypes {\n        release {\n            signingConfig signingConfigs.release\n        }\n    }\n    buildToolsVersion '28.0.3'\n    compileOptions {\n        sourceCompatibility JavaVersion.VERSION_1_8\n        targetCompatibility JavaVersion.VERSION_1_8\n    }\n}\n\nflutter {\n    source '../..'\n}\n\ndependencies {\n    implementation 'android.arch.core:runtime:1.1.1'\n    implementation 'com.android.support:multidex:1.0.0'\n    implementation 'com.android.support.constraint:constraint-layout:1.1.3'\n    testImplementation 'junit:junit:4.12'\n    androidTestImplementation 'com.android.support.test:runner:1.0.2'\n    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'\n    implementation 'com.google.firebase:firebase-core:16.0.7'\n    //Maps Box Maps SDK\n    implementation 'com.mapbox.mapboxsdk:mapbox-android-sdk:7.1.2'\n    //Permission management API\n    implementation 'com.google.android.gms:play-services-maps:16.1.0'\n    //Google play services\n    implementation 'com.google.android.gms:play-services-location:16.0.0'\n    //Bubble Layout\n    implementation 'com.daasuu:BubbleLayout:1.2.0'\n    //Mapbox navigation\n    implementation 'com.mapbox.mapboxsdk:mapbox-android-navigation-ui:0.30.0'\n    //Mapbox Annotation Plugin\n    implementation 'com.mapbox.mapboxsdk:mapbox-android-plugin-annotation-v7:0.5.0'\n    implementation 'com.android.support:design:1.0.0'\n\n}\n\nconfigurations.all {\n    resolutionStrategy {\n        force 'com.android.support:support-v4:27.1.0'\n    }\n}\n\napply plugin: 'com.google.gms.google-services'\napply plugin: 'io.fabric'\n\n\nCan anyone help me with this?",
      "title": "java.lang.NoClassDefFoundError: com.google.firebase.FirebaseApp"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2777,
    "text": "Cloud Firestore 0.9.7 stucks at Installing BoringSSL-GRPC (0.0.2) during pod installTried downgrading the version but didn't work. I am using the latest Xcode and IOS version.\nHelp @shihaohong @Hixie @jonahwilliams @pepegich",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "97",
      "number": "30185",
      "pretext": "Tried downgrading the version but didn't work. I am using the latest Xcode and IOS version.\nHelp @shihaohong @Hixie @jonahwilliams @pepegich",
      "title": "Cloud Firestore 0.9.7 stucks at Installing BoringSSL-GRPC (0.0.2) during pod install"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2778,
    "text": "Infinite loop in TextField with some inputFormatters (iOS)Steps to Reproduce\n\nRun examples/flutter_gallery on iOS (physical device or simulator)\nOpen \"Text fields\" under \"Material Components\"\nEnter one or more numbers into the \"Phone Number\" field\nHit backspace\nThe input field goes into a loop where it repeatedly removes and adds the last character.\n\n\nLogs\n[        ] 🔥  To hot reload your app on the fly, press \"r\". To restart the app entirely, press \"R\".\n[        ] An Observatory debugger and profiler on iPhone X is available at: http://127.0.0.1:8103/\n[        ] For a more detailed help message, press \"h\". To quit, press \"q\".\n[+253478 ms] [DEVICE LOG] 2018-01-06 15:52:03.678301+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4633, Description: Loading Preferences From System CFPrefsD For Search List\n[+13924 ms] [DEVICE LOG] 2018-01-06 15:52:17.602866+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4634, Description: Loading Preferences From System CFPrefsD For Search List\n[   +8 ms] [DEVICE LOG] 2018-01-06 15:52:17.611380+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4635, Description: Loading Preferences From System CFPrefsD For Search List\n[ +234 ms] [DEVICE LOG] 2018-01-06 15:52:17.845901+0200  localhost Runner[24134]: (UIKit) Can't find keyplane that supports type 5 for keyboard iPhone-PortraitChoco-PhonePad; using 2024220450015396792_PortraitChoco_iPhone-Complex-Pad_Default\n[  +63 ms] [DEVICE LOG] 2018-01-06 15:52:17.900540+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4636, Description: Loading Preferences From System CFPrefsD For Search List\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.901476+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4637, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.901791+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4638, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.901973+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4639, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.902255+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463a, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.902480+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463b, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.902748+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463c, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.903001+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463d, Description: Updating Key-Value Observers Of Preferences\n[   +1 ms] [DEVICE LOG] 2018-01-06 15:52:17.903195+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463e, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.903453+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463f, Description: Updating Key-Value Observers Of Preferences\n[   +1 ms] [DEVICE LOG] 2018-01-06 15:52:17.906445+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc58b0, Description: Loading Preferences From System CFPrefsD For Search List\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.907627+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 0 -> 1; styleActivationCount: 0 -> 1\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.907921+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 0 -> 1\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.907992+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activating engine\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.908409+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] feedback engine <_UIFeedbackSystemSoundEngine: 0x6000000bec60: state=0, numberOfClients=1, prewarmCount=0, _isSuspended=0> state changed: Inactive -> Activating\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.910005+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] feedback engine <_UIFeedbackSystemSoundEngine: 0x6000000bec60: state=3, numberOfClients=1, prewarmCount=0, _isSuspended=0> state changed: Activating -> Running\n[+3359 ms] [DEVICE LOG] 2018-01-06 15:52:21.273835+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2\n[        ] [DEVICE LOG] 2018-01-06 15:52:21.273875+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2\n[        ] [DEVICE LOG] 2018-01-06 15:52:21.274731+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1\n[        ] [DEVICE LOG] 2018-01-06 15:52:21.274782+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1\n[+2656 ms] [DEVICE LOG] 2018-01-06 15:52:23.928718+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2\n[        ] [DEVICE LOG] 2018-01-06 15:52:23.928799+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2\n[   +6 ms] [DEVICE LOG] 2018-01-06 15:52:23.929589+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1\n[   +2 ms] [DEVICE LOG] 2018-01-06 15:52:23.929682+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1\n[+3667 ms] [DEVICE LOG] 2018-01-06 15:52:27.608269+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2\n[        ] [DEVICE LOG] 2018-01-06 15:52:27.608319+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2\n[   +2 ms] [DEVICE LOG] 2018-01-06 15:52:27.609012+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1\n[        ] [DEVICE LOG] 2018-01-06 15:52:27.609054+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1\n\nFlutter Doctor\n[✓] Flutter (on Mac OS X 10.13.2 17C88, locale en-SE, channel master)\n    • Flutter at /Users/fsimon/flutter\n    • Framework revision a414fb458d (28 hours ago), 2018-01-06 12:02:33 +0100\n    • Engine revision 12e0e38a8b\n    • Tools Dart version 1.25.0-dev.11.0\n    • Engine Dart version 2.0.0-edge.8d7219a5b6a7c2505ff57f23e7cf80da4c724512\n\n[✓] Android toolchain - develop for Android devices (Android SDK 26.0.3)\n    • Android SDK at /Users/fsimon/Library/Android/sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-26, build-tools 26.0.3\n    • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 9.2)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 9.2, Build version 9C40b\n    • ios-deploy 1.9.2\n    • CocoaPods version 1.3.1\n\n[✓] Android Studio (version 3.0)\n    • Android Studio at /Applications/Android Studio.app/Contents\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n\n[✓] Connected devices\n    • iPhone X                  • 1C6CECD0-0E72-489F-90D7-197A80D82B98 • ios         • iOS 11.2 (simulator)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "98",
      "number": "13961",
      "pretext": "Steps to Reproduce\n\nRun examples/flutter_gallery on iOS (physical device or simulator)\nOpen \"Text fields\" under \"Material Components\"\nEnter one or more numbers into the \"Phone Number\" field\nHit backspace\nThe input field goes into a loop where it repeatedly removes and adds the last character.\n\n\nLogs\n[        ] 🔥  To hot reload your app on the fly, press \"r\". To restart the app entirely, press \"R\".\n[        ] An Observatory debugger and profiler on iPhone X is available at: http://127.0.0.1:8103/\n[        ] For a more detailed help message, press \"h\". To quit, press \"q\".\n[+253478 ms] [DEVICE LOG] 2018-01-06 15:52:03.678301+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4633, Description: Loading Preferences From System CFPrefsD For Search List\n[+13924 ms] [DEVICE LOG] 2018-01-06 15:52:17.602866+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4634, Description: Loading Preferences From System CFPrefsD For Search List\n[   +8 ms] [DEVICE LOG] 2018-01-06 15:52:17.611380+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4635, Description: Loading Preferences From System CFPrefsD For Search List\n[ +234 ms] [DEVICE LOG] 2018-01-06 15:52:17.845901+0200  localhost Runner[24134]: (UIKit) Can't find keyplane that supports type 5 for keyboard iPhone-PortraitChoco-PhonePad; using 2024220450015396792_PortraitChoco_iPhone-Complex-Pad_Default\n[  +63 ms] [DEVICE LOG] 2018-01-06 15:52:17.900540+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4636, Description: Loading Preferences From System CFPrefsD For Search List\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.901476+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4637, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.901791+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4638, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.901973+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc4639, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.902255+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463a, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.902480+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463b, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.902748+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463c, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.903001+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463d, Description: Updating Key-Value Observers Of Preferences\n[   +1 ms] [DEVICE LOG] 2018-01-06 15:52:17.903195+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463e, Description: Updating Key-Value Observers Of Preferences\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.903453+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc463f, Description: Updating Key-Value Observers Of Preferences\n[   +1 ms] [DEVICE LOG] 2018-01-06 15:52:17.906445+0200  localhost Runner[24134]: (Runner) Created Activity ID: 0xc58b0, Description: Loading Preferences From System CFPrefsD For Search List\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.907627+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 0 -> 1; styleActivationCount: 0 -> 1\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.907921+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 0 -> 1\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.907992+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activating engine\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.908409+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] feedback engine <_UIFeedbackSystemSoundEngine: 0x6000000bec60: state=0, numberOfClients=1, prewarmCount=0, _isSuspended=0> state changed: Inactive -> Activating\n[        ] [DEVICE LOG] 2018-01-06 15:52:17.910005+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] feedback engine <_UIFeedbackSystemSoundEngine: 0x6000000bec60: state=3, numberOfClients=1, prewarmCount=0, _isSuspended=0> state changed: Activating -> Running\n[+3359 ms] [DEVICE LOG] 2018-01-06 15:52:21.273835+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2\n[        ] [DEVICE LOG] 2018-01-06 15:52:21.273875+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2\n[        ] [DEVICE LOG] 2018-01-06 15:52:21.274731+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1\n[        ] [DEVICE LOG] 2018-01-06 15:52:21.274782+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1\n[+2656 ms] [DEVICE LOG] 2018-01-06 15:52:23.928718+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2\n[        ] [DEVICE LOG] 2018-01-06 15:52:23.928799+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2\n[   +6 ms] [DEVICE LOG] 2018-01-06 15:52:23.929589+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1\n[   +2 ms] [DEVICE LOG] 2018-01-06 15:52:23.929682+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1\n[+3667 ms] [DEVICE LOG] 2018-01-06 15:52:27.608269+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] activate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 1 -> 2; styleActivationCount: 1 -> 2\n[        ] [DEVICE LOG] 2018-01-06 15:52:27.608319+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] activate feedback engine 0x6000000bec60, clientCount: 1 -> 2\n[   +2 ms] [DEVICE LOG] 2018-01-06 15:52:27.609012+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:FeedbackActivation] deactivate generator <_UIKeyboardFeedbackGenerator: 0x600000125fa0: prepared=1> with style: TurnOn; activationCount: 2 -> 1; styleActivationCount: 2 -> 1\n[        ] [DEVICE LOG] 2018-01-06 15:52:27.609054+0200  localhost Runner[24134]: (UIKit) [com.apple.UIKit:Feedback] deactivate feedback engine 0x6000000bec60, clientCount: 2 -> 1\n\nFlutter Doctor\n[✓] Flutter (on Mac OS X 10.13.2 17C88, locale en-SE, channel master)\n    • Flutter at /Users/fsimon/flutter\n    • Framework revision a414fb458d (28 hours ago), 2018-01-06 12:02:33 +0100\n    • Engine revision 12e0e38a8b\n    • Tools Dart version 1.25.0-dev.11.0\n    • Engine Dart version 2.0.0-edge.8d7219a5b6a7c2505ff57f23e7cf80da4c724512\n\n[✓] Android toolchain - develop for Android devices (Android SDK 26.0.3)\n    • Android SDK at /Users/fsimon/Library/Android/sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-26, build-tools 26.0.3\n    • Java binary at: /Applications/Android Studio.app/Contents/jre/jdk/Contents/Home/bin/java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n\n[✓] iOS toolchain - develop for iOS devices (Xcode 9.2)\n    • Xcode at /Applications/Xcode.app/Contents/Developer\n    • Xcode 9.2, Build version 9C40b\n    • ios-deploy 1.9.2\n    • CocoaPods version 1.3.1\n\n[✓] Android Studio (version 3.0)\n    • Android Studio at /Applications/Android Studio.app/Contents\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-915-b08)\n\n[✓] Connected devices\n    • iPhone X                  • 1C6CECD0-0E72-489F-90D7-197A80D82B98 • ios         • iOS 11.2 (simulator)",
      "title": "Infinite loop in TextField with some inputFormatters (iOS)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2779,
    "text": "Trouble parsing Json Array Here is my simple Todo App using Flutter and Node/Express API's and MongoDB\nI am specifically having a problem with parsing json array of todo items.\nTodo App\nFirst of all when ever I run this project.\nI get this error\nCould not load source 'dart:core/runtime/libobject_patch.dart': .\nNothing in the debug console.\nAfter Reloading a few times, I land on Login Page,\nThen enter credentials and login,\nImmediately, the app is frozen and I get the Above error.\nIf you look at lib/services/todoService.dart  => getAll() method.\nAm I doing something wrong?\nSecondly,\nThe json returned from server is as below, and I need to parse it to a List in Dart.\nI tried following this\nI get the same error as above.\n[ { \"completed\": true, \"completedAt\": 1545314655878, \"_id\": \"5c0244d8ac3baf291808b03a\", \"text\": \"Shop some Groceries\", \"_author\": \"5c02449eac3baf291808b037\", \"__v\": 0 }, { \"completed\": false, \"completedAt\": null, \"_id\": \"5c1b9a14fe8f70360cd0a393\", \"text\": \"Eat before 12\", \"_author\": \"5c02449eac3baf291808b037\", \"__v\": 0 }, { \"completed\": false, \"completedAt\": null, \"_id\": \"5c1ba02efe8f70360cd0a394\", \"text\": \"Test todo\", \"_author\": \"5c02449eac3baf291808b037\", \"__v\": 0 } ]\nMy Todo Class looks like this\nclass Todo {\n  final String text;\n  final String author;\n  final bool completed;\n  final DateTime completedAt;\n  final String id;\n\n  Todo(\n      {this.author = \"\",\n      this.id = \"\",\n      this.completed = false,\n      DateTime completedAt,\n      this.text = \"\"})\n      : completedAt = completedAt ?? DateTime.now();\n\n  factory Todo.fromJson(Map<String, dynamic> json) {\n    return Todo(\n      text: json['text'],\n      author: json['_author'],\n      completed: json['completed'],\n      completedAt: DateTime.fromMillisecondsSinceEpoch(json['completedAt']),\n      id: json['_id'],\n    );\n  }\n}\n\nI cant find a good way to parse it into a List and return it to a FutureBuilder.\nLogs\nI use VS Code, No Errors in the debug Console.\n\n[√] Flutter (Channel beta, v1.0.0, on Microsoft Windows [Version 10.0.17134.472], locale en-IN)\n    • Flutter version 1.0.0 at C:\\flutter\n    • Framework revision 5391447fae (3 weeks ago), 2018-11-29 19:41:26 -0800\n    • Engine revision 7375a0f414\n    • Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)\n\n[√] Android toolchain - develop for Android devices (Android SDK 28.0.3)\n    • Android SDK at C:\\Users\\lightyaer\\AppData\\Local\\Android\\Sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • ANDROID_HOME = C:\\Users\\lightyaer\\AppData\\Local\\Android\\Sdk\n    • Java binary at: C:\\Program Files\\Android\\Android Studio\\jre\\bin\\java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1136-b06)\n    • All Android licenses accepted.\n\n[√] Android Studio (version 3.2)\n    • Android Studio at C:\\Program Files\\Android\\Android Studio\n    X Flutter plugin not installed; this adds Flutter specific functionality.\n    X Dart plugin not installed; this adds Dart specific functionality.\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1136-b06)\n\n[√] VS Code (version 1.30.1)\n    • VS Code at C:\\Users\\lightyaer\\AppData\\Local\\Programs\\Microsoft VS Code\n    • Flutter extension version 2.21.1\n\n[√] Connected device (1 available)\n    • Android SDK built for x86 • emulator-5554 • android-x86 • Android 9 (API 28) (emulator)\n\n• No issues found!",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "99",
      "number": "25627",
      "pretext": "Here is my simple Todo App using Flutter and Node/Express API's and MongoDB\nI am specifically having a problem with parsing json array of todo items.\nTodo App\nFirst of all when ever I run this project.\nI get this error\nCould not load source 'dart:core/runtime/libobject_patch.dart': .\nNothing in the debug console.\nAfter Reloading a few times, I land on Login Page,\nThen enter credentials and login,\nImmediately, the app is frozen and I get the Above error.\nIf you look at lib/services/todoService.dart  => getAll() method.\nAm I doing something wrong?\nSecondly,\nThe json returned from server is as below, and I need to parse it to a List in Dart.\nI tried following this\nI get the same error as above.\n[ { \"completed\": true, \"completedAt\": 1545314655878, \"_id\": \"5c0244d8ac3baf291808b03a\", \"text\": \"Shop some Groceries\", \"_author\": \"5c02449eac3baf291808b037\", \"__v\": 0 }, { \"completed\": false, \"completedAt\": null, \"_id\": \"5c1b9a14fe8f70360cd0a393\", \"text\": \"Eat before 12\", \"_author\": \"5c02449eac3baf291808b037\", \"__v\": 0 }, { \"completed\": false, \"completedAt\": null, \"_id\": \"5c1ba02efe8f70360cd0a394\", \"text\": \"Test todo\", \"_author\": \"5c02449eac3baf291808b037\", \"__v\": 0 } ]\nMy Todo Class looks like this\nclass Todo {\n  final String text;\n  final String author;\n  final bool completed;\n  final DateTime completedAt;\n  final String id;\n\n  Todo(\n      {this.author = \"\",\n      this.id = \"\",\n      this.completed = false,\n      DateTime completedAt,\n      this.text = \"\"})\n      : completedAt = completedAt ?? DateTime.now();\n\n  factory Todo.fromJson(Map<String, dynamic> json) {\n    return Todo(\n      text: json['text'],\n      author: json['_author'],\n      completed: json['completed'],\n      completedAt: DateTime.fromMillisecondsSinceEpoch(json['completedAt']),\n      id: json['_id'],\n    );\n  }\n}\n\nI cant find a good way to parse it into a List and return it to a FutureBuilder.\nLogs\nI use VS Code, No Errors in the debug Console.\n\n[√] Flutter (Channel beta, v1.0.0, on Microsoft Windows [Version 10.0.17134.472], locale en-IN)\n    • Flutter version 1.0.0 at C:\\flutter\n    • Framework revision 5391447fae (3 weeks ago), 2018-11-29 19:41:26 -0800\n    • Engine revision 7375a0f414\n    • Dart version 2.1.0 (build 2.1.0-dev.9.4 f9ebf21297)\n\n[√] Android toolchain - develop for Android devices (Android SDK 28.0.3)\n    • Android SDK at C:\\Users\\lightyaer\\AppData\\Local\\Android\\Sdk\n    • Android NDK location not configured (optional; useful for native profiling support)\n    • Platform android-28, build-tools 28.0.3\n    • ANDROID_HOME = C:\\Users\\lightyaer\\AppData\\Local\\Android\\Sdk\n    • Java binary at: C:\\Program Files\\Android\\Android Studio\\jre\\bin\\java\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1136-b06)\n    • All Android licenses accepted.\n\n[√] Android Studio (version 3.2)\n    • Android Studio at C:\\Program Files\\Android\\Android Studio\n    X Flutter plugin not installed; this adds Flutter specific functionality.\n    X Dart plugin not installed; this adds Dart specific functionality.\n    • Java version OpenJDK Runtime Environment (build 1.8.0_152-release-1136-b06)\n\n[√] VS Code (version 1.30.1)\n    • VS Code at C:\\Users\\lightyaer\\AppData\\Local\\Programs\\Microsoft VS Code\n    • Flutter extension version 2.21.1\n\n[√] Connected device (1 available)\n    • Android SDK built for x86 • emulator-5554 • android-x86 • Android 9 (API 28) (emulator)\n\n• No issues found!",
      "title": "Trouble parsing Json Array "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2780,
    "text": "Request: Left Close/Max/Min buttons variant for custom title barI really like the new title bar style that can be activated by \"editor.titleBarStyle\": \"custom\". However, as a long time Ubuntu user I'm accustomed to my window close/max/min controls being on the left hand side of the title bar. Is is possbible to have another member in the titleBarStyle enum \"custom-left\" or something named similarly to have the controls render on the far left rather than the far right in the order close, minimize, maximize(restore)?\nBelow, the native menu for VS Code (I do not mind the square buttons of the ciustom style as they fit well with the VS brand):",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "0",
      "number": "53812",
      "pretext": "I really like the new title bar style that can be activated by \"editor.titleBarStyle\": \"custom\". However, as a long time Ubuntu user I'm accustomed to my window close/max/min controls being on the left hand side of the title bar. Is is possbible to have another member in the titleBarStyle enum \"custom-left\" or something named similarly to have the controls render on the far left rather than the far right in the order close, minimize, maximize(restore)?\nBelow, the native menu for VS Code (I do not mind the square buttons of the ciustom style as they fit well with the VS brand):",
      "title": "Request: Left Close/Max/Min buttons variant for custom title bar"
    },
    "annotation_approver": null
  },
  {
    "id": 2781,
    "text": "When relaunch vscode after aborted vscode update then Uncaught Exception thrownVSCode Version: 1.17.0.0\nOS Version: Microsoft Windows [Version 10.0.14393]\n\nSteps to Reproduce:\n\nUpdate vscode\nWhile updating launch a second vscode instance\nAbort the Installation on Dialog\nStart vscode again\n\n\nReproduces without extensions: Yes",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "1",
      "number": "37058",
      "pretext": "VSCode Version: 1.17.0.0\nOS Version: Microsoft Windows [Version 10.0.14393]\n\nSteps to Reproduce:\n\nUpdate vscode\nWhile updating launch a second vscode instance\nAbort the Installation on Dialog\nStart vscode again\n\n\nReproduces without extensions: Yes",
      "title": "When relaunch vscode after aborted vscode update then Uncaught Exception thrown"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2782,
    "text": "Integratel Terminal doesn't find commands from PATHVSCode Version:\nVersion 1.3.0\nCommit e724f26\nDate 2016-07-07T16:56:12.476Z\nShell 0.37.6\nRenderer 49.0.2623.75\nNode 5.10.0\nOS Version:\nWindows 7 x64 SP1\n\nI have installed Git for Windows with both the git and Unix commands in the PATH.\nVSCode doesn't seem to find the Unix commands.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "2",
      "number": "8966",
      "pretext": "VSCode Version:\nVersion 1.3.0\nCommit e724f26\nDate 2016-07-07T16:56:12.476Z\nShell 0.37.6\nRenderer 49.0.2623.75\nNode 5.10.0\nOS Version:\nWindows 7 x64 SP1\n\nI have installed Git for Windows with both the git and Unix commands in the PATH.\nVSCode doesn't seem to find the Unix commands.",
      "title": "Integratel Terminal doesn't find commands from PATH"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2783,
    "text": "Color picker wont open when hovering over color box or moving between box and valueVSCode Version: Code - Insiders 1.20.0-insider (8f015e5, 2017-12-21T08:34:32.639Z)\nOS Version: Windows_NT x64 10.0.15063\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nstaticserver\ngao\n0.0.5\n\n\n\n\nSteps to Reproduce:\njust mention a color in hex format and hover arrow of the hex code, this should normally open a in-biult color picker. This is not working only in the insider version, color picker works perfectly in Visual Code.\nhowever, whenever the color picker would appear, it would quickly close itself and very rarely it would work properly.\nThis issue is since 2 previous versions of VSinsider\nReproduces without extensions: Yes",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "3",
      "number": "40634",
      "pretext": "VSCode Version: Code - Insiders 1.20.0-insider (8f015e5, 2017-12-21T08:34:32.639Z)\nOS Version: Windows_NT x64 10.0.15063\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nstaticserver\ngao\n0.0.5\n\n\n\n\nSteps to Reproduce:\njust mention a color in hex format and hover arrow of the hex code, this should normally open a in-biult color picker. This is not working only in the insider version, color picker works perfectly in Visual Code.\nhowever, whenever the color picker would appear, it would quickly close itself and very rarely it would work properly.\nThis issue is since 2 previous versions of VSinsider\nReproduces without extensions: Yes",
      "title": "Color picker wont open when hovering over color box or moving between box and value"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2784,
    "text": "Icons are fuzzy in the UISince the update to 1.13 I am seeing fuzzy icons in the UI of VSCode. I am using the light theme. This is unrelated to the theme I select. Switching between light and dark themes doesn't fix the problem.\n\n\nVSCode Version: Code 1.13.0 (376c52b, 2017-06-08T16:34:53.678Z)\nOS Version: Darwin x64 16.6.0\n\nSteps to Reproduce:\n\nOpen VS Code\nOpen a code file",
    "annotations": [{ "label": 145, "user": 3 }],
    "meta": {
      "": "4",
      "number": "28693",
      "pretext": "Since the update to 1.13 I am seeing fuzzy icons in the UI of VSCode. I am using the light theme. This is unrelated to the theme I select. Switching between light and dark themes doesn't fix the problem.\n\n\nVSCode Version: Code 1.13.0 (376c52b, 2017-06-08T16:34:53.678Z)\nOS Version: Darwin x64 16.6.0\n\nSteps to Reproduce:\n\nOpen VS Code\nOpen a code file",
      "title": "Icons are fuzzy in the UI"
    },
    "annotation_approver": null
  },
  {
    "id": 2785,
    "text": "Test: Automated Smoke TestRef: #25291\nComplexity: 2\nOS:\n\n Windows @alexandrudima\n OS X @weinand\n Linux @Tyriar\n\nAs part of engineering work we have automated major part of our smoke test. To ensure its stability, it is important to run it on different machines.\nRunning automated test\nPrerequisites\n\nUpdate stable to the latest version.\nClose all VS Code instances that will be used in a test (e.g. latest insiders, stable) not to have any interference with the running test that will spawn them itself.\n\nOnce the test is running, please do not interfere with it by doing selection or focusing anywhere within spawned test VS Code instance.\nApproximate running time\n10 minutes minimum (the more failures it has, the more it runs due to adaptive retry strategy of the waiting time).\nProcedure\n\nRun git checkout michelkaporin/smoketest in vscode repository. Currently the code lives on my branch only.\ncd test/smoke\nnpm install\nnpm test -- --latest \"path/to/binary\" --stable  \"path/to/binary\", where latest argument is the path to VS Code executable to conduct testing, and stable is previous stable version (in order to run 'Data Migration' tests). If the latter argument is ommited, 'Data Migration' tests won't run.\n\nExample commands:\nnpm test -- --latest \"C:\\Program Files (x86)\\Microsoft VS Code Insiders\\Code - Insiders.exe\" --stable \"C:\\Program Files (x86)\\Microsoft VS Code\\Code.exe\"\nor\nnpm test -- --latest \"/Applications/Visual Studio Code - Insiders.app/Contents/MacOS/Electron\" --stable \"/Applications/Visual Studio Code.app/Contents/MacOS/Electron\"\nKnown problems\n\n**#27452 bug results in very many failing tests on OS X, once that is fixed, all should pass.\nSpectron spawns multiple cmd windows on Windows OS. This leads to VS Code window to be overlapped by another one. To look at what the test does you need to click on the VS Code title bar to focus on it in every test. (electron-userland/spectron#60)\n\nIf you get failing tests, please dump the log with failed error messages here.\nAny feedback is appreciated 👍",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "5",
      "number": "27456",
      "pretext": "Ref: #25291\nComplexity: 2\nOS:\n\n Windows @alexandrudima\n OS X @weinand\n Linux @Tyriar\n\nAs part of engineering work we have automated major part of our smoke test. To ensure its stability, it is important to run it on different machines.\nRunning automated test\nPrerequisites\n\nUpdate stable to the latest version.\nClose all VS Code instances that will be used in a test (e.g. latest insiders, stable) not to have any interference with the running test that will spawn them itself.\n\nOnce the test is running, please do not interfere with it by doing selection or focusing anywhere within spawned test VS Code instance.\nApproximate running time\n10 minutes minimum (the more failures it has, the more it runs due to adaptive retry strategy of the waiting time).\nProcedure\n\nRun git checkout michelkaporin/smoketest in vscode repository. Currently the code lives on my branch only.\ncd test/smoke\nnpm install\nnpm test -- --latest \"path/to/binary\" --stable  \"path/to/binary\", where latest argument is the path to VS Code executable to conduct testing, and stable is previous stable version (in order to run 'Data Migration' tests). If the latter argument is ommited, 'Data Migration' tests won't run.\n\nExample commands:\nnpm test -- --latest \"C:\\Program Files (x86)\\Microsoft VS Code Insiders\\Code - Insiders.exe\" --stable \"C:\\Program Files (x86)\\Microsoft VS Code\\Code.exe\"\nor\nnpm test -- --latest \"/Applications/Visual Studio Code - Insiders.app/Contents/MacOS/Electron\" --stable \"/Applications/Visual Studio Code.app/Contents/MacOS/Electron\"\nKnown problems\n\n**#27452 bug results in very many failing tests on OS X, once that is fixed, all should pass.\nSpectron spawns multiple cmd windows on Windows OS. This leads to VS Code window to be overlapped by another one. To look at what the test does you need to click on the VS Code title bar to focus on it in every test. (electron-userland/spectron#60)\n\nIf you get failing tests, please dump the log with failed error messages here.\nAny feedback is appreciated 👍",
      "title": "Test: Automated Smoke Test"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2786,
    "text": "Inconsistency between unsaved file dialog and unsaved workspace dialogIssue Type: Bug\nWhen closing files with unsaved changes, a dialog open with the three possible actions: \"Don't save\", \"Cancel\", \"Save\" in that order.\nWhen closing an unsaved workspace, a similar dialog appear with actions \"Save\", \"Cancel\", \"Don't save\" in that order.\nThat feel inconsistent and could cause confusion. The order should probably be the same in every dialog.\nVS Code version: Code 1.42.0 (ae08d54, 2020-02-06T10:51:23.649Z)\nOS version: Linux x64 5.0.0-38-generic\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i5-7260U CPU @ 2.20GHz (4 x 3340)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmetal: disabled_offmultiple_raster_threads: enabled_onoop_rasterization: disabled_offprotected_video_decode: unavailable_offrasterization: disabled_softwareskia_renderer: disabled_offsurface_control: disabled_offsurface_synchronization: enabled_onvideo_decode: unavailable_offviz_display_compositor: enabled_onviz_hit_test_surface_layer: disabled_offwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\n1, 1, 1\n\n\nMemory (System)\n11.62GB (0.87GB free)\n\n\nProcess Argv\nremarques.md --no-sandbox\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (14)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nnpm-intellisense\nchr\n1.3.0\n\n\npath-intellisense\nchr\n1.4.2\n\n\nvscode-eslint\ndba\n2.0.15\n\n\nvscode-npm-script\neg2\n0.3.11\n\n\nsearch-node-modules\njas\n1.3.0\n\n\nvscode-exec-node\nmir\n0.5.1\n\n\nvscode-language-pack-fr\nMS-\n1.42.2\n\n\ncpptools\nms-\n0.26.3\n\n\ndebugger-for-chrome\nmsj\n4.12.6\n\n\nvscode-npm-scripts\ntra\n0.2.1\n\n\nvim\nvsc\n1.12.4\n\n\nnodejs-extension-pack\nwad\n0.1.9\n\n\nvscode-todo-highlight\nway\n1.0.4\n\n\nJavaScriptSnippets\nxab\n1.7.2",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "6",
      "number": "90455",
      "pretext": "Issue Type: Bug\nWhen closing files with unsaved changes, a dialog open with the three possible actions: \"Don't save\", \"Cancel\", \"Save\" in that order.\nWhen closing an unsaved workspace, a similar dialog appear with actions \"Save\", \"Cancel\", \"Don't save\" in that order.\nThat feel inconsistent and could cause confusion. The order should probably be the same in every dialog.\nVS Code version: Code 1.42.0 (ae08d54, 2020-02-06T10:51:23.649Z)\nOS version: Linux x64 5.0.0-38-generic\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i5-7260U CPU @ 2.20GHz (4 x 3340)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmetal: disabled_offmultiple_raster_threads: enabled_onoop_rasterization: disabled_offprotected_video_decode: unavailable_offrasterization: disabled_softwareskia_renderer: disabled_offsurface_control: disabled_offsurface_synchronization: enabled_onvideo_decode: unavailable_offviz_display_compositor: enabled_onviz_hit_test_surface_layer: disabled_offwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\n1, 1, 1\n\n\nMemory (System)\n11.62GB (0.87GB free)\n\n\nProcess Argv\nremarques.md --no-sandbox\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (14)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nnpm-intellisense\nchr\n1.3.0\n\n\npath-intellisense\nchr\n1.4.2\n\n\nvscode-eslint\ndba\n2.0.15\n\n\nvscode-npm-script\neg2\n0.3.11\n\n\nsearch-node-modules\njas\n1.3.0\n\n\nvscode-exec-node\nmir\n0.5.1\n\n\nvscode-language-pack-fr\nMS-\n1.42.2\n\n\ncpptools\nms-\n0.26.3\n\n\ndebugger-for-chrome\nmsj\n4.12.6\n\n\nvscode-npm-scripts\ntra\n0.2.1\n\n\nvim\nvsc\n1.12.4\n\n\nnodejs-extension-pack\nwad\n0.1.9\n\n\nvscode-todo-highlight\nway\n1.0.4\n\n\nJavaScriptSnippets\nxab\n1.7.2",
      "title": "Inconsistency between unsaved file dialog and unsaved workspace dialog"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2787,
    "text": "terminal Issue Type: Bug\ni open terminal, but  its look like is not work !\nVS Code version: Code 1.27.2 (f46c4c4, 2018-09-12T16:17:45.060Z)\nOS version: Windows_NT x64 10.0.17763\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz (8 x 2592)\n\n\nGPU Status\n2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled\n\n\nMemory (System)\n15.88GB (8.52GB free)\n\n\nProcess Argv\nG:\\Program Files\\Microsoft VS Code\\Code.exe\n\n\nScreen Reader\nno\n\n\nVM\n67%\n\n\n\nExtensions (15)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nproject-manager\nale\n9.0.1\n\n\nvscode-svgviewer\ncss\n1.4.6\n\n\nvscode-eslint\ndba\n1.6.0\n\n\nxml\nDot\n2.3.2\n\n\ngitlens\neam\n8.5.6\n\n\nprettier-vscode\nesb\n1.6.1\n\n\nvue-format\nfeb\n0.1.2\n\n\ngit-project-manager\nfel\n1.7.1\n\n\nvscode-attrs-sorter\nmrm\n2.1.0\n\n\ncpptools\nms-\n0.19.0\n\n\nvetur\noct\n0.13.0\n\n\nvscode-icons\nrob\n7.27.0\n\n\npartial-diff\nryu\n1.4.0\n\n\nvim\nvsc\n0.16.6\n\n\ngitblame\nwad\n2.4.4",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "7",
      "number": "60013",
      "pretext": "Issue Type: Bug\ni open terminal, but  its look like is not work !\nVS Code version: Code 1.27.2 (f46c4c4, 2018-09-12T16:17:45.060Z)\nOS version: Windows_NT x64 10.0.17763\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz (8 x 2592)\n\n\nGPU Status\n2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled\n\n\nMemory (System)\n15.88GB (8.52GB free)\n\n\nProcess Argv\nG:\\Program Files\\Microsoft VS Code\\Code.exe\n\n\nScreen Reader\nno\n\n\nVM\n67%\n\n\n\nExtensions (15)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nproject-manager\nale\n9.0.1\n\n\nvscode-svgviewer\ncss\n1.4.6\n\n\nvscode-eslint\ndba\n1.6.0\n\n\nxml\nDot\n2.3.2\n\n\ngitlens\neam\n8.5.6\n\n\nprettier-vscode\nesb\n1.6.1\n\n\nvue-format\nfeb\n0.1.2\n\n\ngit-project-manager\nfel\n1.7.1\n\n\nvscode-attrs-sorter\nmrm\n2.1.0\n\n\ncpptools\nms-\n0.19.0\n\n\nvetur\noct\n0.13.0\n\n\nvscode-icons\nrob\n7.27.0\n\n\npartial-diff\nryu\n1.4.0\n\n\nvim\nvsc\n0.16.6\n\n\ngitblame\nwad\n2.4.4",
      "title": "terminal "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2788,
    "text": "You can set themes that are disabled in extensions panelVSCode Version: insider\nOS Version: Windows 10 64bit\n\nThemes appear in the extensions panel and can be disabled just like normal extensions.\nI think disabled themes shouldn't appear when running the preferences: Color Theme command. Right now you may set a theme that's disabled wihch is a bit weird.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "8",
      "number": "28823",
      "pretext": "VSCode Version: insider\nOS Version: Windows 10 64bit\n\nThemes appear in the extensions panel and can be disabled just like normal extensions.\nI think disabled themes shouldn't appear when running the preferences: Color Theme command. Right now you may set a theme that's disabled wihch is a bit weird.",
      "title": "You can set themes that are disabled in extensions panel"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2789,
    "text": "The incorrect IME position when the first character is enteredVSCode Version: Code 1.15.0 (8b95971, 2017-08-09T20:06:21.685Z)\nOS Version: Darwin x64 17.0.0\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nmaterial-icon-theme\nPKi\n2.1.0\n\n\nvscode-docker\nPet\n0.0.16\n\n\ncode-settings-sync\nSha\n2.8.2\n\n\ngo\nTwe\n0.0.1\n\n\nhtml-css-class-completion\nZig\n1.8.0\n\n\nvscode-color\nans\n0.4.5\n\n\nvscode-eslint\ndba\n1.2.11\n\n\npython\ndon\n0.7.0\n\n\nvscode-babel-coloring\ndza\n0.0.4\n\n\ngitlens\neam\n4.3.3\n\n\nvscode-html-css\necm\n0.1.7\n\n\ntslint\neg2\n0.17.0\n\n\nnim\nkos\n0.5.26\n\n\nGo\nluk\n0.6.63\n\n\nmarkdown-shortcuts\nmdi\n0.8.1\n\n\nprettify-json\nmoh\n0.0.3\n\n\ndebugger-for-chrome\nmsj\n3.1.7\n\n\ncolor-highlight\nnau\n2.3.0\n\n\nvetur\noct\n0.9.3\n\n\nproto\npet\n0.0.2\n\n\nRuby\nreb\n0.13.0\n\n\nPostCSS\nric\n1.0.1\n\n\nstylelint\nshi\n0.28.0\n\n\nbabelrc\nwad\n1.0.0\n\n\nvscode-import-cost\nwix\n2.0.0\n\n\nvscode-proto3\nzxh\n0.1.2\n\n\n\n(5 theme extensions excluded)\n\nSteps to Reproduce:\n\nSwitch to Chinese IME (e.g. Sougou / Baidu)\nSee the screenshot:\n\n\nI reproduced this problem in the monaco editor, I think it should be the text input implementation issue using textarea, did not find this problem in ace editor.\n\nReproduces without extensions: Yes",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "9",
      "number": "32275",
      "pretext": "VSCode Version: Code 1.15.0 (8b95971, 2017-08-09T20:06:21.685Z)\nOS Version: Darwin x64 17.0.0\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nmaterial-icon-theme\nPKi\n2.1.0\n\n\nvscode-docker\nPet\n0.0.16\n\n\ncode-settings-sync\nSha\n2.8.2\n\n\ngo\nTwe\n0.0.1\n\n\nhtml-css-class-completion\nZig\n1.8.0\n\n\nvscode-color\nans\n0.4.5\n\n\nvscode-eslint\ndba\n1.2.11\n\n\npython\ndon\n0.7.0\n\n\nvscode-babel-coloring\ndza\n0.0.4\n\n\ngitlens\neam\n4.3.3\n\n\nvscode-html-css\necm\n0.1.7\n\n\ntslint\neg2\n0.17.0\n\n\nnim\nkos\n0.5.26\n\n\nGo\nluk\n0.6.63\n\n\nmarkdown-shortcuts\nmdi\n0.8.1\n\n\nprettify-json\nmoh\n0.0.3\n\n\ndebugger-for-chrome\nmsj\n3.1.7\n\n\ncolor-highlight\nnau\n2.3.0\n\n\nvetur\noct\n0.9.3\n\n\nproto\npet\n0.0.2\n\n\nRuby\nreb\n0.13.0\n\n\nPostCSS\nric\n1.0.1\n\n\nstylelint\nshi\n0.28.0\n\n\nbabelrc\nwad\n1.0.0\n\n\nvscode-import-cost\nwix\n2.0.0\n\n\nvscode-proto3\nzxh\n0.1.2\n\n\n\n(5 theme extensions excluded)\n\nSteps to Reproduce:\n\nSwitch to Chinese IME (e.g. Sougou / Baidu)\nSee the screenshot:\n\n\nI reproduced this problem in the monaco editor, I think it should be the text input implementation issue using textarea, did not find this problem in ace editor.\n\nReproduces without extensions: Yes",
      "title": "The incorrect IME position when the first character is entered"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2790,
    "text": "Debugger attached. Waiting for the debugger to disconnect...VSCode Version:  1.27.2\nOS Version: Windows 10 Pro 64bit OS,x64-based processor\n\nSteps to Reproduce:\n\nStart Node Debugger\nError\nDebugger attached.\nWaiting for the debugger to disconnect...\n\nDoes this issue occur when all extensions are disabled?: Yes\nI have tried everything as follows  but nothing works->\n\nrestarted vs code\nrestarted my machine\nuninstall node,npm,vs code\n\nplease suggest some solution for this bug.Basically i'm debugging typescript file.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "10",
      "number": "58686",
      "pretext": "VSCode Version:  1.27.2\nOS Version: Windows 10 Pro 64bit OS,x64-based processor\n\nSteps to Reproduce:\n\nStart Node Debugger\nError\nDebugger attached.\nWaiting for the debugger to disconnect...\n\nDoes this issue occur when all extensions are disabled?: Yes\nI have tried everything as follows  but nothing works->\n\nrestarted vs code\nrestarted my machine\nuninstall node,npm,vs code\n\nplease suggest some solution for this bug.Basically i'm debugging typescript file.",
      "title": "Debugger attached. Waiting for the debugger to disconnect..."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2791,
    "text": "Local editor settings not being overridden by ESLint settingsVSCode Version: Version 1.24.0 (1.24.0)\nOS Version: Mac High Sierra 10.13.4\nESLint Extension: 1.4.12\n\nSteps to Reproduce:\n\nThe default behavior for VSCode is to add spaces to objects:\n\nconst x = { a: 123 };\n\n\nI have a .eslintrc config setup with the rule \"object-curly-spacing\": [\"error\", \"never\"]. Linting occurs and reports the default spacing behavior as an error as expected.\nThis behavior can be overridden in the VSCode settings via \"javascript.format.insertSpaceAfterOpeningAndBeforeClosingNonemptyBraces\": false\nI was expecting that the default behavior of VSCode would be overridden by the ESLint rules when formatting code.\n\n\nDoes this issue occur when all extensions are disabled?: No. The ESLint extension must be installed for VSCode to utilize the .eslintrc rules.\nThis may be an issue correctable in the ESLint extension. If so I will move this issue to that project.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "11",
      "number": "51539",
      "pretext": "VSCode Version: Version 1.24.0 (1.24.0)\nOS Version: Mac High Sierra 10.13.4\nESLint Extension: 1.4.12\n\nSteps to Reproduce:\n\nThe default behavior for VSCode is to add spaces to objects:\n\nconst x = { a: 123 };\n\n\nI have a .eslintrc config setup with the rule \"object-curly-spacing\": [\"error\", \"never\"]. Linting occurs and reports the default spacing behavior as an error as expected.\nThis behavior can be overridden in the VSCode settings via \"javascript.format.insertSpaceAfterOpeningAndBeforeClosingNonemptyBraces\": false\nI was expecting that the default behavior of VSCode would be overridden by the ESLint rules when formatting code.\n\n\nDoes this issue occur when all extensions are disabled?: No. The ESLint extension must be installed for VSCode to utilize the .eslintrc rules.\nThis may be an issue correctable in the ESLint extension. If so I will move this issue to that project.",
      "title": "Local editor settings not being overridden by ESLint settings"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2792,
    "text": "Incorrect error in TypeScript codeI'm using version df35236\nFor this code I get an incorrect error.\n  /**\n   * Removes all the children from `<div class=\"children\"></div>` element\n   *\n  */\n  removeChildren() {\n    if (this.element.querySelector('div.children')) {\n      this.element.removeChild(this.element.querySelector('div.children'));\n    }\n  }\nTypeScript compiler does not complain for this code but VSCode throws this error:",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "12",
      "number": "1052",
      "pretext": "I'm using version df35236\nFor this code I get an incorrect error.\n  /**\n   * Removes all the children from `<div class=\"children\"></div>` element\n   *\n  */\n  removeChildren() {\n    if (this.element.querySelector('div.children')) {\n      this.element.removeChild(this.element.querySelector('div.children'));\n    }\n  }\nTypeScript compiler does not complain for this code but VSCode throws this error:",
      "title": "Incorrect error in TypeScript code"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2793,
    "text": "After update throwing : Couldn't find declaration for many modules It was working before.\nnow at import the module:\nimport { Validation } from 'bunnyjs/src/Validation';\n[ts]\nCould not find a declaration file for module 'bunnyjs/src/Validation'. 'c:/Dev/Notanet/frontend-env/cockpit/node_modules/bunnyjs/src/Validation.js' implicitly has an 'any' type.\nTry npm install @types/bunnyjs if it exists or add a new declaration (.d.ts) file containing declare module 'bunnyjs';",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "13",
      "number": "59547",
      "pretext": "It was working before.\nnow at import the module:\nimport { Validation } from 'bunnyjs/src/Validation';\n[ts]\nCould not find a declaration file for module 'bunnyjs/src/Validation'. 'c:/Dev/Notanet/frontend-env/cockpit/node_modules/bunnyjs/src/Validation.js' implicitly has an 'any' type.\nTry npm install @types/bunnyjs if it exists or add a new declaration (.d.ts) file containing declare module 'bunnyjs';",
      "title": "After update throwing : Couldn't find declaration for many modules "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2794,
    "text": "Cannot read property 'forEach' of nullIssue Id: 35acb833-bda7-37c3-14da-3ab8f360751bVersions - 1.28.0-insider (9/20/2018 2:13:44 PM)Stack TypeError: Cannot read property 'forEach' of null/vs/base/browser/ui/menu/menu.ts#158:13 (style)/vs/platform/theme/common/styler.ts#53:20 (call)/vs/base/common/event.ts#140:15 (fire)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#350:26 (applyTheme)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#295:17 (onComplete)/vs/base/common/winjs.base.js#1191:0 (_notify)/vs/base/common/winjs.base.js#867:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1057:0 (onComplete)/vs/base/common/winjs.base.js#1587:0 (then)/vs/base/common/winjs.base.js#762:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1073:0    at Object.g [as _notify] (out/vs/workbench/workbench.m",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "14",
      "number": "59227",
      "pretext": "Issue Id: 35acb833-bda7-37c3-14da-3ab8f360751bVersions - 1.28.0-insider (9/20/2018 2:13:44 PM)Stack TypeError: Cannot read property 'forEach' of null/vs/base/browser/ui/menu/menu.ts#158:13 (style)/vs/platform/theme/common/styler.ts#53:20 (call)/vs/base/common/event.ts#140:15 (fire)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#350:26 (applyTheme)/vs/workbench/services/themes/electron-browser/workbenchThemeService.ts#295:17 (onComplete)/vs/base/common/winjs.base.js#1191:0 (_notify)/vs/base/common/winjs.base.js#867:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1057:0 (onComplete)/vs/base/common/winjs.base.js#1587:0 (then)/vs/base/common/winjs.base.js#762:0 (enter)/vs/base/common/winjs.base.js#1089:0 (_run)/vs/base/common/winjs.base.js#1073:0    at Object.g [as _notify] (out/vs/workbench/workbench.m",
      "title": " Cannot read property 'forEach' of null"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2795,
    "text": "Multi root: revisit workspace file format/cc @sandy081",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "15",
      "number": "32946",
      "pretext": "/cc @sandy081",
      "title": "Multi root: revisit workspace file format"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2796,
    "text": "Show Preview of file while in Go to File... Feature Request\nIt would be awesome if when using Go to File... (Command-P) that a preview of the current highlighted file is show and as you up/down arrow key a different file, you see a preview of the current selected file.",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "16",
      "number": "31170",
      "pretext": "Feature Request\nIt would be awesome if when using Go to File... (Command-P) that a preview of the current highlighted file is show and as you up/down arrow key a different file, you see a preview of the current selected file.",
      "title": "Show Preview of file while in Go to File... "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2797,
    "text": "Git version control can't interact with repos that are already local on the machine.VSCode Version: Code 1.11.2 (6eaebe3, 2017-04-13T07:56:42.517Z)\nOS Version: Darwin x64 16.6.0\nExtensions:\n\n\n\n\nExtension\nAuthor\nVersion\n\n\n\n\npython\ndonjayamanne\n0.6.3\n\n\ncpptools\nms-vscode\n0.11.0\n\n\nvscode-arduino\nvsciot-vscode\n0.1.2\n\n\n\n\nSteps to Reproduce:\n\nopen VS Code\nopen file from git repo already local on machine\ntry to commit from git version control panel\nfails\nsad day",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "17",
      "number": "25704",
      "pretext": "VSCode Version: Code 1.11.2 (6eaebe3, 2017-04-13T07:56:42.517Z)\nOS Version: Darwin x64 16.6.0\nExtensions:\n\n\n\n\nExtension\nAuthor\nVersion\n\n\n\n\npython\ndonjayamanne\n0.6.3\n\n\ncpptools\nms-vscode\n0.11.0\n\n\nvscode-arduino\nvsciot-vscode\n0.1.2\n\n\n\n\nSteps to Reproduce:\n\nopen VS Code\nopen file from git repo already local on machine\ntry to commit from git version control panel\nfails\nsad day",
      "title": "Git version control can't interact with repos that are already local on the machine."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2798,
    "text": "can we go back please the new logo is so ugly!",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "18",
      "number": "39462",
      "pretext": "",
      "title": "can we go back please the new logo is so ugly!"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2799,
    "text": "Cannot click on nodejs built-in modulesTesting #26203\nHave a js file with the following:\nfunction fib(n) {\n    if (n === 1 || n === 0) {\n        throw new Error('Unexpected');\n    }\n    return fib(n - 1) + fib(n - 2);\n}\nfib(3)\n\nStart debugging (F5)\nThe exception widget shows up at the correct location\nThe module.js is clickable but it tries to open a file called module.js from the workspace instead of opening the nodejs built-in module\nClicking on module.js from the call stack works correctly, so perhaps the debug adapter needs to be asked about how to resolve links in the error stackframe, and only if it doesn't know how to do it, the default should be to fallback to trying to open files?\n\n\nOther use-cases would be debugging a remote target where the error stack trace would contain the file paths as installed on the remote machine and these file paths would need to be translated according to the debug configuration to the local filesystem, or otherwise opened in readonly mode by fetching their sources from nodejs\nfyi @weinand @isidorn",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "19",
      "number": "27712",
      "pretext": "Testing #26203\nHave a js file with the following:\nfunction fib(n) {\n    if (n === 1 || n === 0) {\n        throw new Error('Unexpected');\n    }\n    return fib(n - 1) + fib(n - 2);\n}\nfib(3)\n\nStart debugging (F5)\nThe exception widget shows up at the correct location\nThe module.js is clickable but it tries to open a file called module.js from the workspace instead of opening the nodejs built-in module\nClicking on module.js from the call stack works correctly, so perhaps the debug adapter needs to be asked about how to resolve links in the error stackframe, and only if it doesn't know how to do it, the default should be to fallback to trying to open files?\n\n\nOther use-cases would be debugging a remote target where the error stack trace would contain the file paths as installed on the remote machine and these file paths would need to be translated according to the debug configuration to the local filesystem, or otherwise opened in readonly mode by fetching their sources from nodejs\nfyi @weinand @isidorn",
      "title": "Cannot click on nodejs built-in modules"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2800,
    "text": "Terminal sidebar close button not visibleVSCode Version: Code 1.18.0 (dcee220, 2017-11-08T21:19:36.079Z)\nOS Version: Windows_NT ia32 10.0.15063\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nhtml-snippets\nabu\n0.1.0\n\n\ndjango-snippets\nbib\n1.1.0\n\n\nvscode-styled-jsx\nbla\n0.1.1\n\n\nnpm-intellisense\nchr\n1.3.0\n\n\npath-intellisense\nchr\n1.4.2\n\n\ngitignore\ncod\n0.5.0\n\n\nvscode-eslint\ndba\n1.4.3\n\n\ngitlens\neam\n6.0.0\n\n\ntslint\neg2\n1.0.16\n\n\nvscode-npm-script\neg2\n0.3.3\n\n\nvsc-material-theme\nEqu\n1.1.1\n\n\nprettier-vscode\nesb\n0.24.0\n\n\nphp-debug\nfel\n1.11.1\n\n\nphp-intellisense\nfel\n1.5.4\n\n\nbeautify\nHoo\n1.1.1\n\n\nintellij-idea-keybindings\nk--\n0.2.16\n\n\nvscode-github\nKni\n0.23.0\n\n\nMagicPython\nmag\n1.0.12\n\n\nKotlin\nmat\n1.3.0\n\n\nHTMLHint\nmka\n0.4.0\n\n\nvscode-apache\nmrm\n1.1.1\n\n\nvscode-attrs-sorter\nmrm\n2.1.0\n\n\nvscode-jade-snippets\nmrm\n1.0.1\n\n\nvscode-pugbeautify\nmrm\n1.0.2\n\n\nvscode-puglint\nmrm\n2.3.0\n\n\nvscode-scss\nmrm\n0.6.2\n\n\nvscode-stylefmt\nmrm\n2.5.0\n\n\npython\nms-\n0.8.0\n\n\ncpptools\nms-\n0.14.2\n\n\ncsharp\nms-\n1.13.0\n\n\nPowerShell\nms-\n1.5.0\n\n\ntypescript-javascript-grammar\nms-\n0.0.24\n\n\nmaterial-icon-theme\nPKi\n2.2.4\n\n\nvscode-template-literal-editor\npli\n0.8.4\n\n\njava\nred\n0.14.0\n\n\nvscode-icons\nrob\n7.17.0\n\n\nprettier-eslint-vscode\nRob\n0.7.1\n\n\ncode-settings-sync\nSha\n2.8.5\n\n\nshader\nsle\n1.1.2\n\n\ntwig\nwha\n1.0.2\n\n\njinja\nwho\n0.0.8\n\n\nReactSnippets\nxab\n1.4.0\n\n\n\n\nSteps to Reproduce:\n\nPut the terminal panel to the sidebar.\nMake that window smaller.\nClose button is not visible, and not reachable in any way.\n\nAs you see here, the 'X' button should be in the upper right.\n\nReproduces without extensions: Yes",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "20",
      "number": "38009",
      "pretext": "VSCode Version: Code 1.18.0 (dcee220, 2017-11-08T21:19:36.079Z)\nOS Version: Windows_NT ia32 10.0.15063\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nhtml-snippets\nabu\n0.1.0\n\n\ndjango-snippets\nbib\n1.1.0\n\n\nvscode-styled-jsx\nbla\n0.1.1\n\n\nnpm-intellisense\nchr\n1.3.0\n\n\npath-intellisense\nchr\n1.4.2\n\n\ngitignore\ncod\n0.5.0\n\n\nvscode-eslint\ndba\n1.4.3\n\n\ngitlens\neam\n6.0.0\n\n\ntslint\neg2\n1.0.16\n\n\nvscode-npm-script\neg2\n0.3.3\n\n\nvsc-material-theme\nEqu\n1.1.1\n\n\nprettier-vscode\nesb\n0.24.0\n\n\nphp-debug\nfel\n1.11.1\n\n\nphp-intellisense\nfel\n1.5.4\n\n\nbeautify\nHoo\n1.1.1\n\n\nintellij-idea-keybindings\nk--\n0.2.16\n\n\nvscode-github\nKni\n0.23.0\n\n\nMagicPython\nmag\n1.0.12\n\n\nKotlin\nmat\n1.3.0\n\n\nHTMLHint\nmka\n0.4.0\n\n\nvscode-apache\nmrm\n1.1.1\n\n\nvscode-attrs-sorter\nmrm\n2.1.0\n\n\nvscode-jade-snippets\nmrm\n1.0.1\n\n\nvscode-pugbeautify\nmrm\n1.0.2\n\n\nvscode-puglint\nmrm\n2.3.0\n\n\nvscode-scss\nmrm\n0.6.2\n\n\nvscode-stylefmt\nmrm\n2.5.0\n\n\npython\nms-\n0.8.0\n\n\ncpptools\nms-\n0.14.2\n\n\ncsharp\nms-\n1.13.0\n\n\nPowerShell\nms-\n1.5.0\n\n\ntypescript-javascript-grammar\nms-\n0.0.24\n\n\nmaterial-icon-theme\nPKi\n2.2.4\n\n\nvscode-template-literal-editor\npli\n0.8.4\n\n\njava\nred\n0.14.0\n\n\nvscode-icons\nrob\n7.17.0\n\n\nprettier-eslint-vscode\nRob\n0.7.1\n\n\ncode-settings-sync\nSha\n2.8.5\n\n\nshader\nsle\n1.1.2\n\n\ntwig\nwha\n1.0.2\n\n\njinja\nwho\n0.0.8\n\n\nReactSnippets\nxab\n1.4.0\n\n\n\n\nSteps to Reproduce:\n\nPut the terminal panel to the sidebar.\nMake that window smaller.\nClose button is not visible, and not reachable in any way.\n\nAs you see here, the 'X' button should be in the upper right.\n\nReproduces without extensions: Yes",
      "title": "Terminal sidebar close button not visible"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2801,
    "text": "Do not expect app.getPath('userData') to be there before app.once('ready')There is a discussion going on in 08a1700 where @chrmarti saw an issue on startup for new users that never started Code before. I never saw this issue but the fix seems odd to me: we now suddenly create the directory (userData) via mkdirp that is owned by Electron/Chrome, which seems wrong. It should be created by the framework, not us.\nCan we move the getNodeCachedDataDir() into the app.once('ready') callback? That is the place where all code should go that assumes a certain directory structure to be present, not before.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "21",
      "number": "19662",
      "pretext": "There is a discussion going on in 08a1700 where @chrmarti saw an issue on startup for new users that never started Code before. I never saw this issue but the fix seems odd to me: we now suddenly create the directory (userData) via mkdirp that is owned by Electron/Chrome, which seems wrong. It should be created by the framework, not us.\nCan we move the getNodeCachedDataDir() into the app.once('ready') callback? That is the place where all code should go that assumes a certain directory structure to be present, not before.",
      "title": "Do not expect app.getPath('userData') to be there before app.once('ready')"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2802,
    "text": "run codeIssue Type: Bug\ncodes are not able to run the program\nVS Code version: Code 1.30.1 (dea8705, 2018-12-18T18:12:07.165Z)\nOS version: Windows_NT x64 6.1.7600\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-2670QM CPU @ 2.20GHz (8 x 2195)\n\n\nGPU Status\n2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: unavailable_softwarevideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: unavailable_off\n\n\nMemory (System)\n3.95GB (0.90GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (5)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\ngitlens\neam\n9.5.1\n\n\ntslint\neg2\n1.0.43\n\n\nauto-close-tag\nfor\n0.5.6\n\n\nLiveServer\nrit\n5.5.1\n\n\nautoimport\nste\n1.5.3",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "22",
      "number": "69295",
      "pretext": "Issue Type: Bug\ncodes are not able to run the program\nVS Code version: Code 1.30.1 (dea8705, 2018-12-18T18:12:07.165Z)\nOS version: Windows_NT x64 6.1.7600\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-2670QM CPU @ 2.20GHz (8 x 2195)\n\n\nGPU Status\n2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: unavailable_softwarevideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: unavailable_off\n\n\nMemory (System)\n3.95GB (0.90GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (5)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\ngitlens\neam\n9.5.1\n\n\ntslint\neg2\n1.0.43\n\n\nauto-close-tag\nfor\n0.5.6\n\n\nLiveServer\nrit\n5.5.1\n\n\nautoimport\nste\n1.5.3",
      "title": "run code"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2803,
    "text": "Go to definition (F12) doesn't work in node_modules with TSVSCode Version: 1.20.1\nOS Version: Windows 7\n\nSteps to Reproduce:\n\nPress F12 on an import in a node_module file\n\nBehaviour:\nGoes to the Typescript definition same as clicking \"Go to Type Definition\".\nExpected:\nShould go to the implementation.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "23",
      "number": "45409",
      "pretext": "VSCode Version: 1.20.1\nOS Version: Windows 7\n\nSteps to Reproduce:\n\nPress F12 on an import in a node_module file\n\nBehaviour:\nGoes to the Typescript definition same as clicking \"Go to Type Definition\".\nExpected:\nShould go to the implementation.",
      "title": "Go to definition (F12) doesn't work in node_modules with TS"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2804,
    "text": "Horizontal split hides the line the cursor is on if it's below the midpoint of the viewIssue Type: Bug\nHave the current line positioned below the middle of the viewport. Create a horizontal split. The line with the cursor is now hidden in both views. This is super annoying. The line with the cursor should be visible in both views.\nhttps://cl.ly/14374e89477a\nVS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:31:32.854Z)\nOS version: Darwin x64 18.7.0\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz (8 x 2500)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledoop_rasterization: disabled_offprotected_video_decode: unavailable_offrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: enabledviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\n2, 2, 2\n\n\nMemory (System)\n16.00GB (1.01GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "24",
      "number": "81867",
      "pretext": "Issue Type: Bug\nHave the current line positioned below the middle of the viewport. Create a horizontal split. The line with the cursor is now hidden in both views. This is super annoying. The line with the cursor should be visible in both views.\nhttps://cl.ly/14374e89477a\nVS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:31:32.854Z)\nOS version: Darwin x64 18.7.0\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz (8 x 2500)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledoop_rasterization: disabled_offprotected_video_decode: unavailable_offrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: enabledviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\n2, 2, 2\n\n\nMemory (System)\n16.00GB (1.01GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%",
      "title": "Horizontal split hides the line the cursor is on if it's below the midpoint of the view"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2805,
    "text": "Extra pixels under status barThere are some extra pixels under the status bar. Increase the font size to see it more clearly. Also, resizing the window vertically will change the width of the extra pixels.\n\n\n\nVSCode Version: 1.35.0\nOS Version: Windows_NT x64 10.0.18362\n\nSteps to Reproduce:\n\nLaunch VS Code\n\n\nDoes this issue occur when all extensions are disabled?: Yes",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "25",
      "number": "75202",
      "pretext": "There are some extra pixels under the status bar. Increase the font size to see it more clearly. Also, resizing the window vertically will change the width of the extra pixels.\n\n\n\nVSCode Version: 1.35.0\nOS Version: Windows_NT x64 10.0.18362\n\nSteps to Reproduce:\n\nLaunch VS Code\n\n\nDoes this issue occur when all extensions are disabled?: Yes",
      "title": "Extra pixels under status bar"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2806,
    "text": "Git clone should use the new progress API",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "26",
      "number": "48692",
      "pretext": "",
      "title": "Git clone should use the new progress API"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2807,
    "text": "GIt: Remote Changes ShelfFor all the GREAT stuff in VSCode, I can't believe there isn't a \"Remote Changes\" shelf in the SCM view for Git repos (i.e. together with the \"Changes\" and \"Staged Changes\" shelves).  The 3rd party SVN extension has this feature, even the old dinosaur Eclipse has it.\nI was even more confused when I didn't find a duplicate in the issues.  Am I missing something?  If I am, could someone please let me know what do Git users do in VSCode (admittedly I am coming from CVS/SVN and am new to Git) to see remote changes? (without using the 3rd party plugin GitLens which is not easy to navigate with all the clutter).  I assume people just don't 'sync' or 'pull' blindly without knowing what changes are coming in?",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "27",
      "number": "72808",
      "pretext": "For all the GREAT stuff in VSCode, I can't believe there isn't a \"Remote Changes\" shelf in the SCM view for Git repos (i.e. together with the \"Changes\" and \"Staged Changes\" shelves).  The 3rd party SVN extension has this feature, even the old dinosaur Eclipse has it.\nI was even more confused when I didn't find a duplicate in the issues.  Am I missing something?  If I am, could someone please let me know what do Git users do in VSCode (admittedly I am coming from CVS/SVN and am new to Git) to see remote changes? (without using the 3rd party plugin GitLens which is not easy to navigate with all the clutter).  I assume people just don't 'sync' or 'pull' blindly without knowing what changes are coming in?",
      "title": "GIt: Remote Changes Shelf"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2808,
    "text": "Update test explorer beaker icon to match new UI I believe right now this is the icon that is being used by VS Code: https://github.com/microsoft/vscode/blob/master/src/vs/workbench/api/browser/media/test.svg\nIt'd be great if that could be updated to match the new UI.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "28",
      "number": "82123",
      "pretext": "I believe right now this is the icon that is being used by VS Code: https://github.com/microsoft/vscode/blob/master/src/vs/workbench/api/browser/media/test.svg\nIt'd be great if that could be updated to match the new UI.",
      "title": "Update test explorer beaker icon to match new UI "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2809,
    "text": "Make \"editor.scrollBeyondLastLine\" configurable beyond true or falseI'm the kind of person who always presses enter 5 times after running a shell command. I like having some space. But too much space leaves me feeling vulnerable, as if I was stranded in the middle of the ocean. This all or nothing approach with the \"editor.scrollBeyondLastLine\" option isn't working for me.\nCan this option be configurable by the line number? If I want to be able to scroll past the bottom by 5 lines, I should be able to set the value to 5. If I want to have the current behavior, I could set the value to true, or \"all\".",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "29",
      "number": "43294",
      "pretext": "I'm the kind of person who always presses enter 5 times after running a shell command. I like having some space. But too much space leaves me feeling vulnerable, as if I was stranded in the middle of the ocean. This all or nothing approach with the \"editor.scrollBeyondLastLine\" option isn't working for me.\nCan this option be configurable by the line number? If I want to be able to scroll past the bottom by 5 lines, I should be able to set the value to 5. If I want to have the current behavior, I could set the value to true, or \"all\".",
      "title": "Make \"editor.scrollBeyondLastLine\" configurable beyond true or false"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2810,
    "text": "\"Go to File...\" shows files in search.exclude\"Go to File...\" is essentially a search operation and should be be managed by search.exclude, or optionally an additional config, e.g. goto.exclude. Typical use-case is having Explorer reflect true contents of project (e.g. not hiding generated files), while other operations are for navigating files one will edit.\n\nVSCode Version: @latest\nOS Version: OS X\n\nSteps to Reproduce:\n\nInclude node_modules in search.exclude, but not in files.exclude in settings\ncmd + p\nSee excluded files appearing in results",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "30",
      "number": "33444",
      "pretext": "\"Go to File...\" is essentially a search operation and should be be managed by search.exclude, or optionally an additional config, e.g. goto.exclude. Typical use-case is having Explorer reflect true contents of project (e.g. not hiding generated files), while other operations are for navigating files one will edit.\n\nVSCode Version: @latest\nOS Version: OS X\n\nSteps to Reproduce:\n\nInclude node_modules in search.exclude, but not in files.exclude in settings\ncmd + p\nSee excluded files appearing in results",
      "title": "\"Go to File...\" shows files in search.exclude"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2811,
    "text": "Unable to open file with leading space in file name when passed as a parameter from command lineVSCode Version: 1.13.1\nOS Version: Win10 Pro\n\nSteps to Reproduce:\n\nCreate a file with leading space, e.g. \" my file.txt\"\nWhilst in the text file folder, pass the file name to VS Code from command line, e.g. \"c:\\Program Files (x86)\\Microsoft VS Code\\Code.exe\" \" my file.txt\"; VSCode will try to open file \"my file.txt\" (space removed).\nWhen doing the same with full file path, e.g. \"c:\\Program Files (x86)\\Microsoft VS Code\\Code.exe\" \"c:\\ my file.txt\"; VSCode will not open any file\n\nNote: When opening from within Code by File->Open, everything works fine\nReproduces without extensions: Yes",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "31",
      "number": "29764",
      "pretext": "VSCode Version: 1.13.1\nOS Version: Win10 Pro\n\nSteps to Reproduce:\n\nCreate a file with leading space, e.g. \" my file.txt\"\nWhilst in the text file folder, pass the file name to VS Code from command line, e.g. \"c:\\Program Files (x86)\\Microsoft VS Code\\Code.exe\" \" my file.txt\"; VSCode will try to open file \"my file.txt\" (space removed).\nWhen doing the same with full file path, e.g. \"c:\\Program Files (x86)\\Microsoft VS Code\\Code.exe\" \"c:\\ my file.txt\"; VSCode will not open any file\n\nNote: When opening from within Code by File->Open, everything works fine\nReproduces without extensions: Yes",
      "title": "Unable to open file with leading space in file name when passed as a parameter from command line"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2812,
    "text": "Closing script tags not colored not using ligaturesVSCode Version: Code - Insiders 1.12.0-insider (aa05dac, 2017-05-02T00:04:13.700Z)\nOS Version: Darwin x64 16.5.0\nExtensions:\n\n\n\n\nExtension\nAuthor\nVersion\n\n\n\n\nng-template\nAngular\n0.1.3\n\n\ncode-settings-sync\nShan\n2.6.2\n\n\nsort-lines\nTyriar\n1.2.0\n\n\nBookmarks\nalefragnani\n0.14.1\n\n\nproject-manager\nalefragnani\n0.15.1\n\n\npath-intellisense\nchristian-kohler\n1.2.0\n\n\nvscode-eslint\ndbaeumer\n1.2.8\n\n\nvscode-html-css\necmel\n0.1.2\n\n\ntslint\neg2\n0.12.0\n\n\nAngular2\njohnpapa\n2.2.3\n\n\nvscode-icon-theme\njtlowe\n1.5.0\n\n\ntheme-karyfoundation-themes\nkaryfoundation\n11.1.0\n\n\nHTMLHint\nmkaufman\n0.3.3\n\n\nvscode-autoprefixer\nmrmlnc\n2.0.0\n\n\nvscode-stylefmt\nmrmlnc\n2.3.0\n\n\nangular2-inline\nnatewallace\n0.0.17\n\n\ntypescript-hero\nrbbit\n0.12.0\n\n\nproject-snippets\nrebornix\n0.5.0\n\n\nstylelint\nshinnn\n0.24.0\n\n\ndarcula-extended\nsmlombardi\n3.3.2\n\n\nslime\nsmlombardi\n1.15.0\n\n\ntheme-tesla\nsmlombardi\n6.0.0\n\n\nchange-case\nwmaurer\n1.0.0\n\n\n\n\nSteps to Reproduce:\n\nin script tags, the opening angle bracket in closing </script> tags and no longer colored like other tag brackets:\n\n\nfor comparison, some non-script tags:\n\nFurthermore, you can see that this font, Fira Code, uses ligatures for the </ yet the script tags no longer are using them.\nBoth things happened in the current build.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "32",
      "number": "25790",
      "pretext": "VSCode Version: Code - Insiders 1.12.0-insider (aa05dac, 2017-05-02T00:04:13.700Z)\nOS Version: Darwin x64 16.5.0\nExtensions:\n\n\n\n\nExtension\nAuthor\nVersion\n\n\n\n\nng-template\nAngular\n0.1.3\n\n\ncode-settings-sync\nShan\n2.6.2\n\n\nsort-lines\nTyriar\n1.2.0\n\n\nBookmarks\nalefragnani\n0.14.1\n\n\nproject-manager\nalefragnani\n0.15.1\n\n\npath-intellisense\nchristian-kohler\n1.2.0\n\n\nvscode-eslint\ndbaeumer\n1.2.8\n\n\nvscode-html-css\necmel\n0.1.2\n\n\ntslint\neg2\n0.12.0\n\n\nAngular2\njohnpapa\n2.2.3\n\n\nvscode-icon-theme\njtlowe\n1.5.0\n\n\ntheme-karyfoundation-themes\nkaryfoundation\n11.1.0\n\n\nHTMLHint\nmkaufman\n0.3.3\n\n\nvscode-autoprefixer\nmrmlnc\n2.0.0\n\n\nvscode-stylefmt\nmrmlnc\n2.3.0\n\n\nangular2-inline\nnatewallace\n0.0.17\n\n\ntypescript-hero\nrbbit\n0.12.0\n\n\nproject-snippets\nrebornix\n0.5.0\n\n\nstylelint\nshinnn\n0.24.0\n\n\ndarcula-extended\nsmlombardi\n3.3.2\n\n\nslime\nsmlombardi\n1.15.0\n\n\ntheme-tesla\nsmlombardi\n6.0.0\n\n\nchange-case\nwmaurer\n1.0.0\n\n\n\n\nSteps to Reproduce:\n\nin script tags, the opening angle bracket in closing </script> tags and no longer colored like other tag brackets:\n\n\nfor comparison, some non-script tags:\n\nFurthermore, you can see that this font, Fira Code, uses ligatures for the </ yet the script tags no longer are using them.\nBoth things happened in the current build.",
      "title": "Closing script tags not colored not using ligatures"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2813,
    "text": "Update protocol document for CompletionItemKind and SymbolKind?The C/C++ extension has been following the protocol here to define our protocol messages (the bulk of our extension is written in C++) https://github.com/Microsoft/language-server-protocol/blob/master/protocol.md\nI stumbled on this thread #2628 and saw commits that looked like they changed the enum values of CompletionItemKind and SymbolKind.  So it appears that we are sending incorrect values back to VS Code now.\nI just wanted to double-check... are the enum values in VS Code now different than the ones in the protocol?",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "33",
      "number": "28203",
      "pretext": "The C/C++ extension has been following the protocol here to define our protocol messages (the bulk of our extension is written in C++) https://github.com/Microsoft/language-server-protocol/blob/master/protocol.md\nI stumbled on this thread #2628 and saw commits that looked like they changed the enum values of CompletionItemKind and SymbolKind.  So it appears that we are sending incorrect values back to VS Code now.\nI just wanted to double-check... are the enum values in VS Code now different than the ones in the protocol?",
      "title": "Update protocol document for CompletionItemKind and SymbolKind?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2814,
    "text": "[PHP] provideCompletionItems is not called for items inside of -> methods and also for variables starting with $Issue Type: Bug\nIf you register a registerCompletionItemProvider in an exptension for PHP, it is not always called and completion items are not provided in the following cases:\n\nVariable name starts with $\n\n arr_item = new ClassItem;\n a                         // Completion Item Provider called\n$arr_item = new ClassItem;\n$a                          // Completion Item Provider not called\n\nArrow functions\n\n arr_item = new ClassItem;\n arr_item.t          // Completion Item provider called for the keystroke t \n arr_item = new ClassItem;\n arr_item->t          // Completion Item provider not called for the keystroke t \nNOTE: The completion item provider is not registered with any trigger characters.\nVS Code version: Code 1.23.0 (7c7da59, 2018-05-03T15:23:14.634Z)\nOS version: Darwin x64 17.5.0\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz (8 x 2800)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledrasterization: enabledvideo_decode: enabledvideo_encode: enabledvpx_decode: enabledwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\n3, 3, 3\n\n\nMemory (System)\n16.00GB (0.02GB free)\n\n\nProcess Argv\n/Applications/Visual Studio Code.app/Contents/MacOS/Electron -psn_0_10988154\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (37)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nMochaSnippets\nAla\n0.0.1\n\n\nAll-Autocomplete\nAti\n0.0.12\n\n\nvscode-markdownlint\nDav\n0.16.0\n\n\nxml\nDot\n1.9.2\n\n\nEditorConfig\nEdi\n0.12.1\n\n\njquery-snippets\nHri\n1.0.0\n\n\nlatex-workshop\nJam\n5.4.0\n\n\nvscode-jest\nOrt\n2.7.2\n\n\ncode-settings-sync\nSha\n2.9.2\n\n\nquokka-vscode\nWal\n1.0.124\n\n\nhtml-css-class-completion\nZig\n1.17.1\n\n\njslint\najh\n1.2.1\n\n\ncopy-relative-path\nale\n0.0.2\n\n\nnpm-intellisense\nchr\n1.3.0\n\n\ngitignore\ncod\n0.5.0\n\n\nriot-tag\ncri\n0.1.7\n\n\nvscode-eslint\ndba\n1.4.9\n\n\ntslint\neg2\n1.0.28\n\n\nvscode-npm-script\neg2\n0.3.4\n\n\nphp-intellisense\nfel\n2.3.1\n\n\nauto-close-tag\nfor\n0.5.6\n\n\ncode-runner\nfor\n0.9.3\n\n\nginfuru-vscode-jekyll-syntax\ngin\n0.0.5\n\n\ndocthis\njoe\n0.6.0\n\n\nsublime-babel-vscode\njos\n0.2.10\n\n\nprettify-json\nmoh\n0.0.3\n\n\nextension-manifest-editor\nms-\n0.1.5\n\n\npython\nms-\n2018.4.0\n\n\ncpptools\nms-\n0.17.0\n\n\nsublime-keybindings\nms-\n4.0.0\n\n\nwordcount\nms-\n0.1.0\n\n\ndebugger-for-chrome\nmsj\n4.4.3\n\n\nelm\nsbr\n0.17.0\n\n\ncode-spell-checker\nstr\n1.6.10\n\n\nhtml-preview-vscode\ntht\n0.1.1\n\n\ncmake\ntwx\n0.0.17\n\n\nbetter-align\nwwm\n1.1.6",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "34",
      "number": "50004",
      "pretext": "Issue Type: Bug\nIf you register a registerCompletionItemProvider in an exptension for PHP, it is not always called and completion items are not provided in the following cases:\n\nVariable name starts with $\n\n arr_item = new ClassItem;\n a                         // Completion Item Provider called\n$arr_item = new ClassItem;\n$a                          // Completion Item Provider not called\n\nArrow functions\n\n arr_item = new ClassItem;\n arr_item.t          // Completion Item provider called for the keystroke t \n arr_item = new ClassItem;\n arr_item->t          // Completion Item provider not called for the keystroke t \nNOTE: The completion item provider is not registered with any trigger characters.\nVS Code version: Code 1.23.0 (7c7da59, 2018-05-03T15:23:14.634Z)\nOS version: Darwin x64 17.5.0\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz (8 x 2800)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledrasterization: enabledvideo_decode: enabledvideo_encode: enabledvpx_decode: enabledwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\n3, 3, 3\n\n\nMemory (System)\n16.00GB (0.02GB free)\n\n\nProcess Argv\n/Applications/Visual Studio Code.app/Contents/MacOS/Electron -psn_0_10988154\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (37)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nMochaSnippets\nAla\n0.0.1\n\n\nAll-Autocomplete\nAti\n0.0.12\n\n\nvscode-markdownlint\nDav\n0.16.0\n\n\nxml\nDot\n1.9.2\n\n\nEditorConfig\nEdi\n0.12.1\n\n\njquery-snippets\nHri\n1.0.0\n\n\nlatex-workshop\nJam\n5.4.0\n\n\nvscode-jest\nOrt\n2.7.2\n\n\ncode-settings-sync\nSha\n2.9.2\n\n\nquokka-vscode\nWal\n1.0.124\n\n\nhtml-css-class-completion\nZig\n1.17.1\n\n\njslint\najh\n1.2.1\n\n\ncopy-relative-path\nale\n0.0.2\n\n\nnpm-intellisense\nchr\n1.3.0\n\n\ngitignore\ncod\n0.5.0\n\n\nriot-tag\ncri\n0.1.7\n\n\nvscode-eslint\ndba\n1.4.9\n\n\ntslint\neg2\n1.0.28\n\n\nvscode-npm-script\neg2\n0.3.4\n\n\nphp-intellisense\nfel\n2.3.1\n\n\nauto-close-tag\nfor\n0.5.6\n\n\ncode-runner\nfor\n0.9.3\n\n\nginfuru-vscode-jekyll-syntax\ngin\n0.0.5\n\n\ndocthis\njoe\n0.6.0\n\n\nsublime-babel-vscode\njos\n0.2.10\n\n\nprettify-json\nmoh\n0.0.3\n\n\nextension-manifest-editor\nms-\n0.1.5\n\n\npython\nms-\n2018.4.0\n\n\ncpptools\nms-\n0.17.0\n\n\nsublime-keybindings\nms-\n4.0.0\n\n\nwordcount\nms-\n0.1.0\n\n\ndebugger-for-chrome\nmsj\n4.4.3\n\n\nelm\nsbr\n0.17.0\n\n\ncode-spell-checker\nstr\n1.6.10\n\n\nhtml-preview-vscode\ntht\n0.1.1\n\n\ncmake\ntwx\n0.0.17\n\n\nbetter-align\nwwm\n1.1.6",
      "title": "[PHP] provideCompletionItems is not called for items inside of -> methods and also for variables starting with $"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2815,
    "text": "UNITY PROJECT STOPS LOADED AFTER UPDATEIssue Type: Bug\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\Assembly-CSharp.csproj\nfail: OmniSharp.MSBuild.ProjectManager\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\Assembly-CSharp-firstpass.csproj\n[info]: OmniSharp.MSBuild.ProjectManager\nUpdate project: NavMeshComponents\nfail: OmniSharp.MSBuild.ProjectManager\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\Assembly-CSharp-Editor.csproj\nfail: OmniSharp.MSBuild.ProjectManager\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\Assembly-CSharp-Editor-firstpass.csproj\nfail: OmniSharp.MSBuild.ProjectManager\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\NavMeshComponentsEditor.csproj\nVS Code version: Code 1.27.1 (5944e81, 2018-09-06T09:21:18.328Z)\nOS version: Windows_NT x64 10.0.17134\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-5500U CPU @ 2.40GHz (4 x 2394)\n\n\nGPU Status\n2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled\n\n\nMemory (System)\n15.91GB (7.76GB free)\n\n\nProcess Argv\nC:\\Program Files\\Microsoft VS Code\\Code.exe C:\\work\\IDLE-CITY\\Project -r -g C:\\work\\IDLE-CITY\\Project\\Assets\\Scripts\\MainController.cs:1\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (8)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nbracket-pair-colorizer\nCoe\n1.0.59\n\n\ntransformer\ndak\n1.6.0\n\n\njson-tools\neri\n1.0.2\n\n\npython\nms-\n2018.8.0\n\n\ncsharp\nms-\n1.16.0\n\n\npython\ntht\n0.2.3\n\n\nunity-tools\nTob\n1.0.5\n\n\nunity-debug\nUni\n1.3.0",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "35",
      "number": "58429",
      "pretext": "Issue Type: Bug\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\Assembly-CSharp.csproj\nfail: OmniSharp.MSBuild.ProjectManager\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\Assembly-CSharp-firstpass.csproj\n[info]: OmniSharp.MSBuild.ProjectManager\nUpdate project: NavMeshComponents\nfail: OmniSharp.MSBuild.ProjectManager\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\Assembly-CSharp-Editor.csproj\nfail: OmniSharp.MSBuild.ProjectManager\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\Assembly-CSharp-Editor-firstpass.csproj\nfail: OmniSharp.MSBuild.ProjectManager\nAttemped to update project that is not loaded: c:\\work\\IDLE-CITY\\Project\\NavMeshComponentsEditor.csproj\nVS Code version: Code 1.27.1 (5944e81, 2018-09-06T09:21:18.328Z)\nOS version: Windows_NT x64 10.0.17134\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-5500U CPU @ 2.40GHz (4 x 2394)\n\n\nGPU Status\n2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwarerasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled\n\n\nMemory (System)\n15.91GB (7.76GB free)\n\n\nProcess Argv\nC:\\Program Files\\Microsoft VS Code\\Code.exe C:\\work\\IDLE-CITY\\Project -r -g C:\\work\\IDLE-CITY\\Project\\Assets\\Scripts\\MainController.cs:1\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (8)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nbracket-pair-colorizer\nCoe\n1.0.59\n\n\ntransformer\ndak\n1.6.0\n\n\njson-tools\neri\n1.0.2\n\n\npython\nms-\n2018.8.0\n\n\ncsharp\nms-\n1.16.0\n\n\npython\ntht\n0.2.3\n\n\nunity-tools\nTob\n1.0.5\n\n\nunity-debug\nUni\n1.3.0",
      "title": "UNITY PROJECT STOPS LOADED AFTER UPDATE"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2816,
    "text": "Encoding Files ProblemI have noticed a weird behavior, every time I create a new file, it is encoded in ANSI format, instead of the one I expect to have, which is \"files.encoding\": \"utf8\"\nAnybody has an idea on why vs is creating files on ANSI instead of UTF-8??\nI noticed this problem when I was trying to load a js file, it said the first character was invalid, but the code was fine... then I checked into the encoding and it was ANSI... saved the file as UTF-8 using notepad and the problem went away... PLEASE HELP",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "36",
      "number": "67571",
      "pretext": "I have noticed a weird behavior, every time I create a new file, it is encoded in ANSI format, instead of the one I expect to have, which is \"files.encoding\": \"utf8\"\nAnybody has an idea on why vs is creating files on ANSI instead of UTF-8??\nI noticed this problem when I was trying to load a js file, it said the first character was invalid, but the code was fine... then I checked into the encoding and it was ANSI... saved the file as UTF-8 using notepad and the problem went away... PLEASE HELP",
      "title": "Encoding Files Problem"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2817,
    "text": "Viewlets should focus the first item, not the whole treeEnvironment Details:\nVSCode Version : 1.24.0\nOS Version : Win10\nAdditional Details:\nMAS Violated : MAS 2.1.1\nTools Used : Keyboard\nRepro Steps:\n\nLaunch VS Code.\nNavigate to Activity Bar and select \"Explorer\"(Cntrl+Shift+E) button.\nNavigate to \"File Explorer\" treeview items using \"Tab\".\n\nActual:\nKeyboard focus moves to overall treeview items which is non-interactive. Then using downward arrow keys focus goes to items in treeview.\nExpected:\nKeyboard focus should not move to the items as a whole which is non-interactive. The focus should move to first item in the treeview.\nRecommendations:\nRemove tab-index from the outer  that contains all the treeview items.\nor,\nRefer below link which is repository of bug fixes code snippets:\nhttps://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx\nUser Impact:\nThe keyboard only users will move to non-interactive elements on the screen which will be time consuming to reach only interactive elements on the page.\nMAS Reference:\nhttps://microsoft.sharepoint.com/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}\nAttachment for Reference:\nA11y_VSCode_ViewExplorer_Keyboard_TreeViewItems.pptx\nDoes this issue occur when all extensions are disabled?: Yes",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "37",
      "number": "52000",
      "pretext": "Environment Details:\nVSCode Version : 1.24.0\nOS Version : Win10\nAdditional Details:\nMAS Violated : MAS 2.1.1\nTools Used : Keyboard\nRepro Steps:\n\nLaunch VS Code.\nNavigate to Activity Bar and select \"Explorer\"(Cntrl+Shift+E) button.\nNavigate to \"File Explorer\" treeview items using \"Tab\".\n\nActual:\nKeyboard focus moves to overall treeview items which is non-interactive. Then using downward arrow keys focus goes to items in treeview.\nExpected:\nKeyboard focus should not move to the items as a whole which is non-interactive. The focus should move to first item in the treeview.\nRecommendations:\nRemove tab-index from the outer  that contains all the treeview items.\nor,\nRefer below link which is repository of bug fixes code snippets:\nhttps://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx\nUser Impact:\nThe keyboard only users will move to non-interactive elements on the screen which will be time consuming to reach only interactive elements on the page.\nMAS Reference:\nhttps://microsoft.sharepoint.com/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}\nAttachment for Reference:\nA11y_VSCode_ViewExplorer_Keyboard_TreeViewItems.pptx\nDoes this issue occur when all extensions are disabled?: Yes",
      "title": "Viewlets should focus the first item, not the whole tree"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2818,
    "text": "[html] format not work correctly in html file when which has style property in tagVSCode Version:1.8.0\nOS Version:win 10\n\nSteps to Reproduce:\n\nIn html file,when a tag has style property.It's will influence format\n\nfor example\nwhen I press shift+alt+f to format code\n<div>\n    <p style=\"font-size: 16px;\">hello world</p>\n    <p>xxxholic</p>\n</div>\n\nwill become\n<div>\n    <p style=\"font-size: 15px;\">hello world</p>\n<p>xxxholic</p>\n</div>",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "38",
      "number": "17849",
      "pretext": "VSCode Version:1.8.0\nOS Version:win 10\n\nSteps to Reproduce:\n\nIn html file,when a tag has style property.It's will influence format\n\nfor example\nwhen I press shift+alt+f to format code\n<div>\n    <p style=\"font-size: 16px;\">hello world</p>\n    <p>xxxholic</p>\n</div>\n\nwill become\n<div>\n    <p style=\"font-size: 15px;\">hello world</p>\n<p>xxxholic</p>\n</div>",
      "title": "[html] format not work correctly in html file when which has style property in tag"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2819,
    "text": "Editor not rendering soft hyphenI ran into a case where a soft hyphen had snuck into a regex expression, yet the character was not visible in the editor. It doesn't seem to render in most places, including Stack Exchange; it has only shown up for me when pasted into a terminal window like cmd.exe or terminal.app. Full disclosure: I don't know if this is behaviour as intended or not. This could also be an Electron issue.\n\n\nVSCode Version: 1.16.0\nOS Version: Windows 10 Pro 15063.540\n\nSteps to Reproduce:\n\nCopy this regex expression using Ctrl+A: http://rubular.com/r/M58qyBfuxF (Link provided as it seems GitHub scrubs the soft hyphen before posting)\nPaste it into Code, and verify that there is no hyphen visible.\nPaste it into a terminal window and verify that there is a hyphen directly after the $\n\n\nReproduces without extensions: Yes\nAlso reproduces with both the default font and Roboto Mono",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "39",
      "number": "34176",
      "pretext": "I ran into a case where a soft hyphen had snuck into a regex expression, yet the character was not visible in the editor. It doesn't seem to render in most places, including Stack Exchange; it has only shown up for me when pasted into a terminal window like cmd.exe or terminal.app. Full disclosure: I don't know if this is behaviour as intended or not. This could also be an Electron issue.\n\n\nVSCode Version: 1.16.0\nOS Version: Windows 10 Pro 15063.540\n\nSteps to Reproduce:\n\nCopy this regex expression using Ctrl+A: http://rubular.com/r/M58qyBfuxF (Link provided as it seems GitHub scrubs the soft hyphen before posting)\nPaste it into Code, and verify that there is no hyphen visible.\nPaste it into a terminal window and verify that there is a hyphen directly after the $\n\n\nReproduces without extensions: Yes\nAlso reproduces with both the default font and Roboto Mono",
      "title": "Editor not rendering soft hyphen"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2820,
    "text": "Rename file in explorer styling issueClick a file in the explorer to select\nFocus the editor\nFocus the explorer again with the keyboard\nPress enter to rename file\nThe row is highlighted but the input box doesn't appear\nTry renaming other rows, sometimes the input box appears, sometimes not\n\nSometimes the wrong text appears after pressing enter, not sure if that's a separate issue. That happens a couple times in the gif",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "40",
      "number": "66866",
      "pretext": "Click a file in the explorer to select\nFocus the editor\nFocus the explorer again with the keyboard\nPress enter to rename file\nThe row is highlighted but the input box doesn't appear\nTry renaming other rows, sometimes the input box appears, sometimes not\n\nSometimes the wrong text appears after pressing enter, not sure if that's a separate issue. That happens a couple times in the gif",
      "title": "Rename file in explorer styling issue"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2821,
    "text": "OutputWorker: Cannot read property 'getWorkspace' of undefinedWhen opening the output panel:\nshell.js:248 Cannot read property 'getWorkspace' of undefined: TypeError: Cannot read property 'getWorkspace' of undefined\n    at new OutputWorker (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/workbench/parts/output/common/outputWorker.js:25:49)\n    at create (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/base/common/types.js:160:14)\n    at InstantiationService._createInstance (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/platform/instantiation/common/instantiationService.js:141:43)\n    at file:///Users/bpasero/Development/Microsoft/monaco/out/vs/platform/instantiation/common/instantiationService.js:96:33\n    at Module._invokeFactory (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:755:52)\n    at Module._complete (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:776:34)\n    at Module.resolveDependency (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:831:22)\n    at ModuleManager._resolveDependency (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1421:19)\n    at ModuleManager._resolve (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1519:30)\n    at ModuleManager.defineModule (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1027:18)",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "41",
      "number": "8167",
      "pretext": "When opening the output panel:\nshell.js:248 Cannot read property 'getWorkspace' of undefined: TypeError: Cannot read property 'getWorkspace' of undefined\n    at new OutputWorker (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/workbench/parts/output/common/outputWorker.js:25:49)\n    at create (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/base/common/types.js:160:14)\n    at InstantiationService._createInstance (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/platform/instantiation/common/instantiationService.js:141:43)\n    at file:///Users/bpasero/Development/Microsoft/monaco/out/vs/platform/instantiation/common/instantiationService.js:96:33\n    at Module._invokeFactory (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:755:52)\n    at Module._complete (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:776:34)\n    at Module.resolveDependency (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:831:22)\n    at ModuleManager._resolveDependency (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1421:19)\n    at ModuleManager._resolve (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1519:30)\n    at ModuleManager.defineModule (file:///Users/bpasero/Development/Microsoft/monaco/out/vs/loader.js:1027:18)",
      "title": "OutputWorker: Cannot read property 'getWorkspace' of undefined"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2822,
    "text": "Empty line between settings sectionsWe could remove that extra line between sections.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "42",
      "number": "16489",
      "pretext": "We could remove that extra line between sections.",
      "title": "Empty line between settings sections"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2823,
    "text": "AutoImport not working for NPM packages, only in JS/JSXIssue Type: Bug\nI have this weird issue where AutoImport won't work for NPM packages when using JS or JSX. However it DOES work for:\n\nLocal files\nwhen using Typescript instead of JS\n\nI haven't found any issue quite like this, so I finally decided to ask for clues. Any hints at how to fix this would be greatly appreciated.\nVS Code version: Code 1.30.2 (61122f8, 2019-01-07T22:48:31.260Z)\nOS version: Darwin x64 18.2.0\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-8750H CPU @ 2.20GHz (12 x 2200)\n\n\nGPU Status\n2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledrasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\n2, 2, 2\n\n\nMemory (System)\n16.00GB (1.47GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (39)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nscss-lint\nada\n0.1.8\n\n\nrails-partial\naki\n0.1.0\n\n\njest-snippets\nand\n1.6.1\n\n\nng-template\nAng\n0.1.11\n\n\nrails\nbun\n0.8.6\n\n\nvscode-gemfile\nbun\n0.0.2\n\n\nsolargraph\ncas\n0.19.1\n\n\nnpm-intellisense\nchr\n1.3.0\n\n\nbracket-pair-colorizer\nCoe\n1.0.61\n\n\nionic3-vs-ionView-snippets\ndan\n1.0.2\n\n\nvscode-markdownlint\nDav\n0.23.0\n\n\nvscode-eslint\ndba\n1.8.0\n\n\ntslint\neg2\n1.0.42\n\n\nvsc-material-theme\nEqu\n2.6.3\n\n\nprettier-vscode\nesb\n1.7.3\n\n\ntodo-tree\nGru\n0.0.115\n\n\nbeautify\nHoo\n1.4.7\n\n\nRelativePath\njak\n1.4.0\n\n\nvscode-styled-components\njpo\n0.0.25\n\n\nrspec-snippets\nkar\n0.0.4\n\n\ncoffeelinter\nlky\n1.4.0\n\n\nvscode-language-babel\nmgm\n0.0.21\n\n\ndotenv\nmik\n1.0.1\n\n\nruby-rubocop\nmis\n0.7.1\n\n\nvscode-elixir\nmjm\n1.1.0\n\n\nvscode-postcss-sorting\nmrm\n3.0.1\n\n\nvscode-scss\nmrm\n0.6.2\n\n\natom-keybindings\nms-\n3.0.5\n\n\ndebugger-for-chrome\nmsj\n4.11.1\n\n\ncolor-highlight\nnau\n2.3.0\n\n\nvscode-docker\nPet\n0.5.1\n\n\nmaterial-icon-theme\nPKi\n3.6.2\n\n\nruby\nreb\n0.21.0\n\n\nslim\nsia\n0.1.2\n\n\naddDocComments\nste\n0.0.8\n\n\nhighlight-matching-tag\nvin\n0.8.6\n\n\nsimple-ruby-erb\nvor\n0.2.1\n\n\nvscode-js-import\nwan\n0.15.4\n\n\nchange-case\nwma\n1.0.0",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "43",
      "number": "67126",
      "pretext": "Issue Type: Bug\nI have this weird issue where AutoImport won't work for NPM packages when using JS or JSX. However it DOES work for:\n\nLocal files\nwhen using Typescript instead of JS\n\nI haven't found any issue quite like this, so I finally decided to ask for clues. Any hints at how to fix this would be greatly appreciated.\nVS Code version: Code 1.30.2 (61122f8, 2019-01-07T22:48:31.260Z)\nOS version: Darwin x64 18.2.0\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i7-8750H CPU @ 2.20GHz (12 x 2200)\n\n\nGPU Status\n2d_canvas: enabledchecker_imaging: disabled_offflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: enabledrasterization: enabledvideo_decode: enabledvideo_encode: enabledwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\n2, 2, 2\n\n\nMemory (System)\n16.00GB (1.47GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (39)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nscss-lint\nada\n0.1.8\n\n\nrails-partial\naki\n0.1.0\n\n\njest-snippets\nand\n1.6.1\n\n\nng-template\nAng\n0.1.11\n\n\nrails\nbun\n0.8.6\n\n\nvscode-gemfile\nbun\n0.0.2\n\n\nsolargraph\ncas\n0.19.1\n\n\nnpm-intellisense\nchr\n1.3.0\n\n\nbracket-pair-colorizer\nCoe\n1.0.61\n\n\nionic3-vs-ionView-snippets\ndan\n1.0.2\n\n\nvscode-markdownlint\nDav\n0.23.0\n\n\nvscode-eslint\ndba\n1.8.0\n\n\ntslint\neg2\n1.0.42\n\n\nvsc-material-theme\nEqu\n2.6.3\n\n\nprettier-vscode\nesb\n1.7.3\n\n\ntodo-tree\nGru\n0.0.115\n\n\nbeautify\nHoo\n1.4.7\n\n\nRelativePath\njak\n1.4.0\n\n\nvscode-styled-components\njpo\n0.0.25\n\n\nrspec-snippets\nkar\n0.0.4\n\n\ncoffeelinter\nlky\n1.4.0\n\n\nvscode-language-babel\nmgm\n0.0.21\n\n\ndotenv\nmik\n1.0.1\n\n\nruby-rubocop\nmis\n0.7.1\n\n\nvscode-elixir\nmjm\n1.1.0\n\n\nvscode-postcss-sorting\nmrm\n3.0.1\n\n\nvscode-scss\nmrm\n0.6.2\n\n\natom-keybindings\nms-\n3.0.5\n\n\ndebugger-for-chrome\nmsj\n4.11.1\n\n\ncolor-highlight\nnau\n2.3.0\n\n\nvscode-docker\nPet\n0.5.1\n\n\nmaterial-icon-theme\nPKi\n3.6.2\n\n\nruby\nreb\n0.21.0\n\n\nslim\nsia\n0.1.2\n\n\naddDocComments\nste\n0.0.8\n\n\nhighlight-matching-tag\nvin\n0.8.6\n\n\nsimple-ruby-erb\nvor\n0.2.1\n\n\nvscode-js-import\nwan\n0.15.4\n\n\nchange-case\nwma\n1.0.0",
      "title": "AutoImport not working for NPM packages, only in JS/JSX"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2824,
    "text": "File Tree Automatically Focus when closing a fileThis is not a bug\nIt is more an enhancement\nThing is when I close a tab (file), file tree changes it focus automatically to the now active file; this is really annoying because many times I just open and close many files (as I'm sure other devs do) and I have to scroll all ways down/up again! Hope you improve this.\nNote: Sublime Text doesn't automatically focus the file tree after closing a file.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "44",
      "number": "23459",
      "pretext": "This is not a bug\nIt is more an enhancement\nThing is when I close a tab (file), file tree changes it focus automatically to the now active file; this is really annoying because many times I just open and close many files (as I'm sure other devs do) and I have to scroll all ways down/up again! Hope you improve this.\nNote: Sublime Text doesn't automatically focus the file tree after closing a file.",
      "title": "File Tree Automatically Focus when closing a file"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2825,
    "text": "Problem with syntax highlighting (HTML/PHP)Problem with syntax highlighting as reported below:\n\n\nVSCode Version: 1.16.0 (1.16.0)\nOS Version: Mac OSX Sierra 10.12.5\n\nSteps to Reproduce:\n\nWrite HTML code\nInsert multiline pieces of PHP code\nSee that the syntax doesn't recognise the open/close php tag",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "45",
      "number": "34204",
      "pretext": "Problem with syntax highlighting as reported below:\n\n\nVSCode Version: 1.16.0 (1.16.0)\nOS Version: Mac OSX Sierra 10.12.5\n\nSteps to Reproduce:\n\nWrite HTML code\nInsert multiline pieces of PHP code\nSee that the syntax doesn't recognise the open/close php tag",
      "title": "Problem with syntax highlighting (HTML/PHP)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2826,
    "text": "Feature request: Add context menu to branch name in lower leftVersion 1.11 changed the default action when you clicked on the branch name on the lower left from opening an input with git checkout already filled to an input for the git checkout command itself.  The previous pre-filled input was nice in that you could replace \"checkout\" with \"branch\" and create a new branch.\nInstead of cluttering the new popup ui with a branch option you could add a context menu to the branch name in the lower left corner that would contain branch and optionally other common commands which you can access now in the ellipsis menu in the git pane (note: branch is not available right now in the git pane ellipsis menu).",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "46",
      "number": "24249",
      "pretext": "Version 1.11 changed the default action when you clicked on the branch name on the lower left from opening an input with git checkout already filled to an input for the git checkout command itself.  The previous pre-filled input was nice in that you could replace \"checkout\" with \"branch\" and create a new branch.\nInstead of cluttering the new popup ui with a branch option you could add a context menu to the branch name in the lower left corner that would contain branch and optionally other common commands which you can access now in the ellipsis menu in the git pane (note: branch is not available right now in the git pane ellipsis menu).",
      "title": "Feature request: Add context menu to branch name in lower left"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2827,
    "text": "Duplicate setting \"npm.runSilent\" in vscode://defaultsettings/settings.jsonVSCode Version: Code 1.15.1 (41abd21, 2017-08-16T17:15:57.756Z)\nOS Version: Darwin x64 16.7.0\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nmarkdown-toc\nAla\n1.5.6\n\n\nxml\nDot\n1.9.2\n\n\nEditorConfig\nEdi\n0.9.4\n\n\nmaterial-icon-theme\nPKi\n2.1.0\n\n\nvscode-docker\nPet\n0.0.16\n\n\ncode-settings-sync\nSha\n2.8.2\n\n\nhtml-css-class-completion\nZig\n1.8.0\n\n\nhugofy\nakm\n0.1.0\n\n\nvscode-color\nans\n0.4.5\n\n\ncform\naws\n0.0.10\n\n\nbetter-toml\nbun\n0.2.0\n\n\npath-intellisense\nchr\n1.4.2\n\n\ngitignore\ncod\n0.5.0\n\n\ntypewriter\ndan\n1.0.1\n\n\nvscode-eslint\ndba\n1.2.11\n\n\ngithistory\ndon\n0.2.3\n\n\ngitlens\neam\n4.3.3\n\n\nvscode-npm-script\neg2\n0.2.0\n\n\nlambda-snippets\nlog\n0.2.0\n\n\nGo\nluk\n0.6.63\n\n\nprettify-json\nmoh\n0.0.3\n\n\ndebugger-for-chrome\nmsj\n3.1.8\n\n\nvscode-icons\nrob\n7.12.0\n\n\ngitblame\nwad\n2.1.0\n\n\n\n(3 theme extensions excluded)\n\nSteps to Reproduce:\n\nLine 1975 and 2061 are duplicates.\nDoes NOT repro with extensions disabled.\n\n\nReproduces without extensions: No",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "47",
      "number": "32716",
      "pretext": "VSCode Version: Code 1.15.1 (41abd21, 2017-08-16T17:15:57.756Z)\nOS Version: Darwin x64 16.7.0\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nmarkdown-toc\nAla\n1.5.6\n\n\nxml\nDot\n1.9.2\n\n\nEditorConfig\nEdi\n0.9.4\n\n\nmaterial-icon-theme\nPKi\n2.1.0\n\n\nvscode-docker\nPet\n0.0.16\n\n\ncode-settings-sync\nSha\n2.8.2\n\n\nhtml-css-class-completion\nZig\n1.8.0\n\n\nhugofy\nakm\n0.1.0\n\n\nvscode-color\nans\n0.4.5\n\n\ncform\naws\n0.0.10\n\n\nbetter-toml\nbun\n0.2.0\n\n\npath-intellisense\nchr\n1.4.2\n\n\ngitignore\ncod\n0.5.0\n\n\ntypewriter\ndan\n1.0.1\n\n\nvscode-eslint\ndba\n1.2.11\n\n\ngithistory\ndon\n0.2.3\n\n\ngitlens\neam\n4.3.3\n\n\nvscode-npm-script\neg2\n0.2.0\n\n\nlambda-snippets\nlog\n0.2.0\n\n\nGo\nluk\n0.6.63\n\n\nprettify-json\nmoh\n0.0.3\n\n\ndebugger-for-chrome\nmsj\n3.1.8\n\n\nvscode-icons\nrob\n7.12.0\n\n\ngitblame\nwad\n2.1.0\n\n\n\n(3 theme extensions excluded)\n\nSteps to Reproduce:\n\nLine 1975 and 2061 are duplicates.\nDoes NOT repro with extensions disabled.\n\n\nReproduces without extensions: No",
      "title": "Duplicate setting \"npm.runSilent\" in vscode://defaultsettings/settings.json"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2828,
    "text": "moving TypeScript files in explorer fails to update/prompt imports when containing folder is movedVSCode Version: 1.27.2\nOS Version: Win 10\nTypescript: 3.0.3\n\nSteps to Reproduce:\n\nSet update imports on move typescript setting to prompt\nCreate an empty project with 2 typescript files, one importing from the other.\nIn the explorer manually move the depended upon file to a new directory. You will be prompted to update the import. This works fine.\nNow move that directory into ANOTHER new directory.\nThis time there is no prompt and the import will be broken",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "48",
      "number": "58987",
      "pretext": "VSCode Version: 1.27.2\nOS Version: Win 10\nTypescript: 3.0.3\n\nSteps to Reproduce:\n\nSet update imports on move typescript setting to prompt\nCreate an empty project with 2 typescript files, one importing from the other.\nIn the explorer manually move the depended upon file to a new directory. You will be prompted to update the import. This works fine.\nNow move that directory into ANOTHER new directory.\nThis time there is no prompt and the import will be broken",
      "title": "moving TypeScript files in explorer fails to update/prompt imports when containing folder is moved"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2829,
    "text": "[tab completion] multi cursor behaviour different to emmetTesting #9698:\n\nenable tab completion: \"editor.tabCompletion\": true\ncreate a html snippet\n\n     \"Div\": {\n        \"prefix\": \"div\",\n        \"body\": [\n            \"<div>\",\n            \"    ${}\",\n            \"</div>\"\n        ],\n        \"description\": \"New div\"\n    }\n\nin a html file have a multiple lines with content 'div'. Set multiple cursors after each div. Press tab.\n👉  primary div is expanded\ndo the same with multiple lines of span, press tab\n👉  tab character is added at each of the cursors\n\nNot sure if the emmet behaviour is intentional.\nIt would be cool if the expansion happens on every cursor.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "49",
      "number": "9764",
      "pretext": "Testing #9698:\n\nenable tab completion: \"editor.tabCompletion\": true\ncreate a html snippet\n\n     \"Div\": {\n        \"prefix\": \"div\",\n        \"body\": [\n            \"<div>\",\n            \"    ${}\",\n            \"</div>\"\n        ],\n        \"description\": \"New div\"\n    }\n\nin a html file have a multiple lines with content 'div'. Set multiple cursors after each div. Press tab.\n👉  primary div is expanded\ndo the same with multiple lines of span, press tab\n👉  tab character is added at each of the cursors\n\nNot sure if the emmet behaviour is intentional.\nIt would be cool if the expansion happens on every cursor.",
      "title": "[tab completion] multi cursor behaviour different to emmet"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2830,
    "text": "ScrollingWhen scrolling through a file, the view is not fluid. It jumps, skips, pauses and is not a constant view as I move up or down the file.\nVersion: July 2018 (version 1.26)",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "50",
      "number": "57704",
      "pretext": "When scrolling through a file, the view is not fluid. It jumps, skips, pauses and is not a constant view as I move up or down the file.\nVersion: July 2018 (version 1.26)",
      "title": "Scrolling"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2831,
    "text": "Mac OS - Highlighting text in the editor requires two clicks when the integrated terminal is openVSCode Version: 1.2.1\nOS Version: 10.11.5 (15F34) (El Capitan)\n\nSteps to Reproduce:\n\nOpen a file in the editor\nOpen the integrated terminal\nType some commands on the terminal\nTry to copy some text on the editor by highlighting and typing Mac + c\nYou will have to click on the section twice before the text successfully copies",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "51",
      "number": "8633",
      "pretext": "VSCode Version: 1.2.1\nOS Version: 10.11.5 (15F34) (El Capitan)\n\nSteps to Reproduce:\n\nOpen a file in the editor\nOpen the integrated terminal\nType some commands on the terminal\nTry to copy some text on the editor by highlighting and typing Mac + c\nYou will have to click on the section twice before the text successfully copies",
      "title": "Mac OS - Highlighting text in the editor requires two clicks when the integrated terminal is open"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2832,
    "text": "open in broswerPlease natively support opening a file in the browser. LIke a right click menu option. The extensions don't work well especially in chrome.\nthanks",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "52",
      "number": "56153",
      "pretext": "Please natively support opening a file in the browser. LIke a right click menu option. The extensions don't work well especially in chrome.\nthanks",
      "title": "open in broswer"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2833,
    "text": "TreeItem is too slowVSCode Version: 1.25.1\nOS Version: 10.13.5\n\nI've added the following view to Todo+:\n\nI've been benchmarking it against magento2, which contains about 30k files and 2M lines of code. If the user has ag installed in his system the process of finding those todos is quite fast, its takes about 4s to do that on my laptop.\nQuite surprisingly though the bulk of the time is spent on these lines, which are executed about 1k times, since the extension creates about 1k TreeItems under these circumstances:\n\n\n\nAdding the tooltips requires about 500 extra milliseconds, I would expect that number to be near 0ms, I see basically nothing going on here that can justify that number.\n\n\nAdding the commands requires about 4 extra seconds. 4 seconds for doing what? Creating those objects should be almost free, and until those TreeItems get clicked they don't even change anything as far as the user is concerned, I think.\n\n\nAdding the icons requires about 14 extra seconds.  There are less than 10 different images loaded, but for some reason loading them a few hundred times is that slow.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "53",
      "number": "56234",
      "pretext": "VSCode Version: 1.25.1\nOS Version: 10.13.5\n\nI've added the following view to Todo+:\n\nI've been benchmarking it against magento2, which contains about 30k files and 2M lines of code. If the user has ag installed in his system the process of finding those todos is quite fast, its takes about 4s to do that on my laptop.\nQuite surprisingly though the bulk of the time is spent on these lines, which are executed about 1k times, since the extension creates about 1k TreeItems under these circumstances:\n\n\n\nAdding the tooltips requires about 500 extra milliseconds, I would expect that number to be near 0ms, I see basically nothing going on here that can justify that number.\n\n\nAdding the commands requires about 4 extra seconds. 4 seconds for doing what? Creating those objects should be almost free, and until those TreeItems get clicked they don't even change anything as far as the user is concerned, I think.\n\n\nAdding the icons requires about 14 extra seconds.  There are less than 10 different images loaded, but for some reason loading them a few hundred times is that slow.",
      "title": "TreeItem is too slow"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2834,
    "text": "Additional instance starts in wrong virtual desktopVSCode Version:1.0.0\nOS Version: Windows 7\n\nI'm using desktops.exe from SysInternals to add virtual desktops to Windows 7.\nWhen having an open instance of code in a not active desktop, starting an additional instance will not start on the currently active desktop.\nSteps to Reproduce:\n\nOpen VSCode in desktop 1\nSwitch to desktop 2\nStart another VSCode instance: code .. The new instance starts in desktop 1",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "54",
      "number": "5876",
      "pretext": "VSCode Version:1.0.0\nOS Version: Windows 7\n\nI'm using desktops.exe from SysInternals to add virtual desktops to Windows 7.\nWhen having an open instance of code in a not active desktop, starting an additional instance will not start on the currently active desktop.\nSteps to Reproduce:\n\nOpen VSCode in desktop 1\nSwitch to desktop 2\nStart another VSCode instance: code .. The new instance starts in desktop 1",
      "title": "Additional instance starts in wrong virtual desktop"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2835,
    "text": "Intellisense in package.json shows packages inconsistentlyFrom #13835 (comment),\nWhen typing quickly, I'm selecting packages by initials (or something), but when I type slowly, I'm selecting packages by prefix:\n\nOpen package.json\nPut the cursor in \"dependencies\"\nctrl+space, then esc (It doesn't repro unless the intellisense window has been shown on this line)\ntype blu quickly\nThe intellisense window appears again and I expect to see suggestions for all packages that match. Instead I only see \"blackbaud-npi-datamart-ux\", with the 'blu' highlighted. It's always this one package.\n\nIf you type blu more slowly, then it shows 'blu', 'blu-css', 'blu-generator' ...\nAnother example -\nTyping \"blah\" quickly\n\nSlowly:\n\nThe first type of fuzzy matching would be useful but it should be more consistent.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "55",
      "number": "14679",
      "pretext": "From #13835 (comment),\nWhen typing quickly, I'm selecting packages by initials (or something), but when I type slowly, I'm selecting packages by prefix:\n\nOpen package.json\nPut the cursor in \"dependencies\"\nctrl+space, then esc (It doesn't repro unless the intellisense window has been shown on this line)\ntype blu quickly\nThe intellisense window appears again and I expect to see suggestions for all packages that match. Instead I only see \"blackbaud-npi-datamart-ux\", with the 'blu' highlighted. It's always this one package.\n\nIf you type blu more slowly, then it shows 'blu', 'blu-css', 'blu-generator' ...\nAnother example -\nTyping \"blah\" quickly\n\nSlowly:\n\nThe first type of fuzzy matching would be useful but it should be more consistent.",
      "title": "Intellisense in package.json shows packages inconsistently"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2836,
    "text": "Uncaught SyntaxError: Unexpected end of inputIssue Id: 3c0f493a-37c5-196b-ce04-755343aba4dbVersions - 0.10.6-release-  dfc08dcStack SyntaxError: Unexpected end of input    at Object.parse (native)[/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 (V8Protocol.dispatch)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 %28V8Protocol.dispatch%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 (V8Protocol.handleData)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 %28V8Protocol.handleData%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 (V8Protocol.connect)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 %28V8Protocol.connect%29)    at emitOne (events.js:77:13)    at Socket.emit (events.js:169:7)    at readableAddChunk (_stream_readable.js:146:16)    at Socket.Readable.push (_stream_readable.js:110:10)    at Pipe.onread (net.js:523:20)",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "56",
      "number": "2470",
      "pretext": "Issue Id: 3c0f493a-37c5-196b-ce04-755343aba4dbVersions - 0.10.6-release-  dfc08dcStack SyntaxError: Unexpected end of input    at Object.parse (native)[/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 (V8Protocol.dispatch)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L129:21 %28V8Protocol.dispatch%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 (V8Protocol.handleData)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L108:11 %28V8Protocol.handleData%29)[/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 (V8Protocol.connect)](https://github.com/microsoft/vscode/blob/a80232bbcfe8a5cdad1ebc98638673d9dcb02458/src/vs/workbench/parts/debug/node/v8Protocol.ts#L63:8 %28V8Protocol.connect%29)    at emitOne (events.js:77:13)    at Socket.emit (events.js:169:7)    at readableAddChunk (_stream_readable.js:146:16)    at Socket.Readable.push (_stream_readable.js:110:10)    at Pipe.onread (net.js:523:20)",
      "title": " Uncaught SyntaxError: Unexpected end of input"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2837,
    "text": "\"workbench.editor.labelFormat\": \"default\" should show paths with split windowVSCode Version: Version 1.22.2 (1.22.2)\nOS Version: Mac OS 10.11.6\n\nSteps to Reproduce:\n\nMake two identically named files in different subfolders of a project.\nSet   \"workbench.editor.labelFormat\": \"default\" in settings.\nOpen both files. See path info that differentiates them.\nSplit the window and move one over. No longer see path info that differentiates them.\n\nDesired behavior:\nI should still see path info that differentiates them, since there are two different files open with the same title and the goal of \"default\" is to be able to differentiate them by path.\n\nDoes this issue occur when all extensions are disabled?: Yes/No\nYes.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "57",
      "number": "48410",
      "pretext": "VSCode Version: Version 1.22.2 (1.22.2)\nOS Version: Mac OS 10.11.6\n\nSteps to Reproduce:\n\nMake two identically named files in different subfolders of a project.\nSet   \"workbench.editor.labelFormat\": \"default\" in settings.\nOpen both files. See path info that differentiates them.\nSplit the window and move one over. No longer see path info that differentiates them.\n\nDesired behavior:\nI should still see path info that differentiates them, since there are two different files open with the same title and the goal of \"default\" is to be able to differentiate them by path.\n\nDoes this issue occur when all extensions are disabled?: Yes/No\nYes.",
      "title": "\"workbench.editor.labelFormat\": \"default\" should show paths with split window"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2838,
    "text": "Unresponsive VSCode after upgrade to 1.26.Hello!\nAfter installing VSCode 1.26, the GUI is almost completely unresponsive when I open my project. After a few minutes, it can be used. However, the CPU usage will remain at 24% (i have 4 cores). Even after the project or folder is closed.\nIf I open VSCode without a project or folder, it behaves.\nVSCode is responsive if launched with --disable-extensions.\nCould it be an extentions that misbehaves?\nHere is a --status dump:\n\nVersion:          Code 1.26.0 (4e93618, 2018-08-13T16:29:31.933Z)\nOS Version:       Windows_NT x64 10.0.16299\nCPUs:             Intel(R) Core(TM) i7-6820HQ CPU @ 2.70GHz (8 x 2712)\nMemory (System):  15.85GB (3.24GB free)\nVM:               0%\nScreen Reader:    no\nProcess Argv:     C:\\Users\\forsbdan\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe\nGPU Status:       2d_canvas:                    enabled\nchecker_imaging:              disabled_off\nflash_3d:                     enabled\nflash_stage3d:                enabled\nflash_stage3d_baseline:       enabled\ngpu_compositing:              enabled\nmultiple_raster_threads:      enabled_on\nnative_gpu_memory_buffers:    disabled_software\nrasterization:                enabled\nvideo_decode:                 enabled\nvideo_encode:                 enabled\nwebgl:                        enabled\nwebgl2:                       enabled\nCPU %   Mem MB     PID  Process\n0      100   34680  code main\n0       84   22972     shared-process\n0      146   23460     gpu-process\n23     1271   28764     window (Program.cs - demo2 - Visual Studio Code)\n0        6   26188       winpty-process\n0       65   16204         C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\n0       10   19472         console-window-host (Windows internal process)\n0       51   30296       searchService\n0       14   31020       electron-crash-reporter\n0       11   31384       watcherService\n0       10   21716         console-window-host (Windows internal process)\n0      191   33784       extensionHost\n0       38   22568         searchService\n0        3   37816         cmd /s /c \"C:\\Users\\forsbdan.vscode\\extensions\\ms-vscode.csharp-1.15.2.omnisharp\\1.30.1\\OmniSharp.exe -s c:\\src\\demo2\\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4\"\n0       10    7140           console-window-host (Windows internal process)\n0      115   19888           C:\\Users\\forsbdan.vscode\\extensions\\ms-vscode.csharp-1.15.2.omnisharp\\1.30.1\\OmniSharp.exe  -s c:\\src\\demo2\\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4\nWorkspace Stats:\n|  Window (Program.cs - demo2 - Visual Studio Code)\n|    Folder (demo2): 83 files\n|      File types: json(15) cshtml(12) cs(8) js(5) ts(5) scss(5) cache(5)\n|                  map(3) vue(3) txt(2)\n|      Conf files: launch.json(2) tasks.json(2) sln(1) csproj(1)\n|                  package.json(1) tsconfig.json(1) webpack.config.js(1)\n|                  settings.json(1)\n|      Launch Configs: coreclr(2)",
    "annotations": [{ "label": 143, "user": 1 }],
    "meta": {
      "": "58",
      "number": "56364",
      "pretext": "Hello!\nAfter installing VSCode 1.26, the GUI is almost completely unresponsive when I open my project. After a few minutes, it can be used. However, the CPU usage will remain at 24% (i have 4 cores). Even after the project or folder is closed.\nIf I open VSCode without a project or folder, it behaves.\nVSCode is responsive if launched with --disable-extensions.\nCould it be an extentions that misbehaves?\nHere is a --status dump:\n\nVersion:          Code 1.26.0 (4e93618, 2018-08-13T16:29:31.933Z)\nOS Version:       Windows_NT x64 10.0.16299\nCPUs:             Intel(R) Core(TM) i7-6820HQ CPU @ 2.70GHz (8 x 2712)\nMemory (System):  15.85GB (3.24GB free)\nVM:               0%\nScreen Reader:    no\nProcess Argv:     C:\\Users\\forsbdan\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe\nGPU Status:       2d_canvas:                    enabled\nchecker_imaging:              disabled_off\nflash_3d:                     enabled\nflash_stage3d:                enabled\nflash_stage3d_baseline:       enabled\ngpu_compositing:              enabled\nmultiple_raster_threads:      enabled_on\nnative_gpu_memory_buffers:    disabled_software\nrasterization:                enabled\nvideo_decode:                 enabled\nvideo_encode:                 enabled\nwebgl:                        enabled\nwebgl2:                       enabled\nCPU %   Mem MB     PID  Process\n0      100   34680  code main\n0       84   22972     shared-process\n0      146   23460     gpu-process\n23     1271   28764     window (Program.cs - demo2 - Visual Studio Code)\n0        6   26188       winpty-process\n0       65   16204         C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\n0       10   19472         console-window-host (Windows internal process)\n0       51   30296       searchService\n0       14   31020       electron-crash-reporter\n0       11   31384       watcherService\n0       10   21716         console-window-host (Windows internal process)\n0      191   33784       extensionHost\n0       38   22568         searchService\n0        3   37816         cmd /s /c \"C:\\Users\\forsbdan.vscode\\extensions\\ms-vscode.csharp-1.15.2.omnisharp\\1.30.1\\OmniSharp.exe -s c:\\src\\demo2\\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4\"\n0       10    7140           console-window-host (Windows internal process)\n0      115   19888           C:\\Users\\forsbdan.vscode\\extensions\\ms-vscode.csharp-1.15.2.omnisharp\\1.30.1\\OmniSharp.exe  -s c:\\src\\demo2\\Fagkveld.sln --hostPID 33784 --stdio DotNet:enablePackageRestore=false --encoding utf-8 --loglevel information formattingOptions:useTabs=false formattingOptions:tabSize=4 formattingOptions:indentationSize=4\nWorkspace Stats:\n|  Window (Program.cs - demo2 - Visual Studio Code)\n|    Folder (demo2): 83 files\n|      File types: json(15) cshtml(12) cs(8) js(5) ts(5) scss(5) cache(5)\n|                  map(3) vue(3) txt(2)\n|      Conf files: launch.json(2) tasks.json(2) sln(1) csproj(1)\n|                  package.json(1) tsconfig.json(1) webpack.config.js(1)\n|                  settings.json(1)\n|      Launch Configs: coreclr(2)",
      "title": "Unresponsive VSCode after upgrade to 1.26."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2839,
    "text": "ctrl+z/clrl+y after block mode edit does not follow the (multi-line) cursor to follow the undo/redo stepsIssue Type: Bug\ncreate dummy text1\ndo block mode edit inside text1\ncreate more dummy text, preferably so much you dont see text1 any more\nctrl + z\nVS Code version: Code 1.21.1 (79b44aa, 2018-03-14T14:46:47.128Z)\nOS version: Windows_NT x64 10.0.15063\n\nExtensions (4)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nvscode-eslint\ndba\n1.4.7\n\n\ngc-excelviewer\nGra\n2.0.20\n\n\npython\nms-\n2018.2.1\n\n\nvscode-docker\nPet\n0.0.25\n\n\n\n\nReproduces only with extensions",
    "annotations": [{ "label": 146, "user": 1 }],
    "meta": {
      "": "59",
      "number": "46332",
      "pretext": "Issue Type: Bug\ncreate dummy text1\ndo block mode edit inside text1\ncreate more dummy text, preferably so much you dont see text1 any more\nctrl + z\nVS Code version: Code 1.21.1 (79b44aa, 2018-03-14T14:46:47.128Z)\nOS version: Windows_NT x64 10.0.15063\n\nExtensions (4)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nvscode-eslint\ndba\n1.4.7\n\n\ngc-excelviewer\nGra\n2.0.20\n\n\npython\nms-\n2018.2.1\n\n\nvscode-docker\nPet\n0.0.25\n\n\n\n\nReproduces only with extensions",
      "title": "ctrl+z/clrl+y after block mode edit does not follow the (multi-line) cursor to follow the undo/redo steps"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2840,
    "text": "HTML tests are failinghttps://travis-ci.org/Microsoft/vscode/jobs/120206982",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "60",
      "number": "4886",
      "pretext": "https://travis-ci.org/Microsoft/vscode/jobs/120206982",
      "title": "HTML tests are failing"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2841,
    "text": "Settings editor crashes when restoringOpen settings editor\nTab away\nTab back to it",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "61",
      "number": "72813",
      "pretext": "Open settings editor\nTab away\nTab back to it",
      "title": "Settings editor crashes when restoring"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2842,
    "text": "VSCode should do like any decent editor, and provide a drop-down from all search text boxesIssue Type: Feature Request\nDecent editors provide a drop-down list of older search phrases in their search (and replace!) text entry UI elements.\nBut VSCode doesn't seem to do this, for some reason.\nIt's only polite to remember such things for the comfy user, you know :)\nVS Code version: Code 1.32.3 (a3db5be, 2019-03-14T23:43:35.476Z)\nOS version: Windows_NT x64 10.0.18348",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "62",
      "number": "70591",
      "pretext": "Issue Type: Feature Request\nDecent editors provide a drop-down list of older search phrases in their search (and replace!) text entry UI elements.\nBut VSCode doesn't seem to do this, for some reason.\nIt's only polite to remember such things for the comfy user, you know :)\nVS Code version: Code 1.32.3 (a3db5be, 2019-03-14T23:43:35.476Z)\nOS version: Windows_NT x64 10.0.18348",
      "title": "VSCode should do like any decent editor, and provide a drop-down from all search text boxes"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2843,
    "text": "Reopen issue 21518 or 18782Please reopen either #21518 or #18782 . The bug still repros with C/C++.\nvoid foo(int, const char*, const char *) { // also repros when the params are subsets, e.g. a, aa, aaa\n}\nint main()\n{\nfoo(a, ) // signature Help selects the the 3rd param instead of the 2nd.\n}",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "63",
      "number": "65327",
      "pretext": "Please reopen either #21518 or #18782 . The bug still repros with C/C++.\nvoid foo(int, const char*, const char *) { // also repros when the params are subsets, e.g. a, aa, aaa\n}\nint main()\n{\nfoo(a, ) // signature Help selects the the 3rd param instead of the 2nd.\n}",
      "title": "Reopen issue 21518 or 18782"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2844,
    "text": "Cannot specify global problemMatcher in tasks.jsonVSCode Version: 1.38.1\nOS Version: macOS 10.14.4\n\nSteps to Reproduce:\n\nSpecify \"problemMatcher\": [] at top level of tasks.json object\nRun a task (i.e. an npm task) that has no task config\nVS Code still prompts you for how to scan the task output\n\n\nDoes this issue occur when all extensions are disabled?: No\nOther comments\nEssentially, I want to be able to disable the problem matcher by default. Personally, I never use it, and it's just noise, because every time I run a task I haven't ran before, I have to select Never scan the task output, which opens up tasks.json and adds a new config for that task, just so it can specifiy \"problemMatcher\": [].\nRelated issues\n\n#43003\n\nThis issue was closed, but I still have the problem",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "64",
      "number": "81237",
      "pretext": "VSCode Version: 1.38.1\nOS Version: macOS 10.14.4\n\nSteps to Reproduce:\n\nSpecify \"problemMatcher\": [] at top level of tasks.json object\nRun a task (i.e. an npm task) that has no task config\nVS Code still prompts you for how to scan the task output\n\n\nDoes this issue occur when all extensions are disabled?: No\nOther comments\nEssentially, I want to be able to disable the problem matcher by default. Personally, I never use it, and it's just noise, because every time I run a task I haven't ran before, I have to select Never scan the task output, which opens up tasks.json and adds a new config for that task, just so it can specifiy \"problemMatcher\": [].\nRelated issues\n\n#43003\n\nThis issue was closed, but I still have the problem",
      "title": "Cannot specify global problemMatcher in tasks.json"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2845,
    "text": "Can't do column selection with keyboard over Windows remote desktopVSCode Version:alpha 0.10.12 commit 05e203c\nOS Version:Windows 7\n\nSteps to Reproduce:\n\nTry Ctrl+Shift+Alt+arrow keys to \"draw\" a rectangle.\nNothing happens\n\nShift+Alt and mouse works correctly.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "65",
      "number": "4391",
      "pretext": "VSCode Version:alpha 0.10.12 commit 05e203c\nOS Version:Windows 7\n\nSteps to Reproduce:\n\nTry Ctrl+Shift+Alt+arrow keys to \"draw\" a rectangle.\nNothing happens\n\nShift+Alt and mouse works correctly.",
      "title": "Can't do column selection with keyboard over Windows remote desktop"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2846,
    "text": "ARM template type not accepted even though valid.This issue appears to have come back (or some variation of it) #881\n\nVSCode Version:1.10.2 with arm tools 0.3.4\nOS Version: Windows 10",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "66",
      "number": "22970",
      "pretext": "This issue appears to have come back (or some variation of it) #881\n\nVSCode Version:1.10.2 with arm tools 0.3.4\nOS Version: Windows 10",
      "title": "ARM template type not accepted even though valid."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2847,
    "text": "Nonsensical message in extensionManagementService.tsFound while translating to Hungarian. The message is \"Unknown error while\" in\nsrc/vs/platform/extensionManagement/node/extensionManagementService.ts.\nIt does not make any sense, the end is missing. Added by @sandy081 in db7ddfd.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "67",
      "number": "47566",
      "pretext": "Found while translating to Hungarian. The message is \"Unknown error while\" in\nsrc/vs/platform/extensionManagement/node/extensionManagementService.ts.\nIt does not make any sense, the end is missing. Added by @sandy081 in db7ddfd.",
      "title": "Nonsensical message in extensionManagementService.ts"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2848,
    "text": "I am not able to run my code.Issue Type: Bug\nI am not able to run my code in Visual Studio. It is repeatedly showing that  include errors are detected and to update my includePath for which I have already installed vcpkg from GitHub , still the issue is not yet solved.\nVS Code version: Code 1.39.2 (6ab5985, 2019-10-15T15:35:18.241Z)\nOS version: Windows_NT x64 10.0.18362\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i5-8250U CPU @ 1.60GHz (8 x 1800)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwareoop_rasterization: disabled_offprotected_video_decode: enabledrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: unavailable_offviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\nundefined\n\n\nMemory (System)\n7.90GB (3.42GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (2)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\ncode-runner\nfor\n0.9.14\n\n\ncpptools\nms-\n0.26.1",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "68",
      "number": "83854",
      "pretext": "Issue Type: Bug\nI am not able to run my code in Visual Studio. It is repeatedly showing that  include errors are detected and to update my includePath for which I have already installed vcpkg from GitHub , still the issue is not yet solved.\nVS Code version: Code 1.39.2 (6ab5985, 2019-10-15T15:35:18.241Z)\nOS version: Windows_NT x64 10.0.18362\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i5-8250U CPU @ 1.60GHz (8 x 1800)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwareoop_rasterization: disabled_offprotected_video_decode: enabledrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: unavailable_offviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\nundefined\n\n\nMemory (System)\n7.90GB (3.42GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (2)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\ncode-runner\nfor\n0.9.14\n\n\ncpptools\nms-\n0.26.1",
      "title": "I am not able to run my code."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2849,
    "text": "Add option to view default settings in single plain text filePretty self explanatory. I don't have anything against the new UI or anything but I don't like that I can no longer see the default settings in a single plain text file.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "69",
      "number": "17371",
      "pretext": "Pretty self explanatory. I don't have anything against the new UI or anything but I don't like that I can no longer see the default settings in a single plain text file.",
      "title": "Add option to view default settings in single plain text file"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2850,
    "text": "Allow extensions to contribute build task typesCurrently, when I press Ctrl+Shift+B and click \"Configure Build Task...\" then \"Create tasks.json from template\", the only options are MSBuild, maven, .NET Core, and Others.\nI would like to be able to easily create a build task template for my Mono projects. .NET Core does not have the full feature-set of Mono or .NET Framework yet, and even if it did, I still have existing projects using Mono that I'd like to edit with VS Code. Please add this feature!",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "70",
      "number": "66308",
      "pretext": "Currently, when I press Ctrl+Shift+B and click \"Configure Build Task...\" then \"Create tasks.json from template\", the only options are MSBuild, maven, .NET Core, and Others.\nI would like to be able to easily create a build task template for my Mono projects. .NET Core does not have the full feature-set of Mono or .NET Framework yet, and even if it did, I still have existing projects using Mono that I'd like to edit with VS Code. Please add this feature!",
      "title": "Allow extensions to contribute build task types"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2851,
    "text": "spawn osascript ENOENTIssue Id: def759c3-62fe-b6f1-ff32-27679721a3ffVersions - 0.10.8-  f291f4a-  43ff6af-  5b5f4db-  17fa1cbStack Error: spawn osascript ENOENT    at exports._errnoException (util.js:837:11)    at Process.ChildProcess._handle.onexit (internal/child_process.js:178:32)     at onErrorNT (internal/child_process.js:344:16)     at doNTCallback2 (node.js:442:9)    at process._tickCallback (node.js:356:17)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "71",
      "number": "5019",
      "pretext": "Issue Id: def759c3-62fe-b6f1-ff32-27679721a3ffVersions - 0.10.8-  f291f4a-  43ff6af-  5b5f4db-  17fa1cbStack Error: spawn osascript ENOENT    at exports._errnoException (util.js:837:11)    at Process.ChildProcess._handle.onexit (internal/child_process.js:178:32)     at onErrorNT (internal/child_process.js:344:16)     at doNTCallback2 (node.js:442:9)    at process._tickCallback (node.js:356:17)",
      "title": " spawn osascript ENOENT"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2852,
    "text": "Preference to disable Recently Opened FilesVSCode Version: 1.16.1\nOS Version: OSX\n\nI personally have no need for recent files to be promoted to the top of the Goto File results:\n\nThese results actually slow me down, since I a used to Sublime Text fuzzy match behavior. If the \"Recently Opened\" results were not there, I would be able to jump to files in a predictable way.\nAny preference option I'm missing?",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "72",
      "number": "35637",
      "pretext": "VSCode Version: 1.16.1\nOS Version: OSX\n\nI personally have no need for recent files to be promoted to the top of the Goto File results:\n\nThese results actually slow me down, since I a used to Sublime Text fuzzy match behavior. If the \"Recently Opened\" results were not there, I would be able to jump to files in a predictable way.\nAny preference option I'm missing?",
      "title": "Preference to disable Recently Opened Files"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2853,
    "text": "Workbench should provide language agnostic actions to apply all quick fixes to a fileVSCode Version: 1.0.0\nOS Version:\n\nInstead of extensions and language servers providing their own language specific actions the workbench should provide actions to apply all available quick fixes to a file. I think that this is even possible with the extension API today since we can ask for all code actions for a given range.\nSee microsoft/vscode-eslint#70.",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "73",
      "number": "5650",
      "pretext": "VSCode Version: 1.0.0\nOS Version:\n\nInstead of extensions and language servers providing their own language specific actions the workbench should provide actions to apply all available quick fixes to a file. I think that this is even possible with the extension API today since we can ask for all code actions for a given range.\nSee microsoft/vscode-eslint#70.",
      "title": "Workbench should provide language agnostic actions to apply all quick fixes to a file"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2854,
    "text": "Missing localizations in 1.6VSCode Version: 1.6 insiders\nOS Version: doesn't matter\n\nIn Menu Items:\n\nHelp -> Search (Mac OS X)\nView -> Toggle Render Whitespace\nEdit -> Start Dictation (Mac OS X)\n\nIn Settings:\n\nThe below is not localized in User Settings, but is localized in Workspace Settings\n// Place your settings in this file to overwrite the default settings\nThe below is not localized in Keyboard Shortcut settings\n// Place your key bindings in this file to overwrite the defaults\n\nIn Launch.json\nTooltips for all properties except the below are not localized in launch.json\n\nname\ntype\nrequest\npreLaunch task\n\nwhile debugging\n\nThe title for the Debug Console in German appears as \"DebugKonsole\". There should be a space between the \"Debug\" and the \"Console\"\nAdd watch for a symbol. When it is not available, the value says \"not available\" in English\n\nwhile closing unsaved files\nThe dialog box that appears has text that is not localized",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "74",
      "number": "13103",
      "pretext": "VSCode Version: 1.6 insiders\nOS Version: doesn't matter\n\nIn Menu Items:\n\nHelp -> Search (Mac OS X)\nView -> Toggle Render Whitespace\nEdit -> Start Dictation (Mac OS X)\n\nIn Settings:\n\nThe below is not localized in User Settings, but is localized in Workspace Settings\n// Place your settings in this file to overwrite the default settings\nThe below is not localized in Keyboard Shortcut settings\n// Place your key bindings in this file to overwrite the defaults\n\nIn Launch.json\nTooltips for all properties except the below are not localized in launch.json\n\nname\ntype\nrequest\npreLaunch task\n\nwhile debugging\n\nThe title for the Debug Console in German appears as \"DebugKonsole\". There should be a space between the \"Debug\" and the \"Console\"\nAdd watch for a symbol. When it is not available, the value says \"not available\" in English\n\nwhile closing unsaved files\nThe dialog box that appears has text that is not localized",
      "title": "Missing localizations in 1.6"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2855,
    "text": "Other NSDocument apps don't recognize when VS Code changes a documentVSCode Version: 1.34.0\nOS Version: macOS Mojave 10.14.5\n\nSteps to Reproduce:\n\nOpen a document with an NSDocument-aware app such as TextEdit\n(Optionally, open that document with a second NSDocument-aware window)\nOpen that document with VS Code\n(Optionally, open that document with a second VS Code window)\n(Optionally, edit and save the document in the first app, observe that all VS Code windows update)\nEdit and save the document in VS Code, observe that the first app doesn't update (but all other VS Code windows update)\n(Optionally, re-save the document in the first app. Observe that, prior to interacting with the warning dialog, all other NSDocument-aware windows update.)\n\nThis bug impairs working with the same file in multiple applications on macOS.\nThis may be related to NSFilePresenter and presentedItemDidChange of NSDocument.\nDoes this issue occur when all extensions are disabled?: Yes",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "75",
      "number": "74759",
      "pretext": "VSCode Version: 1.34.0\nOS Version: macOS Mojave 10.14.5\n\nSteps to Reproduce:\n\nOpen a document with an NSDocument-aware app such as TextEdit\n(Optionally, open that document with a second NSDocument-aware window)\nOpen that document with VS Code\n(Optionally, open that document with a second VS Code window)\n(Optionally, edit and save the document in the first app, observe that all VS Code windows update)\nEdit and save the document in VS Code, observe that the first app doesn't update (but all other VS Code windows update)\n(Optionally, re-save the document in the first app. Observe that, prior to interacting with the warning dialog, all other NSDocument-aware windows update.)\n\nThis bug impairs working with the same file in multiple applications on macOS.\nThis may be related to NSFilePresenter and presentedItemDidChange of NSDocument.\nDoes this issue occur when all extensions are disabled?: Yes",
      "title": "Other NSDocument apps don't recognize when VS Code changes a document"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2856,
    "text": "Scrolling with touchpad behaves like pressing up and down arrow keysVSCode Version: 1.10.2\nOS Version: Windows 10\n\nSteps to Reproduce:\nI have a laptop with touchpad and scrolling (by touching the touchpad with two fingers and sliding them vertically) file content, files list, terminal window or anything inside VSCode behaves like I am pressing an holding up/down arrow keys. This behavior is not only annoying but it doesn't allow me to scroll say terminal window, because scrolling simply shows the list of last run commands - the same as if you press up arrow key multiple times. Horizontal scrolling behaves as left/right arrow keys - instead of scrolling horizontally, I move the caret. I have a wireless Microsoft mouse and when I use it for scrolling, everything works a expected. When I use my touchpad to scroll in the same way any other application like Chrome, Notepad, whatever, it behaves normally (the caret stays on its location and only the view is scrolled - actually I can normally scroll this GitHub's text area I am writing the issue text in). Does anyone have such problem with VSCode ? Could it because of touchpad drivers (I think mine is \"ELAN Pointing Device\") ?\nList of extensions:\n1 Debugger for Chrome 2.7.0\n2. Git History (git log) 0.2.0\n3. TSLint 0.8.1\n4. vscode-icons 7.4.0\nI tried to disable all extensions and reload VSCode but nothing changed.\nThe video below is made without using arrows keys - only scrolling functionality of my touchpad.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "76",
      "number": "23062",
      "pretext": "VSCode Version: 1.10.2\nOS Version: Windows 10\n\nSteps to Reproduce:\nI have a laptop with touchpad and scrolling (by touching the touchpad with two fingers and sliding them vertically) file content, files list, terminal window or anything inside VSCode behaves like I am pressing an holding up/down arrow keys. This behavior is not only annoying but it doesn't allow me to scroll say terminal window, because scrolling simply shows the list of last run commands - the same as if you press up arrow key multiple times. Horizontal scrolling behaves as left/right arrow keys - instead of scrolling horizontally, I move the caret. I have a wireless Microsoft mouse and when I use it for scrolling, everything works a expected. When I use my touchpad to scroll in the same way any other application like Chrome, Notepad, whatever, it behaves normally (the caret stays on its location and only the view is scrolled - actually I can normally scroll this GitHub's text area I am writing the issue text in). Does anyone have such problem with VSCode ? Could it because of touchpad drivers (I think mine is \"ELAN Pointing Device\") ?\nList of extensions:\n1 Debugger for Chrome 2.7.0\n2. Git History (git log) 0.2.0\n3. TSLint 0.8.1\n4. vscode-icons 7.4.0\nI tried to disable all extensions and reload VSCode but nothing changed.\nThe video below is made without using arrows keys - only scrolling functionality of my touchpad.",
      "title": "Scrolling with touchpad behaves like pressing up and down arrow keys"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2857,
    "text": "Iteration Plan for January 2018Happy 2018 everyone! This plan captures our work in January. This is a 5 week iteration. We will ship early February.\nEndgame\n\nJanuary 29th: Code freeze for the endgame\nFebruary 2nd: Endgame done\n\nThe endgame details for this iteration are tracked in #42374\nPlan Items\nBelow is a summary of the top level plan items. Given the large number of explorations, we'll diverge from our usual practice of having plan items for all bullets upfront. This time we'll add them as we go.\nLegend of annotations:\n\n\n\nMark\nDescription\n\n\n\n\n🏃\nwork in progress\n\n\n✋\nblocked task\n\n\n💪\nstretch goal for this iteration\n\n\n🔴\nmissing issue reference\n\n\n🔵\nmore investigation required to remove uncertainty\n\n\n⚫️\nunder discussion within the team\n\n\n\nInstall/Update\n\n Investigate in improving the update experience on Windows #41676 @joaomoreno\n\nWorkbench\n\n Switch to async dialog API #39536 @bpasero\n Support saving a file in admin mode #1614 @bpasero\n UX for notification improvements #22388 @bpasero @stevencl\n Multi-select in the Explorer, Open Editor #1023 @isidorn\n Reimplement drop downs for Linux/Windows (themable, fixes initial empty contents) #25965 (PR @cleidigh) @bpasero\n Enable Error decorations in explorer #782 @jrieken\n Support natural language search in Settings editor #40957 @roblourens\n Explore improving how a user changes a setting #41040 @roblourens @sandy081\n\nEditor\n\n Text model and storage reimplementation to improve performance #41042 @alexandrudima @rebornix\n Allow to save large files > 256 MB #32503 @bpasero @alexandrudima\n Support language-type independent snippets #13182 @jrieken\n More customization for the caret #41052 @ramya-rao-a\n\nDebug\n\n Launch configs for multi root workspaces #38134 @isidorn\n Support auto attach for node.js subprocess (aka cluster support) #40123 @weinand\n Support to use nvm configuratons in node launch configs #25386 @weinand\n Explore how to run DebugAdapter inside extension #40906 @weinand\n\nTerminal\n\n Improve accessibility of built-in terminal #8339 @Tyriar\n\nSCM\n\n Submodule support @joaomoreno\n Git commit message length counter @joaomoreno\n\nOutput Panel\n\n Show product logs in the output panel #39638 @sandy081\n Make log viewing in output panel more memory efficient #40196 @sandy081\n\nLanguages\nLanguage Server Support\n\n Create a website for LSP @dbaeumer @auchenberg\n Protocol extension for goto implementation microsoft/language-server-protocol#156 @dbaeumer\n\nEmmet\n\n Explore how to improve emmet activation in html and css files #29113 @ramya-rao-a\n\nJavaScript/TypeScript\n\n Adoption of TS 2.7 #41046 @mjbvz\n\nCSS/HTML\n\n Catchup with latest CSS/Less syntax microsoft/vscode-css-languageservice#56 microsoft/vscode-css-languageservice#57 microsoft/vscode-css-languageservice#58 microsoft/vscode-css-languageservice#47 @octref @aeschli\n\nExtensions\n\n  vsce - Warn when package.json misses repository entry #41677 @joaomoreno\n Improve quality of recommended extensions #41054 @ramya-rao-a\n Tastefully extend recommendations to a wider range of file types #38543 @ramya-rao-a\n\nExtension Contributions\n\n Refresh JS Hint support microsoft/vscode-jshint#48 @RMacfarlane\n\nAPI\n\n Migrate proposed Code action API to stable #34664 @jrieken @mjbvz\n Propose API to resolve rename/definition scope #7340 @jrieken @mjbvz\n Explore improving the HTMLPreview support #41047 @mjbvz @jrieken\n Propose refactoring provider API #41048 @jrieken @mjbvz\n Expose logging API to extensions #40053 @roblourens\n Propose Search Provider API @jrieken, @roblourens\n Propose API to create/delete/rename resources for refactorings @jrieken @mjbvz\n Enhance custom tree view API, primary actions, use icons from resource URI, preserve expansion state, improve managing contributions  #27823 @sandy081\n Debug API to create/remove breakpoints @weinand\n Support to modify the root folder of a workspace #35407 @bpasero\n\nPerformance\n\n 🏃 Use ASAR for bundled node modules #41350, #41353 @alexandrudima\n Explore using plain nodejs for helper processes #41685 @alexandrudima\n Explore local storage replacement #18439 @bpasero\n\nServiceability\n\n Issue reporter in separate renderer windows #41041 @RMacfarlane @octref\nLogs\n\n Propagate log level to all processes dynamically #39754 @sandy081\n Support to upload logs #40056 @mjbvz @roblourens\n\n\n\nEngineering\n\n Self-host on @ts-check for our JS code #41678 @joaomoreno @egamma\n 🏃 Support extensions that contribute translations aka \"language packs\" #39178 @dbaeumer @sandy081 @aeschli\n 🏃 Tool to generate/update a language pack from transifex #41682 @aeschli\n Speed up gulp-build @alexandrudima\nImprove issue tracking support bot @chrmarti\n\n Delay action of auto-assignment bot by 15s #33999\n Explore support for detecting duplicate issues #41292\n\n\n\nDocumentations\n\n Make 5min Node.js debugging video to be embedded in docs and uploaded to YouTube channel @auchenberg\n Add Debugging Recipe for VueJS. microsoft/vscode-recipes#55 @auchenberg\n\n\nDeferred\n\n\nImprove documentation of our electron upgrade process #41036 @Tyriar\n\n\nAdd intelisense support for src  attributes and href in html #2037 @octref @aeschli\n\n\nAdopt logging service  @joaomoreno the adoption by the #41680 team\n\n\n💪 Render white space for selection option #1477 @ramya-rao-a\n\n\nProvide API for creating a file based output channel #41672 @sandy081\n\n\n✋ Electron update to 2.0 @Tyriar @bpasero\n\n\n💪 Docathon team\n\n\n✋ Support 32-bit apt repositories #20790 @Tyriar\n\n\nBetter support for webpack TBD\n\n\nSupport web-site for issue reporting @octref @RMacfarlane\n\n\nImprove stability of smoke test, run it as part of the builds #41679 # @joaomoreno\n\n\n💪 Process explorer as a separate renderer window #41045 @RMacfarlane",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "77",
      "number": "41061",
      "pretext": "Happy 2018 everyone! This plan captures our work in January. This is a 5 week iteration. We will ship early February.\nEndgame\n\nJanuary 29th: Code freeze for the endgame\nFebruary 2nd: Endgame done\n\nThe endgame details for this iteration are tracked in #42374\nPlan Items\nBelow is a summary of the top level plan items. Given the large number of explorations, we'll diverge from our usual practice of having plan items for all bullets upfront. This time we'll add them as we go.\nLegend of annotations:\n\n\n\nMark\nDescription\n\n\n\n\n🏃\nwork in progress\n\n\n✋\nblocked task\n\n\n💪\nstretch goal for this iteration\n\n\n🔴\nmissing issue reference\n\n\n🔵\nmore investigation required to remove uncertainty\n\n\n⚫️\nunder discussion within the team\n\n\n\nInstall/Update\n\n Investigate in improving the update experience on Windows #41676 @joaomoreno\n\nWorkbench\n\n Switch to async dialog API #39536 @bpasero\n Support saving a file in admin mode #1614 @bpasero\n UX for notification improvements #22388 @bpasero @stevencl\n Multi-select in the Explorer, Open Editor #1023 @isidorn\n Reimplement drop downs for Linux/Windows (themable, fixes initial empty contents) #25965 (PR @cleidigh) @bpasero\n Enable Error decorations in explorer #782 @jrieken\n Support natural language search in Settings editor #40957 @roblourens\n Explore improving how a user changes a setting #41040 @roblourens @sandy081\n\nEditor\n\n Text model and storage reimplementation to improve performance #41042 @alexandrudima @rebornix\n Allow to save large files > 256 MB #32503 @bpasero @alexandrudima\n Support language-type independent snippets #13182 @jrieken\n More customization for the caret #41052 @ramya-rao-a\n\nDebug\n\n Launch configs for multi root workspaces #38134 @isidorn\n Support auto attach for node.js subprocess (aka cluster support) #40123 @weinand\n Support to use nvm configuratons in node launch configs #25386 @weinand\n Explore how to run DebugAdapter inside extension #40906 @weinand\n\nTerminal\n\n Improve accessibility of built-in terminal #8339 @Tyriar\n\nSCM\n\n Submodule support @joaomoreno\n Git commit message length counter @joaomoreno\n\nOutput Panel\n\n Show product logs in the output panel #39638 @sandy081\n Make log viewing in output panel more memory efficient #40196 @sandy081\n\nLanguages\nLanguage Server Support\n\n Create a website for LSP @dbaeumer @auchenberg\n Protocol extension for goto implementation microsoft/language-server-protocol#156 @dbaeumer\n\nEmmet\n\n Explore how to improve emmet activation in html and css files #29113 @ramya-rao-a\n\nJavaScript/TypeScript\n\n Adoption of TS 2.7 #41046 @mjbvz\n\nCSS/HTML\n\n Catchup with latest CSS/Less syntax microsoft/vscode-css-languageservice#56 microsoft/vscode-css-languageservice#57 microsoft/vscode-css-languageservice#58 microsoft/vscode-css-languageservice#47 @octref @aeschli\n\nExtensions\n\n  vsce - Warn when package.json misses repository entry #41677 @joaomoreno\n Improve quality of recommended extensions #41054 @ramya-rao-a\n Tastefully extend recommendations to a wider range of file types #38543 @ramya-rao-a\n\nExtension Contributions\n\n Refresh JS Hint support microsoft/vscode-jshint#48 @RMacfarlane\n\nAPI\n\n Migrate proposed Code action API to stable #34664 @jrieken @mjbvz\n Propose API to resolve rename/definition scope #7340 @jrieken @mjbvz\n Explore improving the HTMLPreview support #41047 @mjbvz @jrieken\n Propose refactoring provider API #41048 @jrieken @mjbvz\n Expose logging API to extensions #40053 @roblourens\n Propose Search Provider API @jrieken, @roblourens\n Propose API to create/delete/rename resources for refactorings @jrieken @mjbvz\n Enhance custom tree view API, primary actions, use icons from resource URI, preserve expansion state, improve managing contributions  #27823 @sandy081\n Debug API to create/remove breakpoints @weinand\n Support to modify the root folder of a workspace #35407 @bpasero\n\nPerformance\n\n 🏃 Use ASAR for bundled node modules #41350, #41353 @alexandrudima\n Explore using plain nodejs for helper processes #41685 @alexandrudima\n Explore local storage replacement #18439 @bpasero\n\nServiceability\n\n Issue reporter in separate renderer windows #41041 @RMacfarlane @octref\nLogs\n\n Propagate log level to all processes dynamically #39754 @sandy081\n Support to upload logs #40056 @mjbvz @roblourens\n\n\n\nEngineering\n\n Self-host on @ts-check for our JS code #41678 @joaomoreno @egamma\n 🏃 Support extensions that contribute translations aka \"language packs\" #39178 @dbaeumer @sandy081 @aeschli\n 🏃 Tool to generate/update a language pack from transifex #41682 @aeschli\n Speed up gulp-build @alexandrudima\nImprove issue tracking support bot @chrmarti\n\n Delay action of auto-assignment bot by 15s #33999\n Explore support for detecting duplicate issues #41292\n\n\n\nDocumentations\n\n Make 5min Node.js debugging video to be embedded in docs and uploaded to YouTube channel @auchenberg\n Add Debugging Recipe for VueJS. microsoft/vscode-recipes#55 @auchenberg\n\n\nDeferred\n\n\nImprove documentation of our electron upgrade process #41036 @Tyriar\n\n\nAdd intelisense support for src  attributes and href in html #2037 @octref @aeschli\n\n\nAdopt logging service  @joaomoreno the adoption by the #41680 team\n\n\n💪 Render white space for selection option #1477 @ramya-rao-a\n\n\nProvide API for creating a file based output channel #41672 @sandy081\n\n\n✋ Electron update to 2.0 @Tyriar @bpasero\n\n\n💪 Docathon team\n\n\n✋ Support 32-bit apt repositories #20790 @Tyriar\n\n\nBetter support for webpack TBD\n\n\nSupport web-site for issue reporting @octref @RMacfarlane\n\n\nImprove stability of smoke test, run it as part of the builds #41679 # @joaomoreno\n\n\n💪 Process explorer as a separate renderer window #41045 @RMacfarlane",
      "title": "Iteration Plan for January 2018"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2858,
    "text": "Test editor.multicursorModifierTesting #27193\n\n OSX @bpasero\n windows @chrisdias\n linux @isidorn\n\nComplexity: 2\nThere is a new setting editor.multicursorModifier. On Linux and Windows it can have the value \"ctrl\" or \"alt\". On OSX it can have the value \"cmd\" or \"alt\". Please check:\n\nthe default is \"alt\" (just as before). So without any changes, out of the box, multiple cursors are added via \"alt\" + click. It is a known issue that under some Linux distributions, multiple cursors cannot be added via \"alt\" + click, which is used to move windows, this new option aims to also overcome this limitation.\nwhen using alt as the multicursor modifier: ctrl+click (cmd+click) is used for going to definition and opening links. ctrl+alt+click (cmd+alt+click) is used for going to definition and opening in a side editor and for opening a link in a side editor (you can craft a file:/// link to test this)\nwhen using ctrl or cmd as the multicursor modifier, the 3 features swap modifiers and going to definition / opening a link is alt+click. ctrl+alt+click (cmd+alt+click) will still be used for going to definition / opening a link to the side. The hover message on links correctly shows the modifier to be used to open a link.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "78",
      "number": "27480",
      "pretext": "Testing #27193\n\n OSX @bpasero\n windows @chrisdias\n linux @isidorn\n\nComplexity: 2\nThere is a new setting editor.multicursorModifier. On Linux and Windows it can have the value \"ctrl\" or \"alt\". On OSX it can have the value \"cmd\" or \"alt\". Please check:\n\nthe default is \"alt\" (just as before). So without any changes, out of the box, multiple cursors are added via \"alt\" + click. It is a known issue that under some Linux distributions, multiple cursors cannot be added via \"alt\" + click, which is used to move windows, this new option aims to also overcome this limitation.\nwhen using alt as the multicursor modifier: ctrl+click (cmd+click) is used for going to definition and opening links. ctrl+alt+click (cmd+alt+click) is used for going to definition and opening in a side editor and for opening a link in a side editor (you can craft a file:/// link to test this)\nwhen using ctrl or cmd as the multicursor modifier, the 3 features swap modifiers and going to definition / opening a link is alt+click. ctrl+alt+click (cmd+alt+click) will still be used for going to definition / opening a link to the side. The hover message on links correctly shows the modifier to be used to open a link.",
      "title": "Test editor.multicursorModifier"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2859,
    "text": "IDE slows to freeze as more and more output written to internal debug consoleVSCode Version: 1.30.2\nOS Version: Windows 10 x64\n\nSteps to Reproduce:\n\nDebug a GO program\nThe program writes to the standard output\nIDE starts to slow down and finally freeze.",
    "annotations": [{ "label": 143, "user": 3 }],
    "meta": {
      "": "79",
      "number": "66311",
      "pretext": "VSCode Version: 1.30.2\nOS Version: Windows 10 x64\n\nSteps to Reproduce:\n\nDebug a GO program\nThe program writes to the standard output\nIDE starts to slow down and finally freeze.",
      "title": "IDE slows to freeze as more and more output written to internal debug console"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2860,
    "text": "Markdown preview doesn't work correctly when zoomed inWhile smoke testing 1.7.2 eb1f17e",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "80",
      "number": "15542",
      "pretext": "While smoke testing 1.7.2 eb1f17e",
      "title": "Markdown preview doesn't work correctly when zoomed in"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2861,
    "text": "nodejs debugger nodemon crush i use nodemon in project and it work smooth and good but when i try to run exact gulp command 'gulp serve' in vs code debugger it cuase following error:\n'[nodemon] app crashed - waiting for file changes before starting...'\nit work really when i type gulp command but in debug mode i get error.\nthe code is:\nvar nodemonStream = nodemon({\n       script: './Server/Bootstrap.js',\n       inspect:true,\n       done:done\n   });",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "81",
      "number": "65163",
      "pretext": "i use nodemon in project and it work smooth and good but when i try to run exact gulp command 'gulp serve' in vs code debugger it cuase following error:\n'[nodemon] app crashed - waiting for file changes before starting...'\nit work really when i type gulp command but in debug mode i get error.\nthe code is:\nvar nodemonStream = nodemon({\n       script: './Server/Bootstrap.js',\n       inspect:true,\n       done:done\n   });",
      "title": "nodejs debugger nodemon crush "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2862,
    "text": "C++ intelliSense with different file system types (Windows 10)VSCode Version: Code - Insiders 1.17.0-insider (128a4e3, 2017-09-12T05:24:19.607Z)\nOS Version: Windows_NT x64 10.0.15063\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\ncpptools\nms-\n0.12.4\n\n\n\nIntelliSense doesn't seem to be working with SSHFS filesystem type in Windows 10 (no import errors detected and no squiggly lines). I know this is a fringe case, but I'm wondering if there is an easy fix.\nIntelliSense works fine for this example in the local filesystem as well as on an exfat formatted flash drive.\n\nSteps to Reproduce:\n\nInstall winsshfs and mount a remote file system\nOpen the a project locally from that mounted file system\n\n\n\n\nReproduces without extensions: No (Need the C++ extension for intellisense)",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "82",
      "number": "34586",
      "pretext": "VSCode Version: Code - Insiders 1.17.0-insider (128a4e3, 2017-09-12T05:24:19.607Z)\nOS Version: Windows_NT x64 10.0.15063\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\ncpptools\nms-\n0.12.4\n\n\n\nIntelliSense doesn't seem to be working with SSHFS filesystem type in Windows 10 (no import errors detected and no squiggly lines). I know this is a fringe case, but I'm wondering if there is an easy fix.\nIntelliSense works fine for this example in the local filesystem as well as on an exfat formatted flash drive.\n\nSteps to Reproduce:\n\nInstall winsshfs and mount a remote file system\nOpen the a project locally from that mounted file system\n\n\n\n\nReproduces without extensions: No (Need the C++ extension for intellisense)",
      "title": "C++ intelliSense with different file system types (Windows 10)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2863,
    "text": "Test extension API for breakpoints Test for #23188:\nComplexity: 4\n\n Any OS - @jrieken\n\nThe November milestone of VS Code proposes extension API for reading the breakpoints of a workspace and tracking added, removed, and changed breakpoints:\nhttps://github.com/Microsoft/vscode/blob/a42cd0efc5b4baa17075fcd8da1c5e2097419c6f/src/vs/vscode.proposed.d.ts#L251-L329\nVerify:\n\nAPI makes sense (especially the Breakpoint, SourceBreakpoint, FunctionBreakpoint hierarchy and its use of the type discriminator). Will this work if we extend the API to create those types?\nwrite a simple extension that accesses breakpoints and registers for BreakpointsChangeEvents. Please note that accessing breakpoints initially returns an empty array but triggers a subsequent event that has the full set of breakpoints in its added property.",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "83",
      "number": "39553",
      "pretext": "Test for #23188:\nComplexity: 4\n\n Any OS - @jrieken\n\nThe November milestone of VS Code proposes extension API for reading the breakpoints of a workspace and tracking added, removed, and changed breakpoints:\nhttps://github.com/Microsoft/vscode/blob/a42cd0efc5b4baa17075fcd8da1c5e2097419c6f/src/vs/vscode.proposed.d.ts#L251-L329\nVerify:\n\nAPI makes sense (especially the Breakpoint, SourceBreakpoint, FunctionBreakpoint hierarchy and its use of the type discriminator). Will this work if we extend the API to create those types?\nwrite a simple extension that accesses breakpoints and registers for BreakpointsChangeEvents. Please note that accessing breakpoints initially returns an empty array but triggers a subsequent event that has the full set of breakpoints in its added property.",
      "title": "Test extension API for breakpoints "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2864,
    "text": "new behavior of File duplication not smart(file<space>copy.ext)Issue Type: Performance Issue\nThe new behavior of file copy pasting (ctrl+c, ctrl+v) in the proj explorer adds whitespace to the new file name, often causing command line scripts to not work. including double quotes to the script is not always easy, and many devs(probably you as well) have a habit of eliminating whites just to avoid this clumsiness.\nOlder behavior was much better, if not best, file.ext -> file.1.ext\nplz restore it\nVS Code version: Code 1.36.1 (2213894, 2019-07-08T22:59:35.033Z)\nOS version: Windows_NT x64 10.0.17134\n\nWorkspace Info\n|  Window (LightStickUI.1.ino - LightStickUI - Visual Studio Code)\n|  Window (Transparent_Sprite_Demo.ino - Transparent_Sprite_Demo - Visual Studio Code)\n|    Folder (Transparent_Sprite_Demo): 3 files\n|      File types: json(2) ino(1)\n|      Conf files:\n|    Folder (LightStickUI): 6 files\n|      File types: json(3) ino(2) cpp(1)\n|      Conf files:;\n\n\nExtensions (2)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\ncpptools\nms-\n0.24.0\n\n\nvscode-arduino\nvsc\n0.2.27\n\n\n\n(1 theme extensions excluded)",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "84",
      "number": "77406",
      "pretext": "Issue Type: Performance Issue\nThe new behavior of file copy pasting (ctrl+c, ctrl+v) in the proj explorer adds whitespace to the new file name, often causing command line scripts to not work. including double quotes to the script is not always easy, and many devs(probably you as well) have a habit of eliminating whites just to avoid this clumsiness.\nOlder behavior was much better, if not best, file.ext -> file.1.ext\nplz restore it\nVS Code version: Code 1.36.1 (2213894, 2019-07-08T22:59:35.033Z)\nOS version: Windows_NT x64 10.0.17134\n\nWorkspace Info\n|  Window (LightStickUI.1.ino - LightStickUI - Visual Studio Code)\n|  Window (Transparent_Sprite_Demo.ino - Transparent_Sprite_Demo - Visual Studio Code)\n|    Folder (Transparent_Sprite_Demo): 3 files\n|      File types: json(2) ino(1)\n|      Conf files:\n|    Folder (LightStickUI): 6 files\n|      File types: json(3) ino(2) cpp(1)\n|      Conf files:;\n\n\nExtensions (2)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\ncpptools\nms-\n0.24.0\n\n\nvscode-arduino\nvsc\n0.2.27\n\n\n\n(1 theme extensions excluded)",
      "title": "new behavior of File duplication not smart(file<space>copy.ext)"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2865,
    "text": "FTP file gets deleted when drag and drop from file explorerVSCode Version: 1.10.1\nOS Version: Windows 10 Home 1607\n\nSteps to Reproduce:\n\nOpen file explorer\nOpen any ftp folder\n\nDrag an drop any file from FIle Explorer into VSCode\nThe dragged file gets removed from FTP folder, but it is not openend in VSCode",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "85",
      "number": "21868",
      "pretext": "VSCode Version: 1.10.1\nOS Version: Windows 10 Home 1607\n\nSteps to Reproduce:\n\nOpen file explorer\nOpen any ftp folder\n\nDrag an drop any file from FIle Explorer into VSCode\nThe dragged file gets removed from FTP folder, but it is not openend in VSCode",
      "title": "FTP file gets deleted when drag and drop from file explorer"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2866,
    "text": "Cannot run integration test while having the vim extension installed Testing #27456\nI have the vim extension installed and disabled (always), but it looks like the integration test picks it up and enables it. This causes the integration test to fail in the Data Migration -> checks if the Untitled file is restored migrating from stable to latest test\nI will continue by uninstalling the vim extension.",
    "annotations": [{ "label": 144, "user": 3 }],
    "meta": {
      "": "86",
      "number": "27537",
      "pretext": "Testing #27456\nI have the vim extension installed and disabled (always), but it looks like the integration test picks it up and enables it. This causes the integration test to fail in the Data Migration -> checks if the Untitled file is restored migrating from stable to latest test\nI will continue by uninstalling the vim extension.",
      "title": "Cannot run integration test while having the vim extension installed "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2867,
    "text": "Bug when running python code in console after running it using the \"run the selection\" featureIssue Type: Bug\n1- Write print('hello') in a new python file\n2- Select the chunk of code and run it with run selection (shift+enter)\n3- Run the code again, but using run python file in terminal.\nexpected: hello\nactual:\nFile \"\", line 1\n& C:/Users/me/AppData/Local/Programs/Python/Python37-32/python.exe \"mypath/test.py\"\n^\nSyntaxError: invalid syntax\nVS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:35:15.005Z)\nOS version: Windows_NT x64 10.0.18362\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i5-8350U CPU @ 1.70GHz (8 x 1896)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwareoop_rasterization: disabled_offprotected_video_decode: enabledrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: enabledviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\nundefined\n\n\nMemory (System)\n7.85GB (1.38GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "87",
      "number": "81149",
      "pretext": "Issue Type: Bug\n1- Write print('hello') in a new python file\n2- Select the chunk of code and run it with run selection (shift+enter)\n3- Run the code again, but using run python file in terminal.\nexpected: hello\nactual:\nFile \"\", line 1\n& C:/Users/me/AppData/Local/Programs/Python/Python37-32/python.exe \"mypath/test.py\"\n^\nSyntaxError: invalid syntax\nVS Code version: Code 1.38.1 (b37e54c, 2019-09-11T13:35:15.005Z)\nOS version: Windows_NT x64 10.0.18362\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i5-8350U CPU @ 1.70GHz (8 x 1896)\n\n\nGPU Status\n2d_canvas: enabledflash_3d: enabledflash_stage3d: enabledflash_stage3d_baseline: enabledgpu_compositing: enabledmultiple_raster_threads: enabled_onnative_gpu_memory_buffers: disabled_softwareoop_rasterization: disabled_offprotected_video_decode: enabledrasterization: enabledskia_deferred_display_list: disabled_offskia_renderer: disabled_offsurface_synchronization: enabled_onvideo_decode: enabledviz_display_compositor: disabled_offwebgl: enabledwebgl2: enabled\n\n\nLoad (avg)\nundefined\n\n\nMemory (System)\n7.85GB (1.38GB free)\n\n\nProcess Argv\n\n\n\nScreen Reader\nno\n\n\nVM\n0%",
      "title": "Bug when running python code in console after running it using the \"run the selection\" feature"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2868,
    "text": "CLI code snippets do not change font colorWhen I use a code block for something like powershell or t-sql, the colors of the font change, making it easier to parse the text.\nFor example\n\n\nFont color changes in accordance to powershell standards.\nHowever, when I do so for CLI, the color doesn't change, and it should be changing, right?\n\n\nNo font color change in accordance to CLI standards.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "88",
      "number": "68659",
      "pretext": "When I use a code block for something like powershell or t-sql, the colors of the font change, making it easier to parse the text.\nFor example\n\n\nFont color changes in accordance to powershell standards.\nHowever, when I do so for CLI, the color doesn't change, and it should be changing, right?\n\n\nNo font color change in accordance to CLI standards.",
      "title": "CLI code snippets do not change font color"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2869,
    "text": "[SCM tree]: Keep first level indent the same as listPretty similar to #66863. SCM tree should exclude first level items and keep indent at 8px\n\n\n\n\"scm.defaultViewMode\": \"tree\",\n\nIndent for first level items is too big, because of\n\n\"workbench.tree.indent\": 20,",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "89",
      "number": "82420",
      "pretext": "Pretty similar to #66863. SCM tree should exclude first level items and keep indent at 8px\n\n\n\n\"scm.defaultViewMode\": \"tree\",\n\nIndent for first level items is too big, because of\n\n\"workbench.tree.indent\": 20,",
      "title": "[SCM tree]: Keep first level indent the same as list"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2870,
    "text": "When launching attach config unsuccessfully, debug status bar item does not showWhile testing #35904\nI don't know if this is as-designed, but this seems to be one of the original motivation for #31745.\nWhen I'm pressing F5 while not focusing on debug viewlet, I got no visual feedback as to what's the debug target.\nWould it make sense to enable the status bar item on launching debug instead of on launching debug successfully?",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "90",
      "number": "37335",
      "pretext": "While testing #35904\nI don't know if this is as-designed, but this seems to be one of the original motivation for #31745.\nWhen I'm pressing F5 while not focusing on debug viewlet, I got no visual feedback as to what's the debug target.\nWould it make sense to enable the status bar item on launching debug instead of on launching debug successfully?",
      "title": "When launching attach config unsuccessfully, debug status bar item does not show"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2871,
    "text": "Why the dark+ theme's syntax is different from the light+ theme's syntax?Why the dark+ theme's syntax is different from the light+ theme's syntax?\n\n\nVS Code version: Code 1.22.2 (3aeede7, 2018-04-12T16:38:45.278Z)\nOS version: Windows_NT x64 10.0.15063\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i5-4210H CPU @ 2.90GHz (4 x 2893)\n\n\nMemory (System)\n7.89GB (1.54GB free)\n\n\nProcess Argv\nC:\\Program Files\\Microsoft VS Code\\Code.exe\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (9)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nvscode-custom-css\nbe5\n2.7.0\n\n\nmarkdown-preview-github-styles\nbie\n0.1.2\n\n\nvscode-eslint\ndba\n1.4.8\n\n\nEditorConfig\nEdi\n0.12.1\n\n\ntslint\neg2\n1.0.28\n\n\nvsc-material-theme\nEqu\n2.0.1\n\n\ndebugger-for-chrome\nmsj\n4.3.0\n\n\nadvanced-new-file\npat\n1.2.0\n\n\nvscode-icons\nrob\n7.23.0",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "91",
      "number": "48889",
      "pretext": "Why the dark+ theme's syntax is different from the light+ theme's syntax?\n\n\nVS Code version: Code 1.22.2 (3aeede7, 2018-04-12T16:38:45.278Z)\nOS version: Windows_NT x64 10.0.15063\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i5-4210H CPU @ 2.90GHz (4 x 2893)\n\n\nMemory (System)\n7.89GB (1.54GB free)\n\n\nProcess Argv\nC:\\Program Files\\Microsoft VS Code\\Code.exe\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (9)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nvscode-custom-css\nbe5\n2.7.0\n\n\nmarkdown-preview-github-styles\nbie\n0.1.2\n\n\nvscode-eslint\ndba\n1.4.8\n\n\nEditorConfig\nEdi\n0.12.1\n\n\ntslint\neg2\n1.0.28\n\n\nvsc-material-theme\nEqu\n2.0.1\n\n\ndebugger-for-chrome\nmsj\n4.3.0\n\n\nadvanced-new-file\npat\n1.2.0\n\n\nvscode-icons\nrob\n7.23.0",
      "title": "Why the dark+ theme's syntax is different from the light+ theme's syntax?"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2872,
    "text": "No completion proposal for the new presentation property.I've upgraded a task.json to 2.0.0 and get a deprecated warning\n\nTrying to fix by inserting a presentation property, but there is no Intellisense proposal for presentation.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "92",
      "number": "29736",
      "pretext": "I've upgraded a task.json to 2.0.0 and get a deprecated warning\n\nTrying to fix by inserting a presentation property, but there is no Intellisense proposal for presentation.",
      "title": "No completion proposal for the new presentation property."
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2873,
    "text": "Replace textHi Team,\nI have replaced a word from multiple files through find tab and saved all.\nBut again when i open the ts file to edit some other line , its is showing a working directory.\nIt was fixing by VSC restart.\nPlease fix this with an alternative",
    "annotations": [{ "label": 142, "user": 3 }],
    "meta": {
      "": "93",
      "number": "57059",
      "pretext": "Hi Team,\nI have replaced a word from multiple files through find tab and saved all.\nBut again when i open the ts file to edit some other line , its is showing a working directory.\nIt was fixing by VSC restart.\nPlease fix this with an alternative",
      "title": "Replace text"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2874,
    "text": "Text gets selected, to the next line, when the menu bar auto hidesVSCode Version: Code 1.15.1 (41abd21, 2017-08-16T18:07:25.676Z)\nOS Version: Windows_NT x64 10.0.15063\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nspellright\nban\n1.1.16\n\n\npython\ndon\n0.7.0\n\n\ncpptools\nms-\n0.12.3\n\n\ncsharp\nms-\n1.12.1\n\n\nPowerShell\nms-\n1.4.1\n\n\nblank-line-organizer\nrin\n0.1.2\n\n\nsort-lines\nTyr\n1.3.0\n\n\nchange-case\nwma\n1.0.0\n\n\n\n\nSteps to Reproduce:\n\nSet your menu bar to auto hide (i.e. set it to toggle).\nPress Alt to show the menu bar.\nClick somewhere near the beginning of a line of text in your currently visible text document.\nAs the menu bar disappears, notice that the text gets selected, started from the position where you clicked and ending on the line below where you clicked.\n\nExpected:\nNothing should get selected when the menu bar auto hides. The cursor should simply be wherever you clicked.\n\nReproduces without extensions: Yes/No",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "94",
      "number": "32853",
      "pretext": "VSCode Version: Code 1.15.1 (41abd21, 2017-08-16T18:07:25.676Z)\nOS Version: Windows_NT x64 10.0.15063\nExtensions:\n\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nspellright\nban\n1.1.16\n\n\npython\ndon\n0.7.0\n\n\ncpptools\nms-\n0.12.3\n\n\ncsharp\nms-\n1.12.1\n\n\nPowerShell\nms-\n1.4.1\n\n\nblank-line-organizer\nrin\n0.1.2\n\n\nsort-lines\nTyr\n1.3.0\n\n\nchange-case\nwma\n1.0.0\n\n\n\n\nSteps to Reproduce:\n\nSet your menu bar to auto hide (i.e. set it to toggle).\nPress Alt to show the menu bar.\nClick somewhere near the beginning of a line of text in your currently visible text document.\nAs the menu bar disappears, notice that the text gets selected, started from the position where you clicked and ending on the line below where you clicked.\n\nExpected:\nNothing should get selected when the menu bar auto hides. The cursor should simply be wherever you clicked.\n\nReproduces without extensions: Yes/No",
      "title": "Text gets selected, to the next line, when the menu bar auto hides"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2875,
    "text": "Go to current file from viewing diff. VSCode Version: latest\nOS Version: OSX\n\nSteps to Reproduce:\n\nedit a file in a Git repo with VS Code\nclick the Git icon in the left well to see the diff.\n\nI often want to check the differences between two files, and then immediately go to the current working index of that file.\nexpect: (something like) cmd+ right click on the current working index of a Git diff takes you to that file.",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "95",
      "number": "26483",
      "pretext": "VSCode Version: latest\nOS Version: OSX\n\nSteps to Reproduce:\n\nedit a file in a Git repo with VS Code\nclick the Git icon in the left well to see the diff.\n\nI often want to check the differences between two files, and then immediately go to the current working index of that file.\nexpect: (something like) cmd+ right click on the current working index of a Git diff takes you to that file.",
      "title": "Go to current file from viewing diff. "
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2876,
    "text": "unable to update Issue Type: Bug\nI am using vs code, and i am facing issue regarding updates .\nwhy it always require admin permission to start, and\nIt able to fetch updates but does not able to update.\nVS Code version: Code 1.24.0 (6a6e02c, 2018-06-06T17:35:40.560Z)\nOS version: Windows_NT x64 6.0.6002\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i3-5005U CPU @ 2.00GHz (4 x 1995)\n\n\nGPU Status\n2d_canvas: unavailable_softwareflash_3d: unavailable_softwareflash_stage3d: unavailable_softwareflash_stage3d_baseline: unavailable_softwaregpu_compositing: unavailable_softwaremultiple_raster_threads: unavailable_offnative_gpu_memory_buffers: disabled_softwarerasterization: unavailable_softwarevideo_decode: unavailable_softwarevideo_encode: unavailable_softwarevpx_decode: unavailable_softwarewebgl: unavailable_offwebgl2: unavailable_off\n\n\nMemory (System)\n7.92GB (4.17GB free)\n\n\nProcess Argv\nC:\\Program Files\\Microsoft VS Code\\Code.exe\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (4)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nhtml-snippets\nabu\n0.2.1\n\n\nvscode-html-css\necm\n0.2.0\n\n\nbeautify\nHoo\n1.3.0\n\n\ntrimspaces\nigo\n0.0.24",
    "annotations": [{ "label": 148, "user": 1 }],
    "meta": {
      "": "96",
      "number": "51415",
      "pretext": "Issue Type: Bug\nI am using vs code, and i am facing issue regarding updates .\nwhy it always require admin permission to start, and\nIt able to fetch updates but does not able to update.\nVS Code version: Code 1.24.0 (6a6e02c, 2018-06-06T17:35:40.560Z)\nOS version: Windows_NT x64 6.0.6002\n\nSystem Info\n\n\n\nItem\nValue\n\n\n\n\nCPUs\nIntel(R) Core(TM) i3-5005U CPU @ 2.00GHz (4 x 1995)\n\n\nGPU Status\n2d_canvas: unavailable_softwareflash_3d: unavailable_softwareflash_stage3d: unavailable_softwareflash_stage3d_baseline: unavailable_softwaregpu_compositing: unavailable_softwaremultiple_raster_threads: unavailable_offnative_gpu_memory_buffers: disabled_softwarerasterization: unavailable_softwarevideo_decode: unavailable_softwarevideo_encode: unavailable_softwarevpx_decode: unavailable_softwarewebgl: unavailable_offwebgl2: unavailable_off\n\n\nMemory (System)\n7.92GB (4.17GB free)\n\n\nProcess Argv\nC:\\Program Files\\Microsoft VS Code\\Code.exe\n\n\nScreen Reader\nno\n\n\nVM\n0%\n\n\n\nExtensions (4)\n\n\n\nExtension\nAuthor (truncated)\nVersion\n\n\n\n\nhtml-snippets\nabu\n0.2.1\n\n\nvscode-html-css\necm\n0.2.0\n\n\nbeautify\nHoo\n1.3.0\n\n\ntrimspaces\nigo\n0.0.24",
      "title": "unable to update "
    },
    "annotation_approver": null
  },
  {
    "id": 2877,
    "text": "Move editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not workingEnvironment Details:\nVSCode Version : 1.24.1\nAdditional Details:\nMAS Violated: MAS2.1.1\nRepro Steps:\n1)Launch VS Code.\n2)Open Keyboard Shortcuts.\nActual:\nMove editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not working. After using this command, screen starts rotating.\nExpected:\nBoth the commands should work.\nRecommendations:\nRefer below link which is repository of bug fixes code snippets:\nhttps://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx\nMAS Reference\nhttps://microsoft.sharepoint.com/:w:/r/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}\nAttachment for Reference:\n\nDoes this issue occur when all extensions are disabled?: Yes",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "97",
      "number": "53485",
      "pretext": "Environment Details:\nVSCode Version : 1.24.1\nAdditional Details:\nMAS Violated: MAS2.1.1\nRepro Steps:\n1)Launch VS Code.\n2)Open Keyboard Shortcuts.\nActual:\nMove editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not working. After using this command, screen starts rotating.\nExpected:\nBoth the commands should work.\nRecommendations:\nRefer below link which is repository of bug fixes code snippets:\nhttps://microsoft.sharepoint.com/teams/msenable/mas/pages/browse-fixes.aspx\nMAS Reference\nhttps://microsoft.sharepoint.com/:w:/r/teams/msenable/_layouts/15/WopiFrame.aspx?sourcedoc={8492c4eb-c179-40ae-8777-cd044ed725a2}\nAttachment for Reference:\n\nDoes this issue occur when all extensions are disabled?: Yes",
      "title": "Move editor group into next/previous group(Ctrl+Alt+ Left/Right arrow) commands are not working"
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2878,
    "text": "close icon on terminal/console/etc. pane should be \"x\"The terminal, debug output, console, etc. panes at the bottom have a small down arrow for the action to close or hide the pane. clicking on this makes the pane slide down and disappear.\n\nThe down arrow to me suggests that the window will be collapsed down rather than closed. Whenever I click on this I expect there to be an action at the bottom of the editor to restore the pane. Instead I have to use the keyboard or menu to bring these back.\nAs a result, a better icon for this pane would be the close \"x\".\nAlternatively, there should be a visualization at the bottom of the editor that there is a pane that can be restored.",
    "annotations": [{ "label": 146, "user": 3 }],
    "meta": {
      "": "98",
      "number": "9638",
      "pretext": "The terminal, debug output, console, etc. panes at the bottom have a small down arrow for the action to close or hide the pane. clicking on this makes the pane slide down and disappear.\n\nThe down arrow to me suggests that the window will be collapsed down rather than closed. Whenever I click on this I expect there to be an action at the bottom of the editor to restore the pane. Instead I have to use the keyboard or menu to bring these back.\nAs a result, a better icon for this pane would be the close \"x\".\nAlternatively, there should be a visualization at the bottom of the editor that there is a pane that can be restored.",
      "title": "close icon on terminal/console/etc. pane should be \"x\""
    },
    "annotation_approver": "jocharan"
  },
  {
    "id": 2879,
    "text": "ERR Model is disposed!: Error: Model is disposed!ERR Model is disposed!: Error: Model is disposed!\n    at TextModel._assertNotDisposed (file:///Users/jrieken/Code/vscode/out/vs/editor/common/model/textModel.js:245:23)\n    at TextModel.getVersionId (file:///Users/jrieken/Code/vscode/out/vs/editor/common/model/textModel.js:499:18)\n    at OutlinePanel.<anonymous> (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:548:61)\n    at step (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:50:23)\n    at Object.next (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:31:53)\n    at fulfilled (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:22:58)\n    at <anonymous>\n    at process._tickCallback (internal/process/next_tick.js:109:7)",
    "annotations": [{ "label": 148, "user": 3 }],
    "meta": {
      "": "99",
      "number": "53533",
      "pretext": "ERR Model is disposed!: Error: Model is disposed!\n    at TextModel._assertNotDisposed (file:///Users/jrieken/Code/vscode/out/vs/editor/common/model/textModel.js:245:23)\n    at TextModel.getVersionId (file:///Users/jrieken/Code/vscode/out/vs/editor/common/model/textModel.js:499:18)\n    at OutlinePanel.<anonymous> (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:548:61)\n    at step (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:50:23)\n    at Object.next (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:31:53)\n    at fulfilled (file:///Users/jrieken/Code/vscode/out/vs/workbench/parts/outline/electron-browser/outlinePanel.js:22:58)\n    at <anonymous>\n    at process._tickCallback (internal/process/next_tick.js:109:7)",
      "title": "ERR Model is disposed!: Error: Model is disposed!"
    },
    "annotation_approver": "jocharan"
  }
]
