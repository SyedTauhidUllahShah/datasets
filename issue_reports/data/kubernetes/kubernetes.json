[
  {
    "repository": {
      "issue": {
        "author": { "login": "saad-ali" },
        "number": 13565,
        "resourcePath": "/kubernetes/kubernetes/issues/13565",
        "state": "CLOSED",
        "publishedAt": "2015-09-03T18:33:31Z",
        "closedAt": "2015-09-24T23:53:49Z",
        "title": "Indirect dependency on golang test code introduced unintended CLI flags in Kubernetes binaries",
        "bodyText": "From @eparis's comment in the Cinder PR: #13367 (comment)\nPR #13367, Cinder Volume Plugin introduced a dependency on\n\ngithub.com/rackspace/gophercloud/openstack/blockstorage/v1/volumes\ngithub.com/rackspace/gophercloud/openstack/compute/v2/extensions/volumeattach\n\nwhich in turn has a dependency on github.com/golang/go/src/testing\nwhich declares a bunch of command line args (see https://github.com/golang/go/blob/master/src/testing/testing.go#L187).\nWhich means that kubernetes binaries, like kubelet now have flags for golang test code like --test.memprofilerate, --chatty, etc.",
        "bodyHTML": "<p>From <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/eparis/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/eparis\">@eparis</a>'s comment in the Cinder PR: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"103978721\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/13367\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/13367/hovercard?comment_id=137254241&amp;comment_type=issue_comment\" href=\"https://github.com/kubernetes/kubernetes/pull/13367#issuecomment-137254241\">#13367 (comment)</a></p>\n<p>PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"103978721\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/13367\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/13367/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/13367\">#13367</a>, Cinder Volume Plugin introduced a dependency on</p>\n<ul>\n<li><a href=\"https://github.com/rackspace/gophercloud/blob/master/openstack/blockstorage/v1/volumes/fixtures.go#L6\"><code>github.com/rackspace/gophercloud/openstack/blockstorage/v1/volumes</code></a></li>\n<li><a href=\"https://github.com/rackspace/gophercloud/blob/master/openstack/compute/v2/extensions/volumeattach/fixtures.go#L8\"><code>github.com/rackspace/gophercloud/openstack/compute/v2/extensions/volumeattach</code></a></li>\n</ul>\n<p>which in turn has a dependency on <a href=\"https://github.com/golang/go/blob/master/src/testing/testing.go#L187\"><code>github.com/golang/go/src/testing</code></a></p>\n<p>which declares a bunch of command line args (see <a href=\"https://github.com/golang/go/blob/master/src/testing/testing.go#L187\">https://github.com/golang/go/blob/master/src/testing/testing.go#L187</a>).</p>\n<p>Which means that kubernetes binaries, like <code>kubelet</code> now have flags for golang test code like <code>--test.memprofilerate</code>, <code>--chatty</code>, etc.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "jingxu97" },
        "number": 28318,
        "resourcePath": "/kubernetes/kubernetes/issues/28318",
        "state": "CLOSED",
        "publishedAt": "2016-06-30T23:07:35Z",
        "closedAt": "2016-06-30T23:32:49Z",
        "title": "Kubelet on a node dead right before pod is created and assigned to that node cause pod disappear on apiserver",
        "bodyText": "The steps to reproduce the issue, on a working gce cluster\n\nChoose a node, edit /usr/sbin/kubelet-checker.sh, change sleep time to a large number (600s)\nCreate a pod and make sure it is assigned to the node you chose in step 1\nkill kubelet process (kubelet process will be restarted automatically after > 5mins )\nCheck pod status, at the beginning, it is ContainerCreating/(or pending), after a few mins, it disappears (kubectl get pods no long shows it)",
        "bodyHTML": "<p>The steps to reproduce the issue, on a working gce cluster</p>\n<ol>\n<li>Choose a node, edit /usr/sbin/kubelet-checker.sh, change sleep time to a large number (600s)</li>\n<li>Create a pod and make sure it is assigned to the node you chose in step 1</li>\n<li>kill kubelet process (kubelet process will be restarted automatically after &gt; 5mins )</li>\n<li>Check pod status, at the beginning, it is ContainerCreating/(or pending), after a few mins, it disappears (kubectl get pods no long shows it)</li>\n</ol>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "dchen1107" },
        "number": 4128,
        "resourcePath": "/kubernetes/kubernetes/issues/4128",
        "state": "CLOSED",
        "publishedAt": "2015-02-04T21:19:13Z",
        "closedAt": "2015-03-28T03:33:53Z",
        "title": "Configure master node same as slave node",
        "bodyText": "This is pre-requirement for running all master components in a pod. #3853 was filed to run etcd as a pod. To make sure we really run etcd as a pod using Kubernete network model without specifying HostPort from spec, we need config the master node:\n\ncreate networking bridge called cbr0\nconfig docker run with \"--bridge cbr0 --iptables=false\"\nadd default route for master node (on GCE). Need to figure out other cloud providers.",
        "bodyHTML": "<p>This is pre-requirement for running all master components in a pod. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"55698218\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/3853\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/3853/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/3853\">#3853</a> was filed to run etcd as a pod. To make sure we really run etcd as a pod using Kubernete network model without specifying HostPort from spec, we need config the master node:</p>\n<ol>\n<li>create networking bridge called cbr0</li>\n<li>config docker run with \"--bridge cbr0 --iptables=false\"</li>\n<li>add default route for master node (on GCE). Need to figure out other cloud providers.</li>\n</ol>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": null,
        "number": 23062,
        "resourcePath": "/kubernetes/kubernetes/issues/23062",
        "state": "CLOSED",
        "publishedAt": "2016-03-16T17:18:51Z",
        "closedAt": "2016-03-31T16:58:16Z",
        "title": "Alert someone when kubernetes-test-history stops running, and add report date",
        "bodyText": "The software that generates http://storage.googleapis.com/kubernetes-test-history/static/tests-e2e.html has crashed/stopped running a few times, and nobody knew.  Please add an alert.\nAlso, the above report does not indicate what date the report pertains to, which makes it very confusing, and difficult to tell whether the report is up to date (e.g. I looked today, and it was a week old, although this was entirely non-obvious).",
        "bodyHTML": "<p>The software that generates <a rel=\"nofollow\" href=\"http://storage.googleapis.com/kubernetes-test-history/static/tests-e2e.html\">http://storage.googleapis.com/kubernetes-test-history/static/tests-e2e.html</a> has crashed/stopped running a few times, and nobody knew.  Please add an alert.</p>\n<p>Also, the above report does not indicate what date the report pertains to, which makes it very confusing, and difficult to tell whether the report is up to date (e.g. I looked today, and it was a week old, although this was entirely non-obvious).</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "discordianfish" },
        "number": 132,
        "resourcePath": "/kubernetes/kubernetes/issues/132",
        "state": "CLOSED",
        "publishedAt": "2014-06-17T13:17:56Z",
        "closedAt": "2014-06-24T13:07:16Z",
        "title": "Add generic documentation",
        "bodyText": "Hi,\nI'm happy to contribute the missing documentation but I've spend now quite some time on trying to get kubernetes working on a plain docker host and had no success so far.\nI've read the design doc, shell scripts and salt states and this is how I deployed it:\n\nkubelet -config /etc/kubelet.conf -address=0.0.0.0\netcd -peer-addr 10.0.1.115:7001 -addr 10.0.1.115:4001 -discovery https://discovery.etcd.io/\napiserver -address 0.0.0.0 -etcd_servers=http://10.0.1.115:4001 -machines=10.0.1.115\ncontroller-manager -master localhost:8080 -etcd_servers=http://10.0.1.115:4001\n\nKubelet log shows (I've created an empty kubelet.conf to stop it throwing errors, not sure if necessary though):\n2014/06/17 13:01:12 Desired:[]api.ContainerManifest{}\n2014/06/17 13:01:12 Existing:\n[]string{} Desired: map[string]bool{}\n\napiserver prints nothing at all.\ncontroller-manger logs:\netcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=false&sorted=false]\netcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false  | method  GET]\netcd 2014/06/17 13:13:17 DEBUG: watch [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]\netcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]\netcd 2014/06/17 13:13:17 DEBUG: [recv.response.from http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]\netcd 2014/06/17 13:13:17 DEBUG: [recv.success. http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&recursive=false&sorted=false]\n2014/06/17 13:13:17 Synchronization error &etcd.EtcdError{ErrorCode:100, Message:\"Key not found\", Cause:\"/registry\", Index:0x2}\n\nNow running cloudcfg fails:\n./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 run dockerfile/nginx 2 myNginx\n2014/06/17 15:16:03 Error: &errors.errorString{s:\"request [POST http://localhost:10250/api/v1beta1/replicationControllers] failed (404) 404 Not Found\"}\n\nIf I try to create a pod I get:\n./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 -c api/examples/pod.json create /pods\n2014/06/17 15:17:14 Failed to print: &json.SyntaxError{msg:\"invalid character 'N' looking for beginning of value\", Offset:1}",
        "bodyHTML": "<p>Hi,</p>\n<p>I'm happy to contribute the missing documentation but I've spend now quite some time on trying to get kubernetes working on a plain docker host and had no success so far.<br>\nI've read the design doc, shell scripts and salt states and this is how I deployed it:</p>\n<ul>\n<li>kubelet -config /etc/kubelet.conf -address=0.0.0.0</li>\n<li>etcd -peer-addr 10.0.1.115:7001 -addr 10.0.1.115:4001 -discovery <a href=\"https://discovery.etcd.io/\" rel=\"nofollow\">https://discovery.etcd.io/</a></li>\n<li>apiserver -address 0.0.0.0 -etcd_servers=<a href=\"http://10.0.1.115:4001\" rel=\"nofollow\">http://10.0.1.115:4001</a> -machines=10.0.1.115</li>\n<li>controller-manager -master localhost:8080 -etcd_servers=<a href=\"http://10.0.1.115:4001\" rel=\"nofollow\">http://10.0.1.115:4001</a></li>\n</ul>\n<p>Kubelet log shows (I've created an empty kubelet.conf to stop it throwing errors, not sure if necessary though):</p>\n<pre><code>2014/06/17 13:01:12 Desired:[]api.ContainerManifest{}\n2014/06/17 13:01:12 Existing:\n[]string{} Desired: map[string]bool{}\n</code></pre>\n<p>apiserver prints nothing at all.</p>\n<p>controller-manger logs:</p>\n<pre><code>etcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&amp;recursive=false&amp;sorted=false]\netcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&amp;recursive=false&amp;sorted=false  | method  GET]\netcd 2014/06/17 13:13:17 DEBUG: watch [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: get [/registry/controllers http://10.0.1.115:4001] [%!s(MISSING)]\netcd 2014/06/17 13:13:17 DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&amp;recursive=true&amp;wait=true]\netcd 2014/06/17 13:13:17 DEBUG: [send.request.to  http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&amp;recursive=true&amp;wait=true  | method  GET]\netcd 2014/06/17 13:13:17 DEBUG: [recv.response.from http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&amp;recursive=false&amp;sorted=false]\netcd 2014/06/17 13:13:17 DEBUG: [recv.success. http://10.0.1.115:4001/v2/keys/registry/controllers?consistent=true&amp;recursive=false&amp;sorted=false]\n2014/06/17 13:13:17 Synchronization error &amp;etcd.EtcdError{ErrorCode:100, Message:\"Key not found\", Cause:\"/registry\", Index:0x2}\n</code></pre>\n<p>Now running cloudcfg fails:</p>\n<pre><code>./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 run dockerfile/nginx 2 myNginx\n2014/06/17 15:16:03 Error: &amp;errors.errorString{s:\"request [POST http://localhost:10250/api/v1beta1/replicationControllers] failed (404) 404 Not Found\"}\n</code></pre>\n<p>If I try to create a pod I get:</p>\n<pre><code>./cmd/cloudcfg/cloudcfg -h http://localhost:10250 -p 8080:80 -c api/examples/pod.json create /pods\n2014/06/17 15:17:14 Failed to print: &amp;json.SyntaxError{msg:\"invalid character 'N' looking for beginning of value\", Offset:1}\n</code></pre>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "evanphx" },
        "number": 14281,
        "resourcePath": "/kubernetes/kubernetes/issues/14281",
        "state": "CLOSED",
        "publishedAt": "2015-09-21T16:34:26Z",
        "closedAt": "2015-10-21T17:47:16Z",
        "title": "`rolling-update` requires a unique name when specifying a manifest",
        "bodyText": "Per a conversation in slack with @jimmidyson, rolling-update should be able to do the same rename dance when a manifest is specified as when --image is used.\nRight now, it errors out with a message like: error: pods/catalog.yml cannot have the same name as the existing ReplicationController catalog",
        "bodyHTML": "<p>Per a conversation in slack with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/jimmidyson/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jimmidyson\">@jimmidyson</a>, <code>rolling-update</code> should be able to do the same rename dance when a manifest is specified as when <code>--image</code> is used.</p>\n<p>Right now, it errors out with a message like: <code>error: pods/catalog.yml cannot have the same name as the existing ReplicationController catalog</code></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "ihmccreery" },
        "number": 15116,
        "resourcePath": "/kubernetes/kubernetes/issues/15116",
        "state": "CLOSED",
        "publishedAt": "2015-10-05T22:10:48Z",
        "closedAt": "2016-02-05T17:31:19Z",
        "title": "What do we do with 1.0 tests that fail when run against 1.1?",
        "bodyText": "This is a problem we're going to run into as we're running 1.0 e2e tests against 1.1 clusters, (which we do because we need to make sure that 1.1 clusters still operate the way 1.0 clusters did).  If a test fails, but the failure is due to a bad test rather than a problem in 1.1, what do we do?\nFor example, Services/Nodeport e2es are failing on upgrade due to change in error message.  They fail when running 1.0 e2es against a HEAD master (in jobs kubernetes-upgrade-gke-step3-e2e-old and kubernetes-upgrade-gke-step5-e2e-old):\n\nKubernetes e2e suite.Services should check NodePort out-of-range\n\nExpected\n    <string>: Service \"nodeport-range-test\" is invalid: spec.ports[0].nodePort: invalid value '53127', Details: provided port is not in the valid range\nto equal\n    <string>: Service \"nodeport-range-test\" is invalid: spec.ports[0].nodePort: invalid value '53127': provided port is not in the valid range\n\n\nKubernetes e2e suite.Services should prevent NodePort collisions\n\nExpected\n    <string>: Service \"nodeport-collision2\" is invalid: spec.ports[0].nodePort: invalid value '31641', Details: provided port is already allocated\nto equal\n    <string>: Service \"nodeport-collision2\" is invalid: spec.ports[0].nodePort: invalid value '31641': provided port is already allocated\n\n@ixdy @quinton-hoole Any ideas about how to fix this?  This definitely isn't a regression, but it's probably not a good idea to just disable these tests.  My best thought is to cherry-pick the 1.1 test changes into 1.0, and somehow pull these tests HEAD of the 1.0 branch.  That's a lot of mucking around though, and I'm not sure it's worth it.\nFor now, I think we should punt on these specific tests until we have a better idea of how widespread this kind of version-skew problem is going to be.",
        "bodyHTML": "<p>This is a problem we're going to run into as we're running 1.0 e2e tests against 1.1 clusters, (which we do because we need to make sure that 1.1 clusters still operate the way 1.0 clusters did).  If a test fails, but the failure is due to a bad test rather than a problem in 1.1, what do we do?</p>\n<p>For example, Services/Nodeport e2es are failing on upgrade due to change in error message.  They fail when running 1.0 e2es against a HEAD master (in jobs kubernetes-upgrade-gke-step3-e2e-old and kubernetes-upgrade-gke-step5-e2e-old):</p>\n<ul>\n<li>Kubernetes e2e suite.Services should check NodePort out-of-range</li>\n</ul>\n<pre><code>Expected\n    &lt;string&gt;: Service \"nodeport-range-test\" is invalid: spec.ports[0].nodePort: invalid value '53127', Details: provided port is not in the valid range\nto equal\n    &lt;string&gt;: Service \"nodeport-range-test\" is invalid: spec.ports[0].nodePort: invalid value '53127': provided port is not in the valid range\n</code></pre>\n<ul>\n<li>Kubernetes e2e suite.Services should prevent NodePort collisions</li>\n</ul>\n<pre><code>Expected\n    &lt;string&gt;: Service \"nodeport-collision2\" is invalid: spec.ports[0].nodePort: invalid value '31641', Details: provided port is already allocated\nto equal\n    &lt;string&gt;: Service \"nodeport-collision2\" is invalid: spec.ports[0].nodePort: invalid value '31641': provided port is already allocated\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/ixdy/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ixdy\">@ixdy</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/quinton-hoole/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/quinton-hoole\">@quinton-hoole</a> Any ideas about how to fix this?  This definitely isn't a regression, but it's probably not a good idea to just disable these tests.  My best thought is to cherry-pick the 1.1 test changes into 1.0, and somehow pull these tests HEAD of the 1.0 branch.  That's a lot of mucking around though, and I'm not sure it's worth it.</p>\n<p>For now, I think we should punt on these specific tests until we have a better idea of how widespread this kind of version-skew problem is going to be.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "kelbongoo" },
        "number": 14237,
        "resourcePath": "/kubernetes/kubernetes/issues/14237",
        "state": "CLOSED",
        "publishedAt": "2015-09-19T23:38:32Z",
        "closedAt": "2015-09-20T07:58:48Z",
        "title": "GKE cluster running nginx seems to break nodePort",
        "bodyText": "Setting up a cluster on GKE via the gcloud cli tool, and then following this tutorial https://cloud.google.com/container-engine/docs/tutorials/http-balancer, I came across two issues - the first is here #13073.\nIn the tutorial we set up the nginx service with a nodePort like so\nkubectl expose rc my-nginx --target-port=80 --type=NodePort\n\nOf all the nodes in the cluster (I tried 3 and 4 node clusters), the only one who responded to\ncurl (node external ip):(node_port)\n\nwas the node where the nginx container was actually hosted. The other nodes all showed TCP open at the nodePort in nmap but dropped the connection right away and nothing ever got through to nginx.\nNot sure if this is expected behaviour ? From the tutorial it seems like all nodes should return nginx responses\nNext, create a Container Engine service which exposes this nginx Pod on each node in your cluster:",
        "bodyHTML": "<p>Setting up a cluster on GKE via the gcloud cli tool, and then following this tutorial <a rel=\"nofollow\" href=\"https://cloud.google.com/container-engine/docs/tutorials/http-balancer\">https://cloud.google.com/container-engine/docs/tutorials/http-balancer</a>, I came across two issues - the first is here <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"102634948\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/13073\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/13073/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/13073\">#13073</a>.</p>\n<p>In the tutorial we set up the nginx service with a nodePort like so</p>\n<pre><code>kubectl expose rc my-nginx --target-port=80 --type=NodePort\n</code></pre>\n<p>Of all the nodes in the cluster (I tried 3 and 4 node clusters), the only one who responded to</p>\n<pre><code>curl (node external ip):(node_port)\n</code></pre>\n<p>was the node where the nginx container was actually hosted. The other nodes all showed TCP open at the nodePort in nmap but dropped the connection right away and nothing ever got through to nginx.</p>\n<p>Not sure if this is expected behaviour ? From the tutorial it seems like all nodes should return nginx responses</p>\n<pre><code>Next, create a Container Engine service which exposes this nginx Pod on each node in your cluster:\n</code></pre>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "borg286" },
        "number": 10774,
        "resourcePath": "/kubernetes/kubernetes/issues/10774",
        "state": "CLOSED",
        "publishedAt": "2015-07-06T19:35:00Z",
        "closedAt": "2015-08-25T22:52:22Z",
        "title": "Docker k8s setup instructions to have DNS option",
        "bodyText": "Please add a way to add DNS support to the setup instructions for getting a kubernetes cluster using docker.\nThe SkyDNS server already has a yaml file  and would exist as a pod and service just as the current apiserver pods exist.",
        "bodyHTML": "<p>Please add a way to add DNS support to the setup instructions for getting a kubernetes cluster using docker.<br>\nThe SkyDNS server already has a yaml file  and would exist as a pod and service just as the current apiserver pods exist.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "gouyang" },
        "number": 11732,
        "resourcePath": "/kubernetes/kubernetes/issues/11732",
        "state": "CLOSED",
        "publishedAt": "2015-07-23T03:03:54Z",
        "closedAt": "2015-08-17T04:09:32Z",
        "title": "Check local copy of the golang docker image is always failed.",
        "bodyText": "The golang docker image is existed\n$ docker images docker.io/golang\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\ndocker.io/golang    1.4                 124e2127157f        8 days ago          517.2 MB\n\nBut it always say \"You don't have a local copy of the golang docker image\".\n$ sudo make release\nbuild/release.sh\n+++ [0723 10:58:17] Verifying Prerequisites....\nYou don't have a local copy of the golang docker image. This image is 450MB.\nDownload it now? [y/n] n\n\n#11284 fixes this.",
        "bodyHTML": "<p>The golang docker image is existed</p>\n<pre><code>$ docker images docker.io/golang\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\ndocker.io/golang    1.4                 124e2127157f        8 days ago          517.2 MB\n</code></pre>\n<p>But it always say \"You don't have a local copy of the golang docker image\".</p>\n<pre><code>$ sudo make release\nbuild/release.sh\n+++ [0723 10:58:17] Verifying Prerequisites....\nYou don't have a local copy of the golang docker image. This image is 450MB.\nDownload it now? [y/n] n\n</code></pre>\n<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"95094974\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/11284\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/11284/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/11284\">#11284</a> fixes this.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "karlkfi" },
        "number": 15412,
        "resourcePath": "/kubernetes/kubernetes/issues/15412",
        "state": "CLOSED",
        "publishedAt": "2015-10-09T23:54:36Z",
        "closedAt": "2017-06-02T02:43:51Z",
        "title": "[mesos/docker] Flakey Smoke Test - TLS handshake timeout",
        "bodyText": "error: couldn't read version from server: Get https://172.17.0.79:6443/api: net/http: TLS handshake timeout\n\nhttps://teamcity.mesosphere.io/viewLog.html?buildId=56436&buildTypeId=Oss_KubernetesMesos_5SmokeTestsDockerMesos&tab=buildLog&guest=1#_focus=614\nFirst time I've seen this. Documenting for searchability.",
        "bodyHTML": "<pre><code>error: couldn't read version from server: Get https://172.17.0.79:6443/api: net/http: TLS handshake timeout\n</code></pre>\n<p><a rel=\"nofollow\" href=\"https://teamcity.mesosphere.io/viewLog.html?buildId=56436&amp;buildTypeId=Oss_KubernetesMesos_5SmokeTestsDockerMesos&amp;tab=buildLog&amp;guest=1#_focus=614\">https://teamcity.mesosphere.io/viewLog.html?buildId=56436&amp;buildTypeId=Oss_KubernetesMesos_5SmokeTestsDockerMesos&amp;tab=buildLog&amp;guest=1#_focus=614</a></p>\n<p>First time I've seen this. Documenting for searchability.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "mikesimons" },
        "number": 4695,
        "resourcePath": "/kubernetes/kubernetes/issues/4695",
        "state": "CLOSED",
        "publishedAt": "2015-02-21T11:28:13Z",
        "closedAt": "2015-02-21T14:17:32Z",
        "title": "Services with the same name in different namespaces cause kube-proxy to bug out",
        "bodyText": "Should it be possible to have services with the same name in different namespaces? Nothing prevents it and I suspect that it should be possible but there are assumptions in code that cause this not to function correctly.\nHere are some failing tests: https://github.com/mikesimons/kubernetes/compare/proxy_namespace_clash\nAnd here is what lead me to investigate:\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703463   19433 config.go:233] Setting services {Services:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d1e4b42e-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:51 +0000 UTC Labels:map[role:service name:read] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.236.240 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}] Op:0}\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703801   19433 config.go:138] Setting endpoints {Endpoints:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:8080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:7080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink:/api/v1beta1/endpoints/syslog?namespace=default UID:4c837d1f-b929-11e4-bc1b-080027205d4b ResourceVersion:12 CreationTimestamp:2015-02-20 17:52:55 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[172.17.0.3:514]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d24c9595-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:52 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:fc253202-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:41:02 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]}] Op:0}\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703923   19433 proxier.go:459] Received update notice: [{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}]\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.704086   19433 proxier.go:470] Something changed for service \"redis\": stopping it\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.726654   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.729142   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.733688   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.736540   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739764   19433 proxier.go:575] Closed iptables portals for service \"redis\"\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739798   19433 proxier.go:480] Adding new service \"redis\" at 119.9.235.183:6379/TCP (local :0)\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739880   19433 proxier.go:443] Proxying for service \"redis\" on TCP port 37841\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739890   19433 proxier.go:492] info: &{portalIP:[0 0 0 0 0 0 0 0 0 0 255 255 119 9 235 183] portalPort:6379 protocol:TCP proxyPort:37841 socket:0xc20810c4a0 timeout:60000000000 publicIP:[] sessionAffinityType:None stickyMaxAgeMinutes:180}\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742221   19433 proxier.go:103] Accept failed: accept tcp [::]:55814: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742248   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742257   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742261   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742265   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742269   19433 proxier.go:103] Accept failed: use of closed network connection\n\nThe Accept failed: use of closed network connection is spat out indefinitely at a very high rate. All services cease to function (but timeout rather than refuse connection).\nThe thing of note in the logs is that in the \"Setting services\" message both redis instances are present where-as in the \"Recieved update\" message, only the project2 instance is present.\nI believe that this is due to the fact that several places in pkg/proxy/config/config.go create maps keyed on service name only. Here is an example https://github.com/mikesimons/kubernetes/blob/master/pkg/proxy/config/config.go#L214\nAssuming I am correct, what would be the best way to fix this? It doesn't look like the keys of the maps are used anywhere so does this need to be a map at all?",
        "bodyHTML": "<p>Should it be possible to have services with the same name in different namespaces? Nothing prevents it and I suspect that it should be possible but there are assumptions in code that cause this not to function correctly.</p>\n<p>Here are some failing tests: <a href=\"https://github.com/mikesimons/kubernetes/compare/proxy_namespace_clash\">https://github.com/mikesimons/kubernetes/compare/proxy_namespace_clash</a></p>\n<p>And here is what lead me to investigate:</p>\n<pre><code>Feb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703463   19433 config.go:233] Setting services {Services:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d1e4b42e-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:51 +0000 UTC Labels:map[role:service name:read] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.236.240 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}] Op:0}\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703801   19433 config.go:138] Setting endpoints {Endpoints:[{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:8080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID: ResourceVersion: CreationTimestamp:0001-01-01 00:00:00 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[10.0.2.15:7080]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink:/api/v1beta1/endpoints/syslog?namespace=default UID:4c837d1f-b929-11e4-bc1b-080027205d4b ResourceVersion:12 CreationTimestamp:2015-02-20 17:52:55 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[172.17.0.3:514]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project1 SelfLink: UID:d24c9595-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:39:52 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:fc253202-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:41:02 +0000 UTC Labels:map[] Annotations:map[]} Endpoints:[]}] Op:0}\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.703923   19433 proxier.go:459] Received update notice: [{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes-ro Namespace:default SelfLink: UID:458766c2-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[component:apiserver provider:kubernetes] Annotations:map[]} Spec:{Port:80 Protocol:TCP Selector:map[] PortalIP:119.9.9.239 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:syslog Namespace:default SelfLink: UID:4bc04140-b929-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:52:54 +0000 UTC Labels:map[] Annotations:map[]} Spec:{Port:514 Protocol:UDP Selector:map[name:elk] PortalIP:119.9.206.11 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:514 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:redis Namespace:project2 SelfLink: UID:f76c4de3-b99c-11e4-a0c0-080027205d4b ResourceVersion: CreationTimestamp:2015-02-21 07:40:54 +0000 UTC Labels:map[name:read role:service] Annotations:map[]} Spec:{Port:6379 Protocol:TCP Selector:map[name:redis] PortalIP:119.9.235.183 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}} {TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:kubernetes Namespace:default SelfLink: UID:4575a863-b928-11e4-bc1b-080027205d4b ResourceVersion: CreationTimestamp:2015-02-20 17:45:34 +0000 UTC Labels:map[provider:kubernetes component:apiserver] Annotations:map[]} Spec:{Port:443 Protocol:TCP Selector:map[] PortalIP:119.9.53.150 ProxyPort:0 CreateExternalLoadBalancer:false PublicIPs:[] ContainerPort:{Kind:0 IntVal:0 StrVal:} SessionAffinity:None} Status:{}}]\n\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.704086   19433 proxier.go:470] Something changed for service \"redis\": stopping it\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.726654   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.729142   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-CONTAINER -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j REDIRECT --to-ports 55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.733688   19433 iptables.go:186] running iptables -C [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.736540   19433 iptables.go:186] running iptables -D [KUBE-PORTALS-HOST -t nat -m comment --comment redis -p tcp -m tcp -d 119.9.236.240/32 --dport 6379 -j DNAT --to-destination 10.0.2.15:55814]\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739764   19433 proxier.go:575] Closed iptables portals for service \"redis\"\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739798   19433 proxier.go:480] Adding new service \"redis\" at 119.9.235.183:6379/TCP (local :0)\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739880   19433 proxier.go:443] Proxying for service \"redis\" on TCP port 37841\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: I0221 07:41:19.739890   19433 proxier.go:492] info: &amp;{portalIP:[0 0 0 0 0 0 0 0 0 0 255 255 119 9 235 183] portalPort:6379 protocol:TCP proxyPort:37841 socket:0xc20810c4a0 timeout:60000000000 publicIP:[] sessionAffinityType:None stickyMaxAgeMinutes:180}\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742221   19433 proxier.go:103] Accept failed: accept tcp [::]:55814: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742248   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742257   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742261   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742265   19433 proxier.go:103] Accept failed: use of closed network connection\nFeb 21 07:41:19 localhost.localdomain kube-proxy[19433]: E0221 07:41:19.742269   19433 proxier.go:103] Accept failed: use of closed network connection\n</code></pre>\n<p>The <code>Accept failed: use of closed network connection</code> is spat out indefinitely at a very high rate. All services cease to function (but timeout rather than refuse connection).</p>\n<p>The thing of note in the logs is that in the \"Setting services\" message both redis instances are present where-as in the \"Recieved update\" message, only the project2 instance is present.</p>\n<p>I believe that this is due to the fact that several places in pkg/proxy/config/config.go create maps keyed on service name only. Here is an example <a href=\"https://github.com/mikesimons/kubernetes/blob/master/pkg/proxy/config/config.go#L214\">https://github.com/mikesimons/kubernetes/blob/master/pkg/proxy/config/config.go#L214</a></p>\n<p>Assuming I am correct, what would be the best way to fix this? It doesn't look like the keys of the maps are used anywhere so does this need to be a map at all?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "erictune" },
        "number": 7047,
        "resourcePath": "/kubernetes/kubernetes/issues/7047",
        "state": "CLOSED",
        "publishedAt": "2015-04-20T15:26:39Z",
        "closedAt": "2015-06-26T22:00:00Z",
        "title": "Want create-or-update command for kubectl",
        "bodyText": "If you want to build a shell script to reconcile a config file with the state on the apiserver, it is convenient to be able to do \"create or update\" in a single command.",
        "bodyHTML": "<p>If you want to build a shell script to reconcile a config file with the state on the apiserver, it is convenient to be able to do \"create or update\" in a single command.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "marekbiskup" },
        "number": 9937,
        "resourcePath": "/kubernetes/kubernetes/issues/9937",
        "state": "CLOSED",
        "publishedAt": "2015-06-17T13:30:33Z",
        "closedAt": "2018-02-20T11:32:01Z",
        "title": "Create a command-line tool to query yaml and json files",
        "bodyText": "Example usage (details to be decided):\n$ manifest-query -f mypod.yaml  -t {{.metadata.name}}\n\ngolang templates (http://golang.org/pkg/text/template/#pkg-overview) may not be sufficient because they don't handle all characters (e.g. there have been problems with /)\nThe tool will be useful for:\n\nbash scripts that have to manipulate yamls (e.g. for fixing #9849)\nexamples, e.g. here: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/accessing-the-cluster.md#without-kubectl-proxy\n\ncc. @zmerlynn",
        "bodyHTML": "<p>Example usage (details to be decided):</p>\n<pre><code>$ manifest-query -f mypod.yaml  -t {{.metadata.name}}\n</code></pre>\n<p>golang templates (<a rel=\"nofollow\" href=\"http://golang.org/pkg/text/template/#pkg-overview\">http://golang.org/pkg/text/template/#pkg-overview</a>) may not be sufficient because they don't handle all characters (e.g. there have been problems with /)</p>\n<p>The tool will be useful for:</p>\n<ul>\n<li>bash scripts that have to manipulate yamls (e.g. for fixing <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"88646237\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/9849\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/9849/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/9849\">#9849</a>)</li>\n<li>examples, e.g. here: <a href=\"https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/accessing-the-cluster.md#without-kubectl-proxy\">https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/accessing-the-cluster.md#without-kubectl-proxy</a></li>\n</ul>\n<p>cc. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/zmerlynn/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zmerlynn\">@zmerlynn</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "fahadpolash" },
        "number": 9459,
        "resourcePath": "/kubernetes/kubernetes/issues/9459",
        "state": "CLOSED",
        "publishedAt": "2015-06-09T04:25:21Z",
        "closedAt": "2015-06-09T22:18:23Z",
        "title": "Vagrant:Fedora 21 repo failing ",
        "bodyText": "Hello,\nI was trying to bringing up the cluster from the github master branch which is using fedora 21. But I am getting the following errors:\n==> master: Installing, enabling prerequisites\n==> master:\n==> master:\n==> master:  One of the configured repositories failed (Fedora 21 - x86_64),\n==> master:  and yum doesn't have enough cached data to continue. At this point the only\n==> master:  safe thing yum can do is fail. There are a few ways to work \"fix\" this:\n==> master:\n==> master:      1. Contact the upstream for the repository and get them to fix the problem.\n==> master:\n==> master:      2. Reconfigure the baseurl/etc. for the repository, to point to a working\n==> master:         upstream. This is most often useful if you are using a newer\n==> master:         distribution release than is supported by the repository (and the\n==> master:         packages for the previous distribution release still work).\n==> master:\n==> master:      3. Disable the repository, so yum won't use it by default. Yum will then\n==> master:         just ignore the repository until you permanently enable it again or use\n==> master:         --enablerepo for temporary usage:\n==> master:\n==> master:             yum-config-manager --disable fedora\n==> master:\n==> master:      4. Configure the failing repository to be skipped, if it is unavailable.\n==> master:         Note that yum will try to contact the repo. when it runs most commands,\n==> master:         so will have to try and fail each time (and thus. yum will be be much\n==> master:         slower). If it is a very temporary problem though, this is often a nice\n==> master:         compromise:\n==> master:\n==> master:             yum-config-manager --save --setopt=fedora.skip_if_unavailable=true\n==> master:\n==> master: Cannot retrieve metalink for repository: fedora/21/x86_64. Please verify its path and try again\nAny idea?",
        "bodyHTML": "<p>Hello,<br>\nI was trying to bringing up the cluster from the github master branch which is using fedora 21. But I am getting the following errors:</p>\n<p>==&gt; master: Installing, enabling prerequisites<br>\n==&gt; master:<br>\n==&gt; master:<br>\n==&gt; master:  One of the configured repositories failed (Fedora 21 - x86_64),<br>\n==&gt; master:  and yum doesn't have enough cached data to continue. At this point the only<br>\n==&gt; master:  safe thing yum can do is fail. There are a few ways to work \"fix\" this:<br>\n==&gt; master:<br>\n==&gt; master:      1. Contact the upstream for the repository and get them to fix the problem.<br>\n==&gt; master:<br>\n==&gt; master:      2. Reconfigure the baseurl/etc. for the repository, to point to a working<br>\n==&gt; master:         upstream. This is most often useful if you are using a newer<br>\n==&gt; master:         distribution release than is supported by the repository (and the<br>\n==&gt; master:         packages for the previous distribution release still work).<br>\n==&gt; master:<br>\n==&gt; master:      3. Disable the repository, so yum won't use it by default. Yum will then<br>\n==&gt; master:         just ignore the repository until you permanently enable it again or use<br>\n==&gt; master:         --enablerepo for temporary usage:<br>\n==&gt; master:<br>\n==&gt; master:             yum-config-manager --disable fedora<br>\n==&gt; master:<br>\n==&gt; master:      4. Configure the failing repository to be skipped, if it is unavailable.<br>\n==&gt; master:         Note that yum will try to contact the repo. when it runs most commands,<br>\n==&gt; master:         so will have to try and fail each time (and thus. yum will be be much<br>\n==&gt; master:         slower). If it is a very temporary problem though, this is often a nice<br>\n==&gt; master:         compromise:<br>\n==&gt; master:<br>\n==&gt; master:             yum-config-manager --save --setopt=fedora.skip_if_unavailable=true<br>\n==&gt; master:<br>\n==&gt; master: Cannot retrieve metalink for repository: fedora/21/x86_64. Please verify its path and try again</p>\n<p>Any idea?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "erictune" },
        "number": 14528,
        "resourcePath": "/kubernetes/kubernetes/issues/14528",
        "state": "CLOSED",
        "publishedAt": "2015-09-24T23:17:52Z",
        "closedAt": "2015-10-05T23:17:31Z",
        "title": "Too many daemons created by DaemonSet",
        "bodyText": "I created a DaemonSet a few days ago on my 4 node cluster.  I noticed today that I have 6596 Pending pods, and 4 running pods.\nThis is not expected.\nThe running pods are on nodes that have existed for 3 or 4 days.\nThe pending pods all have unset nodeName.  I would not have expected the DaemonSet controller to ever create a pod with this value unset.  I assume they are pending due to port conflicts with the existing pods, as they use a hostPort.",
        "bodyHTML": "<p>I created a DaemonSet a few days ago on my 4 node cluster.  I noticed today that I have 6596 Pending pods, and 4 running pods.</p>\n<p>This is not expected.</p>\n<p>The running pods are on nodes that have existed for 3 or 4 days.</p>\n<p>The pending pods all have unset nodeName.  I would not have expected the DaemonSet controller to ever create a pod with this value unset.  I assume they are pending due to port conflicts with the existing pods, as they use a hostPort.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "aanm" },
        "number": 14743,
        "resourcePath": "/kubernetes/kubernetes/issues/14743",
        "state": "CLOSED",
        "publishedAt": "2015-09-29T14:32:36Z",
        "closedAt": "2015-10-08T22:43:50Z",
        "title": "kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes",
        "bodyText": "I'm getting an error while running the cluster/kube-up.sh with\nexport KUBERNETES_PROVIDER=vagrant\nexport VAGRANT_DEFAULT_PROVIDER=virtualbox\nexport NUM_MINIONS=1\n\nerror:\n==> master: \u2018/srv/salt-overlay/salt/kube-apiserver/basic_auth.csv\u2019 -> \u2018/srv/salt-new/salt/kube-apiserver/basic_auth.csv\u2019\n==> master: \u2018/srv/salt-overlay/pillar/cluster-params.sls\u2019 -> \u2018/srv/salt-new/pillar/cluster-params.sls\u2019\n==> master: +++ Install binaries from tar: kubernetes-server-linux-amd64.tar.gz\n==> master: tar: kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes\n==> master: tar: Exiting with failure status due to previous errors\nThe SSH command responded with a non-zero exit status. Vagrant\nassumes that this means the command failed. The output for this command\nshould be in the log above. Please read the output to determine what\nwent wrong.\n\nam I missing something?",
        "bodyHTML": "<p>I'm getting an error while running the <code>cluster/kube-up.sh</code> with</p>\n<pre><code>export KUBERNETES_PROVIDER=vagrant\nexport VAGRANT_DEFAULT_PROVIDER=virtualbox\nexport NUM_MINIONS=1\n</code></pre>\n<p>error:</p>\n<pre><code>==&gt; master: \u2018/srv/salt-overlay/salt/kube-apiserver/basic_auth.csv\u2019 -&gt; \u2018/srv/salt-new/salt/kube-apiserver/basic_auth.csv\u2019\n==&gt; master: \u2018/srv/salt-overlay/pillar/cluster-params.sls\u2019 -&gt; \u2018/srv/salt-new/pillar/cluster-params.sls\u2019\n==&gt; master: +++ Install binaries from tar: kubernetes-server-linux-amd64.tar.gz\n==&gt; master: tar: kubernetes/server/bin/hyperkube: Wrote only 512 of 10240 bytes\n==&gt; master: tar: Exiting with failure status due to previous errors\nThe SSH command responded with a non-zero exit status. Vagrant\nassumes that this means the command failed. The output for this command\nshould be in the log above. Please read the output to determine what\nwent wrong.\n</code></pre>\n<p>am I missing something?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "DazWilkin" },
        "number": 21635,
        "resourcePath": "/kubernetes/kubernetes/issues/21635",
        "state": "CLOSED",
        "publishedAt": "2016-02-20T22:38:38Z",
        "closedAt": "2016-05-06T16:39:54Z",
        "title": "Please provide a feedback mechanism on kubernetes.io --> github.com/kubernetes/kubernetes",
        "bodyText": "I may be missing something (!) but, when I wish to file bugs against kubernetes.io, instead of a \"Click here to submit feedback\", I'm resorting to Googling a section of text on the kubernetes.io to find the relevant page on github in order to reference the github content for github issues!\nE.g.\nFrom here:\nhttp://kubernetes.io/v1.1/docs/user-guide/connecting-applications.html#securing-the-service\nMust Google to help find this:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/connecting-applications.md#securing-the-service\nFrom here:\nhttp://kubernetes.io/v1.1/examples/https-nginx/README.html\nMust Google to help find this:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.1/examples/https-nginx/README.md\nRecommend providing a feedback feature that facilitates submitting bugs against kubenetes.io pages by either auto-referencing or facilitating finding the reference on the github page.\nAlternatively, do you just accept kubernetes.io references in github issues?",
        "bodyHTML": "<p>I may be missing something (!) but, when I wish to file bugs against kubernetes.io, instead of a \"Click here to submit feedback\", I'm resorting to Googling a section of text on the kubernetes.io to find the relevant page on github in order to reference the github content for github issues!</p>\n<p>E.g.<br>\nFrom here:<br>\n<a rel=\"nofollow\" href=\"http://kubernetes.io/v1.1/docs/user-guide/connecting-applications.html#securing-the-service\">http://kubernetes.io/v1.1/docs/user-guide/connecting-applications.html#securing-the-service</a></p>\n<p>Must Google to help find this:<br>\n<a href=\"https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/connecting-applications.md#securing-the-service\">https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/connecting-applications.md#securing-the-service</a></p>\n<p>From here:<br>\n<a rel=\"nofollow\" href=\"http://kubernetes.io/v1.1/examples/https-nginx/README.html\">http://kubernetes.io/v1.1/examples/https-nginx/README.html</a></p>\n<p>Must Google to help find this:<br>\n<a href=\"https://github.com/kubernetes/kubernetes/blob/release-1.1/examples/https-nginx/README.md\">https://github.com/kubernetes/kubernetes/blob/release-1.1/examples/https-nginx/README.md</a></p>\n<p>Recommend providing a feedback feature that facilitates submitting bugs against kubenetes.io pages by either auto-referencing or facilitating finding the reference on the github page.</p>\n<p>Alternatively, do you just accept kubernetes.io references in github issues?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "lavalamp" },
        "number": 17600,
        "resourcePath": "/kubernetes/kubernetes/issues/17600",
        "state": "CLOSED",
        "publishedAt": "2015-11-20T23:35:46Z",
        "closedAt": "2015-11-23T22:01:53Z",
        "title": "Broken on soak cluster: Kubectl client Update Demo should do a rolling update of a replication controller [Conformance]",
        "bodyText": "http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-soak-continuous-e2e-gce/3939/",
        "bodyHTML": "<p><a rel=\"nofollow\" href=\"http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-soak-continuous-e2e-gce/3939/\">http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-soak-continuous-e2e-gce/3939/</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "caesarxuchao" },
        "number": 8943,
        "resourcePath": "/kubernetes/kubernetes/issues/8943",
        "state": "CLOSED",
        "publishedAt": "2015-05-28T19:54:49Z",
        "closedAt": "2015-05-28T22:29:11Z",
        "title": "'--api-prefix' option in 'kubectl proxy' documentation is wrong",
        "bodyText": "Without --api-prefix, kubectl proxy works just fine,\n$ curl localhost:8001/api/\n{\n  \"versions\": [\n    \"v1beta1\",\n    \"v1beta2\",\n    \"v1beta3\"\n  ]\n}\n\nWith '--api-prefix=xxx-api', it stops work:\n$ curl localhost:8001/xxx-api/\n404 page not found\n\nMy kubectl version:\n$ kubectl version\nClient Version: version.Info{Major:\"0\", Minor:\"17+\", GitVersion:\"v0.17.1-878-g851f6b754241c0-dirty\", GitCommit:\"851f6b754241c01f30c43cddd2f9ca8c7c3f42f5\", GitTreeState:\"dirty\"}\nServer Version: version.Info{Major:\"0\", Minor:\"17+\", GitVersion:\"v0.17.1-893-g58b683fe296da8-dirty\", GitCommit:\"58b683fe296da870fdbc6eb1a32dcf50ed94a8e3\", GitTreeState:\"dirty\"}\n\n@jlowdermilk, could you take a look? Thanks.",
        "bodyHTML": "<p>Without <code>--api-prefix</code>, <code>kubectl proxy</code> works just fine,</p>\n<pre><code>$ curl localhost:8001/api/\n{\n  \"versions\": [\n    \"v1beta1\",\n    \"v1beta2\",\n    \"v1beta3\"\n  ]\n}\n</code></pre>\n<p>With '--api-prefix=xxx-api', it stops work:</p>\n<pre><code>$ curl localhost:8001/xxx-api/\n404 page not found\n</code></pre>\n<p>My <code>kubectl</code> version:</p>\n<pre><code>$ kubectl version\nClient Version: version.Info{Major:\"0\", Minor:\"17+\", GitVersion:\"v0.17.1-878-g851f6b754241c0-dirty\", GitCommit:\"851f6b754241c01f30c43cddd2f9ca8c7c3f42f5\", GitTreeState:\"dirty\"}\nServer Version: version.Info{Major:\"0\", Minor:\"17+\", GitVersion:\"v0.17.1-893-g58b683fe296da8-dirty\", GitCommit:\"58b683fe296da870fdbc6eb1a32dcf50ed94a8e3\", GitTreeState:\"dirty\"}\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/jlowdermilk/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlowdermilk\">@jlowdermilk</a>, could you take a look? Thanks.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "cjcullen" },
        "number": 21330,
        "resourcePath": "/kubernetes/kubernetes/issues/21330",
        "state": "CLOSED",
        "publishedAt": "2016-02-16T20:00:55Z",
        "closedAt": "2019-11-07T02:49:01Z",
        "title": "AllowUnconditionalUpdate is very frightening",
        "bodyText": "We provide an option to violate the semantics of our API, and on top of that, it is the default if you don't pass a resourceVersion. This has the ability to do very bad things.\nIf we want to encourage people to start writing their own controllers on top of kubernetes, we shouldn't make it so easy for them to shoot themselves in the face. Is there a reason this option exists?",
        "bodyHTML": "<p>We provide an option to violate the semantics of our API, and on top of that, it is the default if you don't pass a resourceVersion. This has the ability to do very bad things.</p>\n<p>If we want to encourage people to start writing their own controllers on top of kubernetes, we shouldn't make it so easy for them to shoot themselves in the face. Is there a reason this option exists?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "yissachar" },
        "number": 24139,
        "resourcePath": "/kubernetes/kubernetes/issues/24139",
        "state": "CLOSED",
        "publishedAt": "2016-04-12T15:42:51Z",
        "closedAt": "2016-04-20T15:01:21Z",
        "title": "Bash completion fails to autocomplete files or directories",
        "bodyText": "I've enabled kubectl bash completion as per the instructions:\nsource ./contrib/completions/bash/kubectl\nNow autocompletion works for the kubectl commands. However, if I try to autocomplete a file or directory name, it fails.\n\nkubectl create -f ./ser\nTab to autocomplete directory name\nkubectl create -f ./ser-bash: _filedir: command not found\n\nI am using OSX.",
        "bodyHTML": "<p>I've enabled <code>kubectl</code> bash completion as per the <a href=\"http://kubernetes.io/docs/getting-started-guides/gce/\" rel=\"nofollow\">instructions</a>:</p>\n<p><code>source ./contrib/completions/bash/kubectl</code></p>\n<p>Now autocompletion works for the <code>kubectl</code> commands. However, if I try to autocomplete a file or directory name, it fails.</p>\n<ol>\n<li><code>kubectl create -f ./ser</code></li>\n<li>Tab to autocomplete directory name</li>\n<li><code>kubectl create -f ./ser-bash: _filedir: command not found</code></li>\n</ol>\n<p>I am using OSX.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "justinsb" },
        "number": 30338,
        "resourcePath": "/kubernetes/kubernetes/issues/30338",
        "state": "OPEN",
        "publishedAt": "2016-08-10T03:17:48Z",
        "closedAt": null,
        "title": "Document / rationalize CNI plugin distribution",
        "bodyText": "I believe the correct place to download the CNI plugins is https://storage.googleapis.com/kubernetes-release/network-plugins/cni-c864f0e1ea73719b8f4582402b0847064f9883b0.tar.gz\nA few challenges with that:\n\nIt's not clear which version is the latest of the handful in that directory (they all the same date, and the hash doesn't give any clues)\nWe should probably bundle it instead with the k8s version with which that k8s version is tested\nI'm not entirely sure how this tar file was built\n\nIt would be nice if they were available as individual files also (i.e. expanded form), just like we distribute the key k8s binaries in expanded form and in the the kubernetes.tar.gz file.  On that note they appear to actually depend on each other though, so perhaps they can't be split.",
        "bodyHTML": "<p>I believe the correct place to download the CNI plugins is <code>https://storage.googleapis.com/kubernetes-release/network-plugins/cni-c864f0e1ea73719b8f4582402b0847064f9883b0.tar.gz</code></p>\n<p>A few challenges with that:</p>\n<ul>\n<li>It's not clear which version is the latest of the handful in that directory (they all the same date, and the hash doesn't give any clues)</li>\n<li>We should probably bundle it instead with the k8s version with which that k8s version is tested</li>\n<li>I'm not entirely sure how this tar file was built</li>\n</ul>\n<p>It would be nice if they were available as individual files also (i.e. expanded form), just like we distribute the key k8s binaries in expanded form and in the the kubernetes.tar.gz file.  On that note they appear to actually depend on each other though, so perhaps they can't be split.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "luxas" },
        "number": 24534,
        "resourcePath": "/kubernetes/kubernetes/issues/24534",
        "state": "CLOSED",
        "publishedAt": "2016-04-20T16:03:06Z",
        "closedAt": "2016-04-20T22:15:01Z",
        "title": "v1.3.0-alpha.2 binaries not pushed",
        "bodyText": "It seems like v1.3.0-alpha.2 binaries are not pushed.\n$ curl -I https://storage.googleapis.com/kubernetes-release/release/v1.3.0-alpha.2/bin/linux/amd64/kubectl\nHTTP/1.1 404 Not Found\nX-GUploader-UploadID: AEnB2UqOJabwiZ7oFEUJjl-os0mvuOaqsNMaKo9pQr6RJtv1SjStRaTQ6_XwWj3duuLF1Q7bymVfgcSNvoRQoxNzLHHUOdgUiQ\nContent-Type: application/xml; charset=UTF-8\nContent-Length: 127\nDate: Wed, 20 Apr 2016 16:01:18 GMT\nExpires: Wed, 20 Apr 2016 16:01:18 GMT\nCache-Control: private, max-age=0\nServer: UploadServer\nAlternate-Protocol: 443:quic\nAlt-Svc: quic=\":443\"; ma=2592000; v=\"32,31,30,29,28,27,26,25\"\n@david-mcmahon @bgrant0607 @ixdy",
        "bodyHTML": "<p>It seems like <code>v1.3.0-alpha.2</code> binaries are not pushed.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">curl -I https://storage.googleapis.com/kubernetes-release/release/v1.3.0-alpha.2/bin/linux/amd64/kubectl</span>\n<span class=\"pl-c1\">HTTP/1.1 404 Not Found</span>\n<span class=\"pl-c1\">X-GUploader-UploadID: AEnB2UqOJabwiZ7oFEUJjl-os0mvuOaqsNMaKo9pQr6RJtv1SjStRaTQ6_XwWj3duuLF1Q7bymVfgcSNvoRQoxNzLHHUOdgUiQ</span>\n<span class=\"pl-c1\">Content-Type: application/xml; charset=UTF-8</span>\n<span class=\"pl-c1\">Content-Length: 127</span>\n<span class=\"pl-c1\">Date: Wed, 20 Apr 2016 16:01:18 GMT</span>\n<span class=\"pl-c1\">Expires: Wed, 20 Apr 2016 16:01:18 GMT</span>\n<span class=\"pl-c1\">Cache-Control: private, max-age=0</span>\n<span class=\"pl-c1\">Server: UploadServer</span>\n<span class=\"pl-c1\">Alternate-Protocol: 443:quic</span>\n<span class=\"pl-c1\">Alt-Svc: quic=\":443\"; ma=2592000; v=\"32,31,30,29,28,27,26,25\"</span></pre></div>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/david-mcmahon/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/david-mcmahon\">@david-mcmahon</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/bgrant0607/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bgrant0607\">@bgrant0607</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/ixdy/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ixdy\">@ixdy</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "lavalamp" },
        "number": 3338,
        "resourcePath": "/kubernetes/kubernetes/issues/3338",
        "state": "CLOSED",
        "publishedAt": "2015-01-08T22:15:00Z",
        "closedAt": "2015-12-05T00:45:06Z",
        "title": "Investigate alternative JSON parsers",
        "bodyText": "e.g., I saw this on reddit: http://ugorji.net/blog/go-codecgen\nThe primary reason why this would be worth our time at the moment is to get better error messages when people do things like pass a string to an array, or misspell a field name. Performance will eventually become important as we scale up, but serialization performance is a very tiny issue compared with e.g. the serial health checking when you list minions.",
        "bodyHTML": "<p>e.g., I saw this on reddit: <a rel=\"nofollow\" href=\"http://ugorji.net/blog/go-codecgen\">http://ugorji.net/blog/go-codecgen</a></p>\n<p>The primary reason why this would be worth our time at the moment is to get better error messages when people do things like pass a string to an array, or misspell a field name. Performance will eventually become important as we scale up, but serialization performance is a very tiny issue compared with e.g. the serial health checking when you list minions.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "k8s-github-robot" },
        "number": 32237,
        "resourcePath": "/kubernetes/kubernetes/issues/32237",
        "state": "CLOSED",
        "publishedAt": "2016-09-07T22:18:14Z",
        "closedAt": "2016-09-08T18:53:37Z",
        "title": "ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}",
        "bodyText": "https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke/15018/\nFailed: ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134\nSep  7 14:58:43.497: Couldn't delete ns: \"e2e-tests-thirdparty-dn7z5\": unable to retrieve the complete list of server APIs: company.com/v1: the server could not find the requested resource (&discovery.ErrGroupDiscoveryFailed{Groups:map[unversioned.GroupVersion]error{unversioned.GroupVersion{Group:\"company.com\", Version:\"v1\"}:(*errors.StatusError)(0xc820cece80)}})\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:338",
        "bodyHTML": "<p><a rel=\"nofollow\" href=\"https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke/15018/\">https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke/15018/</a></p>\n<p>Failed: ThirdParty resources Simple Third Party creating/deleting thirdparty objects works [Conformance] {Kubernetes e2e suite}</p>\n<pre><code>/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:134\nSep  7 14:58:43.497: Couldn't delete ns: \"e2e-tests-thirdparty-dn7z5\": unable to retrieve the complete list of server APIs: company.com/v1: the server could not find the requested resource (&amp;discovery.ErrGroupDiscoveryFailed{Groups:map[unversioned.GroupVersion]error{unversioned.GroupVersion{Group:\"company.com\", Version:\"v1\"}:(*errors.StatusError)(0xc820cece80)}})\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:338\n</code></pre>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "stephenR" },
        "number": 7965,
        "resourcePath": "/kubernetes/kubernetes/issues/7965",
        "state": "CLOSED",
        "publishedAt": "2015-05-08T16:58:17Z",
        "closedAt": "2015-05-12T23:26:02Z",
        "title": "Secure kubelet port 10250",
        "bodyText": "The kubelet exposes an unauthenticated endpoint on port 10250. The issues with this:\n\nthere are the debug handlers /exec/ and /run/ that run code in any container on the host\nthese debug handlers are enabled by default\nthe code run in the container runs with full root capabilities (compared to docker's root with a capability bounding set)",
        "bodyHTML": "<p>The kubelet exposes an unauthenticated endpoint on port 10250. The issues with this:</p>\n<ul>\n<li>there are the debug handlers /exec/ and /run/ that run code in any container on the host</li>\n<li>these debug handlers are enabled by default</li>\n<li>the code run in the container runs with full root capabilities (compared to docker's root with a capability bounding set)</li>\n</ul>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "MrHohn" },
        "number": 33289,
        "resourcePath": "/kubernetes/kubernetes/issues/33289",
        "state": "CLOSED",
        "publishedAt": "2016-09-22T16:34:52Z",
        "closedAt": "2016-10-25T20:23:51Z",
        "title": "Rescheduler e2e should not scale kube-dns pods.",
        "bodyText": "In rescheduler e2e test, for It(\"should ensure that critical pod is scheduled in case there is no resources available\"), kube-dns is used as the critical pod target and being scaled out and in.\nHowever, we plan to enable the dns horizontal autoscaling feature in the near future, such as this WIP. If this feature is turned on, the corresponding autoscaler will fight with this rescheduler e2e and maintain the desired number of replicas. Hence this test will be very likely to fail.\nSo probably we should use another critical pod here, or either create a new critical pod that does not exist before in order to protect the ongoing functionalities.\n@piosz  @thockin",
        "bodyHTML": "<p>In <a href=\"https://github.com/kubernetes/kubernetes/blob/master/test/e2e/rescheduler.go\">rescheduler e2e test</a>, for It(\"should ensure that critical pod is scheduled in case there is no resources available\"), kube-dns is used as the critical pod target and being scaled out and in.</p>\n<p>However, we plan to enable the dns horizontal autoscaling feature in the near future, such as this <a href=\"https://github.com/kubernetes/kubernetes/pull/33239\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/33239/hovercard\">WIP</a>. If this feature is turned on, the corresponding autoscaler will fight with this rescheduler e2e and maintain the desired number of replicas. Hence this test will be very likely to fail.</p>\n<p>So probably we should use another critical pod here, or either create a new critical pod that does not exist before in order to protect the ongoing functionalities.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/piosz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/piosz\">@piosz</a>  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/thockin/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/thockin\">@thockin</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "jayunit100" },
        "number": 7989,
        "resourcePath": "/kubernetes/kubernetes/issues/7989",
        "state": "CLOSED",
        "publishedAt": "2015-05-08T21:03:33Z",
        "closedAt": "2016-03-17T22:54:23Z",
        "title": "E2E: Audit density.go iterator and other timeout iterators ",
        "bodyText": "In my last PR @brendanburns duly noted  that you can do declarative style of timeout iteration, which is an important part of the e2e's, rather than a for loop.\n\nlets use the style of iteration in soak_k8petstore.go (pending merge now) with switch -> case statements in density.go as well, and possibly other places.    There are also other examples of this online, (i.e.  https://code.google.com/p/go-wiki/wiki/Timeouts)\nwhile we're at it lets audit util.go and see if it is being used wherever possible in tests to maximize code reuse.  there are now a lot of new utils in it (like RunRC and createNS so on) which weren't there when e2e's were originally created",
        "bodyHTML": "<p>In my last PR <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/brendanburns/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/brendanburns\">@brendanburns</a> duly noted  that you can do declarative style of timeout iteration, which is an important part of the e2e's, rather than a for loop.</p>\n<ul>\n<li>lets use the style of iteration in <code>soak_k8petstore.go</code> (pending merge now) with switch -&gt; case statements in <code>density.go</code> as well, and possibly other places.    There are also other examples of this online, (i.e.  <a href=\"https://code.google.com/p/go-wiki/wiki/Timeouts\" rel=\"nofollow\">https://code.google.com/p/go-wiki/wiki/Timeouts</a>)</li>\n<li>while we're at it lets audit <code>util.go</code> and see if it is being used wherever possible in tests to maximize code reuse.  there are now a lot of new utils in it (like RunRC and createNS so on) which weren't there when e2e's were originally created</li>\n</ul>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "jba" },
        "number": 30069,
        "resourcePath": "/kubernetes/kubernetes/issues/30069",
        "state": "CLOSED",
        "publishedAt": "2016-08-04T11:54:18Z",
        "closedAt": "2016-09-14T15:39:49Z",
        "title": "update Google Cloud API client import paths and more",
        "bodyText": "The Google Cloud API client libraries for Go are making some breaking changes:\n\nThe import paths are changing from google.golang.org/cloud/... to\ncloud.google.com/go/.... For example, if your code imports the BigQuery client\nit currently reads\nimport \"google.golang.org/cloud/bigquery\"\nIt should be changed to\nimport \"cloud.google.com/go/bigquery\"\nClient options are also moving, from google.golang.org/cloud to\ngoogle.golang.org/api/option. Two have also been renamed:\n\nWithBaseGRPC is now WithGRPCConn\nWithBaseHTTP is now WithHTTPClient\n\n\nThe cloud.WithContext and cloud.NewContext methods are gone, as are the\ndeprecated pubsub and container functions that required them. Use the Client\nmethods of these packages instead.\n\nYou should make these changes before September 12, 2016, when the packages at\ngoogle.golang.org/cloud will go away.",
        "bodyHTML": "<p>The Google Cloud API client libraries for Go are making some breaking changes:</p>\n<ul>\n<li>The import paths are changing from <code>google.golang.org/cloud/...</code> to<br>\n<code>cloud.google.com/go/...</code>. For example, if your code imports the BigQuery client<br>\nit currently reads<br>\n<code>import \"google.golang.org/cloud/bigquery\"</code><br>\nIt should be changed to<br>\n<code>import \"cloud.google.com/go/bigquery\"</code></li>\n<li>Client options are also moving, from <code>google.golang.org/cloud</code> to<br>\n<code>google.golang.org/api/option</code>. Two have also been renamed:\n<ul>\n<li><code>WithBaseGRPC</code> is now <code>WithGRPCConn</code></li>\n<li><code>WithBaseHTTP</code> is now <code>WithHTTPClient</code></li>\n</ul>\n</li>\n<li>The <code>cloud.WithContext</code> and <code>cloud.NewContext</code> methods are gone, as are the<br>\ndeprecated pubsub and container functions that required them. Use the <code>Client</code><br>\nmethods of these packages instead.</li>\n</ul>\n<p>You should make these changes before September 12, 2016, when the packages at<br>\n<code>google.golang.org/cloud</code> will go away.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "bgrant0607" },
        "number": 16938,
        "resourcePath": "/kubernetes/kubernetes/issues/16938",
        "state": "CLOSED",
        "publishedAt": "2015-11-06T19:26:31Z",
        "closedAt": "2016-05-27T04:16:43Z",
        "title": "Copy relevant useful docs from https://cloud.google.com/container-engine/docs/",
        "bodyText": "They should have been in github in the first place.\ncc @mansoorj",
        "bodyHTML": "<p>They should have been in github in the first place.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/mansoorj/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mansoorj\">@mansoorj</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "piosz" },
        "number": 5184,
        "resourcePath": "/kubernetes/kubernetes/issues/5184",
        "state": "CLOSED",
        "publishedAt": "2015-03-09T15:47:01Z",
        "closedAt": "2015-03-31T16:27:02Z",
        "title": "Missing service environment variables while starting pod",
        "bodyText": "While debugging #5091 I noticed that when I'm creating guestbook application by running ./cluster/kubectl.sh create -f examples/guestbook there is a chance that frontend pods come up before information about environment variables of redis is propagated. In such case frontend can't connect to database during its whole life.\nIt's actually more general problem, since most of complex application might be affected by this. Also I think create a set of resource (especially by specifying the directory of their config files) should just work no matter of the order of creation.\nI can see few solutions of such problem:\n\nGet rid off HOST:PORT stuff and start using DNS service instead.\nAdd ability to wait for such information being propagated.\nInjecting environment variables to container somehow(?).",
        "bodyHTML": "<p>While debugging <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"59921304\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/5091\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/5091/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/5091\">#5091</a> I noticed that when I'm creating guestbook application by running <code>./cluster/kubectl.sh create -f examples/guestbook</code> there is a chance that frontend pods come up before information about environment variables of redis is propagated. In such case frontend can't connect to database during its whole life.</p>\n<p>It's actually more general problem, since most of complex application might be affected by this. Also I think create a set of resource (especially by specifying the directory of their config files) should just work no matter of the order of creation.</p>\n<p>I can see few solutions of such problem:</p>\n<ol>\n<li>Get rid off HOST:PORT stuff and start using DNS service instead.</li>\n<li>Add ability to wait for such information being propagated.</li>\n<li>Injecting environment variables to container somehow(?).</li>\n</ol>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "krousey" },
        "number": 32224,
        "resourcePath": "/kubernetes/kubernetes/issues/32224",
        "state": "CLOSED",
        "publishedAt": "2016-09-07T20:23:17Z",
        "closedAt": "2016-09-12T09:39:51Z",
        "title": "registered.EnabledVersions returns all registered versions",
        "bodyText": "#20846 introduced a bug where registered.EnabledVersions doesn't consult enabledVersions anymore... I think. I was trying to debug another problem and spotted this.\nI think only clients use this function. APIServer has an entirely different mechanism for tracking enabled/disabled things.\ncc @kubernetes/sig-api-machinery",
        "bodyHTML": "<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"132247360\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/20846\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/20846/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/20846\">#20846</a> introduced a bug where <code>registered.EnabledVersions</code> doesn't consult <code>enabledVersions</code> anymore... I think. I was trying to debug another problem and spotted this.</p>\n<p>I think only clients use this function. APIServer has an entirely different mechanism for tracking enabled/disabled things.</p>\n<p>cc @kubernetes/sig-api-machinery</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "timstclair" },
        "number": 24614,
        "resourcePath": "/kubernetes/kubernetes/issues/24614",
        "state": "CLOSED",
        "publishedAt": "2016-04-21T19:12:44Z",
        "closedAt": "2019-01-12T01:42:21Z",
        "title": "Add a `shellcheck` based pre-submit",
        "bodyText": "shellcheck is a bash script linter which could help catch common problems in our (numerous) bash scripts. I know we'd like to reduce our use of bash scripts, but until then I think being stricter about script quality would be helpful.\n/cc @zmerlynn",
        "bodyHTML": "<p><a href=\"https://github.com/koalaman/shellcheck\">shellcheck</a> is a bash script linter which could help catch common problems in our (numerous) bash scripts. I know we'd like to reduce our use of bash scripts, but until then I think being stricter about script quality would be helpful.</p>\n<p>/cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/zmerlynn/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zmerlynn\">@zmerlynn</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "caesarxuchao" },
        "number": 21932,
        "resourcePath": "/kubernetes/kubernetes/issues/21932",
        "state": "CLOSED",
        "publishedAt": "2016-02-24T23:35:05Z",
        "closedAt": "2017-03-20T23:35:31Z",
        "title": "Remove the generated client for Scale",
        "bodyText": "Scale is a sub-resource, we shouldn't generate a typed client for it.\ncc @lavalamp @madhusudancs",
        "bodyHTML": "<p>Scale is a sub-resource, we shouldn't generate a typed client for it.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/lavalamp/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lavalamp\">@lavalamp</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/madhusudancs/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/madhusudancs\">@madhusudancs</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "justinsb" },
        "number": 22907,
        "resourcePath": "/kubernetes/kubernetes/issues/22907",
        "state": "CLOSED",
        "publishedAt": "2016-03-12T17:55:43Z",
        "closedAt": "2016-03-16T14:56:20Z",
        "title": "AWS: Check that dynamic volumes are deleted",
        "bodyText": "It was reported that dynamic volumes were not deleted on AWS.  Maybe we need to enable the PersistentVolumeRecycler somehow?",
        "bodyHTML": "<p>It was reported that dynamic volumes were not deleted on AWS.  Maybe we need to enable the PersistentVolumeRecycler somehow?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "derekwaynecarr" },
        "number": 12053,
        "resourcePath": "/kubernetes/kubernetes/issues/12053",
        "state": "CLOSED",
        "publishedAt": "2015-07-30T23:33:03Z",
        "closedAt": "2015-08-28T18:31:50Z",
        "title": "Merge NamespaceExists and NamespaceLifecycle admission controllers",
        "bodyText": "We should have a single NamespaceLifecycle plugin that enforces the Namespace rules now that all providers are off NamespaceAutoProvision.\nWe may also want to make this no longer a user choice to configure and hard-wire the server to always run this check first to simplify configuration errors.\nFor an example\n#12039",
        "bodyHTML": "<p>We should have a single NamespaceLifecycle plugin that enforces the Namespace rules now that all providers are off NamespaceAutoProvision.</p>\n<p>We may also want to make this no longer a user choice to configure and hard-wire the server to always run this check first to simplify configuration errors.</p>\n<p>For an example<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"98254648\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/12039\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/12039/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/12039\">#12039</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "lavalamp" },
        "number": 3345,
        "resourcePath": "/kubernetes/kubernetes/issues/3345",
        "state": "CLOSED",
        "publishedAt": "2015-01-08T23:46:41Z",
        "closedAt": "2015-01-23T23:21:43Z",
        "title": "Scaling clusters: 1000's of services in env vars",
        "bodyText": "Clearly you ought to be able to make more services in a k8s cluster than it is reasonable to pass to pods in env vars.\nPossible solutions:\n\nSegment by namespace\nRequire predeclarations\n\n@bgrant0607 I know you hate env vars; do you have a preferred solution for this?",
        "bodyHTML": "<p>Clearly you ought to be able to make more services in a k8s cluster than it is reasonable to pass to pods in env vars.</p>\n<p>Possible solutions:</p>\n<ul>\n<li>Segment by namespace</li>\n<li>Require predeclarations</li>\n</ul>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/bgrant0607/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bgrant0607\">@bgrant0607</a> I know you hate env vars; do you have a preferred solution for this?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "satnam6502" },
        "number": 2385,
        "resourcePath": "/kubernetes/kubernetes/issues/2385",
        "state": "CLOSED",
        "publishedAt": "2014-11-14T20:33:03Z",
        "closedAt": "2014-11-18T05:42:43Z",
        "title": "Pod dependencies on services",
        "bodyText": "One thing that is a bad experience at the moment is the bring-up behaviour of one pod that depends on another the services of another pod. For example, in my logging work the Kibana viewer (pod, service) depends on the Elasticsearch (pod, service). When I try and bring them up together from my Makefile I have an intermediate sate like this for quite a while:\nNAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS\ninflux-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending\nheapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running\nsynthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running\nelasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Pending\nkibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Failed\n\ni.e. the Kibana viewer fails to start up because Elasticsearch is not ready yet. Eventually things start to look better:\nNAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS\ninflux-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending\nheapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running\nsynthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running\nelasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Running\nkibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Running\nkubectl.sh get services\n\nbut even though the pods are marked as Running they are still not quite ready yet and it takes another five minutes or so before one can make queries to Elasticsearch and see log output in Kibana.\nIt would be nice to describe in a pod declaration its dependencies on other services so this can be taken into account during scheudling. For example:\napiVersion: v1beta1\nkind: Pod\nid: kibana-pod\ndesiredState:\n  manifest:\n    version: v1beta1\n    id: kibana-server\n    containers:\n      - name: kibana-image\n        image: kubernetes/kibana:latest\n        ports:\n          - name: kibana-port\n            containerPort: 80\n        dependencies: [elasticsearch]\nlabels:\n  app: kibana-viewer\n\nThis would delay the scheduling of this pod until the pod(s) identified by the elasticsearch service are all in the running state.",
        "bodyHTML": "<p>One thing that is a bad experience at the moment is the bring-up behaviour of one pod that depends on another the services of another pod. For example, in my logging work the Kibana viewer (pod, service) depends on the Elasticsearch (pod, service). When I try and bring them up together from my Makefile I have an intermediate sate like this for quite a while:</p>\n<pre><code>NAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS\ninflux-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending\nheapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running\nsynthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running\nelasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Pending\nkibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Failed\n</code></pre>\n<p>i.e. the Kibana viewer fails to start up because Elasticsearch is not ready yet. Eventually things start to look better:</p>\n<pre><code>NAME                           IMAGE(S)                                                                            HOST                                                           LABELS                      STATUS\ninflux-grafana                 kubernetes/heapster_influxdb,kubernetes/heapster_grafana,dockerfile/elasticsearch   kubernetes-minion-3.c.kubernetes-elk.internal/146.148.76.82    name=influxdb               Pending\nheapster                       kubernetes/heapster                                                                 kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=heapster               Running\nsynthetic-logger-0.25lps-pod   ubuntu:14.04                                                                        kubernetes-minion-1.c.kubernetes-elk.internal/130.211.126.68   name=synth-logging-source   Running\nelasticsearch-pod              dockerfile/elasticsearch                                                            kubernetes-minion-2.c.kubernetes-elk.internal/23.236.59.213    app=elasticsearch           Running\nkibana-pod                     kubernetes/kibana:latest                                                            kubernetes-minion-4.c.kubernetes-elk.internal/130.211.121.21   app=kibana-viewer           Running\nkubectl.sh get services\n</code></pre>\n<p>but even though the pods are marked as Running they are still not quite ready yet and it takes another five minutes or so before one can make queries to Elasticsearch and see log output in Kibana.</p>\n<p>It would be nice to describe in a pod declaration its dependencies on other services so this can be taken into account during scheudling. For example:</p>\n<pre><code>apiVersion: v1beta1\nkind: Pod\nid: kibana-pod\ndesiredState:\n  manifest:\n    version: v1beta1\n    id: kibana-server\n    containers:\n      - name: kibana-image\n        image: kubernetes/kibana:latest\n        ports:\n          - name: kibana-port\n            containerPort: 80\n        dependencies: [elasticsearch]\nlabels:\n  app: kibana-viewer\n</code></pre>\n<p>This would delay the scheduling of this pod until the pod(s) identified by the elasticsearch service are all in the running state.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "yossi-cohen" },
        "number": 20556,
        "resourcePath": "/kubernetes/kubernetes/issues/20556",
        "state": "CLOSED",
        "publishedAt": "2016-02-03T15:33:54Z",
        "closedAt": "2016-02-04T20:37:15Z",
        "title": "service account kube-system/default was not found in ubuntu",
        "bodyText": "hi\ni'm trying to run with k8s version 1.1.7\ni updated config-default  script and removed DenyEscalatingExec key from ADMISSION_CONTROL(otherwise its not working)\nafter that i tried to run kube-ui (or any other pod with kube-system namespace)  without any success\nin  kube-controller-manager.log  i saw the following error\nunable to create pod replica: Pod \"kube-ui-v4-\" is forbidden: service account kube-system/default was not found, retry after the service account is created\ni googled it  and  found the following  solution\n\nGenerate a signing key:\nopenssl genrsa -out /tmp/serviceaccount.key 2048\nUpdate /etc/kubernetes/apiserver:\nKUBE_API_ARGS=\"--service_account_key_file=/tmp/serviceaccount.key\"\nUpdate /etc/kubernetes/controller-manager:\nKUBE_CONTROLLER_MANAGER_ARGS=\"--service_account_private_key_file=/etc/kubernetes/serviceaccount.key\"\n\nbut  i couldn't found the keys in the config files\ni tried to update util.sh\nand set the following flags (thats the only places that i found those keys)\n--tls-private-key-file=/etc/kubernetes/serviceaccount.key\"\n--service-account-private-key-file=/etc/kubernetes/serviceaccount.key \\\nnote: doing kubectl serviceaccount for default namespace returns 1 entry\nBUT kubectl serviceaccount --namespace=kube-system returns NO ENTRIES!\ni'm really desperate :) does anyone have a  clue how to fix this issue\ntks a lot",
        "bodyHTML": "<p>hi</p>\n<p>i'm trying to run with k8s version 1.1.7<br>\ni updated config-default  script and removed DenyEscalatingExec key from ADMISSION_CONTROL(otherwise its not working)</p>\n<p>after that i tried to run kube-ui (or any other pod with kube-system namespace)  without any success</p>\n<p>in  kube-controller-manager.log  i saw the following error</p>\n<p><strong>unable to create pod replica: Pod \"kube-ui-v4-\" is forbidden: service account kube-system/default was not found, retry after the service account is created</strong></p>\n<p>i googled it  and  found the following  solution</p>\n<blockquote>\n<p>Generate a signing key:</p>\n<p>openssl genrsa -out /tmp/serviceaccount.key 2048<br>\nUpdate /etc/kubernetes/apiserver:</p>\n<p>KUBE_API_ARGS=\"--service_account_key_file=/tmp/serviceaccount.key\"<br>\nUpdate /etc/kubernetes/controller-manager:</p>\n<p>KUBE_CONTROLLER_MANAGER_ARGS=\"--service_account_private_key_file=/etc/kubernetes/serviceaccount.key\"</p>\n</blockquote>\n<p>but  i couldn't found the keys in the config files</p>\n<p>i tried to update util.sh</p>\n<p>and set the following flags (thats the only places that i found those keys)<br>\n--tls-private-key-file=/etc/kubernetes/serviceaccount.key\"<br>\n--service-account-private-key-file=/etc/kubernetes/serviceaccount.key \\</p>\n<p>note: doing <strong>kubectl serviceaccount</strong> for default namespace returns 1 entry<br>\nBUT <strong>kubectl serviceaccount --namespace=kube-system</strong> returns NO ENTRIES!</p>\n<p>i'm really desperate :) does anyone have a  clue how to fix this issue</p>\n<p>tks a lot</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "justinsb" },
        "number": 4024,
        "resourcePath": "/kubernetes/kubernetes/issues/4024",
        "state": "CLOSED",
        "publishedAt": "2015-02-02T17:48:40Z",
        "closedAt": "2015-05-18T16:35:37Z",
        "title": "etcd arguments are different for systemd vs non-systemd",
        "bodyText": "kubernetes/cluster/saltbase/salt/etcd/default (used for systemd) and kubernetes/cluster/saltbase/salt/etcd/initd (used for initd) have diverged; in particular DAEMON_ARGS has a different bind_addr.\nAlso, I'm not sure whether etcd.conf is still used at all, but it has another value for the bind addresses (hard-coded to 0.0.0.0).",
        "bodyHTML": "<p>kubernetes/cluster/saltbase/salt/etcd/default (used for systemd) and kubernetes/cluster/saltbase/salt/etcd/initd (used for initd) have diverged; in particular DAEMON_ARGS has a different bind_addr.</p>\n<p>Also, I'm not sure whether etcd.conf is still used at all, but it has another value for the bind addresses (hard-coded to 0.0.0.0).</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "ronaldpetty" },
        "number": 13060,
        "resourcePath": "/kubernetes/kubernetes/issues/13060",
        "state": "CLOSED",
        "publishedAt": "2015-08-21T22:00:50Z",
        "closedAt": "2015-09-16T21:10:49Z",
        "title": "Incorrect Login For Master",
        "bodyText": "Hello,\nFor a brand new (first time) installation using the Vagrant install fails (different reason, suspect python lib issue).  I then switched to using the AWS tutorial, it succeeds after some undocumented fixes (will add new bug for that in docs).  After it starts, if you visit the AWS EC2 master, the authentication is requiring the login from the previous Vagrant install.  I suspect it didn't override with the new AWS settings.  I will keep investigating and try to see if this is a real bug or poor docs.\nRegards.\nRon",
        "bodyHTML": "<p>Hello,</p>\n<p>For a brand new (first time) installation using the Vagrant install fails (different reason, suspect python lib issue).  I then switched to using the AWS tutorial, it succeeds after some undocumented fixes (will add new bug for that in docs).  After it starts, if you visit the AWS EC2 master, the authentication is requiring the login from the previous Vagrant install.  I suspect it didn't override with the new AWS settings.  I will keep investigating and try to see if this is a real bug or poor docs.</p>\n<p>Regards.</p>\n<p>Ron</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "wonderfly" },
        "number": 23389,
        "resourcePath": "/kubernetes/kubernetes/issues/23389",
        "state": "CLOSED",
        "publishedAt": "2016-03-23T17:54:10Z",
        "closedAt": "2016-05-27T22:54:37Z",
        "title": "e2e flake: Kubernetes e2e suite.Kubectl client Kubectl apply should apply a new configuration to an existing RC",
        "bodyText": "Recent failure on #23287 seems flaky. That PR didn't change anything but delete a few dead\nJenkins jobs.\nkubernetes-pull-build-test-e2e-gce/33368/\nStacktrace\n\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:463\nExpected error:\n    <*errors.errorString | 0xc208266480>: {\n        s: \"Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\\n{\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"kind\\\\\\\":\\\\\\\"ReplicationController\\\\\\\",\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"metadata\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"replicas\\\\\\\":1,\\\\\\\"selector\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"},\\\\\\\"template\\\\\\\":{\\\\\\\"metadata\\\\\\\":{\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"containers\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"image\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"ports\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-server\\\\\\\",\\\\\\\"containerPort\\\\\\\":6379}],\\\\\\\"resources\\\\\\\":{}}]}}},\\\\\\\"status\\\\\\\":{\\\\\\\"replicas\\\\\\\":0}}\\\"},\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}}}}}\\nto:\\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\\nfor: \\\"STDIN\\\": replicationControllers \\\"redis-master\\\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\\n [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\\nCommand stdout:\\n\\nstderr:\\nError from server: error when applying patch:\\n{\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"kind\\\\\\\":\\\\\\\"ReplicationController\\\\\\\",\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"metadata\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"replicas\\\\\\\":1,\\\\\\\"selector\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"},\\\\\\\"template\\\\\\\":{\\\\\\\"metadata\\\\\\\":{\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"containers\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"image\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"ports\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-server\\\\\\\",\\\\\\\"containerPort\\\\\\\":6379}],\\\\\\\"resources\\\\\\\":{}}]}}},\\\\\\\"status\\\\\\\":{\\\\\\\"replicas\\\\\\\":0}}\\\"},\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}}}}}\\nto:\\n&{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\\nfor: \\\"STDIN\\\": replicationControllers \\\"redis-master\\\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\\n\\n\",\n    }\n    Error running &{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\n    {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"kind\\\":\\\"ReplicationController\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"redis-master\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"redis-master\\\",\\\"image\\\":\\\"redis\\\",\\\"ports\\\":[{\\\"name\\\":\\\"redis-server\\\",\\\"containerPort\\\":6379}],\\\"resources\\\":{}}]}}},\\\"status\\\":{\\\"replicas\\\":0}}\"},\"creationTimestamp\":null,\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}},\"spec\":{\"selector\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"},\"template\":{\"metadata\":{\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}}}}}\n    to:\n    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\n    for: \"STDIN\": replicationControllers \"redis-master\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n     [] <nil> 0xc20833f3a0 exit status 1 <nil> true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\n    Command stdout:\n\n    stderr:\n    Error from server: error when applying patch:\n    {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"kind\\\":\\\"ReplicationController\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"redis-master\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"redis-master\\\",\\\"image\\\":\\\"redis\\\",\\\"ports\\\":[{\\\"name\\\":\\\"redis-server\\\",\\\"containerPort\\\":6379}],\\\"resources\\\":{}}]}}},\\\"status\\\":{\\\"replicas\\\":0}}\"},\"creationTimestamp\":null,\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}},\"spec\":{\"selector\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"},\"template\":{\"metadata\":{\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}}}}}\n    to:\n    &{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\n    for: \"STDIN\": replicationControllers \"redis-master\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n\n\nnot to have occurred\n\n@spxtr to delegate.",
        "bodyHTML": "<p>Recent failure on <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"142420648\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/23287\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/23287/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/23287\">#23287</a> seems flaky. That PR didn't change anything but delete a few dead<br>\nJenkins jobs.</p>\n<p>kubernetes-pull-build-test-e2e-gce/33368/</p>\n<pre><code>Stacktrace\n\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:463\nExpected error:\n    &lt;*errors.errorString | 0xc208266480&gt;: {\n        s: \"Error running &amp;{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\\n{\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"kind\\\\\\\":\\\\\\\"ReplicationController\\\\\\\",\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"metadata\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"replicas\\\\\\\":1,\\\\\\\"selector\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"},\\\\\\\"template\\\\\\\":{\\\\\\\"metadata\\\\\\\":{\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"containers\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"image\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"ports\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-server\\\\\\\",\\\\\\\"containerPort\\\\\\\":6379}],\\\\\\\"resources\\\\\\\":{}}]}}},\\\\\\\"status\\\\\\\":{\\\\\\\"replicas\\\\\\\":0}}\\\"},\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}}}}}\\nto:\\n&amp;{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\\nfor: \\\"STDIN\\\": replicationControllers \\\"redis-master\\\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\\n [] &lt;nil&gt; 0xc20833f3a0 exit status 1 &lt;nil&gt; true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\\nCommand stdout:\\n\\nstderr:\\nError from server: error when applying patch:\\n{\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"kind\\\\\\\":\\\\\\\"ReplicationController\\\\\\\",\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"metadata\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"replicas\\\\\\\":1,\\\\\\\"selector\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"},\\\\\\\"template\\\\\\\":{\\\\\\\"metadata\\\\\\\":{\\\\\\\"creationTimestamp\\\\\\\":null,\\\\\\\"labels\\\\\\\":{\\\\\\\"app\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"kubectl.kubernetes.io/apply-test\\\\\\\":\\\\\\\"ADDED\\\\\\\",\\\\\\\"role\\\\\\\":\\\\\\\"master\\\\\\\"}},\\\\\\\"spec\\\\\\\":{\\\\\\\"containers\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-master\\\\\\\",\\\\\\\"image\\\\\\\":\\\\\\\"redis\\\\\\\",\\\\\\\"ports\\\\\\\":[{\\\\\\\"name\\\\\\\":\\\\\\\"redis-server\\\\\\\",\\\\\\\"containerPort\\\\\\\":6379}],\\\\\\\"resources\\\\\\\":{}}]}}},\\\\\\\"status\\\\\\\":{\\\\\\\"replicas\\\\\\\":0}}\\\"},\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\"}}}}}\\nto:\\n&amp;{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\\nfor: \\\"STDIN\\\": replicationControllers \\\"redis-master\\\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\\n\\n\",\n    }\n    Error running &amp;{/jenkins-master-data/jobs/kubernetes-pull-build-test-e2e-gce/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.103.210 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-pull-build-test-e2e-gce/workspace/.kube/config apply -f - --namespace=e2e-tests-kubectl-ein3k] []  0xc2083c0510  Error from server: error when applying patch:\n    {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"kind\\\":\\\"ReplicationController\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"redis-master\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"redis-master\\\",\\\"image\\\":\\\"redis\\\",\\\"ports\\\":[{\\\"name\\\":\\\"redis-server\\\",\\\"containerPort\\\":6379}],\\\"resources\\\":{}}]}}},\\\"status\\\":{\\\"replicas\\\":0}}\"},\"creationTimestamp\":null,\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}},\"spec\":{\"selector\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"},\"template\":{\"metadata\":{\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}}}}}\n    to:\n    &amp;{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\n    for: \"STDIN\": replicationControllers \"redis-master\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n     [] &lt;nil&gt; 0xc20833f3a0 exit status 1 &lt;nil&gt; true [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051338 0xc208051380 0xc2080513a8] [0xc208051348 0xc208051378 0xc2080513a0] [0x6da5a0 0x6da6c0 0x6da6c0] 0xc20833a5a0}:\n    Command stdout:\n\n    stderr:\n    Error from server: error when applying patch:\n    {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"kind\\\":\\\"ReplicationController\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"redis-master\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"redis\\\",\\\"kubectl.kubernetes.io/apply-test\\\":\\\"ADDED\\\",\\\"role\\\":\\\"master\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"redis-master\\\",\\\"image\\\":\\\"redis\\\",\\\"ports\\\":[{\\\"name\\\":\\\"redis-server\\\",\\\"containerPort\\\":6379}],\\\"resources\\\":{}}]}}},\\\"status\\\":{\\\"replicas\\\":0}}\"},\"creationTimestamp\":null,\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}},\"spec\":{\"selector\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"},\"template\":{\"metadata\":{\"labels\":{\"kubectl.kubernetes.io/apply-test\":\"ADDED\"}}}}}\n    to:\n    &amp;{0xc20802e410 0xc208089030 e2e-tests-kubectl-ein3k redis-master STDIN 0xc2084972c0 0xc2082ab1d0 566}\n    for: \"STDIN\": replicationControllers \"redis-master\" cannot be updated: the object has been modified; please apply your changes to the latest version and try again\n\n\nnot to have occurred\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/spxtr/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/spxtr\">@spxtr</a> to delegate.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "yifan-gu" },
        "number": 19494,
        "resourcePath": "/kubernetes/kubernetes/issues/19494",
        "state": "CLOSED",
        "publishedAt": "2016-01-11T19:24:54Z",
        "closedAt": "2016-02-01T19:53:36Z",
        "title": "rkt: retrieve image size from rkt api service",
        "bodyText": "As rkt/rkt#1916 is merged, we should be able to return the image size information to kubelet.\ncc @derekparker @sjpotter @jonboulle",
        "bodyHTML": "<p>As <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"123741371\" data-permission-text=\"Title is private\" data-url=\"https://github.com/rkt/rkt/issues/1916\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/rkt/rkt/pull/1916/hovercard\" href=\"https://github.com/rkt/rkt/pull/1916\">rkt/rkt#1916</a> is merged, we should be able to return the image size information to kubelet.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/derekparker/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/derekparker\">@derekparker</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/sjpotter/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sjpotter\">@sjpotter</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/jonboulle/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jonboulle\">@jonboulle</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "msaffitz" },
        "number": 23170,
        "resourcePath": "/kubernetes/kubernetes/issues/23170",
        "state": "CLOSED",
        "publishedAt": "2016-03-18T01:14:36Z",
        "closedAt": "2016-03-18T19:46:02Z",
        "title": "Pods Killed Every 5 Minutes After Upgrading to 1.2",
        "bodyText": "We've successfully been running 1.1.7 on AWS for several months.  After upgrading to 1.2 today nearly all of our pods are killed every 5 minutes.  In the logs we see Killing container with docker id 661a6ded11b9: Need to kill pod., but beyond that there doesn't seem to be a clear cause.  I'm at a bit of loss for where to look next and how to debug this, and love any suggestions / recommendations.",
        "bodyHTML": "<p>We've successfully been running 1.1.7 on AWS for several months.  After upgrading to 1.2 today nearly all of our pods are killed every 5 minutes.  In the logs we see <code>Killing container with docker id 661a6ded11b9: Need to kill pod.</code>, but beyond that there doesn't seem to be a clear cause.  I'm at a bit of loss for where to look next and how to debug this, and love any suggestions / recommendations.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "apackeer" },
        "number": 12137,
        "resourcePath": "/kubernetes/kubernetes/issues/12137",
        "state": "CLOSED",
        "publishedAt": "2015-08-03T05:58:12Z",
        "closedAt": "2015-08-03T23:09:45Z",
        "title": "Named Ports not creating _name._protocol.<service> SRV entries",
        "bodyText": "Hi,\nI have the following service definition for use on GCE:\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth\n  labels:\n    name: auth\nspec:\n  type: LoadBalancer\n  ports:\n  - name: api\n    protocol: TCP\n    port: 8000\n    targetPort: 8000\n  selector:\n    name: auth\n\nWhen i do a SRV lookup to get the port (as per instructions here) for that service using:\ndig SRV _api._TCP.auth.default.cluster.local.\nI get:\nroot@ubuntu:/# dig SRV _api._tcp.auth.default.cluster.local.\n\n; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV _api._tcp.auth.default.cluster.local.\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11979\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;_api._tcp.auth.default.cluster.local. IN SRV\n\n;; AUTHORITY SECTION:\ncluster.local.      60  IN  SOA ns.dns.cluster.local. hostmaster.skydns.local. 1438578000 28800 7200 604800 60\n\n;; Query time: 5 msec\n;; SERVER: 10.111.240.10#53(10.111.240.10)\n;; WHEN: Mon Aug 03 05:44:55 UTC 2015\n;; MSG SIZE  rcvd: 123\n\nThe service does get registered properly and is responding. I can look up the service if I use:\ndig SRV default.cluster.local.\nI get:\n; <<>> DiG 9.9.5-3-Ubuntu <<>> SRV default.cluster.local.\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 54040\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 2\n\n;; QUESTION SECTION:\n;default.cluster.local.     IN  SRV\n\n;; ANSWER SECTION:\ndefault.cluster.local.  30  IN  SRV 10 50 0 kubernetes.default.cluster.local.\ndefault.cluster.local.  30  IN  SRV 10 50 0 auth.default.cluster.local.\n\n;; ADDITIONAL SECTION:\nkubernetes.default.cluster.local. 30 IN A   10.111.240.1\nauth.default.cluster.local. 30 IN A 10.111.247.242\n\n;; Query time: 7 msec\n;; SERVER: 10.111.240.10#53(10.111.240.10)\n;; WHEN: Mon Aug 03 05:46:53 UTC 2015\n;; MSG SIZE  rcvd: 201\n\nWhy doesn't the port number get returned when I lookup the named port via an SRV record?",
        "bodyHTML": "<p>Hi,</p>\n<p>I have the following service definition for use on GCE:</p>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: auth\n  labels:\n    name: auth\nspec:\n  type: LoadBalancer\n  ports:\n  - name: api\n    protocol: TCP\n    port: 8000\n    targetPort: 8000\n  selector:\n    name: auth\n</code></pre>\n<p>When i do a SRV lookup to get the port (as per instructions <a href=\"http://kubernetes.io/v1.0/docs/user-guide/services.html#dns\" rel=\"nofollow\">here</a>) for that service using:<br>\n<code>dig SRV _api._TCP.auth.default.cluster.local.</code></p>\n<p>I get:</p>\n<pre><code>root@ubuntu:/# dig SRV _api._tcp.auth.default.cluster.local.\n\n; &lt;&lt;&gt;&gt; DiG 9.9.5-3-Ubuntu &lt;&lt;&gt;&gt; SRV _api._tcp.auth.default.cluster.local.\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 11979\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;_api._tcp.auth.default.cluster.local. IN SRV\n\n;; AUTHORITY SECTION:\ncluster.local.      60  IN  SOA ns.dns.cluster.local. hostmaster.skydns.local. 1438578000 28800 7200 604800 60\n\n;; Query time: 5 msec\n;; SERVER: 10.111.240.10#53(10.111.240.10)\n;; WHEN: Mon Aug 03 05:44:55 UTC 2015\n;; MSG SIZE  rcvd: 123\n</code></pre>\n<p>The service does get registered properly and is responding. I can look up the service if I use:<br>\n<code>dig SRV default.cluster.local.</code></p>\n<p>I get:</p>\n<pre><code>; &lt;&lt;&gt;&gt; DiG 9.9.5-3-Ubuntu &lt;&lt;&gt;&gt; SRV default.cluster.local.\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 54040\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 2\n\n;; QUESTION SECTION:\n;default.cluster.local.     IN  SRV\n\n;; ANSWER SECTION:\ndefault.cluster.local.  30  IN  SRV 10 50 0 kubernetes.default.cluster.local.\ndefault.cluster.local.  30  IN  SRV 10 50 0 auth.default.cluster.local.\n\n;; ADDITIONAL SECTION:\nkubernetes.default.cluster.local. 30 IN A   10.111.240.1\nauth.default.cluster.local. 30 IN A 10.111.247.242\n\n;; Query time: 7 msec\n;; SERVER: 10.111.240.10#53(10.111.240.10)\n;; WHEN: Mon Aug 03 05:46:53 UTC 2015\n;; MSG SIZE  rcvd: 201\n</code></pre>\n<p>Why doesn't the port number get returned when I lookup the named port via an SRV record?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "bgrant0607" },
        "number": 12828,
        "resourcePath": "/kubernetes/kubernetes/issues/12828",
        "state": "CLOSED",
        "publishedAt": "2015-08-17T21:29:00Z",
        "closedAt": "2015-09-08T17:32:48Z",
        "title": "Support setting env vars in kubectl run",
        "bodyText": "Something like kubectl run --env=\"VAR=value\" image. Multiple --env flags should be accepted. Comma-separate values would get into an escape rathole, so I'd like to avoid that.",
        "bodyHTML": "<p>Something like <code>kubectl run --env=\"VAR=value\" image</code>. Multiple --env flags should be accepted. Comma-separate values would get into an escape rathole, so I'd like to avoid that.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "pwittrock" },
        "number": 26220,
        "resourcePath": "/kubernetes/kubernetes/issues/26220",
        "state": "CLOSED",
        "publishedAt": "2016-05-24T22:19:06Z",
        "closedAt": "2018-02-13T17:54:12Z",
        "title": "Node e2e reporting",
        "bodyText": "It would be useful to have daily or per-pr reports about the health of each distro and publish it to a dashboard somewhere",
        "bodyHTML": "<p>It would be useful to have daily or per-pr reports about the health of each distro and publish it to a dashboard somewhere</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "mingxing" },
        "number": 2728,
        "resourcePath": "/kubernetes/kubernetes/issues/2728",
        "state": "CLOSED",
        "publishedAt": "2014-12-03T03:09:42Z",
        "closedAt": "2014-12-15T22:14:14Z",
        "title": "code.google.com/p/go.tools/cmd/cover moved to godoc.org/golang.org/x/tools/cmd/cover",
        "bodyText": "When I tried to install Kubernetes, I met an issue: 'package code.google.com/p/go.tools/cmd/cover: Get https://code.google.com/p/go/source/checkout?repo=tools: dial tcp 173.194.127.104:443: connection timed out'\nThen I did some research and found the path of covert has been moved to https://godoc.org/golang.org/x/tools/cmd/cover\nHowever in https://github.com/GoogleCloudPlatform/kubernetes/blob/e5e4c8a7d35a0bb981155d84d766eec3e2cd6ffa/Godeps/_workspace/src/github.com/google/gofuzz/.travis.yml, it is still use code.google.com/p/go.tools/cmd/cover, should we update it?\nBest Regards\nSimon",
        "bodyHTML": "<p>When I tried to install Kubernetes, I met an issue: 'package code.google.com/p/go.tools/cmd/cover: Get <a rel=\"nofollow\" href=\"https://code.google.com/p/go/source/checkout?repo=tools\">https://code.google.com/p/go/source/checkout?repo=tools</a>: dial tcp 173.194.127.104:443: connection timed out'<br>\nThen I did some research and found the path of covert has been moved to <a rel=\"nofollow\" href=\"https://godoc.org/golang.org/x/tools/cmd/cover\">https://godoc.org/golang.org/x/tools/cmd/cover</a></p>\n<p>However in <a href=\"https://github.com/GoogleCloudPlatform/kubernetes/blob/e5e4c8a7d35a0bb981155d84d766eec3e2cd6ffa/Godeps/_workspace/src/github.com/google/gofuzz/.travis.yml\">https://github.com/GoogleCloudPlatform/kubernetes/blob/e5e4c8a7d35a0bb981155d84d766eec3e2cd6ffa/Godeps/_workspace/src/github.com/google/gofuzz/.travis.yml</a>, it is still use code.google.com/p/go.tools/cmd/cover, should we update it?</p>\n<p>Best Regards<br>\nSimon</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "thockin" },
        "number": 6132,
        "resourcePath": "/kubernetes/kubernetes/issues/6132",
        "state": "OPEN",
        "publishedAt": "2015-03-28T03:18:16Z",
        "closedAt": null,
        "title": "kubectl should return more information on failure",
        "bodyText": "After making a kubectl create call with bad JSON, kubectl would read-back the master's understanding of what I wrote and show me a diff.  Something like that",
        "bodyHTML": "<p>After making a <code>kubectl create</code> call with bad JSON, kubectl would read-back the master's understanding of what I wrote and show me a diff.  Something like that</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "farcaller" },
        "number": 8753,
        "resourcePath": "/kubernetes/kubernetes/issues/8753",
        "state": "CLOSED",
        "publishedAt": "2015-05-24T08:57:39Z",
        "closedAt": "2015-05-24T20:33:03Z",
        "title": "/minions not available for api v1beta3",
        "bodyText": "Just got a clean deployment of 0.17.1, and there seem to be an issue with kubectl:\nroot@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 get minions\nI0524 08:52:29.291509   19049 selector.go:53] Unable to list \"minions\": the server could not find the requested resource\n\nIndeed, requesting https://10.241.1.1:6443/api/v1beta3/minions returns:\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1beta3\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"the server could not find the requested resource\",\n  \"reason\": \"NotFound\",\n  \"details\": {},\n  \"code\": 404\n}\nSpecifying older api version works though:\nroot@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 --api-version='v1beta2' get minions\nNAME         LABELS    STATUS\n10.243.0.1   <none>    NotReady",
        "bodyHTML": "<p>Just got a clean deployment of 0.17.1, and there seem to be an issue with kubectl:</p>\n<pre><code>root@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 get minions\nI0524 08:52:29.291509   19049 selector.go:53] Unable to list \"minions\": the server could not find the requested resource\n</code></pre>\n<p>Indeed, requesting <code>https://10.241.1.1:6443/api/v1beta3/minions</code> returns:</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>kind<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Status<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>apiVersion<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>v1beta3<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>metadata<span class=\"pl-pds\">\"</span></span>: {},\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>status<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Failure<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>message<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>the server could not find the requested resource<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>reason<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NotFound<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>details<span class=\"pl-pds\">\"</span></span>: {},\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>code<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">404</span>\n}</pre></div>\n<p>Specifying older api version works though:</p>\n<pre><code>root@kube-master:~# /opt/kubernetes/server/bin/kubectl -s 'https://10.241.1.1:6443' --username=farcaller --password=123 --stderrthreshold=10 --v=10 --api-version='v1beta2' get minions\nNAME         LABELS    STATUS\n10.243.0.1   &lt;none&gt;    NotReady\n</code></pre>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "jchambers" },
        "number": 33202,
        "resourcePath": "/kubernetes/kubernetes/issues/33202",
        "state": "CLOSED",
        "publishedAt": "2016-09-21T20:06:52Z",
        "closedAt": "2016-11-17T17:21:39Z",
        "title": "kube-up.sh fails and misreports status using AWS (KUBE_MANIFESTS_TAR_URL: unbound variable)",
        "bodyText": "This is, essentially, a deliberate duplicate of #30495, which was closed by the requester before the underlying issue was resolved. In short, current versions of Kubernetes (I'm using 1.3.7) fail when using kube-up.sh to create a cluster under AWS, but incorrectly report success. This leaves some AWS resources (VPCs, security groups, etc.) in place, but with no actual nodes.\nThe relevant bit of output is as follows:\n./cluster/../cluster/../cluster/aws/../../cluster/common.sh: line 528: KUBE_MANIFESTS_TAR_URL: unbound variable\nKubernetes binaries at /Users/jon/kubernetes/cluster/\nYou may want to add this directory to your PATH in $HOME/.profile\nInstallation successful!\n\nA viable workaround has been described in #30495 (I believe this is why the original author closed the issue), but the issue itself has not actually been fixed. Please accept my apologies for the goofy paperwork shenanigans here, but I did want to make sure the issue didn't get lost.",
        "bodyHTML": "<p>This is, essentially, a deliberate duplicate of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"170806253\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/30495\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/30495/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/30495\">#30495</a>, which was closed by the requester before the underlying issue was resolved. In short, current versions of Kubernetes (I'm using 1.3.7) fail when using <code>kube-up.sh</code> to create a cluster under AWS, but incorrectly report success. This leaves some AWS resources (VPCs, security groups, etc.) in place, but with no actual nodes.</p>\n<p>The relevant bit of output is as follows:</p>\n<pre><code>./cluster/../cluster/../cluster/aws/../../cluster/common.sh: line 528: KUBE_MANIFESTS_TAR_URL: unbound variable\nKubernetes binaries at /Users/jon/kubernetes/cluster/\nYou may want to add this directory to your PATH in $HOME/.profile\nInstallation successful!\n</code></pre>\n<p>A viable workaround has been described in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"170806253\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/30495\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/30495/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/30495\">#30495</a> (I believe this is why the original author closed the issue), but the issue itself has not actually been fixed. Please accept my apologies for the goofy paperwork shenanigans here, but I did want to make sure the issue didn't get lost.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "aaskey" },
        "number": 31488,
        "resourcePath": "/kubernetes/kubernetes/issues/31488",
        "state": "CLOSED",
        "publishedAt": "2016-08-26T02:29:31Z",
        "closedAt": "2016-09-14T17:46:25Z",
        "title": "kubectl proxy failed to run as a service or sudo in GCE",
        "bodyText": "Kubernetes version (use kubectl version):\nClient Version: version.Info{Major:\"1\", Minor:\"3\", GitVersion:\"v1.3.4\", GitCommit:\"dd6b458ef8dbf24aff55795baa68f83383c9b3a9\", GitTreeState:\"clean\", BuildDate:\"2016-08-01T16:45:16Z\", GoVersion:\"go1.6.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"3\", GitVersion:\"v1.3.5\", GitCommit:\"b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5\", GitTreeState:\"clean\", BuildDate:\"2016-08-11T20:21:58Z\", GoVersion:\"go1.6.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\nEnvironment:\n\nCloud provider or hardware configuration: GCP VM & GCP container cluster\nOS (e.g. from /etc/os-release):\n\nNAME=\"CentOS Linux\"\nVERSION=\"7 (Core)\"\nID=\"centos\"\nID_LIKE=\"rhel fedora\"\nVERSION_ID=\"7\"\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\nANSI_COLOR=\"0;31\"\nCPE_NAME=\"cpe:/o:centos:centos:7\"\nHOME_URL=\"https://www.centos.org/\"\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\n\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\nREDHAT_SUPPORT_PRODUCT=\"centos\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\n\n\nKernel (e.g. uname -a):\n\nLinux cm-1 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nWhat happened:\nIn a GCE instance, failed to run kubectl proxy as a service or sudo, the same command ran successfully in command line as current user.\nOn Mac, I can run \"kubectl proxy --port=8080\" or \"sudo kubectl proxy --port=8080\" without problem.\nRun as a service, failed:\n$ sudo systemctl restart kubectlproxy\nJob for kubectlproxy.service failed because the control process exited with error code. See \"systemctl status kubectlproxy.service\" and \"journalctl -xe\" for details.\n\n$ sudo systemctl -l status kubectlproxy.service\n\u25cf kubectlproxy.service - kubectl proxy Service\n   Loaded: loaded (/opt/cm/kubectlproxy.service; linked; vendor preset: disabled)\n   Active: failed (Result: exit-code) since Fri 2016-08-26 02:09:04 UTC; 5s ago\n  Process: 1982 ExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080 (code=exited, status=1/FAILURE)\n Main PID: 1982 (code=exited, status=1/FAILURE)\n\nAug 26 02:09:04 cm-1 systemd[1]: Starting kubectl proxy Service...\nAug 26 02:09:04 cm-1 kubectl[1982]: The connection to the server localhost:8080 was refused - did you specify the right host or port?\nAug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service: main process exited, code=exited, status=1/FAILURE\nAug 26 02:09:04 cm-1 systemd[1]: Failed to start kubectl proxy Service.\nAug 26 02:09:04 cm-1 systemd[1]: Unit kubectlproxy.service entered failed state.\nAug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service failed.\n\nRun as sudo, failed:\n$ sudo /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\nRun as current user, succeeded:\n$ /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080\nStarting to serve on 127.0.0.1:8080\n\nBelow is the systemd service file:\n[Unit]\nDescription=kubectl proxy Service\nAfter=network.target\n\n[Service]\nType=notify\nUser=root\nExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --p\nort=8080\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target",
        "bodyHTML": "<p><strong>Kubernetes version</strong> (use <code>kubectl version</code>):</p>\n<pre><code>Client Version: version.Info{Major:\"1\", Minor:\"3\", GitVersion:\"v1.3.4\", GitCommit:\"dd6b458ef8dbf24aff55795baa68f83383c9b3a9\", GitTreeState:\"clean\", BuildDate:\"2016-08-01T16:45:16Z\", GoVersion:\"go1.6.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"3\", GitVersion:\"v1.3.5\", GitCommit:\"b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5\", GitTreeState:\"clean\", BuildDate:\"2016-08-11T20:21:58Z\", GoVersion:\"go1.6.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre>\n<p><strong>Environment</strong>:</p>\n<ul>\n<li><strong>Cloud provider or hardware configuration</strong>: GCP VM &amp; GCP container cluster</li>\n<li><strong>OS</strong> (e.g. from /etc/os-release):</li>\n</ul>\n<pre><code>NAME=\"CentOS Linux\"\nVERSION=\"7 (Core)\"\nID=\"centos\"\nID_LIKE=\"rhel fedora\"\nVERSION_ID=\"7\"\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\nANSI_COLOR=\"0;31\"\nCPE_NAME=\"cpe:/o:centos:centos:7\"\nHOME_URL=\"https://www.centos.org/\"\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\n\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\nREDHAT_SUPPORT_PRODUCT=\"centos\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\n</code></pre>\n<ul>\n<li><strong>Kernel</strong> (e.g. <code>uname -a</code>):</li>\n</ul>\n<pre><code>Linux cm-1 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre>\n<p><strong>What happened</strong>:<br>\nIn a GCE instance, failed to run kubectl proxy as a service or sudo, the same command ran successfully in command line as current user.</p>\n<p>On Mac, I can run \"kubectl proxy --port=8080\" or \"sudo kubectl proxy --port=8080\" without problem.</p>\n<p>Run as a service, failed:</p>\n<pre><code>$ sudo systemctl restart kubectlproxy\nJob for kubectlproxy.service failed because the control process exited with error code. See \"systemctl status kubectlproxy.service\" and \"journalctl -xe\" for details.\n</code></pre>\n<pre><code>$ sudo systemctl -l status kubectlproxy.service\n\u25cf kubectlproxy.service - kubectl proxy Service\n   Loaded: loaded (/opt/cm/kubectlproxy.service; linked; vendor preset: disabled)\n   Active: failed (Result: exit-code) since Fri 2016-08-26 02:09:04 UTC; 5s ago\n  Process: 1982 ExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080 (code=exited, status=1/FAILURE)\n Main PID: 1982 (code=exited, status=1/FAILURE)\n\nAug 26 02:09:04 cm-1 systemd[1]: Starting kubectl proxy Service...\nAug 26 02:09:04 cm-1 kubectl[1982]: The connection to the server localhost:8080 was refused - did you specify the right host or port?\nAug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service: main process exited, code=exited, status=1/FAILURE\nAug 26 02:09:04 cm-1 systemd[1]: Failed to start kubectl proxy Service.\nAug 26 02:09:04 cm-1 systemd[1]: Unit kubectlproxy.service entered failed state.\nAug 26 02:09:04 cm-1 systemd[1]: kubectlproxy.service failed.\n</code></pre>\n<p>Run as sudo, failed:</p>\n<pre><code>$ sudo /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n</code></pre>\n<p>Run as current user, succeeded:</p>\n<pre><code>$ /usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --port=8080\nStarting to serve on 127.0.0.1:8080\n</code></pre>\n<p>Below is the systemd service file:</p>\n<pre><code>[Unit]\nDescription=kubectl proxy Service\nAfter=network.target\n\n[Service]\nType=notify\nUser=root\nExecStart=/usr/local/share/google/google-cloud-sdk/bin/kubectl proxy --address=127.0.0.1 --p\nort=8080\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "fgrzadkowski" },
        "number": 4235,
        "resourcePath": "/kubernetes/kubernetes/issues/4235",
        "state": "CLOSED",
        "publishedAt": "2015-02-07T05:41:36Z",
        "closedAt": "2016-03-01T19:42:50Z",
        "title": "Kubernetes should support cross-zone cluster",
        "bodyText": "Most applications should be running in multiple zones to increase availability. Kubernetes should support it. I imagine this to work in the following way:\n\nUser sets up cluster in a region in a way that minions are spread evenly across all available zones\nUser creates replication controller for the application with size > 1\nScheduler spreads pods within the same replication controller across available zones\n\nThat way user gets regional availability for free.\nAFAIU this will mostly require changes in how we create cluster and how we schedule pods.\ncc @wojtek-t",
        "bodyHTML": "<p>Most applications should be running in multiple zones to increase availability. Kubernetes should support it. I imagine this to work in the following way:</p>\n<ol>\n<li>User sets up cluster in a region in a way that minions are spread evenly across all available zones</li>\n<li>User creates replication controller for the application with size &gt; 1</li>\n<li>Scheduler spreads pods within the same replication controller across available zones</li>\n</ol>\n<p>That way user gets regional availability for free.</p>\n<p>AFAIU this will mostly require changes in how we create cluster and how we schedule pods.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/wojtek-t/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wojtek-t\">@wojtek-t</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "vishh" },
        "number": 23397,
        "resourcePath": "/kubernetes/kubernetes/issues/23397",
        "state": "CLOSED",
        "publishedAt": "2016-03-23T21:15:56Z",
        "closedAt": "2016-08-31T16:52:49Z",
        "title": "Validate Docker v1.11",
        "bodyText": "Docker v1.11 rcs are starting to show up. Its time to get started with the validation.\nhttps://github.com/docker/docker/releases/tag/v1.11.0-rc1\ncc @kubernetes/sig-node\nEDIT(timstclair):\nTASKS:\n\n e2e tests pass\n performance analysis\n startup tests (bootstrap, restart)\n live upgrade successful\n 1 week soak tests",
        "bodyHTML": "<p>Docker v1.11 rcs are starting to show up. Its time to get started with the validation.</p>\n<p><a href=\"https://github.com/docker/docker/releases/tag/v1.11.0-rc1\">https://github.com/docker/docker/releases/tag/v1.11.0-rc1</a></p>\n<p>cc @kubernetes/sig-node</p>\n<p>EDIT(timstclair):</p>\n<p>TASKS:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> e2e tests pass</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> performance analysis</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> startup tests (bootstrap, restart)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> live upgrade successful</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> 1 week soak tests</li>\n</ul>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "bgrant0607" },
        "number": 24343,
        "resourcePath": "/kubernetes/kubernetes/issues/24343",
        "state": "CLOSED",
        "publishedAt": "2016-04-15T18:23:06Z",
        "closedAt": "2019-05-07T15:53:05Z",
        "title": "Figure out how to handle code in multiple repos",
        "bodyText": "A large monorepo works for Google, but not on github.\nWe hit the ceiling of achievable velocity of a single github repo in early 2015:\nhttps://github.com/kubernetes/kubernetes/graphs/contributors\nThere are many reasons: ACLs, notification management, issue triage, PR reviews, sequentialized submit testing, merge conflicts, etc.\nWe're chipping away at these issues, but we need more than incremental improvement.\nWe've discussed moving a number of things to other repos:\n\nKubelet: #444\nGeneric API infrastructure: #2742\nClient libraries: #5660\nMisc. utilities: #24156\nkubectl\nscheduler\nexamples\ncloud providers + cluster code + \"getting-started guides\"\nauth plugins\nnetwork and storage plugins?\ne2e tests\ncontributor documentation\n\nWe need to seriously think about how to do this.\nKnown issues that need to be addressed:\n\nWe need to get sprawl and package dependencies under control: #4851\nWe need to make most components ordinary clients of the API: #20193\nWe need to figure out dependency management and integration testing\n\nAn example of a Go project on github with good repo hygiene:\nhttps://github.com/deis\nI have no illusions that breaking the project into separate repos will be a silver bullet: it's necessary, but not sufficient. I also know that it will cause some pain. But that pain already exists: cadvisor, heapster, dashboard, contrib, docs, ....\nSpeaking of contrib, it needs to be broken up, too: kubernetes-retired/contrib#762\n@thockin @smarterclayton @lavalamp @mikedanese @dchen1107 @davidopp @ixdy",
        "bodyHTML": "<p>A large monorepo works for Google, but not on github.</p>\n<p>We hit the ceiling of achievable velocity of a single github repo in early 2015:<br>\n<a href=\"https://github.com/kubernetes/kubernetes/graphs/contributors\">https://github.com/kubernetes/kubernetes/graphs/contributors</a></p>\n<p>There are many reasons: ACLs, notification management, issue triage, PR reviews, sequentialized submit testing, merge conflicts, etc.</p>\n<p>We're chipping away at these issues, but we need more than incremental improvement.</p>\n<p>We've discussed moving a number of things to other repos:</p>\n<ul>\n<li>Kubelet: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"37758458\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/444\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/444/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/444\">#444</a></li>\n<li>Generic API infrastructure: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"50872738\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/2742\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/2742/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/2742\">#2742</a></li>\n<li>Client libraries: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"63057747\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/5660\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/5660/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/5660\">#5660</a></li>\n<li>Misc. utilities: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"147865817\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/24156\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/24156/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/24156\">#24156</a></li>\n<li>kubectl</li>\n<li>scheduler</li>\n<li>examples</li>\n<li>cloud providers + cluster code + \"getting-started guides\"</li>\n<li>auth plugins</li>\n<li>network and storage plugins?</li>\n<li>e2e tests</li>\n<li>contributor documentation</li>\n</ul>\n<p>We need to seriously think about how to do this.</p>\n<p>Known issues that need to be addressed:</p>\n<ul>\n<li>We need to get sprawl and package dependencies under control: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"59113560\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/4851\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/4851/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/4851\">#4851</a></li>\n<li>We need to make most components ordinary clients of the API: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"129006279\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/20193\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/20193/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/20193\">#20193</a></li>\n<li>We need to figure out dependency management and integration testing</li>\n</ul>\n<p>An example of a Go project on github with good repo hygiene:<br>\n<a href=\"https://github.com/deis\">https://github.com/deis</a></p>\n<p>I have no illusions that breaking the project into separate repos will be a silver bullet: it's necessary, but not sufficient. I also know that it will cause some pain. But that pain already exists: cadvisor, heapster, dashboard, contrib, docs, ....</p>\n<p>Speaking of contrib, it needs to be broken up, too: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"147813570\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes-retired/contrib/issues/762\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes-retired/contrib/issues/762/hovercard\" href=\"https://github.com/kubernetes-retired/contrib/issues/762\">kubernetes-retired/contrib#762</a></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/thockin/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/thockin\">@thockin</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/smarterclayton/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/smarterclayton\">@smarterclayton</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/lavalamp/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lavalamp\">@lavalamp</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/mikedanese/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mikedanese\">@mikedanese</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/dchen1107/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dchen1107\">@dchen1107</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/davidopp/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/davidopp\">@davidopp</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/ixdy/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ixdy\">@ixdy</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "saad-ali" },
        "number": 20885,
        "resourcePath": "/kubernetes/kubernetes/issues/20885",
        "state": "CLOSED",
        "publishedAt": "2016-02-09T04:34:17Z",
        "closedAt": "2017-06-16T00:28:51Z",
        "title": "Modify E2E tests to use the GCE API instead of gcloud exec",
        "bodyText": "PR #17747 laid the ground work to enable E2E tests to use the GCE API instead of gcloud exec. That PR only modified the PD tests to do so.\nIn order to make E2E more robust, we should switch over all other instances of gcloud exec in E2E tests to use the GCE API instead.",
        "bodyHTML": "<p>PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"118731714\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/17747\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/17747/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/17747\">#17747</a> laid the ground work to enable E2E tests to use the GCE API instead of gcloud exec. That PR only modified the PD tests to do so.</p>\n<p>In order to make E2E more robust, we should switch over all other instances of gcloud exec in E2E tests to use the GCE API instead.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "RichieEscarez" },
        "number": 10130,
        "resourcePath": "/kubernetes/kubernetes/issues/10130",
        "state": "CLOSED",
        "publishedAt": "2015-06-19T22:21:39Z",
        "closedAt": "2015-07-10T19:42:26Z",
        "title": "Add the \"when\" and \"why\" to use the Downward API",
        "bodyText": "We need to add more content around the Downward API to better clarify when and why to use it. We should also try to revise the \"how\" info we provide today to improve/clarify what the specific steps.\n(for example: Prerequisites are xyx. To use the downward API: 1. do this. 2....3...etc..).\nSome details we should make clear:\n\nWhen do i want containers to consume info about the system without coupling to k8s client or REST API?\nDo we say \"use the downward API when your containers need access to information about the cluster in which it resides\"?\nWe should also tie together/link/mention the other content we have on using environment variables and the downward api example.",
        "bodyHTML": "<p>We need to add more content around the <a href=\"https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/downward_api.md\">Downward API</a> to better clarify when and why to use it. We should also try to revise the \"how\" info we provide today to improve/clarify what the specific steps.<br>\n(for example: Prerequisites are xyx. To use the downward API: 1. do this. 2....3...etc..).</p>\n<p>Some details we should make clear:</p>\n<ul>\n<li>When do i want containers to consume info about the system without coupling to k8s client or REST API?</li>\n<li>Do we say \"use the downward API when your containers need access to information about the cluster in which it resides\"?</li>\n<li>We should also tie together/link/mention the other content we have on using <a href=\"https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/environment-guide\">environment variables</a> and the <a href=\"https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/downward-api\">downward api example</a>.</li>\n</ul>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "euank" },
        "number": 26816,
        "resourcePath": "/kubernetes/kubernetes/issues/26816",
        "state": "CLOSED",
        "publishedAt": "2016-06-03T21:36:56Z",
        "closedAt": "2017-12-27T17:55:47Z",
        "title": "rkt: hostPath mounts to non-existent directories fail",
        "bodyText": "A pod referencing a nonexistent host path, such as the one below, does not run under the rkt container runtime.. However, it runs just fine under docker (which, by default, creates nonexistent paths as empty directories when a mount references them).\nThe pod also does not show up in kubectl get pods in any state, though it can still be deleted.\nThe latter is definitely something that should be fixed; the former is an intentional behavioural difference between rkt and docker and might be up to debate.\n\nShould the pod fail? Do we need consistent behavior with docker here? Do we think that blindly creating directories on the host masks typos and is surprising?\n1b) Should rkt change to this behavior, or should we create the directory before referencing it in a bindmount? (e.g. an ExecStartPre=mkdir -p <host directories>)\nPods that fail like this should still be visible in get pods; I think this is jut that rkt failed during stage1 and there's some over-aggressive error handling in our code that masks this pod, but I haven't dived very deeply there yet.\n\nExample failing pod\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: mount-dn\n  name: mount-dne\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: dne\n    hostPath:\n      path: /does/not/exist\n  containers:\n    - name: exit\n      image: busybox\n      command: [\"sh\", \"-c\", \"ls /test; sleep 60\"]\n      volumeMounts:\n      - mountPath: /test\n        name: dne\ncc @yifan-gu @tmrts",
        "bodyHTML": "<p>A pod referencing a nonexistent host path, such as the one below, does not run under the rkt container runtime.. However, it runs just fine under docker (which, by default, creates nonexistent paths as empty directories when a mount references them).</p>\n<p>The pod also does not show up in <code>kubectl get pods</code> in any state, though it can still be deleted.</p>\n<p>The latter is definitely something that should be fixed; the former is an intentional behavioural difference between rkt and docker and might be up to debate.</p>\n<ol>\n<li>Should the pod fail? Do we need consistent behavior with docker here? Do we think that blindly creating directories on the host masks typos and is surprising?<br>\n1b) Should rkt change to this behavior, or should we create the directory before referencing it in a bindmount? (e.g. an <code>ExecStartPre=mkdir -p &lt;host directories&gt;</code>)</li>\n<li>Pods that fail like this should still be visible in <code>get pods</code>; I think this is jut that rkt failed during stage1 and there's some over-aggressive error handling in our code that masks this pod, but I haven't dived very deeply there yet.</li>\n</ol>\n<p>Example failing pod</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">Pod</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">labels</span>:\n    <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">mount-dn</span>\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">mount-dne</span>\n<span class=\"pl-ent\">spec</span>:\n  <span class=\"pl-ent\">restartPolicy</span>: <span class=\"pl-s\">Never</span>\n  <span class=\"pl-ent\">volumes</span>:\n  - <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">dne</span>\n    <span class=\"pl-ent\">hostPath</span>:\n      <span class=\"pl-ent\">path</span>: <span class=\"pl-s\">/does/not/exist</span>\n  <span class=\"pl-ent\">containers</span>:\n    - <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">exit</span>\n      <span class=\"pl-ent\">image</span>: <span class=\"pl-s\">busybox</span>\n      <span class=\"pl-ent\">command</span>: <span class=\"pl-s\">[\"sh\", \"-c\", \"ls /test; sleep 60\"]</span>\n      <span class=\"pl-ent\">volumeMounts</span>:\n      - <span class=\"pl-ent\">mountPath</span>: <span class=\"pl-s\">/test</span>\n        <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">dne</span></pre></div>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/yifan-gu/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yifan-gu\">@yifan-gu</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/tmrts/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tmrts\">@tmrts</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "pstauffer" },
        "number": 22485,
        "resourcePath": "/kubernetes/kubernetes/issues/22485",
        "state": "CLOSED",
        "publishedAt": "2016-03-04T00:34:58Z",
        "closedAt": "2017-06-02T01:51:33Z",
        "title": "fixing detecting docker version",
        "bodyText": "In the getting-started-guides matches the detection of the docker version not with a version 1.1x.\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/master.sh#L108\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/worker.sh#L105",
        "bodyHTML": "<p>In the getting-started-guides matches the detection of the docker version not with a version 1.1x.</p>\n<p><a href=\"https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/master.sh#L108\">https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/master.sh#L108</a></p>\n<p><a href=\"https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/worker.sh#L105\">https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker-multinode/worker.sh#L105</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "bgrant0607" },
        "number": 20154,
        "resourcePath": "/kubernetes/kubernetes/issues/20154",
        "state": "CLOSED",
        "publishedAt": "2016-01-26T15:27:48Z",
        "closedAt": "2016-01-26T18:32:25Z",
        "title": "test-cmd flake: first kubectl command (get nodes) fails",
        "bodyText": "Encountered on #18901\n!!! Error in ./hack/test-cmd.sh:203\n  '[ \"$(kubectl get nodes -o go-template='{{ .apiVersion }}' \"${kube_flags[@]}\")\" == \"v1\" ]' exited with status 1\nCall stack:\n  1: ./hack/test-cmd.sh:203 runTests(...)\n  2: ./hack/test-cmd.sh:1352 main(...)\nExiting with status 1\n\ncc @kargakis",
        "bodyHTML": "<p>Encountered on <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"123009514\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/18901\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/18901/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/18901\">#18901</a></p>\n<pre><code>!!! Error in ./hack/test-cmd.sh:203\n  '[ \"$(kubectl get nodes -o go-template='{{ .apiVersion }}' \"${kube_flags[@]}\")\" == \"v1\" ]' exited with status 1\nCall stack:\n  1: ./hack/test-cmd.sh:203 runTests(...)\n  2: ./hack/test-cmd.sh:1352 main(...)\nExiting with status 1\n</code></pre>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/kargakis/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kargakis\">@kargakis</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "brendandburns" },
        "number": 11965,
        "resourcePath": "/kubernetes/kubernetes/issues/11965",
        "state": "CLOSED",
        "publishedAt": "2015-07-29T03:41:53Z",
        "closedAt": "2017-06-01T17:57:35Z",
        "title": "Use bandwidth shaping on the master",
        "bodyText": "Limit outbound and inbound bandwidth, using the tc tool, see:\nhttps://www.iplocation.net/traffic-control",
        "bodyHTML": "<p>Limit outbound and inbound bandwidth, using the <code>tc</code> tool, see:</p>\n<p><a rel=\"nofollow\" href=\"https://www.iplocation.net/traffic-control\">https://www.iplocation.net/traffic-control</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "thockin" },
        "number": 1988,
        "resourcePath": "/kubernetes/kubernetes/issues/1988",
        "state": "CLOSED",
        "publishedAt": "2014-10-24T17:11:26Z",
        "closedAt": "2015-07-09T05:31:56Z",
        "title": "Add an optional \"why\" clause to ValidationError",
        "bodyText": "I find myself wanting to explain WHY a validation error failed, and there's just no way to pass that down to users.",
        "bodyHTML": "<p>I find myself wanting to explain WHY a validation error failed, and there's just no way to pass that down to users.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "goltermann" },
        "number": 22524,
        "resourcePath": "/kubernetes/kubernetes/issues/22524",
        "state": "CLOSED",
        "publishedAt": "2016-03-04T16:30:42Z",
        "closedAt": "2016-04-20T22:14:46Z",
        "title": "Add go_vet to presubmit",
        "bodyText": "We'd like to stay vet clean, and aren't too far away.",
        "bodyHTML": "<p>We'd like to stay vet clean, and aren't too far away.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "dashpole" },
        "number": 33382,
        "resourcePath": "/kubernetes/kubernetes/issues/33382",
        "state": "CLOSED",
        "publishedAt": "2016-09-23T19:00:02Z",
        "closedAt": "2016-11-01T18:09:21Z",
        "title": "Per-Container Inode Accounting",
        "bodyText": "cc: @derekwaynecarr (how can we get devicemapper support for this?)\nwhen a node has reached its eviction-hard threshold for imagefs.inodes or nodefs.inodes, the eviction manager does not behave optimally.\nThis is because we do not keep track of inodes used per-container, but rather just overall.  In some cases, the pod which is not using many inodes is evicted before a pod that is using many.\nProgress:\n\n Create an end to end test that demonstrates that a pod using a normal amount of disk capacity is evicted before a pod that uses all remaining inodes (but little disk capacity).\n Modify cadvisor api and kubernetes api to include per-container inode usage\n Modify cadvisor to publish per-container inode usage\n Modify kubelet eviction manager to take per-container inode usage into account when evicting",
        "bodyHTML": "<p>cc: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/derekwaynecarr/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/derekwaynecarr\">@derekwaynecarr</a> (how can we get devicemapper support for this?)</p>\n<p>when a node has reached its eviction-hard threshold for imagefs.inodes or nodefs.inodes, the eviction manager does not behave optimally.<br>\nThis is because we do not keep track of inodes used per-container, but rather just overall.  In some cases, the pod which is not using many inodes is evicted before a pod that is using many.</p>\n<p>Progress:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Create an end to end test that demonstrates that a pod using a normal amount of disk capacity is evicted before a pod that uses all remaining inodes (but little disk capacity).</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Modify cadvisor api and kubernetes api to include per-container inode usage</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Modify cadvisor to publish per-container inode usage</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Modify kubelet eviction manager to take per-container inode usage into account when evicting</li>\n</ul>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "roberthbailey" },
        "number": 20820,
        "resourcePath": "/kubernetes/kubernetes/issues/20820",
        "state": "CLOSED",
        "publishedAt": "2016-02-08T16:27:53Z",
        "closedAt": "2018-03-12T05:16:33Z",
        "title": "[docker 1.10] create syscall filters for k8s-supplied components",
        "bodyText": "With docker 1.10, you can create a filter for syscalls that the container is allowed to execute, mainly to reduce the kernel attack surface and make it harder to use a privilege escalation vulnerability in the kernel code.\nFor containers that we provide (master components, add-ons) where we know the expected syscall surface we should explore locking them down by limiting the system calls that they can make.\n/cc @stephenR",
        "bodyHTML": "<p>With docker 1.10, you can create a filter for syscalls that the container is allowed to execute, mainly to reduce the kernel attack surface and make it harder to use a privilege escalation vulnerability in the kernel code.</p>\n<p>For containers that we provide (master components, add-ons) where we know the expected syscall surface we should explore locking them down by limiting the system calls that they can make.</p>\n<p>/cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/stephenR/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/stephenR\">@stephenR</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "skwokmag" },
        "number": 7493,
        "resourcePath": "/kubernetes/kubernetes/issues/7493",
        "state": "CLOSED",
        "publishedAt": "2015-04-29T04:26:11Z",
        "closedAt": "2015-05-02T15:26:10Z",
        "title": "What kind of aws roles do I need to prepare for kubernetes",
        "bodyText": "Hi,\niam has a few roles. Is it \"Grant API access to SAML providers\"?\nThank you.\nskwok",
        "bodyHTML": "<p>Hi,</p>\n<p>iam has a few roles. Is it \"Grant API access to SAML providers\"?<br>\nThank you.</p>\n<p>skwok</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "alex-mohr" },
        "number": 3894,
        "resourcePath": "/kubernetes/kubernetes/issues/3894",
        "state": "CLOSED",
        "publishedAt": "2015-01-28T22:09:52Z",
        "closedAt": "2015-03-04T20:20:18Z",
        "title": "kubectl: rationalize which messages to stderr/stdout",
        "bodyText": "A user reports to google-containers@googlegroups.com (https://groups.google.com/forum/#!topic/google-containers/qHDR-mvh7sM) the following.  I verified similar behavior in kubectl v0.9.1.\nI am using Kubernetes v0.7.0, when I run \"kubectl\" to create pod, I found:\n$ kubectl create -s http://192.168.122.136:8080 -f ./mariadb-pod.yaml\nI0128 22:09:44.258267   30016 restclient.go:133] Waiting for completion of operation 19    --> this is to stderr\nmariadb    --> this is to stdout\nI do not know why the message \"...Waiting for completion ...\" is considered as an error, I think it should be to stdout too.",
        "bodyHTML": "<p>A user reports to <a href=\"mailto:google-containers@googlegroups.com\">google-containers@googlegroups.com</a> (<a rel=\"nofollow\" href=\"https://groups.google.com/forum/#!topic/google-containers/qHDR-mvh7sM\">https://groups.google.com/forum/#!topic/google-containers/qHDR-mvh7sM</a>) the following.  I verified similar behavior in kubectl v0.9.1.</p>\n<p>I am using Kubernetes v0.7.0, when I run \"kubectl\" to create pod, I found:<br>\n$ kubectl create -s <a rel=\"nofollow\" href=\"http://192.168.122.136:8080\">http://192.168.122.136:8080</a> -f ./mariadb-pod.yaml<br>\nI0128 22:09:44.258267   30016 restclient.go:133] Waiting for completion of operation 19    --&gt; this is to stderr<br>\nmariadb    --&gt; this is to stdout</p>\n<p>I do not know why the message \"...Waiting for completion ...\" is considered as an error, I think it should be to stdout too.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "mwielgus" },
        "number": 22938,
        "resourcePath": "/kubernetes/kubernetes/issues/22938",
        "state": "CLOSED",
        "publishedAt": "2016-03-14T11:54:20Z",
        "closedAt": "2016-03-14T18:08:10Z",
        "title": "Version is missing in Heapster configuration",
        "bodyText": "cc: @piosz",
        "bodyHTML": "<p>cc: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/piosz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/piosz\">@piosz</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "derekwaynecarr" },
        "number": 7906,
        "resourcePath": "/kubernetes/kubernetes/issues/7906",
        "state": "CLOSED",
        "publishedAt": "2015-05-07T19:23:26Z",
        "closedAt": "2015-05-08T14:37:18Z",
        "title": "Kubelet is reporting Node cpu capacity as negative value",
        "bodyText": "I have some reports of a Node reporting a a resource capacity for cpu as a negative value.\nosc describe node `hostname -f`\nName:                   <omitted>\nLabels:                 <none>\nCreationTimestamp:      Thu, 07 May 2015 09:50:48 -0400\nConditions:\n  Type          Status  LastHeartbeatTime\nLastTransitionTime                      Reason\n        Message\n  Ready         True    Thu, 07 May 2015 09:52:58 -0400         Thu, 07\nMay 2015 09:51:54 -0400         kubelet is posting ready status\nAddresses:      <omitted>\nCapacity:\n cpu:           -1\n memory:        1884432Ki\nVersion:\n Kernel Version:                3.10.0-229.el7.x86_64\n OS Image:                      Red Hat Enterprise Linux Server 7.1 (Maipo)\n Container Runtime Version:     docker://1.6.0\n Kubelet Version:               v0.14.1-582-gb12d75d\n Kube-Proxy Version:            v0.14.1-582-gb12d75d\nExternalID:                     <omitted>\nPods:                           (0 in total)\nNo events.\n\nAny idea why cpu could report as a negative value?  Is this an expected normal outcome?  In its current state, it prevents any pods from being scheduled to that Node.  Tips on how to debug further are appreciated.\n@dchen1107 @vmarmol - any ideas?",
        "bodyHTML": "<p>I have some reports of a Node reporting a a resource capacity for cpu as a negative value.</p>\n<pre><code>osc describe node `hostname -f`\nName:                   &lt;omitted&gt;\nLabels:                 &lt;none&gt;\nCreationTimestamp:      Thu, 07 May 2015 09:50:48 -0400\nConditions:\n  Type          Status  LastHeartbeatTime\nLastTransitionTime                      Reason\n        Message\n  Ready         True    Thu, 07 May 2015 09:52:58 -0400         Thu, 07\nMay 2015 09:51:54 -0400         kubelet is posting ready status\nAddresses:      &lt;omitted&gt;\nCapacity:\n cpu:           -1\n memory:        1884432Ki\nVersion:\n Kernel Version:                3.10.0-229.el7.x86_64\n OS Image:                      Red Hat Enterprise Linux Server 7.1 (Maipo)\n Container Runtime Version:     docker://1.6.0\n Kubelet Version:               v0.14.1-582-gb12d75d\n Kube-Proxy Version:            v0.14.1-582-gb12d75d\nExternalID:                     &lt;omitted&gt;\nPods:                           (0 in total)\nNo events.\n</code></pre>\n<p>Any idea why cpu could report as a negative value?  Is this an expected normal outcome?  In its current state, it prevents any pods from being scheduled to that Node.  Tips on how to debug further are appreciated.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/dchen1107/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dchen1107\">@dchen1107</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/vmarmol/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vmarmol\">@vmarmol</a> - any ideas?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "luxas" },
        "number": 20070,
        "resourcePath": "/kubernetes/kubernetes/issues/20070",
        "state": "CLOSED",
        "publishedAt": "2016-01-24T20:35:45Z",
        "closedAt": "2016-01-25T06:04:59Z",
        "title": "apiserver proxy: no endpoints available error if Service.spec.ports[*].name is specified",
        "bodyText": "Hello everyone!\nI noticed this strange behaviour today when I was trying to connect to my service via the apiserver proxy:\nspec:\n  ports:\n  - port: 80\n    name: foo\n    targetPort: 3000\n$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"no endpoints available for service \\\"gogs\\\"\",\n  \"reason\": \"ServiceUnavailable\",\n  \"code\": 503\n}\nBut when I deleted the Service.spec.ports[*].name field, it worked as it should:\nspec:\n  ports:\n  - port: 80\n    targetPort: 3000\n$ curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar\nOK\nIs this a known bug? Or is it made this way \"by design\"?\nThis also makes services with two or more ports inaccessible via apiserver proxy\n@ArtfulCoder @thockin",
        "bodyHTML": "<p>Hello everyone!<br>\nI noticed this strange behaviour today when I was trying to connect to my service via the <code>apiserver proxy</code>:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">spec</span>:\n  <span class=\"pl-ent\">ports</span>:\n  - <span class=\"pl-ent\">port</span>: <span class=\"pl-c1\">80</span>\n    <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">foo</span>\n    <span class=\"pl-ent\">targetPort</span>: <span class=\"pl-c1\">3000</span></pre></div>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar</span>\n<span class=\"pl-c1\">{</span>\n<span class=\"pl-c1\">  \"kind\": \"Status\",</span>\n<span class=\"pl-c1\">  \"apiVersion\": \"v1\",</span>\n<span class=\"pl-c1\">  \"metadata\": {},</span>\n<span class=\"pl-c1\">  \"status\": \"Failure\",</span>\n<span class=\"pl-c1\">  \"message\": \"no endpoints available for service \\\"gogs\\\"\",</span>\n<span class=\"pl-c1\">  \"reason\": \"ServiceUnavailable\",</span>\n<span class=\"pl-c1\">  \"code\": 503</span>\n<span class=\"pl-c1\">}</span></pre></div>\n<p>But when I deleted the <code>Service.spec.ports[*].name</code> field, it worked as it should:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">spec</span>:\n  <span class=\"pl-ent\">ports</span>:\n  - <span class=\"pl-ent\">port</span>: <span class=\"pl-c1\">80</span>\n    <span class=\"pl-ent\">targetPort</span>: <span class=\"pl-c1\">3000</span></pre></div>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">curl -sSL localhost:8080/api/v1/proxy/namespaces/default/services/foobar</span>\n<span class=\"pl-c1\">OK</span></pre></div>\n<p>Is this a known bug? Or is it made this way \"by design\"?<br>\nThis also makes services with two or more ports inaccessible via apiserver proxy<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/ArtfulCoder/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ArtfulCoder\">@ArtfulCoder</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/thockin/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/thockin\">@thockin</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "jszczepkowski" },
        "number": 15474,
        "resourcePath": "/kubernetes/kubernetes/issues/15474",
        "state": "CLOSED",
        "publishedAt": "2015-10-12T11:45:05Z",
        "closedAt": "2015-11-17T11:10:02Z",
        "title": "Implement e2e tests for horizontal pod autoscaling of deployments",
        "bodyText": "We need to write e2e tests which combines horizontal pod autoscaling with deployments (scale sub-resourcer should point to a deployment object).\nThe tests should be similar to https://github.com/kubernetes/kubernetes/blob/master/test/e2e/horizontal_pod_autoscaling.go. The library from https://github.com/kubernetes/kubernetes/blob/master/test/e2e/autoscaling_utils.go should be extended and allow creation of a resource consumer as a deployment (currently it is always replication controller). The new tests should create a deployment resource consumer and pass a scale-sub-resource to the deployment to autoscaler.",
        "bodyHTML": "<p>We need to write e2e tests which combines horizontal pod autoscaling with deployments (scale sub-resourcer should point to a deployment object).</p>\n<p>The tests should be similar to <a href=\"https://github.com/kubernetes/kubernetes/blob/master/test/e2e/horizontal_pod_autoscaling.go\">https://github.com/kubernetes/kubernetes/blob/master/test/e2e/horizontal_pod_autoscaling.go</a>. The library from <a href=\"https://github.com/kubernetes/kubernetes/blob/master/test/e2e/autoscaling_utils.go\">https://github.com/kubernetes/kubernetes/blob/master/test/e2e/autoscaling_utils.go</a> should be extended and allow creation of a resource consumer as a deployment (currently it is always replication controller). The new tests should create a deployment resource consumer and pass a scale-sub-resource to the deployment to autoscaler.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "xiaochunyn" },
        "number": 26513,
        "resourcePath": "/kubernetes/kubernetes/issues/26513",
        "state": "CLOSED",
        "publishedAt": "2016-05-30T08:47:09Z",
        "closedAt": "2017-05-31T18:23:30Z",
        "title": "How Webhook works, where can find the details? thanks",
        "bodyText": "1 Is webhook plugin stable?\n2 How webhook works?\n3 How can I use webhook and where can find detailed information.\nThank you very much!",
        "bodyHTML": "<p>1 Is webhook plugin stable?<br>\n2 How webhook works?<br>\n3 How can I use webhook and where can find detailed information.</p>\n<p>Thank you very much!</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "gmarek" },
        "number": 19167,
        "resourcePath": "/kubernetes/kubernetes/issues/19167",
        "state": "CLOSED",
        "publishedAt": "2015-12-29T11:02:32Z",
        "closedAt": "2016-01-11T19:00:03Z",
        "title": "Unit tests are EXTREMELY flaky lately",
        "bodyText": "It's bad. kubertest-test-go suite failed 5 consecutive times because of it.\n@thockin @davidopp @brendandburns @bgrant0607 @dchen1107 @fgrzadkowski @wojtek-t @nikhiljindal @aronchick\nKnown Issues:\n\n pkg/master #19141 (fixed by @wojtek-t in #19195)\n pkg/util/wait #19067 (maybe fixed by @wojtek-t in #19196)\n pkg/storage/etcd, pkg/registry/generic/etcd #18928\n pkg/apiserver #19176\n pkg/client/record/event_test.go #19151\n k8s.io/kubernetes/contrib/mesos/pkg/runtime #19186\n pkg/util/wait: #19223\n pkg/client/unversioned/portforward #19230\n plugin/pkg/scheduler/factory #19229\n pkg/storage #19254\n pkg/client/record #19268",
        "bodyHTML": "<p>It's bad. kubertest-test-go suite failed 5 consecutive times because of it.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/thockin/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/thockin\">@thockin</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/davidopp/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/davidopp\">@davidopp</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/brendandburns/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/brendandburns\">@brendandburns</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/bgrant0607/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bgrant0607\">@bgrant0607</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/dchen1107/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dchen1107\">@dchen1107</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/fgrzadkowski/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fgrzadkowski\">@fgrzadkowski</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/wojtek-t/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wojtek-t\">@wojtek-t</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/nikhiljindal/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nikhiljindal\">@nikhiljindal</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/aronchick/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aronchick\">@aronchick</a></p>\n<p>Known Issues:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> pkg/master <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124075031\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19141\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19141/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19141\">#19141</a> (fixed by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/wojtek-t/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wojtek-t\">@wojtek-t</a> in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124336306\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19195\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/19195/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/19195\">#19195</a>)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> pkg/util/wait <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"123682664\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19067\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19067/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19067\">#19067</a> (maybe fixed by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/wojtek-t/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wojtek-t\">@wojtek-t</a> in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124344773\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19196\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/19196/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/19196\">#19196</a>)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> pkg/storage/etcd, pkg/registry/generic/etcd <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"123079357\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/18928\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/18928/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/18928\">#18928</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> pkg/apiserver <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124228372\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19176\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19176/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19176\">#19176</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> pkg/client/record/event_test.go <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124142018\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19151\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19151/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19151\">#19151</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> k8s.io/kubernetes/contrib/mesos/pkg/runtime <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124313681\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19186\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19186/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19186\">#19186</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> pkg/util/wait: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124543305\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19223\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19223/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19223\">#19223</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> pkg/client/unversioned/portforward <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124602923\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19230\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19230/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19230\">#19230</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> plugin/pkg/scheduler/factory <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124602796\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19229\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19229/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19229\">#19229</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> pkg/storage <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124765686\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19254\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19254/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19254\">#19254</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> pkg/client/record <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"124841696\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19268\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19268/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19268\">#19268</a></li>\n</ul>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "bprashanth" },
        "number": 17805,
        "resourcePath": "/kubernetes/kubernetes/issues/17805",
        "state": "CLOSED",
        "publishedAt": "2015-11-26T00:01:57Z",
        "closedAt": "2017-07-14T17:42:30Z",
        "title": "Fix l7 controller watch on namespaces",
        "bodyText": "Observed odd flakyness when watching Ingresses from the controller scoped to a namespace. Need to get to the bottom of it, but I suspect it's a bug higher up in the stack. For now it isn't that important because the controller watches all namespaces.",
        "bodyHTML": "<p>Observed odd flakyness when watching Ingresses from the controller scoped to a namespace. Need to get to the bottom of it, but I suspect it's a bug higher up in the stack. For now it isn't that important because the controller watches all namespaces.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "proppy" },
        "number": 424,
        "resourcePath": "/kubernetes/kubernetes/issues/424",
        "state": "CLOSED",
        "publishedAt": "2014-07-12T05:34:50Z",
        "closedAt": "2014-07-14T23:58:27Z",
        "title": "support for docker links env variables format",
        "bodyText": "When exposing services through environments variable to running containers, we should support the standard docker format used by links (see below). So developers don't have to modify their discovery code to connect to a service.\n    $ sudo docker run --rm --name web2 --link db:db training/webapp env\n    . . .\n    DB_NAME=/web2/db\n    DB_PORT=tcp://172.17.0.5:5432\n    DB_PORT_5000_TCP=tcp://172.17.0.5:5432\n    DB_PORT_5000_TCP_PROTO=tcp\n    DB_PORT_5000_TCP_PORT=5432\n    DB_PORT_5000_TCP_ADDR=172.17.0.5\n\nThe code writing the env var from service definition is here:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/779cb84625fda1ffd0b87437c2cc002033b2d2fe/pkg/registry/service_registry.go#L46\nI believe it could be easily adapted to follow the same standard defined by docker.",
        "bodyHTML": "<p>When exposing services through environments variable to running containers, we should support the <em>standard</em> docker format used by links (see below). So developers don't have to modify their discovery code to connect to a service.</p>\n<pre><code>    $ sudo docker run --rm --name web2 --link db:db training/webapp env\n    . . .\n    DB_NAME=/web2/db\n    DB_PORT=tcp://172.17.0.5:5432\n    DB_PORT_5000_TCP=tcp://172.17.0.5:5432\n    DB_PORT_5000_TCP_PROTO=tcp\n    DB_PORT_5000_TCP_PORT=5432\n    DB_PORT_5000_TCP_ADDR=172.17.0.5\n</code></pre>\n<p>The code writing the env var from service definition is here:<br>\n<a href=\"https://github.com/GoogleCloudPlatform/kubernetes/blob/779cb84625fda1ffd0b87437c2cc002033b2d2fe/pkg/registry/service_registry.go#L46\">https://github.com/GoogleCloudPlatform/kubernetes/blob/779cb84625fda1ffd0b87437c2cc002033b2d2fe/pkg/registry/service_registry.go#L46</a></p>\n<p>I believe it could be easily adapted to follow the same <em>standard</em> defined by docker.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "dchen1107" },
        "number": 4783,
        "resourcePath": "/kubernetes/kubernetes/issues/4783",
        "state": "CLOSED",
        "publishedAt": "2015-02-24T19:15:04Z",
        "closedAt": "2015-06-05T05:10:31Z",
        "title": "Add e2e test for etcd failures handling",
        "bodyText": "We ran into a lot of issues when etcd crashes. We need tests to ensure all components / daemons are doing the right things etcd is down, and recover later.",
        "bodyHTML": "<p>We ran into a lot of issues when etcd crashes. We need tests to ensure all components / daemons are doing the right things etcd is down, and recover later.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "deads2k" },
        "number": 3800,
        "resourcePath": "/kubernetes/kubernetes/issues/3800",
        "state": "CLOSED",
        "publishedAt": "2015-01-26T15:20:50Z",
        "closedAt": "2015-11-04T16:48:39Z",
        "title": "Remove --auth-path from kubectl",
        "bodyText": "Since the .kubeconfig file was introduced, there is a new way to describe the information contained inside of the existing .kubernetes_auth format.  .kubernetes_auth combined information that described how to recognize the api-server with information about how to authenticate the user to the api-server.  .kubeconfig separates those two concepts into discretely re-useable chunks, but --auth-path was kept for backwards compatibility.\nIf .kubernetes_auth is eliminated, there will be one way to express that information and that will simplify the explanation of how the information is built.  Right now, allowing references to .kuberentes_auth and defaulting to looking at the ~/.kubernetes_auth makes it harder to describe exactly where authentication information is coming from.\nThis would be a breaking change that has ripples affecting e2e tests, so I'd like to be sure there is agreement to the concept before starting a change.\n/cc @jlowdermilk @smarterclayton @liggitt",
        "bodyHTML": "<p>Since the .kubeconfig file was introduced, there is a new way to describe the information contained inside of the existing .kubernetes_auth format.  .kubernetes_auth combined information that described how to recognize the api-server with information about how to authenticate the user to the api-server.  .kubeconfig separates those two concepts into discretely re-useable chunks, but --auth-path was kept for backwards compatibility.</p>\n<p>If .kubernetes_auth is eliminated, there will be one way to express that information and that will simplify the explanation of how the information is built.  Right now, allowing references to .kuberentes_auth and defaulting to looking at the ~/.kubernetes_auth makes it harder to describe exactly where authentication information is coming from.</p>\n<p>This would be a breaking change that has ripples affecting e2e tests, so I'd like to be sure there is agreement to the concept before starting a change.</p>\n<p>/cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/jlowdermilk/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlowdermilk\">@jlowdermilk</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/smarterclayton/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/smarterclayton\">@smarterclayton</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/liggitt/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/liggitt\">@liggitt</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "shuoli84" },
        "number": 13709,
        "resourcePath": "/kubernetes/kubernetes/issues/13709",
        "state": "CLOSED",
        "publishedAt": "2015-09-09T06:27:44Z",
        "closedAt": "2015-10-14T15:22:21Z",
        "title": "kube-proxy's udp proxy dead",
        "bodyText": "EDIT: kubernetes version 1.0.3\nOne of our kubernete machine not able to connect to dns server, and after some trouble shooting, it seems kube-proxy's udp socket dead.\n\nIt stops reading from socket. I don't know how to re-pro yet. All other tcp sockets are working normally. I am trying to trouble shoot this, but I has no knowledge on how to debug a running golang process.\niptables:\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-PORTALS-CONTAINER  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */\nDOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL\nKUBE-NODEPORT-CONTAINER  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */\n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-PORTALS-HOST  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */\nDOCKER     all  --  anywhere            !127.0.0.0/8          ADDRTYPE match dst-type LOCAL\nKUBE-NODEPORT-HOST  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */\n\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nMASQUERADE  all  --  172.16.70.0/24       anywhere\n\nChain DOCKER (2 references)\ntarget     prot opt source               destination\n\nChain KUBE-NODEPORT-CONTAINER (1 references)\ntarget     prot opt source               destination\nREDIRECT   tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 redir ports 46087\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 redir ports 48104\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 redir ports 42337\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 redir ports 32768\n\nChain KUBE-NODEPORT-HOST (1 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 to:192.168.1.30:46087\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 to:192.168.1.30:48104\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 to:192.168.1.30:42337\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 to:192.168.1.30:32768\n\nChain KUBE-PORTALS-CONTAINER (1 references)\ntarget     prot opt source               destination\nREDIRECT   tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https redir ports 37726\nREDIRECT   tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 redir ports 46087\nREDIRECT   udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain redir ports 38046\nREDIRECT   tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain redir ports 47397\nREDIRECT   tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http redir ports 48270\nREDIRECT   tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http redir ports 40069\nREDIRECT   tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http redir ports 54624\nREDIRECT   tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 redir ports 33381\nREDIRECT   tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 redir ports 41223\nREDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 redir ports 52521\nREDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 redir ports 45287\nREDIRECT   tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 redir ports 48104\nREDIRECT   tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 redir ports 42337\nREDIRECT   tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 redir ports 32768\n\nChain KUBE-PORTALS-HOST (1 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https to:192.168.1.30:37726\nDNAT       tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 to:192.168.1.30:46087\nDNAT       udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain to:192.168.1.30:38046\nDNAT       tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain to:192.168.1.30:47397\nDNAT       tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http to:192.168.1.30:48270\nDNAT       tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http to:192.168.1.30:40069\nDNAT       tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http to:192.168.1.30:54624\nDNAT       tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 to:192.168.1.30:33381\nDNAT       tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 to:192.168.1.30:41223\nDNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 to:192.168.1.30:52521\nDNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 to:192.168.1.30:45287\nDNAT       tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 to:192.168.1.30:48104\nDNAT       tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 to:192.168.1.30:42337\nDNAT       tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 to:192.168.1.30:32768\n\nNothing interesting in logs. So I just skip them.",
        "bodyHTML": "<p>EDIT: kubernetes version 1.0.3</p>\n<p>One of our kubernete machine not able to connect to dns server, and after some trouble shooting, it seems kube-proxy's udp socket dead.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/3402035/9754609/8e603762-56fd-11e5-8e8a-c2b2013919f6.png\"><img src=\"https://cloud.githubusercontent.com/assets/3402035/9754609/8e603762-56fd-11e5-8e8a-c2b2013919f6.png\" alt=\"screen shot 2015-09-09 at 2 16 14 pm\" style=\"max-width:100%;\"></a></p>\n<p>It stops reading from socket. I don't know how to re-pro yet. All other tcp sockets are working normally. I am trying to trouble shoot this, but I has no knowledge on how to debug a running golang process.</p>\n<p>iptables:</p>\n<pre><code>Chain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-PORTALS-CONTAINER  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */\nDOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL\nKUBE-NODEPORT-CONTAINER  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */\n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-PORTALS-HOST  all  --  anywhere             anywhere             /* handle ClusterIPs; NOTE: this must be before the NodePort rules */\nDOCKER     all  --  anywhere            !127.0.0.0/8          ADDRTYPE match dst-type LOCAL\nKUBE-NODEPORT-HOST  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL /* handle service NodePorts; NOTE: this must be the last rule in the chain */\n\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nMASQUERADE  all  --  172.16.70.0/24       anywhere\n\nChain DOCKER (2 references)\ntarget     prot opt source               destination\n\nChain KUBE-NODEPORT-CONTAINER (1 references)\ntarget     prot opt source               destination\nREDIRECT   tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 redir ports 46087\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 redir ports 48104\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 redir ports 42337\nREDIRECT   tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 redir ports 32768\n\nChain KUBE-NODEPORT-HOST (1 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  anywhere             anywhere             /* default/myspotlight: */ tcp dpt:31743 to:192.168.1.30:46087\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-prod: */ tcp dpt:32164 to:192.168.1.30:48104\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-office: */ tcp dpt:30059 to:192.168.1.30:42337\nDNAT       tcp  --  anywhere             anywhere             /* default/civet-web-testing: */ tcp dpt:30082 to:192.168.1.30:32768\n\nChain KUBE-PORTALS-CONTAINER (1 references)\ntarget     prot opt source               destination\nREDIRECT   tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https redir ports 37726\nREDIRECT   tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 redir ports 46087\nREDIRECT   udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain redir ports 38046\nREDIRECT   tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain redir ports 47397\nREDIRECT   tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http redir ports 48270\nREDIRECT   tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http redir ports 40069\nREDIRECT   tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http redir ports 54624\nREDIRECT   tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 redir ports 33381\nREDIRECT   tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 redir ports 41223\nREDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 redir ports 52521\nREDIRECT   tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 redir ports 45287\nREDIRECT   tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 redir ports 48104\nREDIRECT   tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 redir ports 42337\nREDIRECT   tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 redir ports 32768\n\nChain KUBE-PORTALS-HOST (1 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  anywhere             192.168.3.1          /* default/kubernetes: */ tcp dpt:https to:192.168.1.30:37726\nDNAT       tcp  --  anywhere             192.168.3.59         /* default/myspotlight: */ tcp dpt:18080 to:192.168.1.30:46087\nDNAT       udp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns */ udp dpt:domain to:192.168.1.30:38046\nDNAT       tcp  --  anywhere             192.168.3.10         /* kube-system/kube-dns:dns-tcp */ tcp dpt:domain to:192.168.1.30:47397\nDNAT       tcp  --  anywhere             192.168.3.126        /* kube-system/kube-ui: */ tcp dpt:http to:192.168.1.30:48270\nDNAT       tcp  --  anywhere             192.168.3.145        /* kube-system/monitoring-grafana: */ tcp dpt:http to:192.168.1.30:40069\nDNAT       tcp  --  anywhere             192.168.3.79         /* kube-system/monitoring-heapster: */ tcp dpt:http to:192.168.1.30:54624\nDNAT       tcp  --  anywhere             192.168.3.23         /* kube-system/elasticsearch-logging: */ tcp dpt:9200 to:192.168.1.30:33381\nDNAT       tcp  --  anywhere             192.168.3.121        /* kube-system/kibana-logging: */ tcp dpt:5601 to:192.168.1.30:41223\nDNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:http */ tcp dpt:8083 to:192.168.1.30:52521\nDNAT       tcp  --  anywhere             192.168.3.71         /* kube-system/monitoring-influxdb:api */ tcp dpt:8086 to:192.168.1.30:45287\nDNAT       tcp  --  anywhere             192.168.3.216        /* default/civet-web-prod: */ tcp dpt:5000 to:192.168.1.30:48104\nDNAT       tcp  --  anywhere             192.168.3.167        /* default/civet-web-office: */ tcp dpt:5000 to:192.168.1.30:42337\nDNAT       tcp  --  anywhere             192.168.3.177        /* default/civet-web-testing: */ tcp dpt:5000 to:192.168.1.30:32768\n</code></pre>\n<p>Nothing interesting in logs. So I just skip them.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "k8s-github-robot" },
        "number": 30962,
        "resourcePath": "/kubernetes/kubernetes/issues/30962",
        "state": "CLOSED",
        "publishedAt": "2016-08-19T03:22:43Z",
        "closedAt": "2016-09-02T23:12:12Z",
        "title": "kubernetes-e2e-gke-staging: broken test run",
        "bodyText": "Failed: https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke-staging/6333/\nRun so broken it didn't make JUnit output!",
        "bodyHTML": "<p>Failed: <a rel=\"nofollow\" href=\"https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke-staging/6333/\">https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-e2e-gke-staging/6333/</a></p>\n<p>Run so broken it didn't make JUnit output!</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "childsb" },
        "number": 8792,
        "resourcePath": "/kubernetes/kubernetes/issues/8792",
        "state": "CLOSED",
        "publishedAt": "2015-05-26T00:53:41Z",
        "closedAt": "2016-05-25T06:46:53Z",
        "title": "make clean ; make WHAT=test/e2e/e2e.test\" fails",
        "bodyText": "when building e2e_test.go using 'make WHAT=test/e2e/e2e' fails with:\n(20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test\nbuild/make-clean.sh\n+++ [0525 20:52:03] Verifying Prerequisites....\n+++ [0525 20:52:03] Cleaning out local _output directory\nrm -rf _output\nrm -rf Godeps/_workspace/pkg\nhack/build-go.sh test/e2e/e2e.test\n+++ [0525 20:52:04] Building go targets for darwin/amd64:\ntest/e2e/e2e.test\n/Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh: line 367: pushd: /Users/bc/dev/git/t/kubernetes/_output/local/go/bin: No such file or directory\n!!! Error in /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361\n'pushd \"$(dirname ${outfile})\" > /dev/null' exited with status 1\nCall stack:\n1: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361 kube::golang::build_binaries_for_platform(...)\n2: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:488 kube::golang::build_binaries(...)\n3: hack/build-go.sh:26 main(...)\nExiting with status 1\nmake: *** [all] Error 1\nHowever 'make all; make (20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test' works",
        "bodyHTML": "<p>when building e2e_test.go using 'make WHAT=test/e2e/e2e' fails with:</p>\n<p>(20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test<br>\nbuild/make-clean.sh<br>\n+++ [0525 20:52:03] Verifying Prerequisites....<br>\n+++ [0525 20:52:03] Cleaning out local _output directory<br>\nrm -rf _output<br>\nrm -rf Godeps/_workspace/pkg<br>\nhack/build-go.sh test/e2e/e2e.test<br>\n+++ [0525 20:52:04] Building go targets for darwin/amd64:<br>\ntest/e2e/e2e.test<br>\n/Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh: line 367: pushd: /Users/bc/dev/git/t/kubernetes/_output/local/go/bin: No such file or directory<br>\n!!! Error in /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361<br>\n'pushd \"$(dirname ${outfile})\" &gt; /dev/null' exited with status 1<br>\nCall stack:<br>\n1: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:361 kube::golang::build_binaries_for_platform(...)<br>\n2: /Users/bc/dev/git/t/kubernetes/hack/lib/golang.sh:488 kube::golang::build_binaries(...)<br>\n3: hack/build-go.sh:26 main(...)<br>\nExiting with status 1<br>\nmake: *** [all] Error 1</p>\n<p>However 'make all; make (20:51:55)[bc-macpro]:[~/dev/git/t/kubernetes/]# make clean ; make WHAT=test/e2e/e2e.test' works</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "lachie83" },
        "number": 14023,
        "resourcePath": "/kubernetes/kubernetes/issues/14023",
        "state": "CLOSED",
        "publishedAt": "2015-09-16T06:33:14Z",
        "closedAt": "2015-09-17T20:41:04Z",
        "title": "Pods hang in pending state indefinitely",
        "bodyText": "I've been working with a 6 node cluster for the last few weeks without issue. Earlier today we ran into an open file issue (https://github.com/kubernetes/kubernetes/pull/12443/files) and I patched and restarted kube-proxy. Since then, all pods deployed to all BUT node-01 get stuck in pending state and there log messages stating the cause.\nI've taking a look at both the following similar issues and they don't appear to be the cause\n#4891\n#3185\nCluster is running v1.0.3\nHere's an example of the state\ndocker run --rm -it lachie83/kubectl:prod get pods --namespace=kube-system -o wide\nNAME                READY     STATUS    RESTARTS   AGE       NODE\nkube-dns-v8-i0yac   0/4       Pending   0          4s        10.1.1.35\nkube-dns-v8-jti2e   0/4       Pending   0          4s        10.1.1.34\n\nget events\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-i0yac\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-i0yac                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-i0yac to 10.1.1.35\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-jti2e                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-jti2e to 10.1.1.34\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-jti2e\n\nscheduler log\nI0916 06:25:42.897814   10076 event.go:203] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"kube-dns-v8-jti2e\", UID:\"c1cafebe-5c3b-11e5-b3c4-020443b6797d\", APIVersion:\"v1\", ResourceVersion:\"670117\", FieldPath:\"\"}): reason: 'scheduled' Successfully assigned kube-dns-v8-jti2e to 10.1.1.34\nI0916 06:25:42.904195   10076 event.go:203] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"kube-dns-v8-i0yac\", UID:\"c1cafc69-5c3b-11e5-b3c4-020443b6797d\", APIVersion:\"v1\", ResourceVersion:\"670118\", FieldPath:\"\"}): reason: 'scheduled' Successfully assigned kube-dns-v8-i0yac to 10.1.1.35\n\ntailing kubelet log file during pod create\ntail -f kubelet.kube-node-03.root.log.INFO.20150916-060744.10668\nI0916 06:25:04.448916   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:25:24.449253   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:25:44.449522   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:04.449774   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:24.450400   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:44.450995   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:04.451501   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:24.451910   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:44.452511   10668 config.go:253] Setting pods for source file : {[] 0 file}\n\nkubelet process\nroot@kube-node-03:/var/log/kubernetes# ps -ef | grep kubelet\nroot     10668     1  1 06:07 ?        00:00:13 /opt/bin/kubelet --address=10.1.1.34 --port=10250 --hostname_override=10.1.1.34 --api_servers=https://kube-master-01.sj.lithium.com:6443 --logtostderr=false --log_dir=/var/log/kubernetes --cluster_dns=10.1.2.53 --config=/etc/kubelet/conf --cluster_domain=prod-kube-sjc1-1.internal --v=4 --tls-cert-file=/etc/kubelet/certs/kubelet.pem --tls-private-key-file=/etc/kubelet/certs/kubelet-key.pem\n\nnode list\ndocker run --rm -it lachie83/kubectl:prod get nodes\nNAME            LABELS                                             STATUS\n10.1.1.30   kubernetes.io/hostname=10.1.1.30,name=node-1   Ready\n10.1.1.32   kubernetes.io/hostname=10.1.1.32,name=node-2   Ready\n10.1.1.34   kubernetes.io/hostname=10.1.1.34,name=node-3   Ready\n10.1.1.35   kubernetes.io/hostname=10.1.1.35,name=node-4   Ready\n10.1.1.42   kubernetes.io/hostname=10.1.1.42,name=node-5   Ready\n10.1.1.43   kubernetes.io/hostname=10.1.1.43,name=node-6   Ready",
        "bodyHTML": "<p>I've been working with a 6 node cluster for the last few weeks without issue. Earlier today we ran into an open file issue (<a href=\"https://github.com/kubernetes/kubernetes/pull/12443/files\">https://github.com/kubernetes/kubernetes/pull/12443/files</a>) and I patched and restarted kube-proxy. Since then, all pods deployed to all BUT node-01 get stuck in pending state and there log messages stating the cause.</p>\n<p>I've taking a look at both the following similar issues and they don't appear to be the cause<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"59214274\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/4891\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/4891/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/4891\">#4891</a><br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"53176226\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/3185\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/3185/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/3185\">#3185</a></p>\n<p>Cluster is running v1.0.3</p>\n<p>Here's an example of the state</p>\n<pre><code>docker run --rm -it lachie83/kubectl:prod get pods --namespace=kube-system -o wide\nNAME                READY     STATUS    RESTARTS   AGE       NODE\nkube-dns-v8-i0yac   0/4       Pending   0          4s        10.1.1.35\nkube-dns-v8-jti2e   0/4       Pending   0          4s        10.1.1.34\n</code></pre>\n<p>get events</p>\n<pre><code>Wed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-i0yac\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-i0yac                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-i0yac to 10.1.1.35\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8-jti2e                 Pod                                                                  scheduled          {scheduler }                Successfully assigned kube-dns-v8-jti2e to 10.1.1.34\nWed, 16 Sep 2015 06:25:42 +0000   Wed, 16 Sep 2015 06:25:42 +0000   1         kube-dns-v8                       ReplicationController                                                successfulCreate   {replication-controller }   Created pod: kube-dns-v8-jti2e\n</code></pre>\n<p>scheduler log</p>\n<pre><code>I0916 06:25:42.897814   10076 event.go:203] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"kube-dns-v8-jti2e\", UID:\"c1cafebe-5c3b-11e5-b3c4-020443b6797d\", APIVersion:\"v1\", ResourceVersion:\"670117\", FieldPath:\"\"}): reason: 'scheduled' Successfully assigned kube-dns-v8-jti2e to 10.1.1.34\nI0916 06:25:42.904195   10076 event.go:203] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"kube-dns-v8-i0yac\", UID:\"c1cafc69-5c3b-11e5-b3c4-020443b6797d\", APIVersion:\"v1\", ResourceVersion:\"670118\", FieldPath:\"\"}): reason: 'scheduled' Successfully assigned kube-dns-v8-i0yac to 10.1.1.35\n</code></pre>\n<p>tailing kubelet log file during pod create</p>\n<pre><code>tail -f kubelet.kube-node-03.root.log.INFO.20150916-060744.10668\nI0916 06:25:04.448916   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:25:24.449253   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:25:44.449522   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:04.449774   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:24.450400   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:26:44.450995   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:04.451501   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:24.451910   10668 config.go:253] Setting pods for source file : {[] 0 file}\nI0916 06:27:44.452511   10668 config.go:253] Setting pods for source file : {[] 0 file}\n</code></pre>\n<p>kubelet process</p>\n<pre><code>root@kube-node-03:/var/log/kubernetes# ps -ef | grep kubelet\nroot     10668     1  1 06:07 ?        00:00:13 /opt/bin/kubelet --address=10.1.1.34 --port=10250 --hostname_override=10.1.1.34 --api_servers=https://kube-master-01.sj.lithium.com:6443 --logtostderr=false --log_dir=/var/log/kubernetes --cluster_dns=10.1.2.53 --config=/etc/kubelet/conf --cluster_domain=prod-kube-sjc1-1.internal --v=4 --tls-cert-file=/etc/kubelet/certs/kubelet.pem --tls-private-key-file=/etc/kubelet/certs/kubelet-key.pem\n</code></pre>\n<p>node list</p>\n<pre><code>docker run --rm -it lachie83/kubectl:prod get nodes\nNAME            LABELS                                             STATUS\n10.1.1.30   kubernetes.io/hostname=10.1.1.30,name=node-1   Ready\n10.1.1.32   kubernetes.io/hostname=10.1.1.32,name=node-2   Ready\n10.1.1.34   kubernetes.io/hostname=10.1.1.34,name=node-3   Ready\n10.1.1.35   kubernetes.io/hostname=10.1.1.35,name=node-4   Ready\n10.1.1.42   kubernetes.io/hostname=10.1.1.42,name=node-5   Ready\n10.1.1.43   kubernetes.io/hostname=10.1.1.43,name=node-6   Ready\n</code></pre>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "zmerlynn" },
        "number": 26304,
        "resourcePath": "/kubernetes/kubernetes/issues/26304",
        "state": "CLOSED",
        "publishedAt": "2016-05-25T21:13:16Z",
        "closedAt": "2018-03-12T10:21:32Z",
        "title": "Fix wait.Until tests to actually verify something other than \"not hanging\"",
        "bodyText": "In review for #26301, it was pointed out that my copy-pasta test was verifying one thing, but then the second half of the test is more of the \"run this code and hope it doesn't hang\" variety.\nFollow-up issue.",
        "bodyHTML": "<p>In review for <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"156849435\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/26301\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/26301/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/26301\">#26301</a>, it was pointed out that my copy-pasta test was verifying one thing, but then the second half of the test is more of the \"run this code and hope it doesn't hang\" variety.</p>\n<p>Follow-up issue.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "caesarxuchao" },
        "number": 16626,
        "resourcePath": "/kubernetes/kubernetes/issues/16626",
        "state": "CLOSED",
        "publishedAt": "2015-10-30T22:26:43Z",
        "closedAt": "2015-11-12T07:52:38Z",
        "title": "Document how to add a new API group",
        "bodyText": "AFAIK, we don't have a documentation on how to add a new API group. Currently we have #16621 and #13146 that try to add a new API group. I will draft a guide to ease the future endeavor of adding groups. And probably by writing this guide, I will see how can we make the API group machinery easier to use.\ncc @mikedanese @timstclair @lavalamp",
        "bodyHTML": "<p>AFAIK, we don't have a documentation on how to add a new API group. Currently we have <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"114360433\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/16621\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/16621/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/16621\">#16621</a> and <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"103076642\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/13146\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/13146/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/13146\">#13146</a> that try to add a new API group. I will draft a guide to ease the future endeavor of adding groups. And probably by writing this guide, I will see how can we make the API group machinery easier to use.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/mikedanese/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mikedanese\">@mikedanese</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/timstclair/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/timstclair\">@timstclair</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/lavalamp/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lavalamp\">@lavalamp</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "nikhiljindal" },
        "number": 16124,
        "resourcePath": "/kubernetes/kubernetes/issues/16124",
        "state": "CLOSED",
        "publishedAt": "2015-10-22T20:27:08Z",
        "closedAt": "2015-10-28T22:25:51Z",
        "title": "Add a user guide readme for deployments",
        "bodyText": "",
        "bodyHTML": ""
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "davidopp" },
        "number": 5585,
        "resourcePath": "/kubernetes/kubernetes/issues/5585",
        "state": "CLOSED",
        "publishedAt": "2015-03-18T04:48:22Z",
        "closedAt": "2015-03-18T19:43:21Z",
        "title": "Investigate possibly broken Docker SIGTERM delivery",
        "bodyText": "@rsokolowski and I found the following behavior last week when we were working with someone who was trying to run some stuff on Kubernetes. Note that this appears to be a Docker problem, not a Kubernetes problem, but I'm filing it here for now so we can verify that it's reproducible before filing a bug against Docker.\nStart two containers on the same machine, each a shell script that traps SIGTERM and prints something like \u201cI received SIGTERM\u201d and quits. Then do \u201cdocker stop\u201d and observe that one container exits via the SIGTERM handler path and the other just gets SIGKILL.",
        "bodyHTML": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/rsokolowski/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rsokolowski\">@rsokolowski</a> and I found the following behavior last week when we were working with someone who was trying to run some stuff on Kubernetes. Note that this appears to be a Docker problem, not a Kubernetes problem, but I'm filing it here for now so we can verify that it's reproducible before filing a bug against Docker.</p>\n<p>Start two containers on the same machine, each a shell script that traps SIGTERM and prints something like \u201cI received SIGTERM\u201d and quits. Then do \u201cdocker stop\u201d and observe that one container exits via the SIGTERM handler path and the other just gets SIGKILL.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "thockin" },
        "number": 4628,
        "resourcePath": "/kubernetes/kubernetes/issues/4628",
        "state": "CLOSED",
        "publishedAt": "2015-02-19T22:38:58Z",
        "closedAt": "2015-03-13T07:28:18Z",
        "title": "Loosen label and annotation validation and related tests",
        "bodyText": "From #4486\nProposed:\n\nlabel values: restrict to [A-Za-z0-9_-.]*\nannotation values: no restriction\nqualified names (label and annotation keys, resource names, volume plugins): loosen restrictions to ([A-Za-z0-9_-.]+/)?[A-Za-z0-9_-.]+ - this should be a strict superset of\nwhat we allow today.  We can say that convention is to use dns-compatible\ndomain + label, but still allow things like FOO/B_A.R.  This DOES NOT allow for foo.com/bat/bat - is that an important affordance to anyone?  We could allow that, but we need to be clear that \"bar/bat\" means (bar, bat) while \"foo.com/bar/bat\" means (foo.com, bar/bat).\n\n@smarterclayton does that give you enough freedom?\n@bgrant0607 does that give you enough consistency?\n@quinton-hoole-google does this make the qualified name type now viable for your use case in kube-proxy?",
        "bodyHTML": "<p>From <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"57944053\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/4486\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/4486/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/4486\">#4486</a></p>\n<p>Proposed:</p>\n<ul>\n<li>label values: restrict to [A-Za-z0-9_-.]*</li>\n<li>annotation values: no restriction</li>\n<li>qualified names (label and annotation keys, resource names, volume plugins): loosen restrictions to ([A-Za-z0-9_-.]+/)?[A-Za-z0-9_-.]+ - this should be a strict superset of<br>\nwhat we allow today.  We can say that convention is to use dns-compatible<br>\ndomain + label, but still allow things like FOO/B_A.R.  This DOES NOT allow for foo.com/bat/bat - is that an important affordance to anyone?  We could allow that, but we need to be clear that \"bar/bat\" means (bar, bat) while \"foo.com/bar/bat\" means (foo.com, bar/bat).</li>\n</ul>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/smarterclayton/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/smarterclayton\">@smarterclayton</a> does that give you enough freedom?<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/bgrant0607/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bgrant0607\">@bgrant0607</a> does that give you enough consistency?<br>\n@quinton-hoole-google does this make the qualified name type now viable for your use case in kube-proxy?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "zmerlynn" },
        "number": 5946,
        "resourcePath": "/kubernetes/kubernetes/issues/5946",
        "state": "CLOSED",
        "publishedAt": "2015-03-25T20:19:16Z",
        "closedAt": "2015-04-20T20:22:38Z",
        "title": "`validate-cluster.sh` should be using `/validate`: wants `kubectl healthy`?",
        "bodyText": "We have a /validate endpoint that GKE has been using to validate the health of running clusters after the API server is available, similar to the kube-up.sh flow of validate-cluster.sh. There's no reason that validate-cluster.sh should actually do this work manually anymore - we should just go through common source. We should probably introduce a kubectl healthy or kubectl healthcheck?",
        "bodyHTML": "<p>We have a <code>/validate</code> endpoint that GKE has been using to validate the health of running clusters after the API server is available, similar to the <code>kube-up.sh</code> flow of <code>validate-cluster.sh</code>. There's no reason that <code>validate-cluster.sh</code> should actually do this work manually anymore - we should just go through common source. We should probably introduce a <code>kubectl healthy</code> or <code>kubectl healthcheck</code>?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "metral" },
        "number": 20514,
        "resourcePath": "/kubernetes/kubernetes/issues/20514",
        "state": "CLOSED",
        "publishedAt": "2016-02-02T21:18:52Z",
        "closedAt": "2016-05-15T12:45:01Z",
        "title": "Running k8s locally via Docker & 1.2.0-alpha.6 does not stand up apiserver",
        "bodyText": "Following the all-in-one install via kubelet on Docker as listed in the docs is not working. I hit a \"no cloud provider specified\" and the controller and apiserver never come up and am plagued with many connection refused errors when hitting 127.0.0.1:8080 even though I set KUBERNETES_PROVIDER https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md#run-it\nI'm running on an Ubuntu VM and don't need cloud provider resources like the LoadBalancer type for Services - I just want a local dev k8s environment.\nHere is a snippet of the logs for the mod that enables nsenter (seeing how there were issues with Secrets not working in hyperkube - see #19069 for related info )\n\ndefault\ndocker-compose file:\nmaster:\n  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\n  net: host\n  pid: host\n  privileged: true\n  volumes:\n    - /:/rootfs:ro\n    - /sys:/sys:ro\n    - /dev:/dev\n    - /var/lib/docker/:/var/lib/docker:rw\n    - /var/lib/kubelet/:/var/lib/kubelet:rw\n    - /var/run:/var/run:rw\n  command: ['./hyperkube', 'kubelet', '--containerized', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local', '--allow-privileged=true', '--v=10']\n\nlogs:\nAttaching to aiok8s_master_1\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:53.489776    6562 server.go:132] Running kubelet in containerized mode (experimental)\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.248665    6562 server.go:351] Using self-signed cert (/var/run/kubernetes/kubelet.crt, /var/run/kubernetes/kubelet.key)\n\ufffd[36mmaster_1 | \ufffd[0mW0202 21:15:54.248985    6562 server.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.\n\ufffd[36mmaster_1 | \ufffd[0mW0202 21:15:54.249026    6562 server.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.249194    6562 plugins.go:71] No cloud provider specified.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.249212    6562 server.go:289] Successfully initialized cloud provider: \"\" from the config file: \"\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.249332    6562 manager.go:128] cAdvisor running in container: \"/docker-daemon/docker/808781db3c41aa36cafc5229b705611b694d581a5c661f03094a875dd9a8ff6e\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.465357    6562 fs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/rootfs major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/rootfs/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/rootfs/mystore major:202 minor:66 fsType: blockSize:0}]\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.494458    6562 machine.go:50] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.494534    6562 manager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID: SystemUUID:7ae5ca5c485b47b88d89d96a71601d80 BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968} {Device:/dev/xvda1 Capacity:42140499968}] DiskMap:map[43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline} 43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.495665    6562 manager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496462    6562 server.go:312] Using root directory: /var/lib/kubelet\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496691    6562 server.go:571] Sending events to api server.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496862    6562 server.go:636] Adding manifest file: etc/kubernetes/manifests\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496911    6562 file.go:47] Watching path \"etc/kubernetes/manifests\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496925    6562 server.go:646] Watching apiserver\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.497132    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/etcd.json\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/services?resourceVersion=0\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.501520    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.501968    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&resourceVersion=0  in 1 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502198    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502042    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/services?resourceVersion=0  in 1 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502666    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502225    6562 common.go:52] Generated UID \"e171033ec56ced6f29a4417f8b61cbf0\" pod \"k8s-etcd\" from etc/kubernetes/manifests/etcd.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502880    6562 common.go:56] Generated Name \"k8s-etcd-127.0.0.1\" for UID \"e171033ec56ced6f29a4417f8b61cbf0\" from URL etc/kubernetes/manifests/etcd.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502892    6562 common.go:61] Using namespace \"default\" for pod \"k8s-etcd-127.0.0.1\" from etc/kubernetes/manifests/etcd.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503109    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/kube-proxy.json\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503393    6562 common.go:52] Generated UID \"e513e62ea641585218de8b3495fc7a21\" pod \"k8s-proxy\" from etc/kubernetes/manifests/kube-proxy.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503419    6562 common.go:56] Generated Name \"k8s-proxy-127.0.0.1\" for UID \"e513e62ea641585218de8b3495fc7a21\" from URL etc/kubernetes/manifests/kube-proxy.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503429    6562 common.go:61] Using namespace \"default\" for pod \"k8s-proxy-127.0.0.1\" from etc/kubernetes/manifests/kube-proxy.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503514    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/master.json\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503983    6562 common.go:52] Generated UID \"eaf8ef5e21f965406a198841c5faa403\" pod \"k8s-master\" from etc/kubernetes/manifests/master.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504018    6562 common.go:56] Generated Name \"k8s-master-127.0.0.1\" for UID \"eaf8ef5e21f965406a198841c5faa403\" from URL etc/kubernetes/manifests/master.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504029    6562 common.go:61] Using namespace \"default\" for pod \"k8s-master-127.0.0.1\" from etc/kubernetes/manifests/master.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504172    6562 config.go:265] Setting pods for source file\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504433    6562 config.go:412] Receiving a new pod \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504492    6562 config.go:412] Receiving a new pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504507    6562 config.go:412] Receiving a new pod \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.505051    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&resourceVersion=0  in 3 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.505072    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.708688    6562 manager.go:191] Setting dockerRoot to /var/lib/docker\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.759597    6562 plugins.go:56] Registering credential provider: .dockercfg\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772006    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/aws-ebs\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772056    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/empty-dir\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772081    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/gce-pd\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772103    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/git-repo\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772125    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/host-path\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772149    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/nfs\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772170    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/secret\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772185    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/iscsi\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772204    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/glusterfs\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772227    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/persistent-claim\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772242    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/rbd\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772263    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/cinder\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772284    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/cephfs\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772327    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/downward-api\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772351    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/fc\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772373    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/flocker\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772498    6562 server.go:608] Started kubelet\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772569    6562 server.go:104] Starting to listen on 0.0.0.0:10250\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:54.772810    6562 kubelet.go:868] Image garbage collection failed: unable to find data for container /\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773100    6562 request.go:546] Request Body: {\"kind\":\"Event\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"127.0.0.1.142f3c86e792e0e6\",\"namespace\":\"default\",\"creationTimestamp\":null},\"involvedObject\":{\"kind\":\"Node\",\"name\":\"127.0.0.1\",\"uid\":\"127.0.0.1\"},\"reason\":\"Starting\",\"message\":\"Starting kubelet.\",\"source\":{\"component\":\"kubelet\",\"host\":\"127.0.0.1\"},\"firstTimestamp\":\"2016-02-02T21:15:54Z\",\"lastTimestamp\":\"2016-02-02T21:15:54Z\",\"count\":1,\"type\":\"Normal\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773187    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" -H \"Content-Type: application/json\" http://localhost:8080/api/v1/namespaces/default/events\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773216    6562 server.go:121] Starting to listen read-only on 0.0.0.0:10255\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773351    6562 server.go:569] Event(api.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"127.0.0.1\", UID:\"127.0.0.1\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'Starting' Starting kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773845    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/events  in 0 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773867    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:54.774060    6562 event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781503    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781526    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781535    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781542    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781549    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781556    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781562    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781569    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781575    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781582    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781589    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.784869    6562 kubelet.go:898] Running in container \"/kubelet\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.788967    6562 container_manager_linux.go:207] Configure resource-only container /docker-daemon with memory limit: 11043009331\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.789028    6562 manager.go:124] Starting to sync pod status with apiserver\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.789054    6562 kubelet.go:2246] Starting kubelet main sync loop.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.792327    6562 generic.go:106] GenericPLEG: Relisting\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.819530    6562 kubelet.go:2278] SyncLoop (ADD, \"file\"): \"\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820696    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820720    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820728    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820735    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820743    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820749    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820756    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820763    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820770    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820776    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820783    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820971    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820996    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821006    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821016    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821024    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/b98145a18fd89935c205528d910e094b6c178ec657f1be1637f922949dc2cd7d: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821034    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/3baa25cf80a8301be5f04d31d181b17023c23ed0e4c7a36df72f174b8b74896f: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821042    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/e94c8a2adb00d32038bde655ce418d482fe3810b81465e26395b2f2abb767047: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821051    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/8da0f1f1d20e5f3b00b0b0bfa994e8682a7b48f294453c15455fa88b1d6ca90b: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821060    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f98816a30a176e1dc9da2d980e4f961e2fffa056d109fe099af5dc6233edc540: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821070    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/c3f9389f06f5c520d0e57a197b284d5af182dad8f6d1f90e179a22edbbf6e132: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821079    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/6882bb8bc6e182316fe58110d3586738e9479a288604fe73396176151595024d: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821088    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/ea35c7a7f264f71ff0c7fc8d724cda79d272994f4dcb841d08944c6dc39d3528: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821096    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/351e59a4c698f8412365798aa8fce50e9029c22d3065f9531e2d9e4be5e3720c: unknown -> running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821105    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d: unknown -> running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821116    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03: unknown -> running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821126    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455: unknown -> running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821134    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540: unknown -> running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821144    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15: unknown -> running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821154    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/6ba5339964aec260e37272fb2d67597a170f2bd203e09e3b7736b72684814f31: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821162    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/e45b33bd1c357e7a18b8f73ba755c568e7563aa854b3351595e0fadbb59b5748: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821171    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/54b9ec78c213e0309daf88c259138a3d4bf5e1ed6309bd63503c666f65105064: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821180    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/5a3cdba1e62db46b6c5440c443439571bfeb5720032c251e3ce7aaac764fec10: unknown -> exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.851415    6562 kubelet.go:2301] SyncLoop (PLEG): ignore irrelevant event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.880172    6562 kubelet.go:2281] SyncLoop (UPDATE, \"file\"): \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0), k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21), k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.952533    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.029929    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.030066    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.030158    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.030284    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031309    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-proxy-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default\",\"uid\":\"e513e62ea641585218de8b3495fc7a21\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"e513e62ea641585218de8b3495fc7a21\",\"kubernetes.io/config.mirror\":\"e513e62ea641585218de8b3495fc7a21\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504500123Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"containers\":[{\"name\":\"kube-proxy\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"proxy\",\"--master=http://127.0.0.1:8080\",\"--v=2\",\"--resource-container=\\\"\\\"\"],\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\",\"securityContext\":{\"privileged\":true}}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031354    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-etcd-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-etcd-127.0.0.1/default\",\"uid\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"kubernetes.io/config.mirror\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504450902Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"volumes\":[{\"name\":\"varetcd\",\"emptyDir\":{}}],\"containers\":[{\"name\":\"etcd\",\"image\":\"gcr.io/google_containers/etcd:2.2.1\",\"command\":[\"/usr/local/bin/etcd\",\"--listen-client-urls=http://127.0.0.1:4001\",\"--advertise-client-urls=http://127.0.0.1:4001\",\"--data-dir=/var/etcd/data\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"varetcd\",\"mountPath\":\"/var/etcd\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031372    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" -H \"Content-Type: application/json\" http://localhost:8080/api/v1/namespaces/default/pods\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031378    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-master-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-master-127.0.0.1/default\",\"uid\":\"eaf8ef5e21f965406a198841c5faa403\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"eaf8ef5e21f965406a198841c5faa403\",\"kubernetes.io/config.mirror\":\"eaf8ef5e21f965406a198841c5faa403\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504513882Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"volumes\":[{\"name\":\"data\",\"emptyDir\":{}}],\"containers\":[{\"name\":\"controller-manager\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"controller-manager\",\"--master=127.0.0.1:8080\",\"--min-resync-period=3m\",\"--service-account-private-key-file=/srv/kubernetes/server.key\",\"--root-ca-file=/srv/kubernetes/ca.crt\",\"--v=2\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/srv/kubernetes\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"apiserver\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"apiserver\",\"--service-cluster-ip-range=10.0.0.1/24\",\"--insecure-bind-address=127.0.0.1\",\"--etcd-servers=http://127.0.0.1:4001\",\"--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota\",\"--client-ca-file=/srv/kubernetes/ca.crt\",\"--basic-auth-file=/srv/kubernetes/basic_auth.csv\",\"--min-request-timeout=300\",\"--tls-cert-file=/srv/kubernetes/server.cert\",\"--tls-private-key-file=/srv/kubernetes/server.key\",\"--token-auth-file=/srv/kubernetes/known_tokens.csv\",\"--allow-privileged=true\",\"--v=4\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/srv/kubernetes\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"scheduler\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"scheduler\",\"--master=127.0.0.1:8080\",\"--v=2\"],\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"setup\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/setup-files.sh\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/data\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031432    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"Content-Type: application/json\" -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/namespaces/default/pods\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031439    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"Content-Type: application/json\" -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/namespaces/default/pods\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031799    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031802    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031814    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031819    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031800    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031841    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033798    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033828    6562 kubelet.go:1626] Mirror pod not available\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033824    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033852    6562 kubelet.go:1626] Mirror pod not available\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033930    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033960    6562 kubelet.go:1626] Mirror pod not available\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.034347    6562 volumes.go:114] Used volume plugin \"kubernetes.io/empty-dir\" for varetcd\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.034395    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd]\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.044950    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.046068    6562 volumes.go:114] Used volume plugin \"kubernetes.io/empty-dir\" for data\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.046118    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data]\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.053737    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078701    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078723    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078731    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078738    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078745    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078752    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078758    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078765    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078772    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078778    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078785    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.079802    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.141312    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.171516    6562 manager.go:339] Container inspect result: {ID:40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Created:2016-02-02 19:45:35.144413858 +0000 UTC Path:/usr/local/bin/etcd Args:[--listen-client-urls=http://127.0.0.1:4001 --advertise-client-urls=http://127.0.0.1:4001 --data-dir=/var/etcd/data] Config:0xc20824b380 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:772 ExitCode:0 Error: StartedAt:2016-02-02 19:45:35.353220983 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:fbea2d67e6339d5aac386091030eb8c5bd7c82e9f0a3d29d4254dd4ed6f725d5 Node:<nil> NetworkSettings:0xc20866f300 SysInitPath: ResolvConfPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/resolv.conf HostnamePath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hostname HostsPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hosts LogPath:/var/lib/docker/containers/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540-json.log Name:/k8s_etcd.7e452b0b_k8s-etcd-127.0.0.1_default_e171033ec56ced6f29a4417f8b61cbf0_361132f4 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd Destination:/var/etcd Mode: RW:true} {Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/containers/etcd/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208318280 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.171926    6562 manager.go:339] Container inspect result: {ID:1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Created:2016-02-02 21:12:56.37330813 +0000 UTC Path:/hyperkube Args:[proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=\"\"] Config:0xc20876e340 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4969 ExitCode:0 Error: StartedAt:2016-02-02 21:12:56.66700108 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc208338800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03-json.log Name:/k8s_kube-proxy.4e393dc3_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_07ffcf9b Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e513e62ea641585218de8b3495fc7a21/containers/kube-proxy/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc2084cd180 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.172408    6562 manager.go:339] Container inspect result: {ID:db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Created:2016-02-02 21:14:59.392327351 +0000 UTC Path:/setup-files.sh Args:[] Config:0xc2084bd1e0 State:{Running:false Paused:false Restarting:false OOMKilled:false Pid:0 ExitCode:1 Error: StartedAt:2016-02-02 21:14:59.601273593 +0000 UTC FinishedAt:2016-02-02 21:15:00.138896081 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:<nil> NetworkSettings:0xc20846c800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/resolv.conf HostnamePath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hostname HostsPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hosts LogPath:/var/lib/docker/containers/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f-json.log Name:/k8s_setup.7df70a9a_k8s-master-127.0.0.1_default_eaf8ef5e21f965406a198841c5faa403_f5904769 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data Destination:/data Mode: RW:true} {Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/containers/setup/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208206500 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.175011    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.231588    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.232542    6562 manager.go:339] Container inspect result: {ID:212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455 Created:2016-02-02 21:12:55.664721048 +0000 UTC Path:/pause Args:[] Config:0xc208036820 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4952 ExitCode:0 Error: StartedAt:2016-02-02 21:12:55.935180921 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:8950680a606cf7a0b7916dbf4a435b35d28d75c705999847eddb5ed38eb53204 Node:<nil> NetworkSettings:0xc20866f500 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455-json.log Name:/k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_32c906a6 Driver:aufs Mounts:[] Volumes:map[] VolumesRW:map[] HostConfig:0xc208319180 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.232693    6562 manager.go:1630] Syncing Pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\": &{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:k8s-proxy-127.0.0.1 GenerateName: Namespace:default SelfLink:/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default UID:e513e62ea641585218de8b3495fc7a21 ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[kubernetes.io/config.source:file kubernetes.io/config.seen:2016-02-02T21:15:54.504500123Z kubernetes.io/config.hash:e513e62ea641585218de8b3495fc7a21]} Spec:{Volumes:[] Containers:[{Name:kube-proxy Image:gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6 Command:[/hyperkube proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=\"\"] Args:[] WorkingDir: Ports:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[] LivenessProbe:<nil> ReadinessProbe:<nil> Lifecycle:<nil> TerminationMessagePath:/dev/termination-log ImagePullPolicy:IfNotPresent SecurityContext:0xc208422600 Stdin:false StdinOnce:false TTY:false}] RestartPolicy:Always TerminationGracePeriodSeconds:0xc20841d548 ActiveDeadlineSeconds:<nil> DNSPolicy:ClusterFirst NodeSelector:map[] ServiceAccountName: NodeName:127.0.0.1 SecurityContext:0xc2083b8080 ImagePullSecrets:[]} Status:{Phase: Conditions:[] Message: Reason: HostIP: PodIP: StartTime:<nil> ContainerStatuses:[]}}\n\n\nwith nsenter modification\ndocker-compose file:\nmaster:\n  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\n  net: host\n  pid: host\n  privileged: true\n  volumes:\n    - /sys:/sys:ro\n    - /dev:/dev\n    - /var/lib/docker/:/var/lib/docker:rw\n    - /var/lib/kubelet/:/var/lib/kubelet:rw\n    - /var/run:/var/run:rw\n  command: ['nsenter', '--target=1', '--mount', '--wd=.', '--', './hyperkube', 'kubelet', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local']\n\nlogs:\nAttaching to aiok8s_master_1\nserver.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.\nserver.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.\nplugins.go:71] No cloud provider specified.\nmanager.go:128] cAdvisor running in container: \"/docker-daemon/docker/307029613f92612249c358cfcb9796055c4d264503e63f3806dac25175a10177\"\nfs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/ major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/mystore major:202 minor:66 fsType: blockSize:0}]\nmachine.go:93] Failed to get system UUID: open /etc/machine-id: no such file or directory\nmanager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID:1463833d523b452349b56f17534ffabe SystemUUID: BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvda1 Capacity:42140499968} {Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968}] DiskMap:map[43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\nmanager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Ubuntu 14.04.3 LTS DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\nserver.go:636] Adding manifest file: etc/kubernetes/manifests\nserver.go:646] Watching apiserver\nmanager.go:191] Setting dockerRoot to /var/lib/docker\nplugins.go:56] Registering credential provider: .dockercfg\nserver.go:608] Started kubelet\nkubelet.go:868] Image garbage collection failed: unable to find data for container /\nserver.go:104] Starting to listen on 0.0.0.0:10250\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nkubelet.go:898] Running in container \"/kubelet\"\nmanager.go:124] Starting to sync pod status with apiserver\nkubelet.go:2246] Starting kubelet main sync loop.\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nfactory.go:245] Registering Docker factory\nfactory.go:94] Registering Raw factory\nmanager.go:1005] Started watching for new ooms in manager\noomparser.go:198] OOM parser using kernel log file: \"/var/log/kern.log\"\nmanager.go:249] Starting recovery of all containers\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:254] Recovery completed\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 1\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=controller-manager pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 3\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available",
        "bodyHTML": "<p>Following the all-in-one install via kubelet on Docker as listed in the docs is not working. I hit a \"no cloud provider specified\" and the controller and apiserver never come up and am plagued with many connection refused errors when hitting 127.0.0.1:8080 even though I set KUBERNETES_PROVIDER <a href=\"https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md#run-it\">https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md#run-it</a></p>\n<p>I'm running on an Ubuntu VM and don't need cloud provider resources like the LoadBalancer type for Services - I just want a local dev k8s environment.</p>\n<p>Here is a snippet of the logs for the mod that enables nsenter (seeing how there were issues with Secrets not working in hyperkube - see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"123685826\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19069\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/kubernetes/kubernetes/pull/19069/hovercard\" href=\"https://github.com/kubernetes/kubernetes/pull/19069\">#19069</a> for related info )</p>\n<hr>\n<p><strong>default</strong></p>\n<p>docker-compose file:</p>\n<pre><code>master:\n  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\n  net: host\n  pid: host\n  privileged: true\n  volumes:\n    - /:/rootfs:ro\n    - /sys:/sys:ro\n    - /dev:/dev\n    - /var/lib/docker/:/var/lib/docker:rw\n    - /var/lib/kubelet/:/var/lib/kubelet:rw\n    - /var/run:/var/run:rw\n  command: ['./hyperkube', 'kubelet', '--containerized', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local', '--allow-privileged=true', '--v=10']\n</code></pre>\n<p>logs:</p>\n<pre><code>Attaching to aiok8s_master_1\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:53.489776    6562 server.go:132] Running kubelet in containerized mode (experimental)\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.248665    6562 server.go:351] Using self-signed cert (/var/run/kubernetes/kubelet.crt, /var/run/kubernetes/kubelet.key)\n\ufffd[36mmaster_1 | \ufffd[0mW0202 21:15:54.248985    6562 server.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.\n\ufffd[36mmaster_1 | \ufffd[0mW0202 21:15:54.249026    6562 server.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.249194    6562 plugins.go:71] No cloud provider specified.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.249212    6562 server.go:289] Successfully initialized cloud provider: \"\" from the config file: \"\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.249332    6562 manager.go:128] cAdvisor running in container: \"/docker-daemon/docker/808781db3c41aa36cafc5229b705611b694d581a5c661f03094a875dd9a8ff6e\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.465357    6562 fs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/rootfs major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/rootfs/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/rootfs/mystore major:202 minor:66 fsType: blockSize:0}]\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.494458    6562 machine.go:50] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.494534    6562 manager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID: SystemUUID:7ae5ca5c485b47b88d89d96a71601d80 BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968} {Device:/dev/xvda1 Capacity:42140499968}] DiskMap:map[43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline} 43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.495665    6562 manager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496462    6562 server.go:312] Using root directory: /var/lib/kubelet\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496691    6562 server.go:571] Sending events to api server.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496862    6562 server.go:636] Adding manifest file: etc/kubernetes/manifests\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496911    6562 file.go:47] Watching path \"etc/kubernetes/manifests\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.496925    6562 server.go:646] Watching apiserver\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.497132    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/etcd.json\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&amp;resourceVersion=0\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.500812    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/services?resourceVersion=0\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.501520    6562 round_trippers.go:261] curl -k -v -XGET  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&amp;resourceVersion=0\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.501968    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D127.0.0.1&amp;resourceVersion=0  in 1 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502198    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502042    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/services?resourceVersion=0  in 1 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502666    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502225    6562 common.go:52] Generated UID \"e171033ec56ced6f29a4417f8b61cbf0\" pod \"k8s-etcd\" from etc/kubernetes/manifests/etcd.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502880    6562 common.go:56] Generated Name \"k8s-etcd-127.0.0.1\" for UID \"e171033ec56ced6f29a4417f8b61cbf0\" from URL etc/kubernetes/manifests/etcd.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.502892    6562 common.go:61] Using namespace \"default\" for pod \"k8s-etcd-127.0.0.1\" from etc/kubernetes/manifests/etcd.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503109    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/kube-proxy.json\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503393    6562 common.go:52] Generated UID \"e513e62ea641585218de8b3495fc7a21\" pod \"k8s-proxy\" from etc/kubernetes/manifests/kube-proxy.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503419    6562 common.go:56] Generated Name \"k8s-proxy-127.0.0.1\" for UID \"e513e62ea641585218de8b3495fc7a21\" from URL etc/kubernetes/manifests/kube-proxy.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503429    6562 common.go:61] Using namespace \"default\" for pod \"k8s-proxy-127.0.0.1\" from etc/kubernetes/manifests/kube-proxy.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503514    6562 file.go:135] Reading config file \"etc/kubernetes/manifests/master.json\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.503983    6562 common.go:52] Generated UID \"eaf8ef5e21f965406a198841c5faa403\" pod \"k8s-master\" from etc/kubernetes/manifests/master.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504018    6562 common.go:56] Generated Name \"k8s-master-127.0.0.1\" for UID \"eaf8ef5e21f965406a198841c5faa403\" from URL etc/kubernetes/manifests/master.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504029    6562 common.go:61] Using namespace \"default\" for pod \"k8s-master-127.0.0.1\" from etc/kubernetes/manifests/master.json\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504172    6562 config.go:265] Setting pods for source file\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504433    6562 config.go:412] Receiving a new pod \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504492    6562 config.go:412] Receiving a new pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.504507    6562 config.go:412] Receiving a new pod \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.505051    6562 round_trippers.go:280] GET http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D127.0.0.1&amp;resourceVersion=0  in 3 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.505072    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.708688    6562 manager.go:191] Setting dockerRoot to /var/lib/docker\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.759597    6562 plugins.go:56] Registering credential provider: .dockercfg\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772006    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/aws-ebs\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772056    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/empty-dir\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772081    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/gce-pd\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772103    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/git-repo\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772125    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/host-path\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772149    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/nfs\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772170    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/secret\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772185    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/iscsi\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772204    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/glusterfs\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772227    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/persistent-claim\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772242    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/rbd\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772263    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/cinder\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772284    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/cephfs\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772327    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/downward-api\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772351    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/fc\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772373    6562 plugins.go:273] Loaded volume plugin \"kubernetes.io/flocker\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772498    6562 server.go:608] Started kubelet\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.772569    6562 server.go:104] Starting to listen on 0.0.0.0:10250\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:54.772810    6562 kubelet.go:868] Image garbage collection failed: unable to find data for container /\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773100    6562 request.go:546] Request Body: {\"kind\":\"Event\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"127.0.0.1.142f3c86e792e0e6\",\"namespace\":\"default\",\"creationTimestamp\":null},\"involvedObject\":{\"kind\":\"Node\",\"name\":\"127.0.0.1\",\"uid\":\"127.0.0.1\"},\"reason\":\"Starting\",\"message\":\"Starting kubelet.\",\"source\":{\"component\":\"kubelet\",\"host\":\"127.0.0.1\"},\"firstTimestamp\":\"2016-02-02T21:15:54Z\",\"lastTimestamp\":\"2016-02-02T21:15:54Z\",\"count\":1,\"type\":\"Normal\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773187    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" -H \"Content-Type: application/json\" http://localhost:8080/api/v1/namespaces/default/events\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773216    6562 server.go:121] Starting to listen read-only on 0.0.0.0:10255\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773351    6562 server.go:569] Event(api.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"127.0.0.1\", UID:\"127.0.0.1\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason: 'Starting' Starting kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773845    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/events  in 0 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.773867    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:54.774060    6562 event.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781503    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781526    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781535    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781542    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781549    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781556    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781562    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781569    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781575    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781582    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.781589    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.784869    6562 kubelet.go:898] Running in container \"/kubelet\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.788967    6562 container_manager_linux.go:207] Configure resource-only container /docker-daemon with memory limit: 11043009331\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.789028    6562 manager.go:124] Starting to sync pod status with apiserver\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.789054    6562 kubelet.go:2246] Starting kubelet main sync loop.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.792327    6562 generic.go:106] GenericPLEG: Relisting\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.819530    6562 kubelet.go:2278] SyncLoop (ADD, \"file\"): \"\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820696    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820720    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820728    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820735    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820743    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820749    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820756    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820763    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820770    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820776    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820783    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820971    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.820996    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821006    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821016    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821024    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/b98145a18fd89935c205528d910e094b6c178ec657f1be1637f922949dc2cd7d: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821034    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/3baa25cf80a8301be5f04d31d181b17023c23ed0e4c7a36df72f174b8b74896f: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821042    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/e94c8a2adb00d32038bde655ce418d482fe3810b81465e26395b2f2abb767047: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821051    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/8da0f1f1d20e5f3b00b0b0bfa994e8682a7b48f294453c15455fa88b1d6ca90b: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821060    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f98816a30a176e1dc9da2d980e4f961e2fffa056d109fe099af5dc6233edc540: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821070    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/c3f9389f06f5c520d0e57a197b284d5af182dad8f6d1f90e179a22edbbf6e132: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821079    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/6882bb8bc6e182316fe58110d3586738e9479a288604fe73396176151595024d: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821088    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/ea35c7a7f264f71ff0c7fc8d724cda79d272994f4dcb841d08944c6dc39d3528: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821096    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/351e59a4c698f8412365798aa8fce50e9029c22d3065f9531e2d9e4be5e3720c: unknown -&gt; running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821105    6562 generic.go:138] GenericPLEG: eaf8ef5e21f965406a198841c5faa403/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d: unknown -&gt; running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821116    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03: unknown -&gt; running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821126    6562 generic.go:138] GenericPLEG: e513e62ea641585218de8b3495fc7a21/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455: unknown -&gt; running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821134    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540: unknown -&gt; running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821144    6562 generic.go:138] GenericPLEG: e171033ec56ced6f29a4417f8b61cbf0/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15: unknown -&gt; running\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821154    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/6ba5339964aec260e37272fb2d67597a170f2bd203e09e3b7736b72684814f31: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821162    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/e45b33bd1c357e7a18b8f73ba755c568e7563aa854b3351595e0fadbb59b5748: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821171    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/54b9ec78c213e0309daf88c259138a3d4bf5e1ed6309bd63503c666f65105064: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.821180    6562 generic.go:138] GenericPLEG: e1376f76a07b85e8b0e4c363ff0fa6c1/5a3cdba1e62db46b6c5440c443439571bfeb5720032c251e3ce7aaac764fec10: unknown -&gt; exited\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.851415    6562 kubelet.go:2301] SyncLoop (PLEG): ignore irrelevant event: &amp;pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.880172    6562 kubelet.go:2281] SyncLoop (UPDATE, \"file\"): \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0), k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21), k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:54.952533    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &amp;pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"7156b3867d55872accfa7003f92fef1aae99d2351867721e9375784063360f18\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.029929    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.030066    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.030158    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.030284    6562 kubelet.go:1619] Creating a mirror pod for static pod \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\"\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031309    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-proxy-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default\",\"uid\":\"e513e62ea641585218de8b3495fc7a21\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"e513e62ea641585218de8b3495fc7a21\",\"kubernetes.io/config.mirror\":\"e513e62ea641585218de8b3495fc7a21\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504500123Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"containers\":[{\"name\":\"kube-proxy\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"proxy\",\"--master=http://127.0.0.1:8080\",\"--v=2\",\"--resource-container=\\\"\\\"\"],\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\",\"securityContext\":{\"privileged\":true}}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031354    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-etcd-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-etcd-127.0.0.1/default\",\"uid\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"kubernetes.io/config.mirror\":\"e171033ec56ced6f29a4417f8b61cbf0\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504450902Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"volumes\":[{\"name\":\"varetcd\",\"emptyDir\":{}}],\"containers\":[{\"name\":\"etcd\",\"image\":\"gcr.io/google_containers/etcd:2.2.1\",\"command\":[\"/usr/local/bin/etcd\",\"--listen-client-urls=http://127.0.0.1:4001\",\"--advertise-client-urls=http://127.0.0.1:4001\",\"--data-dir=/var/etcd/data\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"varetcd\",\"mountPath\":\"/var/etcd\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031372    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" -H \"Content-Type: application/json\" http://localhost:8080/api/v1/namespaces/default/pods\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031378    6562 request.go:546] Request Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"k8s-master-127.0.0.1\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/pods/namespaces/k8s-master-127.0.0.1/default\",\"uid\":\"eaf8ef5e21f965406a198841c5faa403\",\"creationTimestamp\":null,\"annotations\":{\"kubernetes.io/config.hash\":\"eaf8ef5e21f965406a198841c5faa403\",\"kubernetes.io/config.mirror\":\"eaf8ef5e21f965406a198841c5faa403\",\"kubernetes.io/config.seen\":\"2016-02-02T21:15:54.504513882Z\",\"kubernetes.io/config.source\":\"file\"}},\"spec\":{\"volumes\":[{\"name\":\"data\",\"emptyDir\":{}}],\"containers\":[{\"name\":\"controller-manager\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"controller-manager\",\"--master=127.0.0.1:8080\",\"--min-resync-period=3m\",\"--service-account-private-key-file=/srv/kubernetes/server.key\",\"--root-ca-file=/srv/kubernetes/ca.crt\",\"--v=2\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/srv/kubernetes\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"apiserver\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"apiserver\",\"--service-cluster-ip-range=10.0.0.1/24\",\"--insecure-bind-address=127.0.0.1\",\"--etcd-servers=http://127.0.0.1:4001\",\"--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota\",\"--client-ca-file=/srv/kubernetes/ca.crt\",\"--basic-auth-file=/srv/kubernetes/basic_auth.csv\",\"--min-request-timeout=300\",\"--tls-cert-file=/srv/kubernetes/server.cert\",\"--tls-private-key-file=/srv/kubernetes/server.key\",\"--token-auth-file=/srv/kubernetes/known_tokens.csv\",\"--allow-privileged=true\",\"--v=4\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/srv/kubernetes\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"scheduler\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/hyperkube\",\"scheduler\",\"--master=127.0.0.1:8080\",\"--v=2\"],\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"setup\",\"image\":\"gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\",\"command\":[\"/setup-files.sh\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/data\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeName\":\"127.0.0.1\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{}}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031432    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"Content-Type: application/json\" -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/namespaces/default/pods\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031439    6562 round_trippers.go:261] curl -k -v -XPOST  -H \"Content-Type: application/json\" -H \"User-Agent: hyperkube/v1.2.0 (linux/amd64) kubernetes/cf7d2af\" http://localhost:8080/api/v1/namespaces/default/pods\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031799    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031802    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031814    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031819    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031800    6562 round_trippers.go:280] POST http://localhost:8080/api/v1/namespaces/default/pods  in 0 milliseconds\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.031841    6562 round_trippers.go:286] Response Headers:\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033798    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033828    6562 kubelet.go:1626] Mirror pod not available\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033824    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033852    6562 kubelet.go:1626] Mirror pod not available\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033930    6562 kubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.033960    6562 kubelet.go:1626] Mirror pod not available\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.034347    6562 volumes.go:114] Used volume plugin \"kubernetes.io/empty-dir\" for varetcd\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.034395    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd]\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.044950    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.046068    6562 volumes.go:114] Used volume plugin \"kubernetes.io/empty-dir\" for data\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.046118    6562 nsenter_mount.go:174] findmnt command: nsenter [--mount=/rootfs/proc/1/ns/mnt -- /bin/findmnt -o target --noheadings --target /var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data]\n\ufffd[36mmaster_1 | \ufffd[0mE0202 21:15:55.053737    6562 nsenter_mount.go:179] Failed to nsenter mount, return file doesn't exist: exit status 1\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078701    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078723    6562 docker.go:344] Docker Container: /kubernetes_proxy_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078731    6562 docker.go:344] Docker Container: /kubernetes_etcd_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078738    6562 docker.go:344] Docker Container: /kubernetes_kube2sky_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078745    6562 docker.go:344] Docker Container: /kubernetes_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078752    6562 docker.go:344] Docker Container: /kubernetes_skydns_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078758    6562 docker.go:344] Docker Container: /tender_kilby is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078765    6562 docker.go:344] Docker Container: /lonely_darwin is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078772    6562 docker.go:344] Docker Container: /distracted_euclid is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078778    6562 docker.go:344] Docker Container: /angry_blackwell is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.078785    6562 docker.go:344] Docker Container: /discovery is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.079802    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.141312    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &amp;pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"5654bc0ca6347879e7f0f8f9e515f860e1f7c0b10466f36a28db632e5551eb05\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.171516    6562 manager.go:339] Container inspect result: {ID:40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Created:2016-02-02 19:45:35.144413858 +0000 UTC Path:/usr/local/bin/etcd Args:[--listen-client-urls=http://127.0.0.1:4001 --advertise-client-urls=http://127.0.0.1:4001 --data-dir=/var/etcd/data] Config:0xc20824b380 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:772 ExitCode:0 Error: StartedAt:2016-02-02 19:45:35.353220983 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:fbea2d67e6339d5aac386091030eb8c5bd7c82e9f0a3d29d4254dd4ed6f725d5 Node:&lt;nil&gt; NetworkSettings:0xc20866f300 SysInitPath: ResolvConfPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/resolv.conf HostnamePath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hostname HostsPath:/var/lib/docker/containers/6273439f792fd5751269eeb480f811619f567f9e9674ca027a23f5dc6036ad15/hosts LogPath:/var/lib/docker/containers/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540-json.log Name:/k8s_etcd.7e452b0b_k8s-etcd-127.0.0.1_default_e171033ec56ced6f29a4417f8b61cbf0_361132f4 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/volumes/kubernetes.io~empty-dir/varetcd Destination:/var/etcd Mode: RW:true} {Source:/var/lib/kubelet/pods/e171033ec56ced6f29a4417f8b61cbf0/containers/etcd/40f96c0fa31420594632aeebbe4498399591e1560ee3e53bf863f1abafa14540 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208318280 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.171926    6562 manager.go:339] Container inspect result: {ID:1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Created:2016-02-02 21:12:56.37330813 +0000 UTC Path:/hyperkube Args:[proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=\"\"] Config:0xc20876e340 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4969 ExitCode:0 Error: StartedAt:2016-02-02 21:12:56.66700108 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:&lt;nil&gt; NetworkSettings:0xc208338800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03-json.log Name:/k8s_kube-proxy.4e393dc3_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_07ffcf9b Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/e513e62ea641585218de8b3495fc7a21/containers/kube-proxy/1e948590da098f3badaf69634735c969da27581b5b1c9713161b0d8416fb1c03 Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc2084cd180 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.172408    6562 manager.go:339] Container inspect result: {ID:db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Created:2016-02-02 21:14:59.392327351 +0000 UTC Path:/setup-files.sh Args:[] Config:0xc2084bd1e0 State:{Running:false Paused:false Restarting:false OOMKilled:false Pid:0 ExitCode:1 Error: StartedAt:2016-02-02 21:14:59.601273593 +0000 UTC FinishedAt:2016-02-02 21:15:00.138896081 +0000 UTC} Image:3703c0c24d5ec2911736f0359436ef93634159a40638d748e8669e2a26b4b3dd Node:&lt;nil&gt; NetworkSettings:0xc20846c800 SysInitPath: ResolvConfPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/resolv.conf HostnamePath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hostname HostsPath:/var/lib/docker/containers/f16817ab3755b02ef03c50ffedcfa17381b9a17c1e496b2ed0b46e471331783d/hosts LogPath:/var/lib/docker/containers/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f-json.log Name:/k8s_setup.7df70a9a_k8s-master-127.0.0.1_default_eaf8ef5e21f965406a198841c5faa403_f5904769 Driver:aufs Mounts:[{Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/volumes/kubernetes.io~empty-dir/data Destination:/data Mode: RW:true} {Source:/var/lib/kubelet/pods/eaf8ef5e21f965406a198841c5faa403/containers/setup/db40f9ec3096c96547332eed039f890f4fcdb8a32f43ae7bc935f84c1065903f Destination:/dev/termination-log Mode: RW:true}] Volumes:map[] VolumesRW:map[] HostConfig:0xc208206500 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.175011    6562 docker.go:344] Docker Container: /aiok8s_master_1 is not managed by kubelet.\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.231588    6562 kubelet.go:2304] SyncLoop (PLEG): \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\", event: &amp;pleg.PodLifecycleEvent{ID:\"eaf8ef5e21f965406a198841c5faa403\", Type:\"ContainerDied\", Data:\"1435caee705a6726172a74ac9359a30e09278f431057de033f29c35e5de3be28\"}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.232542    6562 manager.go:339] Container inspect result: {ID:212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455 Created:2016-02-02 21:12:55.664721048 +0000 UTC Path:/pause Args:[] Config:0xc208036820 State:{Running:true Paused:false Restarting:false OOMKilled:false Pid:4952 ExitCode:0 Error: StartedAt:2016-02-02 21:12:55.935180921 +0000 UTC FinishedAt:0001-01-01 00:00:00 +0000 UTC} Image:8950680a606cf7a0b7916dbf4a435b35d28d75c705999847eddb5ed38eb53204 Node:&lt;nil&gt; NetworkSettings:0xc20866f500 SysInitPath: ResolvConfPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/resolv.conf HostnamePath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hostname HostsPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/hosts LogPath:/var/lib/docker/containers/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455/212a6979c9177338c817901297c19165d6c6c8f300f49d640562c7ebdbaa1455-json.log Name:/k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_e513e62ea641585218de8b3495fc7a21_32c906a6 Driver:aufs Mounts:[] Volumes:map[] VolumesRW:map[] HostConfig:0xc208319180 ExecIDs:[] RestartCount:0 AppArmorProfile:}\n\ufffd[36mmaster_1 | \ufffd[0mI0202 21:15:55.232693    6562 manager.go:1630] Syncing Pod \"k8s-proxy-127.0.0.1_default(e513e62ea641585218de8b3495fc7a21)\": &amp;{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:k8s-proxy-127.0.0.1 GenerateName: Namespace:default SelfLink:/api/v1/pods/namespaces/k8s-proxy-127.0.0.1/default UID:e513e62ea641585218de8b3495fc7a21 ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:&lt;nil&gt; DeletionGracePeriodSeconds:&lt;nil&gt; Labels:map[] Annotations:map[kubernetes.io/config.source:file kubernetes.io/config.seen:2016-02-02T21:15:54.504500123Z kubernetes.io/config.hash:e513e62ea641585218de8b3495fc7a21]} Spec:{Volumes:[] Containers:[{Name:kube-proxy Image:gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6 Command:[/hyperkube proxy --master=http://127.0.0.1:8080 --v=2 --resource-container=\"\"] Args:[] WorkingDir: Ports:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[] LivenessProbe:&lt;nil&gt; ReadinessProbe:&lt;nil&gt; Lifecycle:&lt;nil&gt; TerminationMessagePath:/dev/termination-log ImagePullPolicy:IfNotPresent SecurityContext:0xc208422600 Stdin:false StdinOnce:false TTY:false}] RestartPolicy:Always TerminationGracePeriodSeconds:0xc20841d548 ActiveDeadlineSeconds:&lt;nil&gt; DNSPolicy:ClusterFirst NodeSelector:map[] ServiceAccountName: NodeName:127.0.0.1 SecurityContext:0xc2083b8080 ImagePullSecrets:[]} Status:{Phase: Conditions:[] Message: Reason: HostIP: PodIP: StartTime:&lt;nil&gt; ContainerStatuses:[]}}\n</code></pre>\n<hr>\n<p><strong>with nsenter modification</strong></p>\n<p>docker-compose file:</p>\n<pre><code>master:\n  image: gcr.io/google_containers/hyperkube-amd64:v1.2.0-alpha.6\n  net: host\n  pid: host\n  privileged: true\n  volumes:\n    - /sys:/sys:ro\n    - /dev:/dev\n    - /var/lib/docker/:/var/lib/docker:rw\n    - /var/lib/kubelet/:/var/lib/kubelet:rw\n    - /var/run:/var/run:rw\n  command: ['nsenter', '--target=1', '--mount', '--wd=.', '--', './hyperkube', 'kubelet', '--hostname-override=127.0.0.1', '--address=0.0.0.0', '--api-servers=http://localhost:8080', '--config=etc/kubernetes/manifests', '--cluster-dns=10.0.0.10', '--cluster-domain=cluster.local']\n</code></pre>\n<p>logs:</p>\n<pre><code>Attaching to aiok8s_master_1\nserver.go:413] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.\nserver.go:374] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.\nplugins.go:71] No cloud provider specified.\nmanager.go:128] cAdvisor running in container: \"/docker-daemon/docker/307029613f92612249c358cfcb9796055c4d264503e63f3806dac25175a10177\"\nfs.go:105] Filesystem partitions: map[/dev/xvda1:{mountpoint:/ major:202 minor:1 fsType: blockSize:0} /dev/xvde1:{mountpoint:/var major:202 minor:65 fsType: blockSize:0} /dev/xvde2:{mountpoint:/mystore major:202 minor:66 fsType: blockSize:0}]\nmachine.go:93] Failed to get system UUID: open /etc/machine-id: no such file or directory\nmanager.go:163] Machine: {NumCores:4 CpuFrequency:2593816 MemoryCapacity:15775727616 MachineID:1463833d523b452349b56f17534ffabe SystemUUID: BootID:9001093f-1aae-4506-807f-5916e4feb16f Filesystems:[{Device:/dev/xvda1 Capacity:42140499968} {Device:/dev/xvde1 Capacity:75962195968} {Device:/dev/xvde2 Capacity:75962195968}] DiskMap:map[43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:deadline} 43:192:{Name:nbd12 Major:43 Minor:192 Size:0 Scheduler:deadline} 43:224:{Name:nbd14 Major:43 Minor:224 Size:0 Scheduler:deadline} 43:48:{Name:nbd3 Major:43 Minor:48 Size:0 Scheduler:deadline} 43:64:{Name:nbd4 Major:43 Minor:64 Size:0 Scheduler:deadline} 43:160:{Name:nbd10 Major:43 Minor:160 Size:0 Scheduler:deadline} 43:80:{Name:nbd5 Major:43 Minor:80 Size:0 Scheduler:deadline} 43:112:{Name:nbd7 Major:43 Minor:112 Size:0 Scheduler:deadline} 43:208:{Name:nbd13 Major:43 Minor:208 Size:0 Scheduler:deadline} 43:96:{Name:nbd6 Major:43 Minor:96 Size:0 Scheduler:deadline} 202:0:{Name:xvda Major:202 Minor:0 Size:42949672960 Scheduler:deadline} 43:16:{Name:nbd1 Major:43 Minor:16 Size:0 Scheduler:deadline} 43:176:{Name:nbd11 Major:43 Minor:176 Size:0 Scheduler:deadline} 43:240:{Name:nbd15 Major:43 Minor:240 Size:0 Scheduler:deadline} 43:32:{Name:nbd2 Major:43 Minor:32 Size:0 Scheduler:deadline} 43:128:{Name:nbd8 Major:43 Minor:128 Size:0 Scheduler:deadline} 43:144:{Name:nbd9 Major:43 Minor:144 Size:0 Scheduler:deadline} 202:64:{Name:xvde Major:202 Minor:64 Size:161061273600 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:bc:76:4e:10:f4:f3 Speed:0 Mtu:1500} {Name:eth1 MacAddress:bc:76:4e:11:82:a7 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:15775727616 Cores:[{Id:0 Threads:[0 1 2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\nmanager.go:169] Version: {KernelVersion:3.13.0-71-generic ContainerOsVersion:Ubuntu 14.04.3 LTS DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\nserver.go:636] Adding manifest file: etc/kubernetes/manifests\nserver.go:646] Watching apiserver\nmanager.go:191] Setting dockerRoot to /var/lib/docker\nplugins.go:56] Registering credential provider: .dockercfg\nserver.go:608] Started kubelet\nkubelet.go:868] Image garbage collection failed: unable to find data for container /\nserver.go:104] Starting to listen on 0.0.0.0:10250\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nkubelet.go:898] Running in container \"/kubelet\"\nmanager.go:124] Starting to sync pod status with apiserver\nkubelet.go:2246] Starting kubelet main sync loop.\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nfactory.go:245] Registering Docker factory\nfactory.go:94] Registering Raw factory\nmanager.go:1005] Started watching for new ooms in manager\noomparser.go:198] OOM parser using kernel log file: \"/var/log/kern.log\"\nmanager.go:249] Starting recovery of all containers\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:254] Recovery completed\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 1\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 10s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 2\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\nmanager.go:1980] Back-off 10s restarting failed container=controller-manager pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=apiserver pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\nmanager.go:1980] Back-off 20s restarting failed container=setup pod=k8s-master-127.0.0.1_default(eaf8ef5e21f965406a198841c5faa403)\npod_workers.go:125] Error syncing pod eaf8ef5e21f965406a198841c5faa403, skipping: not all containers have started: 0 != 3\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nevent.go:201] Unable to write event: 'Post http://localhost:8080/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8080: connection refused' (may retry after sleeping)\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-etcd-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nmanager.go:390] Failed to update status for pod \"_()\": Get http://localhost:8080/api/v1/namespaces/default/pods/k8s-master-127.0.0.1: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1621] Failed creating a mirror pod for \"k8s-etcd-127.0.0.1_default(e171033ec56ced6f29a4417f8b61cbf0)\": Post http://localhost:8080/api/v1/namespaces/default/pods: dial tcp 127.0.0.1:8080: connection refused\nkubelet.go:1626] Mirror pod not available\n\n</code></pre>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "nikhiljindal" },
        "number": 15910,
        "resourcePath": "/kubernetes/kubernetes/issues/15910",
        "state": "CLOSED",
        "publishedAt": "2015-10-20T00:08:54Z",
        "closedAt": "2018-02-15T10:33:03Z",
        "title": "Cross links dont work in api reference docs when viewed using htmlpreview",
        "bodyText": "For ex: links in https://htmlpreview.github.io/?https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/api-reference/extensions/v1beta1/definitions.html dont work when they point to docs outside this one.\nLinks work fine when the same doc is viewed on kubernetes.io (ex: http://kubernetes.io/v1.0/docs/api-reference/definitions.html).\nLooks like a bug in htmlpreview. Need to figure out a way to fix this.\ncc @caesarxuchao",
        "bodyHTML": "<p>For ex: links in <a rel=\"nofollow\" href=\"https://htmlpreview.github.io/?https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/api-reference/extensions/v1beta1/definitions.html\">https://htmlpreview.github.io/?https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/api-reference/extensions/v1beta1/definitions.html</a> dont work when they point to docs outside this one.</p>\n<p>Links work fine when the same doc is viewed on kubernetes.io (ex: <a rel=\"nofollow\" href=\"http://kubernetes.io/v1.0/docs/api-reference/definitions.html\">http://kubernetes.io/v1.0/docs/api-reference/definitions.html</a>).</p>\n<p>Looks like a bug in htmlpreview. Need to figure out a way to fix this.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/caesarxuchao/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/caesarxuchao\">@caesarxuchao</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": null,
        "number": 19416,
        "resourcePath": "/kubernetes/kubernetes/issues/19416",
        "state": "CLOSED",
        "publishedAt": "2016-01-08T17:36:13Z",
        "closedAt": "2016-02-26T18:16:51Z",
        "title": "Add Ubernetes Lite e2e test scheduling pods with attached volumes into correct zone",
        "bodyText": "Part of #19413",
        "bodyHTML": "<p>Part of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"125654309\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/19413\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/19413/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/19413\">#19413</a></p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "thockin" },
        "number": 6265,
        "resourcePath": "/kubernetes/kubernetes/issues/6265",
        "state": "CLOSED",
        "publishedAt": "2015-04-01T01:09:15Z",
        "closedAt": "2015-04-27T23:18:52Z",
        "title": "DeepHashObject should probably set DisableMethods and DisablePointerMethods for spew",
        "bodyText": "For max consistency.  Some String() methods might hash collide.",
        "bodyHTML": "<p>For max consistency.  Some String() methods might hash collide.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "sacashgit" },
        "number": 6667,
        "resourcePath": "/kubernetes/kubernetes/issues/6667",
        "state": "CLOSED",
        "publishedAt": "2015-04-10T00:07:13Z",
        "closedAt": "2015-04-28T23:43:00Z",
        "title": "Service Name Resolution is not working",
        "bodyText": "Hi There\nI have a ubuntu multi-node cluster set up - running. I am able to create pods, controllers and services at will. But I am facing the issue where service name resolution is not working. As part of the guestbook example, I have created redis-slave, redis-master and frontend services. But when containers are trying to talk to each other through this service interface, it is not working.\nE.g\n\nWhen PHP application try and talk to redis-slave:6379 - it fails (This call dont get routed any where)\nWhen redis slave try to synch with master using redis-master , it doesnt work.\nWhen I try and ping redis-master from inside the container, it comes back with unknown host error.\n\nSome logs to show you the problem\n[8] 10 Apr 00:03:57.902 # Unable to connect to MASTER: Connection timed out\n[8] 10 Apr 00:03:58.905 * Connecting to MASTER redis-master:6379\n[8] 10 Apr 00:03:58.909 # Unable to connect to MASTER: Connection timed out\n[8] 10 Apr 00:03:59.913 * Connecting to MASTER redis-master:6379\nTemporary failure in name resolution [tcp://redis-slave:6379]' in /vendor/predis/predis/lib/Predis/Connection/AbstractConnection.php:141\nI do see Kube-Proxy is making the right iptable entries.\nI0409 13:04:47.690367   20353 proxier.go:556] Opened iptables from-containers portal for service \"redis-master\" on TCP 11.1.1.67:6379\nI0409 13:04:47.696276   20353 proxier.go:567] Opened iptables from-host portal for service \"redis-master\" on TCP 11.1.1.67:6379\nI0409 13:11:50.223702   20353 proxier.go:556] Opened iptables from-containers portal for service \"redis-slave\" on TCP 11.1.1.55:6379\nI0409 13:11:50.252093   20353 proxier.go:567] Opened iptables from-host portal for service \"redis-slave\" on TCP 11.1.1.55:6379\nI0409 13:14:17.024773   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 11.1.1.58:8000\nI0409 13:14:17.034296   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 11.1.1.58:8000\nI0409 13:14:17.046845   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 10.64.80.83:8000\nI0409 13:14:17.057679   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 10.64.80.83:8000\nI0409 13:14:17.067562   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 10.64.80.84:8000\nI0409 13:14:17.077085   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 10.64.80.84:8000\nDo you know what is missing in the set up which is causing this problem ? I do not have any DNS (skydns) running here and think that it is desirable but not needed.\nWhat might be going wrong here ?",
        "bodyHTML": "<p>Hi There</p>\n<p>I have a ubuntu multi-node cluster set up - running. I am able to create pods, controllers and services at will. But I am facing the issue where service name resolution is not working. As part of the guestbook example, I have created redis-slave, redis-master and frontend services. But when containers are trying to talk to each other through this service interface, it is not working.</p>\n<p>E.g</p>\n<ol>\n<li>When PHP application try and talk to redis-slave:6379 - it fails (This call dont get routed any where)</li>\n<li>When redis slave try to synch with master using redis-master , it doesnt work.</li>\n<li>When I try and ping redis-master from inside the container, it comes back with unknown host error.</li>\n</ol>\n<p>Some logs to show you the problem</p>\n<p>[8] 10 Apr 00:03:57.902 # Unable to connect to MASTER: Connection timed out<br>\n[8] 10 Apr 00:03:58.905 * Connecting to MASTER redis-master:6379<br>\n[8] 10 Apr 00:03:58.909 # Unable to connect to MASTER: Connection timed out<br>\n[8] 10 Apr 00:03:59.913 * Connecting to MASTER redis-master:6379</p>\n<p>Temporary failure in name resolution [tcp://redis-slave:6379]' in /vendor/predis/predis/lib/Predis/Connection/AbstractConnection.php:141</p>\n<p>I do see Kube-Proxy is making the right iptable entries.<br>\nI0409 13:04:47.690367   20353 proxier.go:556] Opened iptables from-containers portal for service \"redis-master\" on TCP 11.1.1.67:6379<br>\nI0409 13:04:47.696276   20353 proxier.go:567] Opened iptables from-host portal for service \"redis-master\" on TCP 11.1.1.67:6379<br>\nI0409 13:11:50.223702   20353 proxier.go:556] Opened iptables from-containers portal for service \"redis-slave\" on TCP 11.1.1.55:6379<br>\nI0409 13:11:50.252093   20353 proxier.go:567] Opened iptables from-host portal for service \"redis-slave\" on TCP 11.1.1.55:6379<br>\nI0409 13:14:17.024773   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 11.1.1.58:8000<br>\nI0409 13:14:17.034296   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 11.1.1.58:8000<br>\nI0409 13:14:17.046845   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 10.64.80.83:8000<br>\nI0409 13:14:17.057679   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 10.64.80.83:8000<br>\nI0409 13:14:17.067562   20353 proxier.go:556] Opened iptables from-containers portal for service \"frontend\" on TCP 10.64.80.84:8000<br>\nI0409 13:14:17.077085   20353 proxier.go:567] Opened iptables from-host portal for service \"frontend\" on TCP 10.64.80.84:8000</p>\n<p>Do you know what is missing in the set up which is causing this problem ? I do not have any DNS (skydns) running here and think that it is desirable but not needed.</p>\n<p>What might be going wrong here ?</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "alexanderguzhva" },
        "number": 13633,
        "resourcePath": "/kubernetes/kubernetes/issues/13633",
        "state": "CLOSED",
        "publishedAt": "2015-09-07T03:47:37Z",
        "closedAt": "2015-09-08T21:14:42Z",
        "title": "vagrant-based cluster is broken",
        "bodyText": "I took the last version from github several days ago, installed without any problems\nhost machine\n[root@linux-c56a cluster]$ ./kubectl.sh version\nClient Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.1\", GitCommit:\"9bae6636d5fcf436a3b14d219ced195e10e21fe8\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.1\", GitCommit:\"9bae6636d5fcf436a3b14d219ced195e10e21fe8\", GitTreeState:\"clean\"}\n\nminion-1, everything is fine\n[root@kubernetes-minion-1 vagrant]# docker ps\nCONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n6603d4e83556        gcr.io/google_containers/heapster_grafana:v0.7    \"/kuisp -p 8080 -c /o\"   8 minutes ago       Up 8 minutes                                                         k8s_grafana.d03a4af7_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_576e5681\nc8fcb0448ffa        gcr.io/google_containers/heapster_influxdb:v0.3   \"/run.sh\"                9 minutes ago       Up 9 minutes                                                         k8s_influxdb.e4e63a11_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_4b7be40b\n097522fb0214        gcr.io/google_containers/kube2sky:1.11            \"/kube2sky -domain=cl\"   10 minutes ago      Up 10 minutes                                                        k8s_kube2sky.9e5dd6c0_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_88c205c6\ncd70ece47d5f        gcr.io/google_containers/etcd:2.0.9               \"/usr/local/bin/etcd \"   10 minutes ago      Up 10 minutes                                                        k8s_etcd.f2082b87_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_2fac6c01\nb2065f47fd00        gcr.io/google_containers/exechealthz:1.0          \"/exechealthz '-cmd=n\"   10 minutes ago      Up 10 minutes                                                        k8s_healthz.75190edc_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a9976982\ne328d461b7c2        gcr.io/google_containers/skydns:2015-03-11-001    \"/skydns -machines=ht\"   10 minutes ago      Up 10 minutes                                                        k8s_skydns.8afa9a39_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_9444aa84\n336b74f063c9        gcr.io/google_containers/heapster:v0.17.0         \"/heapster --source=k\"   10 minutes ago      Up 10 minutes                                                        k8s_heapster.f0daed14_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_b0e636eb\nf9584b644df5        gcr.io/google_containers/kube-ui:v1.1             \"/kube-ui\"               10 minutes ago      Up 10 minutes                                                        k8s_kube-ui.3b8d5a14_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_8855cb76\nebbe49b70250        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes       0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp   k8s_POD.c5371ceb_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_d9581395\n81eae8e8fafe        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.6e934112_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a96650e1\nbc84d79785ae        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.9db2f941_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_249307cc\nec7f57f9ab52        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.7be6d81d_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_bf93650f\n\nminion-2. Completely empty\n[root@kubernetes-minion-2 vagrant]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\nminion-3. Completely empty\n[root@kubernetes-minion-3 vagrant]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\nok, let us run redis from the example (replication factor is 3). Here is what I see after 10 minutes of waiting\n[root@linux-c56a cluster]$ ./kubectl.sh get pods\nNAME                   READY     STATUS    RESTARTS   AGE\nredis-1btiw            0/1       Pending   0          48s\nredis-1lfi0            0/1       Pending   0          4m\nredis-333g6            0/1       Pending   0          3m\nredis-5hpg8            0/1       Pending   0          7m\nredis-5lsy5            0/1       Pending   0          17s\nredis-7be1q            0/1       Pending   0          4m\nredis-9jjwo            0/1       Pending   0          3m\nredis-aospw            0/1       Pending   0          48s\nredis-c96tg            0/1       Pending   0          14m\nredis-eoffa            0/1       Pending   0          7m\nredis-exgmi            0/1       Pending   0          11m\nredis-fkp2a            0/1       Pending   0          48s\nredis-fosf3            0/1       Pending   0          17s\nredis-l483e            0/1       Pending   0          3m\nredis-pc3bl            0/1       Pending   0          8m\nredis-sentinel-4bdy5   0/1       Pending   0          10m\nredis-sentinel-4ch8m   0/1       Pending   0          3m\nredis-sentinel-59x65   0/1       Pending   0          14m\nredis-sentinel-97a10   0/1       Pending   0          3m\nredis-sentinel-abw3f   0/1       Pending   0          11m\nredis-sentinel-bjyth   0/1       Pending   0          7m\nredis-sentinel-c94br   0/1       Pending   0          3m\nredis-sentinel-chkmu   0/1       Pending   0          7m\nredis-sentinel-dtc89   0/1       Pending   0          7m\nredis-sentinel-fxzrq   0/1       Pending   0          17s\nredis-sentinel-h4ixm   0/1       Pending   0          11m\nredis-sentinel-hph84   0/1       Pending   0          11m\nredis-sentinel-j1svl   0/1       Pending   0          7m\nredis-sentinel-kfx09   0/1       Pending   0          17s\nredis-sentinel-mp18j   0/1       Pending   0          7m\nredis-sentinel-t9t0v   0/1       Pending   0          10m\nredis-sentinel-tkyh9   0/1       Pending   0          3m\nredis-sentinel-udu4k   0/1       Pending   0          17s\nredis-sentinel-ukv95   0/1       Pending   0          10m\nredis-sentinel-xrniw   0/1       Pending   0          7m\nredis-sentinel-yhwad   0/1       Pending   0          3m\nredis-sentinel-ysawg   0/1       Pending   0          3m\nredis-slcg7            0/1       Pending   0          7m\nredis-spati            0/1       Pending   0          17s\nredis-taasu            0/1       Pending   0          11m\nredis-v6vif            0/1       Pending   0          8m\nredis-yqlbl            0/1       Pending   0          4m\nredis-yvc2u            0/1       Pending   0          11m\nredis-zkktb            0/1       Pending   0          8m",
        "bodyHTML": "<p>I took the last version from github several days ago, installed without any problems<br>\nhost machine</p>\n<pre><code>[root@linux-c56a cluster]$ ./kubectl.sh version\nClient Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.1\", GitCommit:\"9bae6636d5fcf436a3b14d219ced195e10e21fe8\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.1\", GitCommit:\"9bae6636d5fcf436a3b14d219ced195e10e21fe8\", GitTreeState:\"clean\"}\n</code></pre>\n<p>minion-1, everything is fine</p>\n<pre><code>[root@kubernetes-minion-1 vagrant]# docker ps\nCONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n6603d4e83556        gcr.io/google_containers/heapster_grafana:v0.7    \"/kuisp -p 8080 -c /o\"   8 minutes ago       Up 8 minutes                                                         k8s_grafana.d03a4af7_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_576e5681\nc8fcb0448ffa        gcr.io/google_containers/heapster_influxdb:v0.3   \"/run.sh\"                9 minutes ago       Up 9 minutes                                                         k8s_influxdb.e4e63a11_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_4b7be40b\n097522fb0214        gcr.io/google_containers/kube2sky:1.11            \"/kube2sky -domain=cl\"   10 minutes ago      Up 10 minutes                                                        k8s_kube2sky.9e5dd6c0_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_88c205c6\ncd70ece47d5f        gcr.io/google_containers/etcd:2.0.9               \"/usr/local/bin/etcd \"   10 minutes ago      Up 10 minutes                                                        k8s_etcd.f2082b87_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_2fac6c01\nb2065f47fd00        gcr.io/google_containers/exechealthz:1.0          \"/exechealthz '-cmd=n\"   10 minutes ago      Up 10 minutes                                                        k8s_healthz.75190edc_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a9976982\ne328d461b7c2        gcr.io/google_containers/skydns:2015-03-11-001    \"/skydns -machines=ht\"   10 minutes ago      Up 10 minutes                                                        k8s_skydns.8afa9a39_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_9444aa84\n336b74f063c9        gcr.io/google_containers/heapster:v0.17.0         \"/heapster --source=k\"   10 minutes ago      Up 10 minutes                                                        k8s_heapster.f0daed14_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_b0e636eb\nf9584b644df5        gcr.io/google_containers/kube-ui:v1.1             \"/kube-ui\"               10 minutes ago      Up 10 minutes                                                        k8s_kube-ui.3b8d5a14_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_8855cb76\nebbe49b70250        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes       0.0.0.0:8083-&gt;8083/tcp, 0.0.0.0:8086-&gt;8086/tcp   k8s_POD.c5371ceb_monitoring-influx-grafana-v1-v1hqr_kube-system_2803e698-535b-11e5-977a-080027fdddda_d9581395\n81eae8e8fafe        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.6e934112_kube-dns-v8-e4km3_kube-system_280396ad-535b-11e5-977a-080027fdddda_a96650e1\nbc84d79785ae        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.9db2f941_kube-ui-v1-d66nx_kube-system_28038284-535b-11e5-977a-080027fdddda_249307cc\nec7f57f9ab52        gcr.io/google_containers/pause:0.8.0              \"/pause\"                 10 minutes ago      Up 10 minutes                                                        k8s_POD.7be6d81d_monitoring-heapster-v8-dtp1l_kube-system_28036950-535b-11e5-977a-080027fdddda_bf93650f\n</code></pre>\n<p>minion-2. Completely empty</p>\n<pre><code>[root@kubernetes-minion-2 vagrant]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre>\n<p>minion-3. Completely empty</p>\n<pre><code>[root@kubernetes-minion-3 vagrant]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre>\n<p>ok, let us run redis from the example (replication factor is 3). Here is what I see after 10 minutes of waiting</p>\n<pre><code>[root@linux-c56a cluster]$ ./kubectl.sh get pods\nNAME                   READY     STATUS    RESTARTS   AGE\nredis-1btiw            0/1       Pending   0          48s\nredis-1lfi0            0/1       Pending   0          4m\nredis-333g6            0/1       Pending   0          3m\nredis-5hpg8            0/1       Pending   0          7m\nredis-5lsy5            0/1       Pending   0          17s\nredis-7be1q            0/1       Pending   0          4m\nredis-9jjwo            0/1       Pending   0          3m\nredis-aospw            0/1       Pending   0          48s\nredis-c96tg            0/1       Pending   0          14m\nredis-eoffa            0/1       Pending   0          7m\nredis-exgmi            0/1       Pending   0          11m\nredis-fkp2a            0/1       Pending   0          48s\nredis-fosf3            0/1       Pending   0          17s\nredis-l483e            0/1       Pending   0          3m\nredis-pc3bl            0/1       Pending   0          8m\nredis-sentinel-4bdy5   0/1       Pending   0          10m\nredis-sentinel-4ch8m   0/1       Pending   0          3m\nredis-sentinel-59x65   0/1       Pending   0          14m\nredis-sentinel-97a10   0/1       Pending   0          3m\nredis-sentinel-abw3f   0/1       Pending   0          11m\nredis-sentinel-bjyth   0/1       Pending   0          7m\nredis-sentinel-c94br   0/1       Pending   0          3m\nredis-sentinel-chkmu   0/1       Pending   0          7m\nredis-sentinel-dtc89   0/1       Pending   0          7m\nredis-sentinel-fxzrq   0/1       Pending   0          17s\nredis-sentinel-h4ixm   0/1       Pending   0          11m\nredis-sentinel-hph84   0/1       Pending   0          11m\nredis-sentinel-j1svl   0/1       Pending   0          7m\nredis-sentinel-kfx09   0/1       Pending   0          17s\nredis-sentinel-mp18j   0/1       Pending   0          7m\nredis-sentinel-t9t0v   0/1       Pending   0          10m\nredis-sentinel-tkyh9   0/1       Pending   0          3m\nredis-sentinel-udu4k   0/1       Pending   0          17s\nredis-sentinel-ukv95   0/1       Pending   0          10m\nredis-sentinel-xrniw   0/1       Pending   0          7m\nredis-sentinel-yhwad   0/1       Pending   0          3m\nredis-sentinel-ysawg   0/1       Pending   0          3m\nredis-slcg7            0/1       Pending   0          7m\nredis-spati            0/1       Pending   0          17s\nredis-taasu            0/1       Pending   0          11m\nredis-v6vif            0/1       Pending   0          8m\nredis-yqlbl            0/1       Pending   0          4m\nredis-yvc2u            0/1       Pending   0          11m\nredis-zkktb            0/1       Pending   0          8m\n</code></pre>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "AaronTao1990" },
        "number": 25792,
        "resourcePath": "/kubernetes/kubernetes/issues/25792",
        "state": "CLOSED",
        "publishedAt": "2016-05-18T07:31:01Z",
        "closedAt": "2016-05-19T01:13:08Z",
        "title": "failed to set up cluster via docker",
        "bodyText": "I am following this tutorial Installing a Kubernetes Master Node via Docker to set up a cluster via Dockar\u3002\nAfter setting up etcd and flannel. I tried to restart docker with --bip & --mtu option.\n\nYou now need to edit the docker configuration to activate new flags. Again, this is system specific.\nThis may be in /etc/default/docker or /etc/systemd/service/docker.service or it may be elsewhere.\nRegardless, you need to add the following to the docker command line:\n--bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}\n\nAnd after I ran\nps aux | grep docker\n\nI found my docker is already started with the right options:\nroot     55351  0.1  0.0 994216 34940 ?        Ssl  14:25   0:07 /usr/bin/docker daemon -H fd:// --bip=10.1.65.1/24 --mtu=8972\n\nBut when I tried to run hello-world procedure with the following command\nsudo docker run hello-world\n\nThe error occurs\ndocker: Error response from daemon: Container command '/hello' not found or does not exist..\n\nBTW, I can run hello-world find with --bip & --mtu removed.\nAny kind of help will be appreciated :)",
        "bodyHTML": "<p>I am following this tutorial <a href=\"http://kubernetes.io/docs/getting-started-guides/docker-multinode/master/\" rel=\"nofollow\">Installing a Kubernetes Master Node via Docker</a> to set up a cluster via Dockar\u3002</p>\n<p>After setting up etcd and flannel. I tried to restart docker with --bip &amp; --mtu option.</p>\n<blockquote>\n<p>You now need to edit the docker configuration to activate new flags. Again, this is system specific.<br>\nThis may be in /etc/default/docker or /etc/systemd/service/docker.service or it may be elsewhere.<br>\nRegardless, you need to add the following to the docker command line:<br>\n--bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}</p>\n</blockquote>\n<p>And after I ran</p>\n<pre><code>ps aux | grep docker\n</code></pre>\n<p>I found my docker is already started with the right options:</p>\n<pre><code>root     55351  0.1  0.0 994216 34940 ?        Ssl  14:25   0:07 /usr/bin/docker daemon -H fd:// --bip=10.1.65.1/24 --mtu=8972\n</code></pre>\n<p>But when I tried to run hello-world procedure with the following command</p>\n<pre><code>sudo docker run hello-world\n</code></pre>\n<p>The error occurs</p>\n<pre><code>docker: Error response from daemon: Container command '/hello' not found or does not exist..\n</code></pre>\n<p>BTW, I can run hello-world find with --bip &amp; --mtu removed.</p>\n<p>Any kind of help will be appreciated :)</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "LihuaWu" },
        "number": 32991,
        "resourcePath": "/kubernetes/kubernetes/issues/32991",
        "state": "CLOSED",
        "publishedAt": "2016-09-18T08:53:59Z",
        "closedAt": "2017-07-28T02:44:02Z",
        "title": "when using client-go library to communicate with master, client list pods with watch hang forever",
        "bodyText": "Problem: the following codes just hang and never returns\n clientset, err := kubernetes.NewForConfig(config)\n pods, err := clientset.Core().Pods(\"\").List(api.ListOptions{Watch:true})\n\nwhile codes like:\nwatch, err := clientset.Core().Pods(\"\").Watch(api.ListOptions{})\n\nwork as expected.\nIMHO,  the first use is kind of misleading.\n\nIs this a request for help? (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):\nWhat keywords did you search in Kubernetes issues before filing this one? (If you have found any duplicates, you should instead reply there.):\n\nIs this a BUG REPORT or FEATURE REQUEST? (choose one):\n\nKubernetes version (use kubectl version):\nEnvironment:\n\nCloud provider or hardware configuration:\nOS (e.g. from /etc/os-release):\nKernel (e.g. uname -a):\nInstall tools:\nOthers:\n\nWhat happened:\nWhat you expected to happen:\nHow to reproduce it (as minimally and precisely as possible):\nAnything else do we need to know:",
        "bodyHTML": "<p>Problem: the following codes just hang and never returns</p>\n<pre><code> clientset, err := kubernetes.NewForConfig(config)\n pods, err := clientset.Core().Pods(\"\").List(api.ListOptions{Watch:true})\n</code></pre>\n<p>while codes like:</p>\n<pre><code>watch, err := clientset.Core().Pods(\"\").Watch(api.ListOptions{})\n</code></pre>\n<p>work as expected.<br>\nIMHO,  the first use is kind of misleading.</p>\n\n<p><strong>Is this a request for help?</strong> (If yes, you should use our troubleshooting guide and community support channels, see <a rel=\"nofollow\" href=\"http://kubernetes.io/docs/troubleshooting/\">http://kubernetes.io/docs/troubleshooting/</a>.):</p>\n<p><strong>What keywords did you search in Kubernetes issues before filing this one?</strong> (If you have found any duplicates, you should instead reply there.):</p>\n<hr>\n<p><strong>Is this a BUG REPORT or FEATURE REQUEST?</strong> (choose one):</p>\n\n<p><strong>Kubernetes version</strong> (use <code>kubectl version</code>):</p>\n<p><strong>Environment</strong>:</p>\n<ul>\n<li><strong>Cloud provider or hardware configuration</strong>:</li>\n<li><strong>OS</strong> (e.g. from /etc/os-release):</li>\n<li><strong>Kernel</strong> (e.g. <code>uname -a</code>):</li>\n<li><strong>Install tools</strong>:</li>\n<li><strong>Others</strong>:</li>\n</ul>\n<p><strong>What happened</strong>:</p>\n<p><strong>What you expected to happen</strong>:</p>\n<p><strong>How to reproduce it</strong> (as minimally and precisely as possible):</p>\n<p><strong>Anything else do we need to know</strong>:</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "satyajitpadhyewit" },
        "number": 10983,
        "resourcePath": "/kubernetes/kubernetes/issues/10983",
        "state": "CLOSED",
        "publishedAt": "2015-07-09T12:08:23Z",
        "closedAt": "2018-02-12T15:28:11Z",
        "title": "--cpuset-cpus supported in kubernetes?",
        "bodyText": "I want to know if --cpuset-cpus in supported in kubernetes so that we can launch containers with that property. Initiated at #10570  but I am not finding enough resources if it can.",
        "bodyHTML": "<p>I want to know if --cpuset-cpus in supported in kubernetes so that we can launch containers with that property. Initiated at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"92188342\" data-permission-text=\"Title is private\" data-url=\"https://github.com/kubernetes/kubernetes/issues/10570\" data-hovercard-type=\"issue\" data-hovercard-url=\"/kubernetes/kubernetes/issues/10570/hovercard\" href=\"https://github.com/kubernetes/kubernetes/issues/10570\">#10570</a>  but I am not finding enough resources if it can.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "feiskyer" },
        "number": 22842,
        "resourcePath": "/kubernetes/kubernetes/issues/22842",
        "state": "CLOSED",
        "publishedAt": "2016-03-11T06:46:45Z",
        "closedAt": "2016-03-13T01:33:14Z",
        "title": "Cluster failed to initialize on GCE",
        "bodyText": "I tried to setup a trusty cluster on GCE, but it failed with Cluster failed to initialize within 300 seconds.\nHere is my environment setting:\nexport KUBERNETES_PROVIDER=gce\nexport KUBE_GCE_MASTER_PROJECT=ubuntu-os-cloud\nexport KUBE_GCE_MASTER_IMAGE=ubuntu-1404-trusty-v20160304\nexport KUBE_OS_DISTRIBUTION=trusty\n./cluster/kube-up.sh\n\nAm I missing something? CC @dchen1107\nBy the way, this problem has also been confirmed by @Random-Liu .",
        "bodyHTML": "<p>I tried to setup a trusty cluster on GCE, but it failed with <code>Cluster failed to initialize within 300 seconds</code>.</p>\n<p>Here is my environment setting:</p>\n<pre><code>export KUBERNETES_PROVIDER=gce\nexport KUBE_GCE_MASTER_PROJECT=ubuntu-os-cloud\nexport KUBE_GCE_MASTER_IMAGE=ubuntu-1404-trusty-v20160304\nexport KUBE_OS_DISTRIBUTION=trusty\n./cluster/kube-up.sh\n</code></pre>\n<p>Am I missing something? CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/dchen1107/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dchen1107\">@dchen1107</a></p>\n<p>By the way, this problem has also been confirmed by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/Random-Liu/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Random-Liu\">@Random-Liu</a> .</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "erictune" },
        "number": 14385,
        "resourcePath": "/kubernetes/kubernetes/issues/14385",
        "state": "CLOSED",
        "publishedAt": "2015-09-22T22:36:40Z",
        "closedAt": "2015-09-24T21:25:22Z",
        "title": "Default Job Parallelism from Completions",
        "bodyText": "Currently a job's .spec.parallelism defaults to 2.  @bgrant0607 suggested defaulting .spec.parallelism to .spec.completions.  This will allow users to leave it unspecified in most cases, and it will do something intuitive. (Maybe this was how it was in @soltysh original PR, can't recall if it was that or 1).\nThe current default of 2 I liked because it encourages users to think about making their containers concurrency-safe.  But so does setting it from .spec.completions.  And people will ask why it was\nI don't like defaulting to 1 as much because I think people will usually want to override it when they have multiple completions.",
        "bodyHTML": "<p>Currently a job's <code>.spec.parallelism</code> defaults to 2.  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/bgrant0607/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bgrant0607\">@bgrant0607</a> suggested defaulting <code>.spec.parallelism</code> to <code>.spec.completions</code>.  This will allow users to leave it unspecified in most cases, and it will do something intuitive. (Maybe this was how it was in <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/soltysh/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soltysh\">@soltysh</a> original PR, can't recall if it was that or 1).</p>\n<p>The current default of 2 I liked because it encourages users to think about making their containers concurrency-safe.  But so does setting it from <code>.spec.completions</code>.  And people will ask why it was</p>\n<p>I don't like defaulting to 1 as much because I think people will usually want to override it when they have multiple completions.</p>"
      }
    }
  },
  {
    "repository": {
      "issue": {
        "author": { "login": "hhy5861" },
        "number": 28144,
        "resourcePath": "/kubernetes/kubernetes/issues/28144",
        "state": "CLOSED",
        "publishedAt": "2016-06-28T06:31:07Z",
        "closedAt": "2016-06-29T18:57:21Z",
        "title": "About redis from the main issue, I can not connect master service ip connection pod slave",
        "bodyText": "I am from official instances did a redis master-slave, but I can not connect to the master service allocation in the slave containers such as ip: 10.254.31.52, contrary came in they could not connect to the master slave service assigned ip ask this. what causes the network can not communicate?\nThank you!\nservice ip\n\nslave container in ping master service ip:\n\nmaster container in ping slave service ip:\n\nnew start service",
        "bodyHTML": "<p>I am from official instances did a redis master-slave, but I can not connect to the master service allocation in the slave containers such as ip: 10.254.31.52, contrary came in they could not connect to the master slave service assigned ip ask this. what causes the network can not communicate?</p>\n<p>Thank you!</p>\n<p>service ip<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/8274977/16405760/ab25f4ba-3d3c-11e6-9227-ce2c433430e3.png\"><img src=\"https://cloud.githubusercontent.com/assets/8274977/16405760/ab25f4ba-3d3c-11e6-9227-ce2c433430e3.png\" alt=\"1\" style=\"max-width:100%;\"></a></p>\n<p>slave container in ping master service ip:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/8274977/16405776/c0d9b1de-3d3c-11e6-8687-f0fccc47467a.png\"><img src=\"https://cloud.githubusercontent.com/assets/8274977/16405776/c0d9b1de-3d3c-11e6-8687-f0fccc47467a.png\" alt=\"2\" style=\"max-width:100%;\"></a></p>\n<p>master container in ping slave service ip:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/8274977/16405809/fce52ed8-3d3c-11e6-8c56-be9ff1b09bbc.png\"><img src=\"https://cloud.githubusercontent.com/assets/8274977/16405809/fce52ed8-3d3c-11e6-8c56-be9ff1b09bbc.png\" alt=\"3\" style=\"max-width:100%;\"></a></p>\n<p>new start service<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/8274977/16421258/f231f9c2-3d86-11e6-8ff8-e3aa6c42fb40.png\"><img src=\"https://cloud.githubusercontent.com/assets/8274977/16421258/f231f9c2-3d86-11e6-8ff8-e3aa6c42fb40.png\" alt=\"4\" style=\"max-width:100%;\"></a></p>"
      }
    }
  }
]
