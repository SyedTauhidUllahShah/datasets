[
    {
      "repository": {
        "issue": {
          "author": { "login": "betterenvi" },
          "number": 15383,
          "resourcePath": "/tensorflow/tensorflow/issues/15383",
          "state": "CLOSED",
          "publishedAt": "2017-12-15T06:02:16Z",
          "closedAt": "2017-12-19T06:13:54Z",
          "title": "Feature request: Use placeholders to specify the inputs of TFGAN model.",
          "bodyText": "",
          "bodyHTML": ""
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "EdwardVincentMa" },
          "number": 22826,
          "resourcePath": "/tensorflow/tensorflow/issues/22826",
          "state": "CLOSED",
          "publishedAt": "2018-10-09T03:18:04Z",
          "closedAt": "2018-10-20T00:54:17Z",
          "title": "Win10 C++ TF1.9, error LNK2001, build by bazel  !",
          "bodyText": "I have generated the TensorFlowV1.9's .so and .lib file successfully on Win10,  but when I use this in VS2017, it has errors as bellow :\nMFCTestTF1.9.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)\" (?GetVarint32PtrFallback@core@tensorflow@@YAPBDPBD0PAI@Z)\n1>D:\\ProgramData\\VS2017 Project\\MFCTestTF1.9\\Release\\MFCTestTF1.9.exe : fatal error LNK1120: 1 \u4e2a\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u547d\u4ee4\n1>\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cMFCTestTF1.9.vcxproj\u201d\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\nAnd I also build TensorFlowV1.8 with CMAKE, it work OK without LNK error.  But V1.9 can not build by CMAKE.",
          "bodyHTML": "<p>I have generated the TensorFlowV1.9's .so and .lib file successfully on Win10,  but when I use this in VS2017, it has errors as bellow :</p>\n<p>MFCTestTF1.9.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)\" (?GetVarint32PtrFallback@core@tensorflow@@YAPBDPBD0PAI@Z)<br>\n1&gt;D:\\ProgramData\\VS2017 Project\\MFCTestTF1.9\\Release\\MFCTestTF1.9.exe : fatal error LNK1120: 1 \u4e2a\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u547d\u4ee4<br>\n1&gt;\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cMFCTestTF1.9.vcxproj\u201d\u7684\u64cd\u4f5c - \u5931\u8d25\u3002</p>\n<p>And I also build TensorFlowV1.8 with CMAKE, it work OK without LNK error.  But V1.9 can not build by CMAKE.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "homingjui" },
          "number": 21037,
          "resourcePath": "/tensorflow/tensorflow/issues/21037",
          "state": "CLOSED",
          "publishedAt": "2018-07-22T17:20:01Z",
          "closedAt": "2018-08-08T19:43:56Z",
          "title": "tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory",
          "bodyText": "can anyone help me??\nTraceback (most recent call last):\nFile \"/Users/meow/generate_tfrecord.py\", line 99, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n_sys.exit(main(argv))\nFile \"/Users/meow/generate_tfrecord.py\", line 85, in main\nwriter = tf.python_io.TFRecordWriter(FLAGS.output_path)\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py\", line 112, in init\ncompat.as_bytes(path), compat.as_bytes(compression_type), status)\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory",
          "bodyHTML": "<p>can anyone help me??</p>\n<p>Traceback (most recent call last):<br>\nFile \"/Users/meow/generate_tfrecord.py\", line 99, in <br>\ntf.app.run()<br>\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run<br>\n_sys.exit(main(argv))<br>\nFile \"/Users/meow/generate_tfrecord.py\", line 85, in main<br>\nwriter = tf.python_io.TFRecordWriter(FLAGS.output_path)<br>\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py\", line 112, in <strong>init</strong><br>\ncompat.as_bytes(path), compat.as_bytes(compression_type), status)<br>\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in <strong>exit</strong><br>\nc_api.TF_GetCode(self.status.status))<br>\ntensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "jayendra13" },
          "number": 452,
          "resourcePath": "/tensorflow/tensorflow/issues/452",
          "state": "CLOSED",
          "publishedAt": "2015-12-09T07:39:14Z",
          "closedAt": "2017-01-23T22:18:37Z",
          "title": "bazel build error for PolymerElements",
          "bodyText": "I am trying to build tensorflow from source, and bazel is giving some unrelated error\neddie7@albus:~/lab/tensorflow$ git pull\nAlready up-to-date.\neddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n.......\nERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@iron-validatable-behavior//': https://github.com/PolymerElements/iron-validatable-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 129.792s\n\n\nFYI: I have also build bazel from source",
          "bodyHTML": "<p>I am trying to build tensorflow from source, and bazel is giving some unrelated error</p>\n<pre><code>eddie7@albus:~/lab/tensorflow$ git pull\nAlready up-to-date.\neddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n.......\nERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@iron-validatable-behavior//': https://github.com/PolymerElements/iron-validatable-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 129.792s\n\n</code></pre>\n<p><em>FYI: I have also build bazel from source</em></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "rickragv" },
          "number": 19640,
          "resourcePath": "/tensorflow/tensorflow/issues/19640",
          "state": "OPEN",
          "publishedAt": "2018-05-30T08:28:23Z",
          "closedAt": null,
          "title": "Tensorflow Serving not using multi GPU/CUDA cores ",
          "bodyText": "I'm using an AWS g3.8xlarge instance which has 2 GPUs.\nTF serving is able to detect both GPUs and initialise them but while running the model it only uses 1 GPU to the maximum.\nWe are on version 1.7, even though the client sends upto 32 requests in parallel, the model server only uses the first GPU\n\n06_09_21",
          "bodyHTML": "<p>I'm using an AWS g3.8xlarge instance which has 2 GPUs.</p>\n<p>TF serving is able to detect both GPUs and initialise them but while running the model it only uses 1 GPU to the maximum.</p>\n<p>We are on version 1.7, even though the client sends upto 32 requests in parallel, the model server only uses the first GPU<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/6717323/40708575-66e52d36-6411-11e8-9e01-d95a0861827d.jpg\"><img src=\"https://user-images.githubusercontent.com/6717323/40708575-66e52d36-6411-11e8-9e01-d95a0861827d.jpg\" alt=\"40657873-faca0f68-6366-11e8-963c-3d5ba1db4e2c\" style=\"max-width:100%;\"></a></p>\n<p>06_09_21</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "mrbrantofgithub" },
          "number": 14761,
          "resourcePath": "/tensorflow/tensorflow/issues/14761",
          "state": "CLOSED",
          "publishedAt": "2017-11-21T14:41:33Z",
          "closedAt": "2017-12-20T01:41:25Z",
          "title": "tensorflow lite: error when convert frozen model to lite format",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 14.04\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\n1.3.0\nPython version:\n2.7\nBazel version (if compiling from source):\n0.7.0\nGCC/Compiler version (if compiling from source):\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nCUDA/cuDNN version:\ncuda8.0/cudnn6.0\n\nI tried to convert squeezenet frozen model to lite format with the following command:\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3\"\nthe output is shown below:\n2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)\n2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)\n2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)\n2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.\n2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).\n2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\nThen I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3\"\noutput:\n2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)\n2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)\n2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)\n2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.\n2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).\n2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\nAlthough I installed tensorflow with \"pip install tensorflow-gpu\", in order to convert model to lite format, I git clone the tensorflow files and  configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.3.0</li>\n<li><strong>Python version</strong>:<br>\n2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.7.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\ncuda8.0/cudnn6.0</li>\n</ul>\n<p>I tried to convert squeezenet frozen model to lite format with the following command:<br>\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3\"</p>\n<p>the output is shown below:<br>\n2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)<br>\n2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)<br>\n2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)<br>\n2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.<br>\n2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).<br>\n2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze</p>\n<p>Then I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.<br>\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3\"</p>\n<p>output:<br>\n2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)<br>\n2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)<br>\n2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)<br>\n2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/642252829cccf56627749c3c3b8dc65cc0099893/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/642252829cccf56627749c3c3b8dc65cc0099893\"><tt>6422528</tt></a> bytes, theoretical optimal value: 4816896 bytes.<br>\n2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).<br>\n2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze</p>\n<p>Although I installed tensorflow with \"pip install tensorflow-gpu\", in order to convert model to lite format, I git clone the tensorflow files and  configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "delip" },
          "number": 111,
          "resourcePath": "/tensorflow/tensorflow/issues/111",
          "state": "CLOSED",
          "publishedAt": "2015-11-11T01:10:11Z",
          "closedAt": "2015-11-11T22:54:36Z",
          "title": "configure script hardcodes location of cuda that makes it fail on OSX",
          "bodyText": "Cuda installation on OSX is at $CUDA_TOOLKIT_PATH/lib (not lib64), and on OSX the shared libraries are end in .dylib (not .so).\n  if [ -e \"$CUDA_TOOLKIT_PATH/lib64/libcudart.so.7.0\" ]; then\n    break\n  fi\n  echo \"Invalid path to CUDA 7.0 toolkit. ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so.7.0 cannot be found\"",
          "bodyHTML": "<p>Cuda installation on OSX is at $CUDA_TOOLKIT_PATH/lib (not lib64), and on OSX the shared libraries are end in .dylib (not .so).</p>\n<pre><code>  if [ -e \"$CUDA_TOOLKIT_PATH/lib64/libcudart.so.7.0\" ]; then\n    break\n  fi\n  echo \"Invalid path to CUDA 7.0 toolkit. ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so.7.0 cannot be found\"\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "insectatorious" },
          "number": 8199,
          "resourcePath": "/tensorflow/tensorflow/issues/8199",
          "state": "CLOSED",
          "publishedAt": "2017-03-08T14:55:38Z",
          "closedAt": "2017-06-16T22:00:52Z",
          "title": "Tensorboard scalar summary graph distorts after resizing and toggling log scale on y-axis ",
          "bodyText": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone although a search for distorted image tensorboard doesn't help much...\nEnvironment info\nOperating System: 16.04 LTS\nFirefox: 51.0.1 (64-bit)\nTF: 1.0 (installed via pip)\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$> sudo ls -l /usr/local/cudnn/*\n/usr/local/cudnn/include:\ntotal 100\n-r--r--r-- 1 root root 99658 Feb 20 11:27 cudnn.h\n\n/usr/local/cudnn/lib64:\ntotal 150908\nlrwxrwxrwx 1 root root       13 Feb 20 11:27 libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       35 Feb 16 17:01 libcudnn.so.4 -> /usr/local/cuda/lib64/libcudnn.so.4\nlrwxrwxrwx 1 root root       39 Feb 16 17:01 libcudnn.so.4.0.7 -> /usr/local/cuda/lib64/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       18 Feb 20 11:27 libcudnn.so.5 -> libcudnn.so.5.1.10\n-rwxr-xr-x 1 root root 84163560 Feb 20 11:27 libcudnn.so.5.1.10\n\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nStandard TF pip url.\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\n$> python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n1.0.0\n\nSteps to reproduce (Firefox only)\n\nOn the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg cost or accuracy) by expanding the tab. \nClick on the expand icon \nEnable log scale of y-axis \nDisable log scale of y-axis (note the bug happens regardless of whether you do this) \nClick on expand icon to shrink the graph.\n\nThe graph is now overflowing: \nWhat other attempted solutions have you tried?\nTried to reproduce in Chromium 55.0.2883.87 but unable to.",
          "bodyHTML": "<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>None although a search for distorted image tensorboard doesn't help much...</p>\n<h3>Environment info</h3>\n<p>Operating System: 16.04 LTS<br>\nFirefox: 51.0.1 (64-bit)<br>\nTF: 1.0 (installed via pip)</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<pre><code>$&gt; sudo ls -l /usr/local/cudnn/*\n/usr/local/cudnn/include:\ntotal 100\n-r--r--r-- 1 root root 99658 Feb 20 11:27 cudnn.h\n\n/usr/local/cudnn/lib64:\ntotal 150908\nlrwxrwxrwx 1 root root       13 Feb 20 11:27 libcudnn.so -&gt; libcudnn.so.5\nlrwxrwxrwx 1 root root       35 Feb 16 17:01 libcudnn.so.4 -&gt; /usr/local/cuda/lib64/libcudnn.so.4\nlrwxrwxrwx 1 root root       39 Feb 16 17:01 libcudnn.so.4.0.7 -&gt; /usr/local/cuda/lib64/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       18 Feb 20 11:27 libcudnn.so.5 -&gt; libcudnn.so.5.1.10\n-rwxr-xr-x 1 root root 84163560 Feb 20 11:27 libcudnn.so.5.1.10\n</code></pre>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed:<br>\nStandard TF pip url.</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<pre><code>$&gt; python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n1.0.0\n</code></pre>\n<h3>Steps to reproduce (Firefox only)</h3>\n<ol>\n<li>On the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg <em>cost</em> or <em>accuracy</em>) by <strong>expanding the tab</strong>. <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/252960/23708510/0bcd8b5a-040e-11e7-8a19-e2eb7ee208af.png\"><img src=\"https://cloud.githubusercontent.com/assets/252960/23708510/0bcd8b5a-040e-11e7-8a19-e2eb7ee208af.png\" alt=\"tensorboard - mozilla firefox_027\" style=\"max-width:100%;\"></a></li>\n<li><strong>Click on the expand icon</strong> <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/252960/23708524/1227e784-040e-11e7-9c5c-e8f466df5581.png\"><img src=\"https://cloud.githubusercontent.com/assets/252960/23708524/1227e784-040e-11e7-9c5c-e8f466df5581.png\" alt=\"tensorboard - mozilla firefox_028\" style=\"max-width:100%;\"></a></li>\n<li><strong>Enable</strong> log scale of y-axis <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/252960/23708530/162e403a-040e-11e7-8628-fbd04ae05642.png\"><img src=\"https://cloud.githubusercontent.com/assets/252960/23708530/162e403a-040e-11e7-8628-fbd04ae05642.png\" alt=\"tensorboard - mozilla firefox_029\" style=\"max-width:100%;\"></a></li>\n<li><strong>Disable</strong> log scale of y-axis (note the bug happens regardless of whether you do this) <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/252960/23708524/1227e784-040e-11e7-9c5c-e8f466df5581.png\"><img src=\"https://cloud.githubusercontent.com/assets/252960/23708524/1227e784-040e-11e7-9c5c-e8f466df5581.png\" alt=\"tensorboard - mozilla firefox_028\" style=\"max-width:100%;\"></a></li>\n<li><strong>Click on expand icon</strong> to shrink the graph.</li>\n</ol>\n<p>The graph is now overflowing: <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/252960/23708531/18f031a2-040e-11e7-9e22-8c76415ddb14.png\"><img src=\"https://cloud.githubusercontent.com/assets/252960/23708531/18f031a2-040e-11e7-9e22-8c76415ddb14.png\" alt=\"tensorboard - mozilla firefox_030\" style=\"max-width:100%;\"></a></p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>Tried to reproduce in Chromium 55.0.2883.87 but unable to.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "leandroBorgesFerreira" },
          "number": 15645,
          "resourcePath": "/tensorflow/tensorflow/issues/15645",
          "state": "CLOSED",
          "publishedAt": "2017-12-26T18:48:54Z",
          "closedAt": "2017-12-27T08:57:05Z",
          "title": "Tensorflow lite 0.1.1 causing Build to fail",
          "bodyText": "I am trying to use tensrflow-lite in Android. When I add\ncompile 'org.tensorflow:tensorflow-lite:0.1.1'\nI get:\nError:Execution failed for task ':sample:transformClassesWithJarMergingForDebug'.\n> com.android.build.api.transform.TransformException: java.util.zip.ZipException: duplicate entry: R.class\n\nI am using multidex and AGP 2.3.3.\nWhen I take tensorflow-lite off, the app builds correctly. When I put it back, the build fails. I believe this is a bug in the library.",
          "bodyHTML": "<p>I am trying to use tensrflow-lite in Android. When I add</p>\n<p>compile 'org.tensorflow:tensorflow-lite:0.1.1'</p>\n<p>I get:</p>\n<pre><code>Error:Execution failed for task ':sample:transformClassesWithJarMergingForDebug'.\n&gt; com.android.build.api.transform.TransformException: java.util.zip.ZipException: duplicate entry: R.class\n</code></pre>\n<p>I am using multidex and AGP 2.3.3.</p>\n<p>When I take tensorflow-lite off, the app builds correctly. When I put it back, the build fails. I believe this is a bug in the library.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "nothasson" },
          "number": 21851,
          "resourcePath": "/tensorflow/tensorflow/issues/21851",
          "state": "CLOSED",
          "publishedAt": "2018-08-24T11:28:43Z",
          "closedAt": "2018-08-31T01:45:31Z",
          "title": "How to install tensorflow in python3.7?",
          "bodyText": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nPython version:\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
          "bodyHTML": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a rel=\"nofollow\" href=\"https://stackoverflow.com/questions/tagged/tensorflow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n<li><strong>Python version</strong>:</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "yanghoonkim" },
          "number": 10074,
          "resourcePath": "/tensorflow/tensorflow/issues/10074",
          "state": "CLOSED",
          "publishedAt": "2017-05-21T09:09:46Z",
          "closedAt": "2017-05-21T11:25:41Z",
          "title": "fatal problem with saving variables",
          "bodyText": "I coded a simple feedforward neural network and it works very well.\nI tried to save the computation time, and created:\nself.total_time = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')\nin the fnn class.\nand i tried to print the total training time per some training epoch.\nI made it to grow with time:\n# Check & Print training time\ntill_now = time.time() - start_time\nself.total_time += till_now\nprint_time(self.total_time.eval())\nand the result look something like this :\nEpoch :   0 | Evaluation :  115 | Learning Rate : 0.50\nTraining Loss :         0.040919\nValidation Loss :      0.0741969\nValidation Accuracy :      97.77%\nTotal time cost : 0.38 seconds\nEpoch :   1 | Evaluation :  116 | Learning Rate : 0.50\nTraining Loss :        0.0417941\nValidation Loss :       0.073841\nValidation Accuracy :      97.73%\nTotal time cost : 0.71 seconds\nEpoch :   2 | Evaluation :  117 | Learning Rate : 0.50\nTraining Loss :        0.0334573\nValidation Loss :      0.0745566\nValidation Accuracy :      97.75%\nTotal time cost : 1.01 seconds\nHowever, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of variable total_time and it initialized as 0 which is the value i first give to.\nI also checked tf.global_variables() include self.total_time.\nWhat is wrong?",
          "bodyHTML": "<p>I coded a simple feedforward neural network and it works very well.</p>\n<p>I tried to save the computation time, and created:<br>\n<strong>self.total_time</strong> = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')<br>\nin the fnn class.</p>\n<p>and i tried to print the total training time per some training epoch.<br>\n<strong>I made it to grow with time:</strong></p>\n<p><strong># Check &amp; Print training time</strong><br>\ntill_now = time.time() - start_time<br>\nself.total_time += till_now<br>\nprint_time(self.total_time.eval())</p>\n<p><strong>and the result look something like this :</strong></p>\n<h2>Epoch :   0 | Evaluation :  115 | Learning Rate : 0.50</h2>\n<p>Training Loss :         0.040919<br>\nValidation Loss :      0.0741969<br>\nValidation Accuracy :      97.77%<br>\nTotal time cost : 0.38 seconds</p>\n<h2>Epoch :   1 | Evaluation :  116 | Learning Rate : 0.50</h2>\n<p>Training Loss :        0.0417941<br>\nValidation Loss :       0.073841<br>\nValidation Accuracy :      97.73%<br>\nTotal time cost : 0.71 seconds</p>\n<h2>Epoch :   2 | Evaluation :  117 | Learning Rate : 0.50</h2>\n<p>Training Loss :        0.0334573<br>\nValidation Loss :      0.0745566<br>\nValidation Accuracy :      97.75%<br>\nTotal time cost : 1.01 seconds</p>\n<p>However, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of <strong>variable total_time</strong> and it initialized as 0 which is the value i first give to.<br>\nI also checked tf.global_variables() include self.total_time.</p>\n<p>What is wrong?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "vladfi1" },
          "number": 4431,
          "resourcePath": "/tensorflow/tensorflow/issues/4431",
          "state": "CLOSED",
          "publishedAt": "2016-09-18T02:59:41Z",
          "closedAt": "2016-11-18T00:12:16Z",
          "title": "Forward mode ad, directional derivatives",
          "bodyText": "Say I have outputs = f(inputs) and g of the same shape as inputs. I'd like to compute the directional derivative of outputs with respect to inputs in the direction g - in other words, the derivative of f(inputs + alpha * g) with respect to alpha at the point alpha=0.\nThis is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?",
          "bodyHTML": "<p>Say I have <code>outputs = f(inputs)</code> and <code>g</code> of the same shape as <code>inputs</code>. I'd like to compute the directional derivative of <code>outputs</code> with respect to <code>inputs</code> in the direction <code>g</code> - in other words, the derivative of <code>f(inputs + alpha * g)</code> with respect to <code>alpha</code> at the point <code>alpha=0</code>.</p>\n<p>This is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "tampler" },
          "number": 11937,
          "resourcePath": "/tensorflow/tensorflow/issues/11937",
          "state": "CLOSED",
          "publishedAt": "2017-08-01T09:39:58Z",
          "closedAt": "2018-08-22T19:49:42Z",
          "title": "TPU support",
          "bodyText": "I wanna add support for my Tensor Processing Unit chip in TensorFlow.\nMy TPU is implemented as an accelerator for ARM v7 32bit processor and implements multiple arithmetic kernels, similar to GPU. It implements a simple memory mapped interface, SGDMA and vector instructions over tensors. I added vector extension to GCC 7.1.1 and can run bare metal C++ nets on embedded Ubuntu 16.04\nI also checked the TF port  for Raspberry Pi 3, but it looks outdated and barely supported.\nI'm not currently aware about the scope of work, but believe that should not be that complex, given open examples from GPU vendors and already existing port for Google TPU\nAnyone interested in joining this project is highly welcome! Advise, links and code examples are much appreciated\nThank you",
          "bodyHTML": "<p>I wanna add support for my Tensor Processing Unit chip in TensorFlow.</p>\n<p>My TPU is implemented as an accelerator for ARM v7 32bit processor and implements multiple arithmetic kernels, similar to GPU. It implements a simple memory mapped interface, SGDMA and vector instructions over tensors. I added vector extension to GCC 7.1.1 and can run bare metal C++ nets on embedded Ubuntu 16.04</p>\n<p>I also checked the TF port  for Raspberry Pi 3, but it looks outdated and barely supported.</p>\n<p>I'm not currently aware about the scope of work, but believe that should not be that complex, given open examples from GPU vendors and already existing port for Google TPU</p>\n<p>Anyone interested in joining this project is highly welcome! Advise, links and code examples are much appreciated</p>\n<p>Thank you</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "colmantse" },
          "number": 18108,
          "resourcePath": "/tensorflow/tensorflow/issues/18108",
          "state": "CLOSED",
          "publishedAt": "2018-03-30T03:41:54Z",
          "closedAt": "2018-04-02T17:41:57Z",
          "title": "building from source with branch r1.7 gives tf1.5.1 after building wheel",
          "bodyText": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04.9\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below):1.5.1 after successful building, but i want 1.7\nPython version: 3.6\nBazel version (if compiling from source): build label 0.11.1\nGCC/Compiler version (if compiling from source):5.4.0 when i type gcc --version, 7.2.0 shown in python terminal\nCUDA/cuDNN version:cuda 9, cudnn 7\nGPU model and memory: gtx1080 ti 11 gb\nExact command to reproduce: following this https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nI've make sure i'd pull everything from tensorflow. i remove all other branch and check out r1.7, building was successful. No errors and stuff. The wheel i got says tensorflow-1.5.1-cp36 ... etc. , i go on to install it, tf.version = 1.5.1 . I am confused how to build tf 1.7 from source.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
          "bodyHTML": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a rel=\"nofollow\" href=\"https://stackoverflow.com/questions/tagged/tensorflow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:ubuntu 16.04.9</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:source</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.5.1 after successful building, but i want 1.7</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: build label 0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:5.4.0 when i type gcc --version, 7.2.0 shown in python terminal</li>\n<li><strong>CUDA/cuDNN version</strong>:cuda 9, cudnn 7</li>\n<li><strong>GPU model and memory</strong>: gtx1080 ti 11 gb</li>\n<li><strong>Exact command to reproduce</strong>: following this <a href=\"https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03\">https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03</a></li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>I've make sure i'd pull everything from tensorflow. i remove all other branch and check out r1.7, building was successful. No errors and stuff. The wheel i got says tensorflow-1.5.1-cp36 ... etc. , i go on to install it, tf.<strong>version</strong> = 1.5.1 . I am confused how to build tf 1.7 from source.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "yakotaki" },
          "number": 19463,
          "resourcePath": "/tensorflow/tensorflow/issues/19463",
          "state": "CLOSED",
          "publishedAt": "2018-05-22T14:00:10Z",
          "closedAt": "2018-05-22T14:10:45Z",
          "title": "AttributeError: 'NoneType' object has no attribute 'rfind'",
          "bodyText": "OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n-TensorFlow installed from (source):\ntensorflow v1.6.0-0-gd2e24b6039 1.6.0:\nPython 3.5:\nReproduce\ngit clone tensorflow\npython3 tensorflow/examples/speech_commands/train.py\npython3 tensorflow/examples/speech_commands/freeze.py \n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \n--output_file=/tmp/my_frozen_graph.pb\nError appears:\n/home/lukas/.local/lib/python3.5/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\nfrom ._conv import register_converters as _register_converters\n2018-05-22 21:59:00.562103: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\nConverted 6 variables to const ops.\nTraceback (most recent call last):\nFile \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 180, in \ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\nFile \"/home/lukas/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\nFile \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 124, in main\nos.path.dirname(FLAGS.output_file),\nFile \"/usr/lib/python3.5/posixpath.py\", line 148, in dirname\ni = p.rfind(sep) + 1\nAttributeError: 'NoneType' object has no attribute 'rfind'\nThanks",
          "bodyHTML": "<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\n-<strong>TensorFlow installed from (source)</strong>:<br>\n<strong>tensorflow v1.6.0-0-gd2e24b6039 1.6.0</strong>:<br>\n<strong>Python 3.5</strong>:</p>\n<p><strong>Reproduce</strong><br>\ngit clone tensorflow<br>\npython3 tensorflow/examples/speech_commands/train.py<br>\npython3 tensorflow/examples/speech_commands/freeze.py <br>\n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 <br>\n--output_file=/tmp/my_frozen_graph.pb</p>\n<p><strong>Error appears:</strong><br>\n/home/lukas/.local/lib/python3.5/site-packages/h5py/<strong>init</strong>.py:36: FutureWarning: Conversion of the second argument of issubdtype from <code>float</code> to <code>np.floating</code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type</code>.<br>\nfrom ._conv import register_converters as _register_converters<br>\n2018-05-22 21:59:00.562103: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX<br>\nConverted 6 variables to const ops.<br>\nTraceback (most recent call last):<br>\nFile \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 180, in <br>\ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)<br>\nFile \"/home/lukas/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run<br>\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))<br>\nFile \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 124, in main<br>\nos.path.dirname(FLAGS.output_file),<br>\nFile \"/usr/lib/python3.5/posixpath.py\", line 148, in dirname<br>\ni = p.rfind(sep) + 1<br>\nAttributeError: 'NoneType' object has no attribute 'rfind'</p>\n<p><strong>Thanks</strong></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "pronobis" },
          "number": 459,
          "resourcePath": "/tensorflow/tensorflow/issues/459",
          "state": "CLOSED",
          "publishedAt": "2015-12-09T22:48:23Z",
          "closedAt": "2016-06-06T18:55:11Z",
          "title": "Single scalar summary point not visible in the plot",
          "bodyText": "When only one event is available for a scalar summary, the plot remains empty, as shown below (here the value is 2.0):\n\nIt would be great to see one point corresponding to the value instead. The value does appear in the JSON/CSV file.",
          "bodyHTML": "<p>When only one event is available for a scalar summary, the plot remains empty, as shown below (here the value is 2.0):<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/331795/11701319/b1919762-9e83-11e5-8f85-fa1740c52c57.png\"><img src=\"https://cloud.githubusercontent.com/assets/331795/11701319/b1919762-9e83-11e5-8f85-fa1740c52c57.png\" alt=\"snapshot2\" style=\"max-width:100%;\"></a><br>\nIt would be great to see one point corresponding to the value instead. The value does appear in the JSON/CSV file.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "leesunfreshing" },
          "number": 10171,
          "resourcePath": "/tensorflow/tensorflow/issues/10171",
          "state": "CLOSED",
          "publishedAt": "2017-05-24T19:13:29Z",
          "closedAt": "2017-05-26T00:33:35Z",
          "title": "tf.contrib.rnn.decoder does not require explicitly build encoder?",
          "bodyText": "As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.\nThe tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?",
          "bodyHTML": "<p>As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.<br>\nThe tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "iNLyze" },
          "number": 3864,
          "resourcePath": "/tensorflow/tensorflow/issues/3864",
          "state": "CLOSED",
          "publishedAt": "2016-08-16T22:42:25Z",
          "closedAt": "2016-08-18T23:07:14Z",
          "title": "Tensorflow r.0.10, CUDA 8.0, cuDNN 5.1 core dumped, CUDA_ERROR_OUT_OF_MEMORY",
          "bodyText": "On running a benchmark with MNIST data on a CNN (source below) tensorflow first complains about memory allocation and then appears to have trouble using cuDNN 5.1\nDetailed script and output at the bottom.\nProblem appears to affect cuDNN specifically as I could run CUDA examples as well as matmul on tensorflow without problems.\nEnvironment info\nOperating System: ubuntu 16.04\nuname -a\nLinux  4.4.0-34-generic #53-Ubuntu SMP Wed Jul 27 16:06:39 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\nInstalled version of CUDA and cuDNN:\nls -l $CUDA_HOME/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Aug 15 22:51 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Aug 15 22:51 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn_static.a\nEnvironment variables\necho $LD_LIBRARY_PATH\n/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\necho $CUDA_HOME\n/usr/local/cuda\necho $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin/:/usr/local/cuda/bin/\nTensorflow version\nCompiled from source, r0.10, built into pip package and installed this pip wheel\nConfigured with cuDNN path and version set to system default\n\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\nsee /// OUTPUT /// at the end\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nn.a.\nversion r0.10\nThe output of bazel version\nbazel version\nBuild label: 0.3.1-2016-08-15 (@936c2c2)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Sun Aug 14 23:07:32 2016 (1471216052)\nBuild timestamp: 1471216052\nBuild timestamp as int: 1471216052\n\nSteps to reproduce: run this script\nimport numpy\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\nload data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nreshape to be [samples][channels][width][height]\nX_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\nnormalize inputs from 0-255 to 0-1\nX_train = X_train / 255\nX_test = X_test / 255\none hot encode outputs\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\ndefine a simple CNN model\ndef baseline_model():\n##### create model\nmodel = Sequential()\nmodel.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n##### Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nreturn model\nbuild the model\nmodel = baseline_model()\nFit the model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)\nFinal evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n///////////// OUTPUT /////////////////\nUsing TensorFlow backend.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 148.69MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:840] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 148.69M (155910144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/10\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\nAborted (core dumped)",
          "bodyHTML": "<p>On running a benchmark with MNIST data on a CNN (source below) tensorflow first complains about memory allocation and then appears to have trouble using cuDNN 5.1<br>\nDetailed script and output at the bottom.<br>\nProblem appears to affect cuDNN specifically as I could run CUDA examples as well as matmul on tensorflow without problems.</p>\n<h3>Environment info</h3>\n<p>Operating System: ubuntu 16.04<br>\nuname -a<br>\nLinux  4.4.0-34-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"115998800\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/53\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/53/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/53\">#53</a>-Ubuntu SMP Wed Jul 27 16:06:39 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</p>\n<p>Installed version of CUDA and cuDNN:<br>\nls -l $CUDA_HOME/lib64/libcud*<br>\n-rw-r--r-- 1 root root   560184 Aug 15 22:51 /usr/local/cuda/lib64/libcudadevrt.a<br>\nlrwxrwxrwx 1 root root       16 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0<br>\nlrwxrwxrwx 1 root root       19 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.27<br>\n-rwxr-xr-x 1 root root   394472 Aug 15 22:51 /usr/local/cuda/lib64/libcudart.so.8.0.27<br>\n-rw-r--r-- 1 root root   737516 Aug 15 22:51 /usr/local/cuda/lib64/libcudart_static.a<br>\nlrwxrwxrwx 1 root root       13 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so -&gt; libcudnn.so.5<br>\nlrwxrwxrwx 1 root root       17 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5 -&gt; libcudnn.so.5.1.5<br>\n-rwxr-xr-x 1 root root 79337624 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5<br>\n-rw-r--r-- 1 root root 69756172 Aug 15 23:39 /usr/local/cuda/lib64/libcudnn_static.a</p>\n<h3>Environment variables</h3>\n<p>echo $LD_LIBRARY_PATH<br>\n/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64<br>\necho $CUDA_HOME<br>\n/usr/local/cuda<br>\necho $PATH<br>\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin/:/usr/local/cuda/bin/</p>\n<h3>Tensorflow version</h3>\n<p>Compiled from source, r0.10, built into pip package and installed this pip wheel<br>\nConfigured with cuDNN path and version set to system default</p>\n<ol>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.<br>\nsee /// OUTPUT /// at the end</li>\n</ol>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\nn.a.<br>\nversion r0.10</li>\n<li>The output of <code>bazel version</code><br>\nbazel version<br>\nBuild label: 0.3.1-2016-08-15 (@936c2c2)<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Sun Aug 14 23:07:32 2016 (1471216052)<br>\nBuild timestamp: 1471216052<br>\nBuild timestamp as int: 1471216052</li>\n</ol>\n<h3>Steps to reproduce: run this script</h3>\n<p>import numpy<br>\nfrom keras.datasets import mnist<br>\nfrom keras.models import Sequential<br>\nfrom keras.layers import Dense<br>\nfrom keras.layers import Dropout<br>\nfrom keras.layers import Flatten<br>\nfrom keras.layers.convolutional import Convolution2D<br>\nfrom keras.layers.convolutional import MaxPooling2D<br>\nfrom keras.utils import np_utils</p>\n<h5>fix random seed for reproducibility</h5>\n<p>seed = 7<br>\nnumpy.random.seed(seed)</p>\n<h5>load data</h5>\n<p>(X_train, y_train), (X_test, y_test) = mnist.load_data()</p>\n<h5>reshape to be [samples][channels][width][height]</h5>\n<p>X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')<br>\nX_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')</p>\n<h5>normalize inputs from 0-255 to 0-1</h5>\n<p>X_train = X_train / 255<br>\nX_test = X_test / 255</p>\n<h5>one hot encode outputs</h5>\n<p>y_train = np_utils.to_categorical(y_train)<br>\ny_test = np_utils.to_categorical(y_test)<br>\nnum_classes = y_test.shape[1]</p>\n<h5>define a simple CNN model</h5>\n<p>def baseline_model():<br>\n##### create model<br>\nmodel = Sequential()<br>\nmodel.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))<br>\nmodel.add(MaxPooling2D(pool_size=(2, 2)))<br>\nmodel.add(Dropout(0.2))<br>\nmodel.add(Flatten())<br>\nmodel.add(Dense(128, activation='relu'))<br>\nmodel.add(Dense(num_classes, activation='softmax'))<br>\n##### Compile model<br>\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br>\nreturn model</p>\n<h5>build the model</h5>\n<p>model = baseline_model()</p>\n<h5>Fit the model</h5>\n<p>model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)</p>\n<h5>Final evaluation of the model</h5>\n<p>scores = model.evaluate(X_test, y_test, verbose=0)<br>\nprint(\"CNN Error: %.2f%%\" % (100-scores[1]*100))</p>\n<h3>///////////// OUTPUT /////////////////</h3>\n<p>Using TensorFlow backend.<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally<br>\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:<br>\nname: GeForce GTX 1070<br>\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683<br>\npciBusID 0000:01:00.0<br>\nTotal memory: 7.91GiB<br>\nFree memory: 148.69MiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:840] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 148.69M (155910144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nTrain on 60000 samples, validate on 10000 samples<br>\nEpoch 1/10<br>\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR<br>\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM<br>\nF tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms(&amp;algorithms)<br>\nAborted (core dumped)</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "sml0820" },
          "number": 18763,
          "resourcePath": "/tensorflow/tensorflow/issues/18763",
          "state": "CLOSED",
          "publishedAt": "2018-04-21T21:57:29Z",
          "closedAt": "2018-04-22T23:22:41Z",
          "title": "Multiple Classes fails in Eager Mode (\"tf.keras.Model\")",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nBazel version:\nN/A\nCUDA/cuDNN version:\nN/A\nGPU model and memory:\nN/A\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu\nTensorFlow installed from (source or binary):\nInstalled utilizing pip\nTensorFlow version (use command below):\n1.7\nPython version:\n3.6\nExact command to reproduce:\n\nimport tensorflow as tf  \nimport tensorflow.contrib.eager as tfe  \n\ntfe.enable_eager_execution()\n\nclass CustomLayer(tf.keras.Model):\n    def __init__(self):\n        super(CustomLayer, self).__init__()\n        print(\"blah\")\n\nclass CustomNetwork(tf.keras.Model):\n    def __init__(self):\n        super(CustomNetwork, self).__init__()\n        self.custom_layers = CustomLayer()\n\n    def forward(self, x, y=None):\n        x = self.custom_layers(x)\n\nCustomNetwork().forward(tf.convert_to_tensor([1]))\n\nDescribe the problem\nTrying to utilize multiple classes fails in tensorflow eager mode utilizing \"tf.keras.Model\". If I change \"tf.keras.Model\" to \"tfe.Network\" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:\nSource code / logs\nblah\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-12-9afa9b91ddef> in <module>()\n----> 1 CustomNetwork().forward(tf.convert_to_tensor([1]))\n\n<ipython-input-11-484119102aec> in forward(self, x, y)\n      5 \n      6     def forward(self, x, y=None):\n----> 7         x = self.custom_layers(x)\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)\n    237     \"\"\"\n    238     # Actually call the layer (optionally building it).\n--> 239     output = super(Layer, self).__call__(inputs, **kwargs)\n    240     if context.executing_eagerly():\n    241       return output\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    712 \n    713         if not in_deferred_mode:\n--> 714           outputs = self.call(inputs, *args, **kwargs)\n    715           if outputs is None:\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in call(self, inputs, training, mask)\n    635     outputs, _ = self._run_internal_graph(inputs,\n    636                                           training=training,\n--> 637                                           mask=masks)\n    638     return outputs\n    639 \n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\n    770     # does not return a list the same size as `call`\n    771     tensor_map = {}\n--> 772     for x, y, mask in zip(self.inputs, inputs, masks):\n    773       tensor_map[str(id(x))] = (y, mask)\n    774 \n\nTypeError: zip argument #1 must support iteration",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo</li>\n<li><strong>Bazel version</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nN/A</li>\n<li><strong>GPU model and memory</strong>:<br>\nN/A</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nTried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nInstalled utilizing pip</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.7</li>\n<li><strong>Python version</strong>:<br>\n3.6</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>import tensorflow as tf  \nimport tensorflow.contrib.eager as tfe  \n\ntfe.enable_eager_execution()\n\nclass CustomLayer(tf.keras.Model):\n    def __init__(self):\n        super(CustomLayer, self).__init__()\n        print(\"blah\")\n\nclass CustomNetwork(tf.keras.Model):\n    def __init__(self):\n        super(CustomNetwork, self).__init__()\n        self.custom_layers = CustomLayer()\n\n    def forward(self, x, y=None):\n        x = self.custom_layers(x)\n\nCustomNetwork().forward(tf.convert_to_tensor([1]))\n</code></pre>\n<h3>Describe the problem</h3>\n<p>Trying to utilize multiple classes fails in tensorflow eager mode utilizing \"tf.keras.Model\". If I change \"tf.keras.Model\" to \"tfe.Network\" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:</p>\n<h3>Source code / logs</h3>\n<pre><code>blah\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-12-9afa9b91ddef&gt; in &lt;module&gt;()\n----&gt; 1 CustomNetwork().forward(tf.convert_to_tensor([1]))\n\n&lt;ipython-input-11-484119102aec&gt; in forward(self, x, y)\n      5 \n      6     def forward(self, x, y=None):\n----&gt; 7         x = self.custom_layers(x)\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)\n    237     \"\"\"\n    238     # Actually call the layer (optionally building it).\n--&gt; 239     output = super(Layer, self).__call__(inputs, **kwargs)\n    240     if context.executing_eagerly():\n    241       return output\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    712 \n    713         if not in_deferred_mode:\n--&gt; 714           outputs = self.call(inputs, *args, **kwargs)\n    715           if outputs is None:\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in call(self, inputs, training, mask)\n    635     outputs, _ = self._run_internal_graph(inputs,\n    636                                           training=training,\n--&gt; 637                                           mask=masks)\n    638     return outputs\n    639 \n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\n    770     # does not return a list the same size as `call`\n    771     tensor_map = {}\n--&gt; 772     for x, y, mask in zip(self.inputs, inputs, masks):\n    773       tensor_map[str(id(x))] = (y, mask)\n    774 \n\nTypeError: zip argument #1 must support iteration\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "kinDSa" },
          "number": 7683,
          "resourcePath": "/tensorflow/tensorflow/issues/7683",
          "state": "CLOSED",
          "publishedAt": "2017-02-20T05:54:43Z",
          "closedAt": "2017-02-27T19:14:02Z",
          "title": "Package not Reslove.",
          "bodyText": "Hello,\nI am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:\nimport org.tensorflow.DataType;\nimport org.tensorflow.Graph;\nimport org.tensorflow.Session;\nimport org.tensorflow.Tensor;\nimport org.tensorflow.TensorFlow;\nCan any one help to find out where some thing is missing.",
          "bodyHTML": "<p>Hello,</p>\n<p>I am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:</p>\n<p>import org.tensorflow.DataType;<br>\nimport org.tensorflow.Graph;<br>\nimport org.tensorflow.Session;<br>\nimport org.tensorflow.Tensor;<br>\nimport org.tensorflow.TensorFlow;</p>\n<p>Can any one help to find out where some thing is missing.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "harpribot" },
          "number": 2138,
          "resourcePath": "/tensorflow/tensorflow/issues/2138",
          "state": "CLOSED",
          "publishedAt": "2016-04-27T20:08:35Z",
          "closedAt": "2016-06-28T18:56:24Z",
          "title": "embedding_attention_seq2seq fails / embedding_rnn_seq2seq works",
          "bodyText": "Environment info\nOperating System: Ubuntu 14.04\nInstalled version of CUDA and cuDNN: Cuda 7.0 and CUDNN 6.5 v4\nSo when I use a simple Embedding RNN Sequence to Sequence Model like this\n# choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        # embedding model\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n\nThe above implementation works perfectly, but when I just change the model from simple embedding seq2seq to Embedding Attention Seq2Seq, like this,\n\n        # choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n\n\nI get segmentation fault, with absolutely no information. My memory does not run out, neither my CPU, as I tried this with\nbatch_size =1 \nsee.memory_dim = 1\n\nand still got the same segmentation fault.\nI get the same error, and the above configuration can certainly not eat my RAM.\nThis is a potential bug, if I am not getting something worng. The LSTM and GRU cell just takes the size of the hidden layer as parameter, which is a scaler.\nTHE BUG REPORT\nThe Debug result\n(gdb) run train_script_lstm_attn.py \nStarting program: /lusr/bin/python train_script_lstm_attn.py\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n[New Thread 0x7fffd7a25700 (LWP 45650)]\n[New Thread 0x7fffd7224700 (LWP 45651)]\n[New Thread 0x7fffd4a23700 (LWP 45652)]\n[New Thread 0x7fffd2222700 (LWP 45653)]\n[New Thread 0x7fffcfa21700 (LWP 45654)]\n[New Thread 0x7fffcd220700 (LWP 45655)]\n[New Thread 0x7fffcaa1f700 (LWP 45656)]\n[Thread 0x7fffcaa1f700 (LWP 45656) exited]\n[Thread 0x7fffcfa21700 (LWP 45654) exited]\n[Thread 0x7fffd7a25700 (LWP 45650) exited]\n[Thread 0x7fffd2222700 (LWP 45653) exited]\n[Thread 0x7fffd7224700 (LWP 45651) exited]\n[Thread 0x7fffcd220700 (LWP 45655) exited]\n[Thread 0x7fffd4a23700 (LWP 45652) exited]\n[New Thread 0x7fffcaa1f700 (LWP 45661)]\n[New Thread 0x7fffcd220700 (LWP 46103)]\n[New Thread 0x7fffcfa21700 (LWP 46104)]\n[New Thread 0x7fffd2222700 (LWP 46105)]\n[New Thread 0x7ffed22bf700 (LWP 46106)]\n[New Thread 0x7ffed1abe700 (LWP 46107)]\n[New Thread 0x7ffed12bd700 (LWP 46108)]\n[New Thread 0x7ffed0abc700 (LWP 46109)]\n[New Thread 0x7ffec3fff700 (LWP 46110)]\n[New Thread 0x7ffec37fe700 (LWP 46111)]\n[New Thread 0x7ffec2ffd700 (LWP 46112)]\n[New Thread 0x7ffeb77ff700 (LWP 46114)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\n[New Thread 0x7ffeb6ffe700 (LWP 46115)]\n[New Thread 0x7ffeb67fd700 (LWP 46116)]\n[New Thread 0x7ffea2bff700 (LWP 46117)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:42:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:42:00.0)\n[New Thread 0x7ffea23fe700 (LWP 46118)]\n[New Thread 0x7ffea1bfd700 (LWP 46119)]\n[New Thread 0x7ffea13fc700 (LWP 46120)]\n[New Thread 0x7ffea0bfb700 (LWP 46121)]\n[New Thread 0x7ffe8bfff700 (LWP 46122)]\n[New Thread 0x7ffe8b7fe700 (LWP 46123)]\n[New Thread 0x7ffe8affd700 (LWP 46124)]\n[New Thread 0x7ffe8a7fc700 (LWP 46125)]\n[New Thread 0x7ffe89ffb700 (LWP 46126)]\n[New Thread 0x7ffe897fa700 (LWP 46127)]\n[New Thread 0x7ffe88ff9700 (LWP 46128)]\n[New Thread 0x7ffe7bfff700 (LWP 46129)]\n[New Thread 0x7ffe3d23c700 (LWP 46167)]\n\nProgram received signal SIGSEGV, Segmentation fault.\n__memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n2143    ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S: No such file or directory.\n\nThe Backtrace is attached below\n(gdb) backtrace\n#0  __memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n#1  0x00007fffedd56ae1 in tensorflow::Tensor::FromProto(tensorflow::Allocator*, tensorflow::TensorProto const&) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fffedc4577f in tensorflow::ThreadPoolDevice::MakeTensorFromProto(tensorflow::TensorProto const&, tensorflow::AllocatorAttributes, tensorflow::Tensor*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fffecea4f76 in tensorflow::ConstantOp::ConstantOp(tensorflow::OpKernelConstruction*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fffecea50f2 in tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fffedd408bd in tensorflow::CreateOpKernel(tensorflow::DeviceType, tensorflow::DeviceBase*, tensorflow::Allocator*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fffedc1caf4 in tensorflow::CreateNonCachedKernel(tensorflow::Device*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fffedc10c97 in std::_Function_handler<tensorflow::Status (tensorflow::NodeDef const&, tensorflow::OpKernel**), tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*)::{lambda(tensorflow::NodeDef const&, tensorflow::OpKernel**)#2}>::_M_invoke(std::_Any_data const&, tensorflow::NodeDef const&, tensorflow::OpKernel**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fffedc287de in tensorflow::(anonymous namespace)::ExecutorImpl::Initialize() ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fffedc292b3 in tensorflow::NewLocalExecutor(tensorflow::LocalExecutorParams const&, tensorflow::Graph const*, tensorflow::Executor**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fffedc1608d in tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00007fffedc36dda in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Device*, tensorflow::Graph**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00007fffeda06d9d in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00007fffeda07e6a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std---Type <return> to continue, or q <return> to quit---\n::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00007fffeda0a992 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00007fffedc0b7c7 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00007fffedc0bc11 in TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00007fffece8dff5 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00007fffece8e661 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#19 0x00007fffece7a4d7 in _wrap_TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#20 0x000000000049968d in call_function (oparg=<optimized out>, pp_stack=0x7fffffffdb20) at ../Python/ceval.c:4020\n#21 PyEval_EvalFrameEx (f=f@entry=\n    Frame 0xe341a40, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 628, in _run_fn (session=<SwigPyObject at remote 0x7fffb3547b70>, feed_dict={}, fetch_list=[], target_list=['init'], options=None, run_metadata=<TF_Buffer(this=<SwigPyObject at remote 0x7ffe3d2864e0>) at remote 0x7ffe496ff990>), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#22 0x00000000004a1c9a in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, \n    kwcount=<optimized out>, kws=<optimized out>, argcount=238295616, args=<optimized out>, locals=0x0, \n    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252\n#23 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526\n#24 0x0000000000505f96 in PyObject_Call (func=<function at remote 0x7ffe3d39d938>, arg=<optimized out>, kw=<optimized out>)\n    at ../Objects/abstract.c:2529\n#25 0x000000000049b07a in ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, \n    pp_stack=0x7fffffffdd60, func=<function at remote 0x7ffe3d39d938>) at ../Python/ceval.c:4333\n---Type <return> to continue, or q <return> to quit---\n#26 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe341820, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 644, in _do_call (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Ne...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2705\n#27 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25330, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#28 0x000000000049ab45 in fast_function (nk=0, na=8, n=<optimized out>, pp_stack=0x7fffffffdf50, \n    func=<function at remote 0x7fffc3e28de8>) at ../Python/ceval.c:4116\n#29 call_function (oparg=<optimized out>, pp_stack=0x7fffffffdf50) at ../Python/ceval.c:4041\n#30 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe3415e0, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 637, in _do_run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neu...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#31 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25230, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#32 0x000000000049ab45 in fast_function (nk=0, na=7, n=<optimized out>, pp_stack=0x7fffffffe140, \n    func=<function at remote 0x7fffc3e28d70>) at ../Python/ceval.c:4116\n#33 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe140) at ../Python/ceval.c:4041\n#34 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe33d060, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 564, in _run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variabl---Type <return> to continue, or q <return> to quit---\nes': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#35 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25030, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#36 0x000000000049ab45 in fast_function (nk=0, na=6, n=<optimized out>, pp_stack=0x7fffffffe330, \n    func=<function at remote 0x7fffc3e28cf8>) at ../Python/ceval.c:4116\n#37 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe330) at ../Python/ceval.c:4041\n#38 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x1648a420, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 340, in run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#39 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e22a30, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x7fffc3e24608, defcount=3, \n    closure=0x0) at ../Python/ceval.c:3252\n#40 0x0000000000499a52 in fast_function (nk=0, na=2, n=<optimized out>, pp_stack=0x7fffffffe520, \n    func=<function at remote 0x7fffc3e28b18>) at ../Python/ceval.c:4116\n#41 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe520) at ../Python/ceval.c:4041\n#42 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7ffe3d280de0, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 125, in __start_session (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_t---Type <return> to continue, or q <return> to quit---\nype_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#43 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe670, \n    func=<function at remote 0x7fffb4449b18>) at ../Python/ceval.c:4106\n#44 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe670) at ../Python/ceval.c:4041\n#45 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7fffb3511050, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 62, in form_model_graph (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#46 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe7c0, \n    func=<function at remote 0x7fffb4449938>) at ../Python/ceval.c:4106\n#47 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe7c0) at ../Python/ceval.c:4041\n#48 PyEval_EvalFrameEx (f=f@entry=Frame 0x7ffff7ebf7b0, for file train_script_lstm_attn.py, line 11, in <module> (), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#49 0x00000000004a1634 in PyEval_EvalCodeEx (closure=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, \nPython Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n    locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:3252\nPython Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n#50 PyEval_EvalCode (locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:667\n#51 run_mod.42576 (mod=mod@entry=0x9c1f30, filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n---Type <return> to continue, or q <return> to quit---\n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), flags=flags@entry=0x7fffffffe970, \n    arena=arena@entry=0x9aa9c0) at ../Python/pythonrun.c:1370\n#52 0x000000000044e4a5 in PyRun_FileExFlags (fp=fp@entry=0x976cd0, \n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", start=start@entry=257, \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:1356\n#53 0x000000000044ec9f in PyRun_SimpleFileExFlags (fp=fp@entry=0x976cd0, filename=<optimized out>, \n    filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:948\n#54 0x000000000044ed9b in PyRun_AnyFileExFlags (fp=fp@entry=0x976cd0, \n---Type <return> to continue, or q <return> to quit---\n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, \n    flags=flags@entry=0x7fffffffe970) at ../Python/pythonrun.c:752\n#55 0x000000000044f904 in Py_Main (argc=<optimized out>, argv=0x7fffffffeb28) at ../Modules/main.c:640\n#56 0x00007ffff7818ec5 in __libc_start_main (main=0x44f9c2 <main>, argc=2, argv=0x7fffffffeb28, init=<optimized out>, \n    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffeb18) at libc-start.c:287\n#57 0x0000000000578c4e in _start ()\n(gdb)",
          "bodyHTML": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04</p>\n<p>Installed version of CUDA and cuDNN: Cuda 7.0 and CUDNN 6.5 v4</p>\n<p>So when I use a simple Embedding RNN Sequence to Sequence Model like this</p>\n<pre><code># choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        # embedding model\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n</code></pre>\n<p>The above implementation works perfectly, but when I just change the model from simple embedding seq2seq to Embedding Attention Seq2Seq, like this,</p>\n<pre><code>\n        # choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n\n</code></pre>\n<p>I get segmentation fault, with absolutely no information. My memory does not run out, neither my CPU, as I tried this with</p>\n<pre><code>batch_size =1 \nsee.memory_dim = 1\n</code></pre>\n<p>and still got the same segmentation fault.</p>\n<p>I get the same error, and the above configuration can certainly not eat my RAM.</p>\n<p>This is a potential bug, if I am not getting something worng. The LSTM and GRU cell just takes the size of the hidden layer as parameter, which is a scaler.</p>\n<p>THE BUG REPORT</p>\n<p>The Debug result</p>\n<pre><code>(gdb) run train_script_lstm_attn.py \nStarting program: /lusr/bin/python train_script_lstm_attn.py\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n[New Thread 0x7fffd7a25700 (LWP 45650)]\n[New Thread 0x7fffd7224700 (LWP 45651)]\n[New Thread 0x7fffd4a23700 (LWP 45652)]\n[New Thread 0x7fffd2222700 (LWP 45653)]\n[New Thread 0x7fffcfa21700 (LWP 45654)]\n[New Thread 0x7fffcd220700 (LWP 45655)]\n[New Thread 0x7fffcaa1f700 (LWP 45656)]\n[Thread 0x7fffcaa1f700 (LWP 45656) exited]\n[Thread 0x7fffcfa21700 (LWP 45654) exited]\n[Thread 0x7fffd7a25700 (LWP 45650) exited]\n[Thread 0x7fffd2222700 (LWP 45653) exited]\n[Thread 0x7fffd7224700 (LWP 45651) exited]\n[Thread 0x7fffcd220700 (LWP 45655) exited]\n[Thread 0x7fffd4a23700 (LWP 45652) exited]\n[New Thread 0x7fffcaa1f700 (LWP 45661)]\n[New Thread 0x7fffcd220700 (LWP 46103)]\n[New Thread 0x7fffcfa21700 (LWP 46104)]\n[New Thread 0x7fffd2222700 (LWP 46105)]\n[New Thread 0x7ffed22bf700 (LWP 46106)]\n[New Thread 0x7ffed1abe700 (LWP 46107)]\n[New Thread 0x7ffed12bd700 (LWP 46108)]\n[New Thread 0x7ffed0abc700 (LWP 46109)]\n[New Thread 0x7ffec3fff700 (LWP 46110)]\n[New Thread 0x7ffec37fe700 (LWP 46111)]\n[New Thread 0x7ffec2ffd700 (LWP 46112)]\n[New Thread 0x7ffeb77ff700 (LWP 46114)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\n[New Thread 0x7ffeb6ffe700 (LWP 46115)]\n[New Thread 0x7ffeb67fd700 (LWP 46116)]\n[New Thread 0x7ffea2bff700 (LWP 46117)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:42:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:42:00.0)\n[New Thread 0x7ffea23fe700 (LWP 46118)]\n[New Thread 0x7ffea1bfd700 (LWP 46119)]\n[New Thread 0x7ffea13fc700 (LWP 46120)]\n[New Thread 0x7ffea0bfb700 (LWP 46121)]\n[New Thread 0x7ffe8bfff700 (LWP 46122)]\n[New Thread 0x7ffe8b7fe700 (LWP 46123)]\n[New Thread 0x7ffe8affd700 (LWP 46124)]\n[New Thread 0x7ffe8a7fc700 (LWP 46125)]\n[New Thread 0x7ffe89ffb700 (LWP 46126)]\n[New Thread 0x7ffe897fa700 (LWP 46127)]\n[New Thread 0x7ffe88ff9700 (LWP 46128)]\n[New Thread 0x7ffe7bfff700 (LWP 46129)]\n[New Thread 0x7ffe3d23c700 (LWP 46167)]\n\nProgram received signal SIGSEGV, Segmentation fault.\n__memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n2143    ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S: No such file or directory.\n</code></pre>\n<p>The Backtrace is attached below</p>\n<pre><code>(gdb) backtrace\n#0  __memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n#1  0x00007fffedd56ae1 in tensorflow::Tensor::FromProto(tensorflow::Allocator*, tensorflow::TensorProto const&amp;) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fffedc4577f in tensorflow::ThreadPoolDevice::MakeTensorFromProto(tensorflow::TensorProto const&amp;, tensorflow::AllocatorAttributes, tensorflow::Tensor*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fffecea4f76 in tensorflow::ConstantOp::ConstantOp(tensorflow::OpKernelConstruction*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fffecea50f2 in tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fffedd408bd in tensorflow::CreateOpKernel(tensorflow::DeviceType, tensorflow::DeviceBase*, tensorflow::Allocator*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&amp;, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fffedc1caf4 in tensorflow::CreateNonCachedKernel(tensorflow::Device*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&amp;, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fffedc10c97 in std::_Function_handler&lt;tensorflow::Status (tensorflow::NodeDef const&amp;, tensorflow::OpKernel**), tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&amp;, tensorflow::Device*, tensorflow::Graph*)::{lambda(tensorflow::NodeDef const&amp;, tensorflow::OpKernel**)#2}&gt;::_M_invoke(std::_Any_data const&amp;, tensorflow::NodeDef const&amp;, tensorflow::OpKernel**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fffedc287de in tensorflow::(anonymous namespace)::ExecutorImpl::Initialize() ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fffedc292b3 in tensorflow::NewLocalExecutor(tensorflow::LocalExecutorParams const&amp;, tensorflow::Graph const*, tensorflow::Executor**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fffedc1608d in tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&amp;, tensorflow::Device*, tensorflow::Graph*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00007fffedc36dda in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Device*, tensorflow::Graph**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00007fffeda06d9d in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice&lt;std::string&gt;, tensorflow::gtl::ArraySlice&lt;std::string&gt;, tensorflow::gtl::ArraySlice&lt;std::string&gt;, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00007fffeda07e6a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&amp;, std::vector&lt;std::pair&lt;std::string, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::string, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---\n::allocator&lt;tensorflow::Tensor&gt; &gt;*, tensorflow::RunMetadata*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00007fffeda0a992 in tensorflow::DirectSession::Run(std::vector&lt;std::pair&lt;std::string, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::string, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std::allocator&lt;tensorflow::Tensor&gt; &gt;*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00007fffedc0b7c7 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00007fffedc0bc11 in TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00007fffece8dff5 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector&lt;std::pair&lt;char const*, tagPyArrayObject*&gt;, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00007fffece8e661 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector&lt;std::pair&lt;char const*, tagPyArrayObject*&gt;, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#19 0x00007fffece7a4d7 in _wrap_TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#20 0x000000000049968d in call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffffffdb20) at ../Python/ceval.c:4020\n#21 PyEval_EvalFrameEx (f=f@entry=\n    Frame 0xe341a40, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 628, in _run_fn (session=&lt;SwigPyObject at remote 0x7fffb3547b70&gt;, feed_dict={}, fetch_list=[], target_list=['init'], options=None, run_metadata=&lt;TF_Buffer(this=&lt;SwigPyObject at remote 0x7ffe3d2864e0&gt;) at remote 0x7ffe496ff990&gt;), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#22 0x00000000004a1c9a in PyEval_EvalCodeEx (closure=&lt;optimized out&gt;, defcount=&lt;optimized out&gt;, defs=0x0, \n    kwcount=&lt;optimized out&gt;, kws=&lt;optimized out&gt;, argcount=238295616, args=&lt;optimized out&gt;, locals=0x0, \n    globals=&lt;optimized out&gt;, co=&lt;optimized out&gt;) at ../Python/ceval.c:3252\n#23 function_call.15337 (func=&lt;optimized out&gt;, arg=&lt;optimized out&gt;, kw=&lt;optimized out&gt;) at ../Objects/funcobject.c:526\n#24 0x0000000000505f96 in PyObject_Call (func=&lt;function at remote 0x7ffe3d39d938&gt;, arg=&lt;optimized out&gt;, kw=&lt;optimized out&gt;)\n    at ../Objects/abstract.c:2529\n#25 0x000000000049b07a in ext_do_call (nk=&lt;optimized out&gt;, na=&lt;optimized out&gt;, flags=&lt;optimized out&gt;, \n    pp_stack=0x7fffffffdd60, func=&lt;function at remote 0x7ffe3d39d938&gt;) at ../Python/ceval.c:4333\n---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---\n#26 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe341820, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 644, in _do_call (self=&lt;InteractiveSession(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__': 'train_script_lstm_attn.py', 'lstm_net': &lt;Ne...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2705\n#27 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25330, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;, \n    args=&lt;optimized out&gt;, argcount=&lt;optimized out&gt;, kws=&lt;optimized out&gt;, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#28 0x000000000049ab45 in fast_function (nk=0, na=8, n=&lt;optimized out&gt;, pp_stack=0x7fffffffdf50, \n    func=&lt;function at remote 0x7fffc3e28de8&gt;) at ../Python/ceval.c:4116\n#29 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffffffdf50) at ../Python/ceval.c:4041\n#30 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe3415e0, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 637, in _do_run (self=&lt;InteractiveSession(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__': 'train_script_lstm_attn.py', 'lstm_net': &lt;Neu...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#31 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25230, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;, \n    args=&lt;optimized out&gt;, argcount=&lt;optimized out&gt;, kws=&lt;optimized out&gt;, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#32 0x000000000049ab45 in fast_function (nk=0, na=7, n=&lt;optimized out&gt;, pp_stack=0x7fffffffe140, \n    func=&lt;function at remote 0x7fffc3e28d70&gt;) at ../Python/ceval.c:4116\n#33 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffffffe140) at ../Python/ceval.c:4041\n#34 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe33d060, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 564, in _run (self=&lt;InteractiveSession(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variabl---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---\nes': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__': 'train_script_lstm_attn.py', 'lstm_net': &lt;Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#35 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25030, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;, \n    args=&lt;optimized out&gt;, argcount=&lt;optimized out&gt;, kws=&lt;optimized out&gt;, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#36 0x000000000049ab45 in fast_function (nk=0, na=6, n=&lt;optimized out&gt;, pp_stack=0x7fffffffe330, \n    func=&lt;function at remote 0x7fffc3e28cf8&gt;) at ../Python/ceval.c:4116\n#37 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffffffe330) at ../Python/ceval.c:4041\n#38 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x1648a420, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 340, in run (self=&lt;InteractiveSession(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__': 'train_script_lstm_attn.py', 'lstm_net': &lt;Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#39 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e22a30, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;, \n    args=&lt;optimized out&gt;, argcount=&lt;optimized out&gt;, kws=&lt;optimized out&gt;, kwcount=0, defs=0x7fffc3e24608, defcount=3, \n    closure=0x0) at ../Python/ceval.c:3252\n#40 0x0000000000499a52 in fast_function (nk=0, na=2, n=&lt;optimized out&gt;, pp_stack=0x7fffffffe520, \n    func=&lt;function at remote 0x7fffc3e28b18&gt;) at ../Python/ceval.c:4116\n#41 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffffffe520) at ../Python/ceval.c:4041\n#42 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7ffe3d280de0, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 125, in __start_session (self=&lt;NeuralNet(dec_memory=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_t---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---\nype_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#43 0x0000000000499ef2 in fast_function (nk=&lt;optimized out&gt;, na=&lt;optimized out&gt;, n=1, pp_stack=0x7fffffffe670, \n    func=&lt;function at remote 0x7fffb4449b18&gt;) at ../Python/ceval.c:4106\n#44 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffffffe670) at ../Python/ceval.c:4041\n#45 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7fffb3511050, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 62, in form_model_graph (self=&lt;NeuralNet(dec_memory=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#46 0x0000000000499ef2 in fast_function (nk=&lt;optimized out&gt;, na=&lt;optimized out&gt;, n=1, pp_stack=0x7fffffffe7c0, \n    func=&lt;function at remote 0x7fffb4449938&gt;) at ../Python/ceval.c:4106\n#47 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffffffe7c0) at ../Python/ceval.c:4041\n#48 PyEval_EvalFrameEx (f=f@entry=Frame 0x7ffff7ebf7b0, for file train_script_lstm_attn.py, line 11, in &lt;module&gt; (), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#49 0x00000000004a1634 in PyEval_EvalCodeEx (closure=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, \nPython Exception &lt;class 'UnicodeDecodeError'&gt; 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n    locals=&lt;unknown at remote 0x7ffff7ebf928&gt;, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:3252\nPython Exception &lt;class 'UnicodeDecodeError'&gt; 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n#50 PyEval_EvalCode (locals=&lt;unknown at remote 0x7ffff7ebf928&gt;, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:667\n#51 run_mod.42576 (mod=mod@entry=0x9c1f30, filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__': 'train_script_lstm_attn.py', 'lstm_net': &lt;NeuralNet(dec_memory=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---\n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__': 'train_script_lstm_attn.py', 'lstm_net': &lt;NeuralNet(dec_memory=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), flags=flags@entry=0x7fffffffe970, \n    arena=arena@entry=0x9aa9c0) at ../Python/pythonrun.c:1370\n#52 0x000000000044e4a5 in PyRun_FileExFlags (fp=fp@entry=0x976cd0, \n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", start=start@entry=257, \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__': 'train_script_lstm_attn.py', 'lstm_net': &lt;NeuralNet(dec_memory=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': &lt;module at remote 0x7fffb442f9b8&gt;, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': &lt;module at remote 0x7ffff7f90b08&gt;, '__file__': 'train_script_lstm_attn.py', 'lstm_net': &lt;NeuralNet(dec_memory=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [&lt;Variable(_variable=&lt;Tensor(_consumers=[&lt;Operation(_graph=&lt;...&gt;, _control_inputs=[], _outputs=[&lt;Tensor(_consumers=[], _value_index=0, _shape=&lt;TensorShape(_dims=[&lt;Dimension(_value=130088) at remote 0x7ffea024e310&gt;, &lt;Dimension(_value=200) at remote 0x7ffea024e190&gt;]) at remote 0x7ffea024e290&gt;, _op=&lt;...&gt;, _dtype=&lt;DType(_type_enum=101) at remote 0x7fffc463a810&gt;) at remote 0x7ffea024e3d0&gt;], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '&lt;module&gt;', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:1356\n#53 0x000000000044ec9f in PyRun_SimpleFileExFlags (fp=fp@entry=0x976cd0, filename=&lt;optimized out&gt;, \n    filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:948\n#54 0x000000000044ed9b in PyRun_AnyFileExFlags (fp=fp@entry=0x976cd0, \n---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---\n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, \n    flags=flags@entry=0x7fffffffe970) at ../Python/pythonrun.c:752\n#55 0x000000000044f904 in Py_Main (argc=&lt;optimized out&gt;, argv=0x7fffffffeb28) at ../Modules/main.c:640\n#56 0x00007ffff7818ec5 in __libc_start_main (main=0x44f9c2 &lt;main&gt;, argc=2, argv=0x7fffffffeb28, init=&lt;optimized out&gt;, \n    fini=&lt;optimized out&gt;, rtld_fini=&lt;optimized out&gt;, stack_end=0x7fffffffeb18) at libc-start.c:287\n#57 0x0000000000578c4e in _start ()\n(gdb) \n\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "srinathmadasu" },
          "number": 9570,
          "resourcePath": "/tensorflow/tensorflow/issues/9570",
          "state": "CLOSED",
          "publishedAt": "2017-05-01T14:20:43Z",
          "closedAt": "2017-05-01T18:44:22Z",
          "title": "Tensorflow inconsistence results every run",
          "bodyText": "Please go to Stack Overflow for help and support:\nhttp://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug or a feature request.\nThe form below must be filled out.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nBazel version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
          "bodyHTML": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a rel=\"nofollow\" href=\"http://stackoverflow.com/questions/tagged/tensorflow\">http://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug or a feature request.</li>\n<li>The form below must be filled out.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "leerduo" },
          "number": 96,
          "resourcePath": "/tensorflow/tensorflow/issues/96",
          "state": "CLOSED",
          "publishedAt": "2015-11-10T16:10:54Z",
          "closedAt": "2015-11-11T06:19:59Z",
          "title": "tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.",
          "bodyText": "Hello,I got the error when i  execute:\"pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\"--------[ tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.].",
          "bodyHTML": "<p>Hello,I got the error when i  execute:\"pip install <a href=\"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl%22--------%5B\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\"--------[</a> tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.].</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "jxua" },
          "number": 11633,
          "resourcePath": "/tensorflow/tensorflow/issues/11633",
          "state": "CLOSED",
          "publishedAt": "2017-07-20T06:50:25Z",
          "closedAt": "2017-07-20T16:56:16Z",
          "title": "when connect mnist,download mnist data,show network connection error.",
          "bodyText": "I know if the error show ,just our company can not connect to mnist, can i manual download mnist data, and use it? how can i do this?",
          "bodyHTML": "<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/22673941/28404110-88b436e0-6d5a-11e7-9344-596a22f5d64b.png\"><img src=\"https://user-images.githubusercontent.com/22673941/28404110-88b436e0-6d5a-11e7-9344-596a22f5d64b.png\" alt=\"image\" style=\"max-width:100%;\"></a><br>\nI know if the error show ,just our company can not connect to mnist, can i manual download mnist data, and use it? how can i do this?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ypxie" },
          "number": 2269,
          "resourcePath": "/tensorflow/tensorflow/issues/2269",
          "state": "CLOSED",
          "publishedAt": "2016-05-08T07:10:33Z",
          "closedAt": "2016-05-08T21:49:30Z",
          "title": "How to resize one tensor to (e.g., 1.5 * its original shape)?",
          "bodyText": "The tensor shape is not fixed, and can change with different input.",
          "bodyHTML": "<p>The tensor shape is not fixed, and can change with different input.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "lsorber" },
          "number": 11665,
          "resourcePath": "/tensorflow/tensorflow/issues/11665",
          "state": "CLOSED",
          "publishedAt": "2017-07-21T11:37:38Z",
          "closedAt": "2018-01-23T23:40:32Z",
          "title": "Feature request: add a `local_init_feed_dict` to `tf.train.Scaffold`",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.2.0\nPython version: 3.6.1\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: See below.\n\nDescribe the problem\nFeature request: add a local_init_feed_dict to tf.train.Scaffold. It would be useful to be able to create local variables (which are not saved or restored) and have them initialized by a tf.train.MonitoredTrainingSession with a feed_dict. In the example below, the variable X_var is forced to be part of the GLOBAL_VARIABLES collection in order to be able to initialize the variable with a feed_dict. This has the undesirable consequence that the variable will be saved to disk.\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\n\n# Data that we wish to sample, but not save to disk.\nX = np.eye(15, dtype=np.float32)\n\n# Create a graph that samples rows from X randomly.\ngraph = tf.Graph()\nwith graph.as_default():\n    X_init = tf.placeholder(tf.float32, shape=X.shape)\n    # Here, we want to use tf.GraphKeys.LOCAL_VARIABLES,\n    # but can't because there is no feed_dict for that collection in tf.train.Scaffold.\n    X_var = tf.Variable(X_init, trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n    queue = tf.RandomShuffleQueue(\n        capacity=X.shape[0],\n        min_after_dequeue=1,\n        dtypes=[tf.float32],\n        shapes=[X.shape[1]])\n    enqueue_op = queue.enqueue_many([X_var])\n    row = queue.dequeue()\n\n# Sample a few rows from X.\nwith graph.as_default():\n    sess_params = {\n        'scaffold': tf.train.Scaffold(\n            init_feed_dict={X_init: X},\n            init_fn=lambda scaffold, sess: sess.run(enqueue_op))\n    }\n    with tf.train.MonitoredTrainingSession(**sess_params) as sess:\n        print(sess.run(row))\n        print(sess.run(row))",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2.0</li>\n<li><strong>Python version</strong>: 3.6.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: See below.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Feature request: add a <code>local_init_feed_dict</code> to <code>tf.train.Scaffold</code>. It would be useful to be able to create local variables (which are not saved or restored) and have them initialized by a <code>tf.train.MonitoredTrainingSession</code> with a <code>feed_dict</code>. In the example below, the variable <code>X_var</code> is forced to be part of the <code>GLOBAL_VARIABLES</code> collection in order to be able to initialize the variable with a <code>feed_dict</code>. This has the undesirable consequence that the variable will be saved to disk.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">tensorflow</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">tf</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">numpy</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">np</span>\n\n<span class=\"pl-c\"># Data that we wish to sample, but not save to disk.</span>\n<span class=\"pl-v\">X</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">np</span>.<span class=\"pl-en\">eye</span>(<span class=\"pl-c1\">15</span>, <span class=\"pl-s1\">dtype</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">np</span>.<span class=\"pl-s1\">float32</span>)\n\n<span class=\"pl-c\"># Create a graph that samples rows from X randomly.</span>\n<span class=\"pl-s1\">graph</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">Graph</span>()\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">graph</span>.<span class=\"pl-en\">as_default</span>():\n    <span class=\"pl-v\">X_init</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">placeholder</span>(<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">float32</span>, <span class=\"pl-s1\">shape</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">X</span>.<span class=\"pl-s1\">shape</span>)\n    <span class=\"pl-c\"># Here, we want to use tf.GraphKeys.LOCAL_VARIABLES,</span>\n    <span class=\"pl-c\"># but can't because there is no feed_dict for that collection in tf.train.Scaffold.</span>\n    <span class=\"pl-v\">X_var</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">Variable</span>(<span class=\"pl-v\">X_init</span>, <span class=\"pl-s1\">trainable</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-s1\">collections</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">GraphKeys</span>.<span class=\"pl-v\">GLOBAL_VARIABLES</span>])\n    <span class=\"pl-s1\">queue</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">RandomShuffleQueue</span>(\n        <span class=\"pl-s1\">capacity</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">X</span>.<span class=\"pl-s1\">shape</span>[<span class=\"pl-c1\">0</span>],\n        <span class=\"pl-s1\">min_after_dequeue</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>,\n        <span class=\"pl-s1\">dtypes</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">float32</span>],\n        <span class=\"pl-s1\">shapes</span><span class=\"pl-c1\">=</span>[<span class=\"pl-v\">X</span>.<span class=\"pl-s1\">shape</span>[<span class=\"pl-c1\">1</span>]])\n    <span class=\"pl-s1\">enqueue_op</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">queue</span>.<span class=\"pl-en\">enqueue_many</span>([<span class=\"pl-v\">X_var</span>])\n    <span class=\"pl-s1\">row</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">queue</span>.<span class=\"pl-en\">dequeue</span>()\n\n<span class=\"pl-c\"># Sample a few rows from X.</span>\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">graph</span>.<span class=\"pl-en\">as_default</span>():\n    <span class=\"pl-s1\">sess_params</span> <span class=\"pl-c1\">=</span> {\n        <span class=\"pl-s\">'scaffold'</span>: <span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">train</span>.<span class=\"pl-v\">Scaffold</span>(\n            <span class=\"pl-s1\">init_feed_dict</span><span class=\"pl-c1\">=</span>{<span class=\"pl-v\">X_init</span>: <span class=\"pl-v\">X</span>},\n            <span class=\"pl-s1\">init_fn</span><span class=\"pl-c1\">=</span><span class=\"pl-k\">lambda</span> <span class=\"pl-s1\">scaffold</span>, <span class=\"pl-s1\">sess</span>: <span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">enqueue_op</span>))\n    }\n    <span class=\"pl-k\">with</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">train</span>.<span class=\"pl-v\">MonitoredTrainingSession</span>(<span class=\"pl-c1\">**</span><span class=\"pl-s1\">sess_params</span>) <span class=\"pl-k\">as</span> <span class=\"pl-s1\">sess</span>:\n        <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">row</span>))\n        <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">row</span>))</pre></div>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "zuoxingdong" },
          "number": 3185,
          "resourcePath": "/tensorflow/tensorflow/issues/3185",
          "state": "CLOSED",
          "publishedAt": "2016-07-04T14:29:26Z",
          "closedAt": "2016-07-07T16:49:01Z",
          "title": "Udacity Notebook with \"None\" kernel",
          "bodyText": "I use Jupyter notebook to open the .ipynb files, but it shows a red \"None\" kernel on top right corner and all lines of code cannot run.\nMethod I use:\n\nBuild a new directory and extract .ipynb files from examples/udacity to the directory\nIn terminal, run jupyter notebook",
          "bodyHTML": "<p>I use Jupyter notebook to open the .ipynb files, but it shows a red \"None\" kernel on top right corner and all lines of code cannot run.</p>\n<p>Method I use:</p>\n<ol>\n<li>Build a new directory and extract .ipynb files from <code>examples/udacity</code> to the directory</li>\n<li>In terminal, run <code>jupyter notebook</code></li>\n</ol>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "dbakshee" },
          "number": 11099,
          "resourcePath": "/tensorflow/tensorflow/issues/11099",
          "state": "CLOSED",
          "publishedAt": "2017-06-28T04:26:13Z",
          "closedAt": "2018-01-24T20:22:44Z",
          "title": "Typo in illustrating figure for XLA/Concatenation operation",
          "bodyText": "Illustrating image for Concatenate\nsuggests Concat({ 2x4, 2x8 }, dimension=0) is 2x12. Should be dimension=1, and same for the other examples.",
          "bodyHTML": "<p>Illustrating image for <a href=\"https://www.tensorflow.org/performance/xla/operation_semantics#concatenate\" rel=\"nofollow\">Concatenate</a><br>\nsuggests Concat({ 2x4, 2x8 }, dimension=0) is 2x12. Should be dimension=1, and same for the other examples.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "lijhong" },
          "number": 12764,
          "resourcePath": "/tensorflow/tensorflow/issues/12764",
          "state": "CLOSED",
          "publishedAt": "2017-09-02T12:12:36Z",
          "closedAt": "2017-09-02T19:31:07Z",
          "title": "the side &deep model   is not good compare with deep model and wide model ",
          "bodyText": "these days ,I'm learning the wide & deep model ,and run the  wide_n_deep_tutorial.py, so the anwser looks like this:\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=deep\nTraining data is downloaded to /tmp/tmpFB4dsd\nTest data is downloaded to /tmp/tmpomj5Pi\n2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpWei9wK\naccuracy: 0.850071\naccuracy_baseline: 0.763774\nauc: 0.894038\nauc_precision_recall: 0.743199\naverage_loss: 0.393638\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 39.3179\nprediction/mean: 0.242167\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=wide\nTraining data is downloaded to /tmp/tmpFJdWft\nTest data is downloaded to /tmp/tmpjB5nm7\n2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpuPHsDx\naccuracy: 0.835391\naccuracy_baseline: 0.763774\nauc: 0.882763\nauc_precision_recall: 0.694257\naverage_loss: 0.352975\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 35.2563\nprediction/mean: 0.240918\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py\nTraining data is downloaded to /tmp/tmpDdWc_T\nTest data is downloaded to /tmp/tmpFF0PZJ\n2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\nyour machine and could speed up CPU computations.\n2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\nur machine and could speed up CPU computations.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\nmodel directory = /tmp/tmpq2M7SE\naccuracy: 0.820834\naccuracy_baseline: 0.763774\nauc: 0.850518\nauc_precision_recall: 0.676198\naverage_loss: 0.424271\nglobal_step: 2000\nlabel/mean: 0.236226\nloss: 42.3776\nprediction/mean: 0.256489\nI don't know why looks likes this, so someone can help me? thank you.",
          "bodyHTML": "<p>these days ,I'm learning the wide &amp; deep model ,and run the  wide_n_deep_tutorial.py, so the anwser looks like this:</p>\n<p>XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=deep<br>\nTraining data is downloaded to /tmp/tmpFB4dsd<br>\nTest data is downloaded to /tmp/tmpomj5Pi<br>\n2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on<br>\nyour machine and could speed up CPU computations.<br>\n2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on<br>\nyour machine and could speed up CPU computations.<br>\n2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo<br>\nur machine and could speed up CPU computations.<br>\nWARNING:tensorflow:Casting &lt;dtype: 'float32'&gt; labels to bool.<br>\nWARNING:tensorflow:Casting &lt;dtype: 'float32'&gt; labels to bool.<br>\nmodel directory = /tmp/tmpWei9wK<br>\naccuracy: 0.850071<br>\naccuracy_baseline: 0.763774<br>\nauc: 0.894038<br>\nauc_precision_recall: 0.743199<br>\naverage_loss: 0.393638<br>\nglobal_step: 2000<br>\nlabel/mean: 0.236226<br>\nloss: 39.3179<br>\nprediction/mean: 0.242167</p>\n<p>XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=wide<br>\nTraining data is downloaded to /tmp/tmpFJdWft<br>\nTest data is downloaded to /tmp/tmpjB5nm7<br>\n2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on<br>\nyour machine and could speed up CPU computations.<br>\n2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on<br>\nyour machine and could speed up CPU computations.<br>\n2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo<br>\nur machine and could speed up CPU computations.<br>\nWARNING:tensorflow:Casting &lt;dtype: 'float32'&gt; labels to bool.<br>\nWARNING:tensorflow:Casting &lt;dtype: 'float32'&gt; labels to bool.<br>\nmodel directory = /tmp/tmpuPHsDx<br>\naccuracy: 0.835391<br>\naccuracy_baseline: 0.763774<br>\nauc: 0.882763<br>\nauc_precision_recall: 0.694257<br>\naverage_loss: 0.352975<br>\nglobal_step: 2000<br>\nlabel/mean: 0.236226<br>\nloss: 35.2563<br>\nprediction/mean: 0.240918</p>\n<p>XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py<br>\nTraining data is downloaded to /tmp/tmpDdWc_T<br>\nTest data is downloaded to /tmp/tmpFF0PZJ<br>\n2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on<br>\nyour machine and could speed up CPU computations.<br>\n2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on<br>\nyour machine and could speed up CPU computations.<br>\n2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo<br>\nur machine and could speed up CPU computations.<br>\nWARNING:tensorflow:Casting &lt;dtype: 'float32'&gt; labels to bool.<br>\nWARNING:tensorflow:Casting &lt;dtype: 'float32'&gt; labels to bool.<br>\nmodel directory = /tmp/tmpq2M7SE<br>\naccuracy: 0.820834<br>\naccuracy_baseline: 0.763774<br>\nauc: 0.850518<br>\nauc_precision_recall: 0.676198<br>\naverage_loss: 0.424271<br>\nglobal_step: 2000<br>\nlabel/mean: 0.236226<br>\nloss: 42.3776<br>\nprediction/mean: 0.256489</p>\n<p>I don't know why looks likes this, so someone can help me? thank you.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "apiro" },
          "number": 17358,
          "resourcePath": "/tensorflow/tensorflow/issues/17358",
          "state": "CLOSED",
          "publishedAt": "2018-03-01T15:33:26Z",
          "closedAt": "2018-03-01T19:00:43Z",
          "title": "Distributed training: Evaluation and inference best practices",
          "bodyText": "I understand tensorflow distributed training and I implemented my own script.\nWhat I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.\nLet's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.\nMy intuition to achieve this goal is to do the following:\n...\nelif FLAGS.job_name == \"worker\":\n\n    if FLAGS.task_index <= (len(cluster_dict[\"worker\"][:-2]) - 1):\n         logging.info(\"Training worker started\")\n         ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                train_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.TRAIN\n                )\n               with tf.train.MonitoredTrainingSession(\n                    is_chief=(FLAGS.task_index == 0),\n                    master=server.target,\n                    checkpoint_dir=ckpt_dir,\n                    config=config_proto,\n                    hooks=hooks\n                ) as mon_sess:\n                    while not mon_sess.should_stop():\n                        res = train_model.train(...)\n                        ...\n\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-2]) - 1):\n         logging.info(\"Evaluation worker started\")\n         ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                eval_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.EVAL\n                )\n                ...\n\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-1]) - 1):\n        logging.info(\"Inference worker started\")\n        ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                infer_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.INFER\n                )\n                ...\n\nNow, what about the evaluation and inference sessions?\nFor training, I can use tf.train.MonitoredTrainingSession, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use tf.Session.\nRegarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls  eval_model.eval(...) or  infer_model.infer(...), but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to \"periodically\" is to sleep the thread.\nWhat do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?\nAlberto",
          "bodyHTML": "<p>I understand tensorflow distributed training and I implemented my own script.</p>\n<p>What I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.</p>\n<p>Let's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.</p>\n<p>My intuition to achieve this goal is to do the following:</p>\n<pre><code>...\nelif FLAGS.job_name == \"worker\":\n\n    if FLAGS.task_index &lt;= (len(cluster_dict[\"worker\"][:-2]) - 1):\n         logging.info(\"Training worker started\")\n         ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                train_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.TRAIN\n                )\n               with tf.train.MonitoredTrainingSession(\n                    is_chief=(FLAGS.task_index == 0),\n                    master=server.target,\n                    checkpoint_dir=ckpt_dir,\n                    config=config_proto,\n                    hooks=hooks\n                ) as mon_sess:\n                    while not mon_sess.should_stop():\n                        res = train_model.train(...)\n                        ...\n\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-2]) - 1):\n         logging.info(\"Evaluation worker started\")\n         ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                eval_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.EVAL\n                )\n                ...\n\n   elif FLAGS.task_index == (len(cluster_dict[\"worker\"][-1]) - 1):\n        logging.info(\"Inference worker started\")\n        ...\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster,\n                ps_tasks=len(cluster_dict[\"ps\"])\n            )):\n                infer_model = Model(\n                    mode=tf.contrib.learn.ModeKeys.INFER\n                )\n                ...\n</code></pre>\n<p>Now, what about the evaluation and inference sessions?<br>\nFor training, I can use <code>tf.train.MonitoredTrainingSession</code>, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use <code>tf.Session</code>.</p>\n<p>Regarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls  <code>eval_model.eval(...)</code> or  <code>infer_model.infer(...)</code>, but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to \"periodically\" is to sleep the thread.</p>\n<p>What do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?</p>\n<p>Alberto</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "orpillar" },
          "number": 14170,
          "resourcePath": "/tensorflow/tensorflow/issues/14170",
          "state": "CLOSED",
          "publishedAt": "2017-11-02T03:23:27Z",
          "closedAt": "2017-11-03T01:45:06Z",
          "title": "List of functions could be improved with \"const std::string&\" or \"std::string&\" instead \"std::string\"",
          "bodyText": "After a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":\nAfter a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":\n\nc/c_api_function.cc:  static string Normalize(string name);\nSuggestion: string& name;\nc/c_api_function.cc:string NodeNameMapping::Normalize(string name) {\nSuggestion: string& name\n\n\n\ncompiler/jit/graph_to_functiondef.cc:  string NormalizeHelper(string name) const;\nSuggestion: string& name\ncompiler/jit/graph_to_functiondef.cc:  string UniquifyHelper(string name);\nSuggestion: const string&\ncompiler/jit/graph_to_functiondef.cc:string NodeNameMapping::NormalizeHelper(string name) const {\nSuggestion: string& name\ncompiler/jit/graph_to_functiondef.cc:string NodeNameMapping::UniquifyHelper(string name) {\nSuggestion: const string&\ncompiler/tf2xla/dump_graph.cc:string MakeUniquePath(string name) {\nSuggestion: string& name\ncompiler/xla/service/llvm_ir/llvm_util.cc:string IrName(string a) {\nSuggestion: string& a\ncompiler/xla/service/llvm_ir/llvm_util.cc:string SanitizeFunctionName(string function_name) {\nSuggestion: string& function_name\ncompiler/xla/service/llvm_ir/llvm_util.h:string IrName(string a);\nSuggestion: string& a\ncompiler/xla/service/llvm_ir/llvm_util.h:string SanitizeFunctionName(string function_name);\nSuggestion: string& function_name\ncompiler/xla/util.cc:string SanitizeFileName(string file_name) {\nSuggestion: string& file_name\ncompiler/xla/util.h:string SanitizeFileName(string file_name);\nSuggestion: string& file_name\n\n\n\ncontrib/verbs/rdma.cc:RdmaBuffer::RdmaBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaAckBuffer::RdmaAckBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaMessageBuffer::RdmaMessageBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.cc:RdmaTensorBuffer::RdmaTensorBuffer(RdmaChannel* channel, string name)\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaAckBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaMessageBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\ncontrib/verbs/rdma.h:  explicit RdmaTensorBuffer(RdmaChannel* channel, string name);\nSuggestion: const string& name\n\n\n\ncore/common_runtime/step_stats_collector.cc:static int ExtractGpuWithStreamAll(string device_name) {\nSuggestion: string& device_name\ncore/common_runtime/step_stats_collector.cc:static int ExtractGpuWithoutStream(string device_name) {\nSuggestion: string& device_name\ncore/kernels/ops_util.cc:string SanitizeThreadSuffix(string suffix) {\nSuggestion: const string& suffix\ncore/kernels/ops_util.h:string SanitizeThreadSuffix(string suffix);\nSuggestion: const string& suffix\ncore/kernels/xsmm_conv2d.cc:static void chk_libxsmm_err(libxsmm_dnn_err_t status, string msg) {\nSuggestion: const string& msg\n\n\n\nstream_executor/platform.cc:PlatformKind PlatformKindFromString(string kind) {\nSuggestion: const string& kind\nstream_executor/platform.h:PlatformKind PlatformKindFromString(string platform_string);\nSuggestion: const string& platform_string",
          "bodyHTML": "<p>After a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":</p>\n<p>After a quick scan in the latest tensorflow \"master\" branch, here is the list of functions which could improve passing parameter by \"std::string\":</p>\n<ol>\n<li>c/c_api_function.cc:  static string Normalize(string name);<br>\nSuggestion: string&amp; name;</li>\n<li>c/c_api_function.cc:string NodeNameMapping::Normalize(string name) {<br>\nSuggestion: string&amp; name</li>\n</ol>\n<h1></h1>\n<ol start=\"3\">\n<li>compiler/jit/graph_to_functiondef.cc:  string NormalizeHelper(string name) const;<br>\nSuggestion: string&amp; name</li>\n<li>compiler/jit/graph_to_functiondef.cc:  string UniquifyHelper(string name);<br>\nSuggestion: const string&amp;</li>\n<li>compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::NormalizeHelper(string name) const {<br>\nSuggestion: string&amp; name</li>\n<li>compiler/jit/graph_to_functiondef.cc:string NodeNameMapping::UniquifyHelper(string name) {<br>\nSuggestion: const string&amp;</li>\n<li>compiler/tf2xla/dump_graph.cc:string MakeUniquePath(string name) {<br>\nSuggestion: string&amp; name</li>\n<li>compiler/xla/service/llvm_ir/llvm_util.cc:string IrName(string a) {<br>\nSuggestion: string&amp; a</li>\n<li>compiler/xla/service/llvm_ir/llvm_util.cc:string SanitizeFunctionName(string function_name) {<br>\nSuggestion: string&amp; function_name</li>\n<li>compiler/xla/service/llvm_ir/llvm_util.h:string IrName(string a);<br>\nSuggestion: string&amp; a</li>\n<li>compiler/xla/service/llvm_ir/llvm_util.h:string SanitizeFunctionName(string function_name);<br>\nSuggestion: string&amp; function_name</li>\n<li>compiler/xla/util.cc:string SanitizeFileName(string file_name) {<br>\nSuggestion: string&amp; file_name</li>\n<li>compiler/xla/util.h:string SanitizeFileName(string file_name);<br>\nSuggestion: string&amp; file_name</li>\n</ol>\n<h1></h1>\n<ol start=\"14\">\n<li>contrib/verbs/rdma.cc:RdmaBuffer::RdmaBuffer(RdmaChannel* channel, string name)<br>\nSuggestion: const string&amp; name</li>\n<li>contrib/verbs/rdma.cc:RdmaAckBuffer::RdmaAckBuffer(RdmaChannel* channel, string name)<br>\nSuggestion: const string&amp; name</li>\n<li>contrib/verbs/rdma.cc:RdmaMessageBuffer::RdmaMessageBuffer(RdmaChannel* channel, string name)<br>\nSuggestion: const string&amp; name</li>\n<li>contrib/verbs/rdma.cc:RdmaTensorBuffer::RdmaTensorBuffer(RdmaChannel* channel, string name)<br>\nSuggestion: const string&amp; name</li>\n<li>contrib/verbs/rdma.h:  explicit RdmaBuffer(RdmaChannel* channel, string name);<br>\nSuggestion: const string&amp; name</li>\n<li>contrib/verbs/rdma.h:  explicit RdmaAckBuffer(RdmaChannel* channel, string name);<br>\nSuggestion: const string&amp; name</li>\n<li>contrib/verbs/rdma.h:  explicit RdmaMessageBuffer(RdmaChannel* channel, string name);<br>\nSuggestion: const string&amp; name</li>\n<li>contrib/verbs/rdma.h:  explicit RdmaTensorBuffer(RdmaChannel* channel, string name);<br>\nSuggestion: const string&amp; name</li>\n</ol>\n<h1></h1>\n<ol start=\"22\">\n<li>core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithStreamAll(string device_name) {<br>\nSuggestion: string&amp; device_name</li>\n<li>core/common_runtime/step_stats_collector.cc:static int ExtractGpuWithoutStream(string device_name) {<br>\nSuggestion: string&amp; device_name</li>\n<li>core/kernels/ops_util.cc:string SanitizeThreadSuffix(string suffix) {<br>\nSuggestion: const string&amp; suffix</li>\n<li>core/kernels/ops_util.h:string SanitizeThreadSuffix(string suffix);<br>\nSuggestion: const string&amp; suffix</li>\n<li>core/kernels/xsmm_conv2d.cc:static void chk_libxsmm_err(libxsmm_dnn_err_t status, string msg) {<br>\nSuggestion: const string&amp; msg</li>\n</ol>\n<h1></h1>\n<ol start=\"27\">\n<li>stream_executor/platform.cc:PlatformKind PlatformKindFromString(string kind) {<br>\nSuggestion: const string&amp; kind</li>\n<li>stream_executor/platform.h:PlatformKind PlatformKindFromString(string platform_string);<br>\nSuggestion: const string&amp; platform_string</li>\n</ol>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "sibyjackgrove" },
          "number": 18952,
          "resourcePath": "/tensorflow/tensorflow/issues/18952",
          "state": "CLOSED",
          "publishedAt": "2018-04-29T00:38:12Z",
          "closedAt": "2018-05-01T18:23:31Z",
          "title": "Feature request: Option to create dataset from a subset of the columns in the CSV file using tf.contrib.data.make_csv_dataset()",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.8\nPython version:  3.6\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory:NA\nExact command to reproduce:\ntf.contrib.data.make_csv_dataset()\n\nDescribe the problem\nThe tf.contrib.data.make_csv_dataset() is a very useful feature which allows us to convert CSV files directly into at dataset without having to use Pandas library (like shown here). However it is missing an important feature which Pandas had, that is to read a subset of the columns in the CSV file.\nFor example the following code:\ndataset=tf.contrib.data.make_csv_dataset(file_pattern='./data/power_data/MISO_power_data1.csv',batch_size=24,shuffle=False)\ndataset = dataset.batch(4)\nX_iter = dataset.make_one_shot_iterator()\nX_batch = X_iter.get_next()\nX_batch\n\nresults in following dataset:\n{'Actual_Load_MWh': <tf.Tensor 'IteratorGetNext_9:0' shape=(?, ?) dtype=float32>,\n 'Hour_Ending': <tf.Tensor 'IteratorGetNext_9:1' shape=(?, ?) dtype=int32>,\n 'Market_Day': <tf.Tensor 'IteratorGetNext_9:2' shape=(?, ?) dtype=int32>,\n 'Wind_MWh': <tf.Tensor 'IteratorGetNext_9:3' shape=(?, ?) dtype=float32>}\n\nHowever I don't want feature columns for 'Hour_Ending'  and  'Market_Day' in my dataset (since they are not relevant training data) . This could be done in Pandas using code below:\ndf_input=pd.read_csv('./data/power_data/MISO_power_data1.csv',\n                         usecols=['Wind_MWh', 'Actual_Load_MWh'], nrows=24)\n\nI know the easy solution would be to create a CSV file having only the feature columns I want. But it would be a great utility feature to add before make_csv_dataset() migrates out of contrib into core TF. I can submit a PR for this if required.",
          "bodyHTML": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Custom</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8</li>\n<li><strong>Python version</strong>:  3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>:NA</li>\n<li><strong>Exact command to reproduce</strong>:<br>\ntf.contrib.data.make_csv_dataset()</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The <code>tf.contrib.data.make_csv_dataset()</code> is a very useful feature which allows us to convert CSV files directly into at dataset without having to use Pandas library (like shown <a href=\"https://stackoverflow.com/questions/49116343/dataset-api-flat-map-method-producing-error-for-same-code-which-works-with-ma/49140725#49140725\" rel=\"nofollow\">here</a>). However it is missing an important feature which Pandas had, that is to read a subset of the columns in the CSV file.<br>\nFor example the following code:</p>\n<pre><code>dataset=tf.contrib.data.make_csv_dataset(file_pattern='./data/power_data/MISO_power_data1.csv',batch_size=24,shuffle=False)\ndataset = dataset.batch(4)\nX_iter = dataset.make_one_shot_iterator()\nX_batch = X_iter.get_next()\nX_batch\n</code></pre>\n<p>results in following dataset:</p>\n<pre><code>{'Actual_Load_MWh': &lt;tf.Tensor 'IteratorGetNext_9:0' shape=(?, ?) dtype=float32&gt;,\n 'Hour_Ending': &lt;tf.Tensor 'IteratorGetNext_9:1' shape=(?, ?) dtype=int32&gt;,\n 'Market_Day': &lt;tf.Tensor 'IteratorGetNext_9:2' shape=(?, ?) dtype=int32&gt;,\n 'Wind_MWh': &lt;tf.Tensor 'IteratorGetNext_9:3' shape=(?, ?) dtype=float32&gt;}\n</code></pre>\n<p>However I don't want feature columns for 'Hour_Ending'  and  'Market_Day' in my dataset (since they are not relevant training data) . This could be done in Pandas using code below:</p>\n<pre><code>df_input=pd.read_csv('./data/power_data/MISO_power_data1.csv',\n                         usecols=['Wind_MWh', 'Actual_Load_MWh'], nrows=24)\n</code></pre>\n<p>I know the easy solution would be to create a CSV file having only the feature columns I want. But it would be a great utility feature to add before <code>make_csv_dataset()</code> migrates out of contrib into core TF. I can submit a PR for this if required.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "bb4242" },
          "number": 4917,
          "resourcePath": "/tensorflow/tensorflow/issues/4917",
          "state": "CLOSED",
          "publishedAt": "2016-10-12T18:29:30Z",
          "closedAt": "2016-10-12T21:02:55Z",
          "title": "QueueRunner deadlock when using all CPUs",
          "bodyText": "I'm building an input pipeline following the guidelines here.  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using tf.py_func), and return the processed results to an output queue.  I'd like to use QueueRunner's ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:\nimport numpy as np\nimport multiprocessing\nimport tensorflow as tf\n\nn_cpus = multiprocessing.cpu_count()\n\nsess = tf.Session()\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\nmult = tf.mul(a, b)\n\ndef python_op(x):\n    print \"python_op called with {}\".format(x)\n    # In my real function, the np.cos and np.sin calls are replaced by\n    # python calculations I can't do in tensorflow\n    y = np.cos(x)\n    z = sess.run(mult, feed_dict={a: y, b: x})\n    print \"intermediate result is {}\".format(z)\n    return np.sin(z)\n\nn_inputs = n_cpus\ninput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nload_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))\n\noutput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nget_result = output_queue.dequeue_many(n_inputs)\n\ndef processing_pipeline():\n    input_value = input_queue.dequeue()\n    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))\n\n# Here's the problem: If we use all CPUs here, the program will deadlock.\n# If we change cpus to (cpus-1), it works as expected.\nrunner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))\n\ncoord = tf.train.Coordinator()\nrunner.create_threads(sess, coord=coord, start=True)\n\nprint \"Loading input\"\nsess.run(load_input)\nsess.run(input_queue.close())\n\ntry:\n    print \"waiting for result\"\n    result = sess.run(get_result)\n    print \"RESULT: {}\".format(result)\nexcept tf.errors.OutOfRangeError:\n    print \"Input exhausted\"\n\ncoord.request_stop()\ncoord.join()\nprint \"Done\"\nThe program above deadlocks waiting for sess.run to complete in python_op:\n$ python queuetest.py                                                                                                                                                                                                                                                  \nLoading input\npython_op called with [ 0.65624136]\n python_op called with [ 0.80651367]\npython_op called with [ 0.31998941]\n python_op called with [ 0.726421]\n python_op called with [ 0.33133706]\npython_op called with [ 0.4912357]\npython_op called with [ 0.27365881]\npython_op called with [ 0.32846987]\n\nThis is running on an 8-core machine; you can see that 8 python_ops are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing (n_cpus) to (n_cpus-1) in the line that creates the tf.train.QueueRunner, then the program runs to completion:\n$ python queuetest.py                                                                                                                                                                                                                                                        \nLoading input\npython_op called with [ 0.40804103]\npython_op called with [ 0.0182138]\npython_op called with [ 0.17579727]\n python_op called with [ 0.29143187]\npython_op called with [ 0.11612369]\n intermediate result is [ 0.37454084]\npython_op called with [ 0.679506]\npython_op called with [ 0.50754625]\nintermediate result is [ 0.01821078]\nintermediate result is [ 0.52857631]\n intermediate result is [ 0.27914321]\nwaiting for result\npython_op called with [ 0.68288684]\n intermediate result is [ 0.44356483]\nintermediate result is [ 0.11534163]\n intermediate result is [ 0.17308778]\nintermediate result is [ 0.52975237]\nRESULT: [[ 0.36584523]\n [ 0.01820978]\n [ 0.50430447]\n [ 0.42916203]\n [ 0.11508605]\n [ 0.27553213]\n [ 0.1722248 ]\n [ 0.50531965]]\nDone\n\nThe program also completes successfully if we pass in fewer examples than CPUs in the input queue.\nI realize it's somewhat awkward for python_op to call back into the tensorflow session.  However, the threading and queues section of the manual states:\n\"The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.\"\nSo, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?\nAs a side note, one option to work around my problems would be to break python_op into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since python_op's real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.\nOS: Linux\nTensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)",
          "bodyHTML": "<p>I'm building an input pipeline following the guidelines <a href=\"https://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html\" rel=\"nofollow\">here</a>.  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using <code>tf.py_func</code>), and return the processed results to an output queue.  I'd like to use <code>QueueRunner</code>'s ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">numpy</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">np</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">multiprocessing</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">tensorflow</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">tf</span>\n\n<span class=\"pl-s1\">n_cpus</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">multiprocessing</span>.<span class=\"pl-en\">cpu_count</span>()\n\n<span class=\"pl-s1\">sess</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">Session</span>()\n<span class=\"pl-s1\">a</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">placeholder</span>(<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">float32</span>)\n<span class=\"pl-s1\">b</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">placeholder</span>(<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">float32</span>)\n<span class=\"pl-s1\">mult</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">mul</span>(<span class=\"pl-s1\">a</span>, <span class=\"pl-s1\">b</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">python_op</span>(<span class=\"pl-s1\">x</span>):\n    <span class=\"pl-k\">print</span> <span class=\"pl-s\">\"python_op called with {}\"</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">x</span>)\n    <span class=\"pl-c\"># In my real function, the np.cos and np.sin calls are replaced by</span>\n    <span class=\"pl-c\"># python calculations I can't do in tensorflow</span>\n    <span class=\"pl-s1\">y</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">np</span>.<span class=\"pl-en\">cos</span>(<span class=\"pl-s1\">x</span>)\n    <span class=\"pl-s1\">z</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">mult</span>, <span class=\"pl-s1\">feed_dict</span><span class=\"pl-c1\">=</span>{<span class=\"pl-s1\">a</span>: <span class=\"pl-s1\">y</span>, <span class=\"pl-s1\">b</span>: <span class=\"pl-s1\">x</span>})\n    <span class=\"pl-k\">print</span> <span class=\"pl-s\">\"intermediate result is {}\"</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">z</span>)\n    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">np</span>.<span class=\"pl-en\">sin</span>(<span class=\"pl-s1\">z</span>)\n\n<span class=\"pl-s1\">n_inputs</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">n_cpus</span>\n<span class=\"pl-s1\">input_queue</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">FIFOQueue</span>(<span class=\"pl-c1\">10000</span>, [<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">float32</span>], <span class=\"pl-s1\">shapes</span><span class=\"pl-c1\">=</span>[<span class=\"pl-c1\">1</span>])\n<span class=\"pl-s1\">load_input</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">input_queue</span>.<span class=\"pl-en\">enqueue_many</span>(<span class=\"pl-s1\">np</span>.<span class=\"pl-s1\">random</span>.<span class=\"pl-en\">random</span>((<span class=\"pl-s1\">n_inputs</span>, <span class=\"pl-c1\">1</span>)))\n\n<span class=\"pl-s1\">output_queue</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">FIFOQueue</span>(<span class=\"pl-c1\">10000</span>, [<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">float32</span>], <span class=\"pl-s1\">shapes</span><span class=\"pl-c1\">=</span>[<span class=\"pl-c1\">1</span>])\n<span class=\"pl-s1\">get_result</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">output_queue</span>.<span class=\"pl-en\">dequeue_many</span>(<span class=\"pl-s1\">n_inputs</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">processing_pipeline</span>():\n    <span class=\"pl-s1\">input_value</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">input_queue</span>.<span class=\"pl-en\">dequeue</span>()\n    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">output_queue</span>.<span class=\"pl-en\">enqueue</span>(<span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">py_func</span>(<span class=\"pl-s1\">python_op</span>, [<span class=\"pl-s1\">input_value</span>], [<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">float32</span>], <span class=\"pl-c1\">False</span>))\n\n<span class=\"pl-c\"># Here's the problem: If we use all CPUs here, the program will deadlock.</span>\n<span class=\"pl-c\"># If we change cpus to (cpus-1), it works as expected.</span>\n<span class=\"pl-s1\">runner</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">train</span>.<span class=\"pl-v\">QueueRunner</span>(<span class=\"pl-s1\">output_queue</span>, [<span class=\"pl-en\">processing_pipeline</span>()] <span class=\"pl-c1\">*</span> (<span class=\"pl-s1\">n_cpus</span>))\n\n<span class=\"pl-s1\">coord</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">train</span>.<span class=\"pl-v\">Coordinator</span>()\n<span class=\"pl-s1\">runner</span>.<span class=\"pl-en\">create_threads</span>(<span class=\"pl-s1\">sess</span>, <span class=\"pl-s1\">coord</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">coord</span>, <span class=\"pl-s1\">start</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">print</span> <span class=\"pl-s\">\"Loading input\"</span>\n<span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">load_input</span>)\n<span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">input_queue</span>.<span class=\"pl-en\">close</span>())\n\n<span class=\"pl-k\">try</span>:\n    <span class=\"pl-k\">print</span> <span class=\"pl-s\">\"waiting for result\"</span>\n    <span class=\"pl-s1\">result</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">get_result</span>)\n    <span class=\"pl-k\">print</span> <span class=\"pl-s\">\"RESULT: {}\"</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">result</span>)\n<span class=\"pl-k\">except</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">errors</span>.<span class=\"pl-v\">OutOfRangeError</span>:\n    <span class=\"pl-k\">print</span> <span class=\"pl-s\">\"Input exhausted\"</span>\n\n<span class=\"pl-s1\">coord</span>.<span class=\"pl-en\">request_stop</span>()\n<span class=\"pl-s1\">coord</span>.<span class=\"pl-en\">join</span>()\n<span class=\"pl-k\">print</span> <span class=\"pl-s\">\"Done\"</span></pre></div>\n<p>The program above deadlocks waiting for <code>sess.run</code> to complete in <code>python_op</code>:</p>\n<pre><code>$ python queuetest.py                                                                                                                                                                                                                                                  \nLoading input\npython_op called with [ 0.65624136]\n python_op called with [ 0.80651367]\npython_op called with [ 0.31998941]\n python_op called with [ 0.726421]\n python_op called with [ 0.33133706]\npython_op called with [ 0.4912357]\npython_op called with [ 0.27365881]\npython_op called with [ 0.32846987]\n</code></pre>\n<p>This is running on an 8-core machine; you can see that 8 <code>python_op</code>s are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing <code>(n_cpus)</code> to <code>(n_cpus-1)</code> in the line that creates the <code>tf.train.QueueRunner</code>, then the program runs to completion:</p>\n<pre><code>$ python queuetest.py                                                                                                                                                                                                                                                        \nLoading input\npython_op called with [ 0.40804103]\npython_op called with [ 0.0182138]\npython_op called with [ 0.17579727]\n python_op called with [ 0.29143187]\npython_op called with [ 0.11612369]\n intermediate result is [ 0.37454084]\npython_op called with [ 0.679506]\npython_op called with [ 0.50754625]\nintermediate result is [ 0.01821078]\nintermediate result is [ 0.52857631]\n intermediate result is [ 0.27914321]\nwaiting for result\npython_op called with [ 0.68288684]\n intermediate result is [ 0.44356483]\nintermediate result is [ 0.11534163]\n intermediate result is [ 0.17308778]\nintermediate result is [ 0.52975237]\nRESULT: [[ 0.36584523]\n [ 0.01820978]\n [ 0.50430447]\n [ 0.42916203]\n [ 0.11508605]\n [ 0.27553213]\n [ 0.1722248 ]\n [ 0.50531965]]\nDone\n</code></pre>\n<p>The program also completes successfully if we pass in fewer examples than CPUs in the input queue.</p>\n<p>I realize it's somewhat awkward for <code>python_op</code> to call back into the tensorflow session.  However, the <a href=\"https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html#threading-and-queues\" rel=\"nofollow\">threading and queues</a> section of the manual states:</p>\n<p>\"The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.\"</p>\n<p>So, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?</p>\n<p>As a side note, one option to work around my problems would be to break <code>python_op</code> into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since <code>python_op</code>'s real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.</p>\n<p>OS: Linux<br>\nTensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ThatsDevraj" },
          "number": 8273,
          "resourcePath": "/tensorflow/tensorflow/issues/8273",
          "state": "CLOSED",
          "publishedAt": "2017-03-10T12:02:14Z",
          "closedAt": "2017-03-10T16:18:36Z",
          "title": "Error:",
          "bodyText": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nEnvironment info\nOperating System:\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nWhat other attempted solutions have you tried?\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).",
          "bodyHTML": "<p>NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.</p>\n<p>For general support from the community, see <a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">StackOverflow</a>.<br>\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed<br>\nout of scope for GitHub Issues and point people to StackOverflow.</p>\n<p>For bugs or installation issues, please provide the following information.<br>\nThe more information you provide, the more easily we will be able to offer<br>\nhelp and advice.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<h3>Environment info</h3>\n<p>Operating System:</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed:</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)</li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<h3>What other attempted solutions have you tried?</h3>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "lepangdan" },
          "number": 21142,
          "resourcePath": "/tensorflow/tensorflow/issues/21142",
          "state": "CLOSED",
          "publishedAt": "2018-07-26T03:46:24Z",
          "closedAt": "2018-07-27T08:04:11Z",
          "title": "Feature Request: Provide tf.pow with supporting  broadcasting?",
          "bodyText": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nPython version:\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",
          "bodyHTML": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a rel=\"nofollow\" href=\"https://stackoverflow.com/questions/tagged/tensorflow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n<li><strong>Python version</strong>:</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": null,
          "number": 1279,
          "resourcePath": "/tensorflow/tensorflow/issues/1279",
          "state": "CLOSED",
          "publishedAt": "2016-02-24T22:13:59Z",
          "closedAt": "2016-02-25T07:05:30Z",
          "title": "Arch doesn't support it",
          "bodyText": "After sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl or as root, I got:\ntensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\nIt seems it doesn't work with Python 3. What should I do?",
          "bodyHTML": "<p>After <code>sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl</code> or as root, I got:<br>\n<code>tensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.</code></p>\n<p>It seems it doesn't work with Python 3. What should I do?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "shoyer" },
          "number": 17877,
          "resourcePath": "/tensorflow/tensorflow/issues/17877",
          "state": "CLOSED",
          "publishedAt": "2018-03-21T01:39:58Z",
          "closedAt": "2018-04-18T17:35:45Z",
          "title": "tf.manip.roll silently ignores negative axes",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\nTensorFlow installed from (source or binary): unknown\nTensorFlow version (use command below): 1.6.0\nPython version: 3.6.3\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce:\n\nimport tensorflow as tf\ntf.InteractiveSession()\nprint(tf.manip.roll(tf.range(5), -1, axis=0).eval())\n# [1 2 3 4 0]\nprint(tf.manip.roll(tf.range(5), -1, axis=-1).eval())\n# [0 1 2 3 4]\n\nDescribe the problem\naxis=-1 and axis=0 should be equivalent, if tf.manip.roll() works like numpy.roll() and other TensorFlow/NumPy functions that accept negative axes. However, instead negative axes are silently ignored. At the very least, TensorFlow should have raised an informative error.",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Google Colab</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: unknown</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.6.0</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>import tensorflow as tf\ntf.InteractiveSession()\nprint(tf.manip.roll(tf.range(5), -1, axis=0).eval())\n# [1 2 3 4 0]\nprint(tf.manip.roll(tf.range(5), -1, axis=-1).eval())\n# [0 1 2 3 4]\n</code></pre>\n<h3>Describe the problem</h3>\n<p><code>axis=-1</code> and <code>axis=0</code> should be equivalent, if <code>tf.manip.roll()</code> works like <code>numpy.roll()</code> and other TensorFlow/NumPy functions that accept negative axes. However, instead negative axes are silently ignored. At the very least, TensorFlow should have raised an informative error.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "puneet336" },
          "number": 5447,
          "resourcePath": "/tensorflow/tensorflow/issues/5447",
          "state": "CLOSED",
          "publishedAt": "2016-11-07T11:39:46Z",
          "closedAt": "2016-12-08T07:52:18Z",
          "title": "invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive]     ",
          "bodyText": "Hi all,\nI am trying to compile tensorflow-0.11 + bazel 0.3.2 on RHEL 6 with cuda 7.0 + cudnn 7.5.5.0 + gcc 4.9.\nThe compilation command is :\nEXTRA_BAZEL_ARGS=\"--jobs 10\" bazel build -c opt --config=cuda --jobs 10 //tensorflow/tools/pip_package:build_pip_package\nCompilation of rule '//tensorflow/stream_executor:stream_executor' fails with cuda specific message.\nI have latest version of compilers at non standard path , hence i had modified some variables in CROSSTOOL.tpl + third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl. I am attaching compilation error logs, CROSSTOOL.tpl and third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl for your reference.\nThough i am able to compile cpu-only version of tensorflow successfully.\nPlease let me know if any information is needed from my side.\nEagerly awaiting your replies.\ncrosstool_wrapper_driver_is_not_gcc.tpl.txt\nCROSSTOOL.tpl.txt\ntensorflow_build2.log.txt",
          "bodyHTML": "<p>Hi all,<br>\nI am trying to compile tensorflow-0.11 + bazel 0.3.2 on RHEL 6 with cuda 7.0 + cudnn 7.5.5.0 + gcc 4.9.<br>\nThe compilation command is :<br>\nEXTRA_BAZEL_ARGS=\"--jobs 10\" bazel build -c opt --config=cuda --jobs 10 //tensorflow/tools/pip_package:build_pip_package</p>\n<p>Compilation of rule '//tensorflow/stream_executor:stream_executor' fails with cuda specific message.</p>\n<p>I have latest version of compilers at non standard path , hence i had modified some variables in CROSSTOOL.tpl + third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl. I am attaching <strong>compilation error logs</strong>, <strong>CROSSTOOL.tpl</strong> and <strong>third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl</strong> for your reference.</p>\n<p>Though i am able to compile cpu-only version of tensorflow successfully.<br>\nPlease let me know if any information is needed from my side.<br>\nEagerly awaiting your replies.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/575209/crosstool_wrapper_driver_is_not_gcc.tpl.txt\">crosstool_wrapper_driver_is_not_gcc.tpl.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/575211/CROSSTOOL.tpl.txt\">CROSSTOOL.tpl.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/575210/tensorflow_build2.log.txt\">tensorflow_build2.log.txt</a></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "AnlanSun" },
          "number": 7908,
          "resourcePath": "/tensorflow/tensorflow/issues/7908",
          "state": "CLOSED",
          "publishedAt": "2017-02-27T00:38:02Z",
          "closedAt": "2017-06-16T20:35:02Z",
          "title": "Changing computer make the pretrained model fail",
          "bodyText": "I switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.\nusing TF:1.0",
          "bodyHTML": "<p>I switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.</p>\n<p>using TF:1.0</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ethereon" },
          "number": 4419,
          "resourcePath": "/tensorflow/tensorflow/issues/4419",
          "state": "CLOSED",
          "publishedAt": "2016-09-16T20:33:04Z",
          "closedAt": "2017-06-16T22:05:08Z",
          "title": "tf.get_variable without an explicit initializer fails for integer types",
          "bodyText": "The following fails (shape and name are arbitrary):\ntf.get_variable(name='foo', shape=(42,), dtype=tf.int32)\n\nException: TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.\nIn contrast, using tf.float32 works just fine.\nThe problem appears to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658\nIf an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that sqrt(3)==1.7320...), which of course conflicts with the requested integer type.\nWhile this can be mitigated by doing something like:\ntf.get_variable(name='foo', dtype=tf.int32, initializer=tf.zeros_initializer(shape=(42,), dtype=tf.int32))\n\nit feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).\nTested on the current master.",
          "bodyHTML": "<p>The following fails (shape and name are arbitrary):</p>\n<pre><code>tf.get_variable(name='foo', shape=(42,), dtype=tf.int32)\n</code></pre>\n<p>Exception: <code>TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.</code></p>\n<p>In contrast, using <code>tf.float32</code> works just fine.</p>\n<p>The problem appears to be <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658</a></p>\n<p>If an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py#L275\"><code>sqrt(3)==1.7320...</code></a>), which of course conflicts with the requested integer type.</p>\n<p>While this can be mitigated by doing something like:</p>\n<pre><code>tf.get_variable(name='foo', dtype=tf.int32, initializer=tf.zeros_initializer(shape=(42,), dtype=tf.int32))\n</code></pre>\n<p>it feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).</p>\n<p>Tested on the current master.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "bean-du" },
          "number": 806,
          "resourcePath": "/tensorflow/tensorflow/issues/806",
          "state": "CLOSED",
          "publishedAt": "2016-01-19T02:39:26Z",
          "closedAt": "2016-01-19T04:00:05Z",
          "title": "google-tensorflow",
          "bodyText": "",
          "bodyHTML": ""
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "kocabey" },
          "number": 4021,
          "resourcePath": "/tensorflow/tensorflow/issues/4021",
          "state": "CLOSED",
          "publishedAt": "2016-08-24T18:32:50Z",
          "closedAt": "2016-12-21T09:16:48Z",
          "title": "Improving Google Indexing for the Documentation",
          "bodyText": "Whenever I run a Google search on a TensorFlow functionality, say, tf.reshape, it gives me the entire documentation, not the specific documentation related to that functionality.\nCurrently the way I use is  to run a search with ctrl + f to find specific documentation related to what I search for.\nNumpy has that property, i.e. when you run a Google search on np.reshape, you get the specific page.\nIt would be a nice improvement for the documentation if someone fixes the Google indexing for the documentation page, especially for the users who frequently use Google search for the documentation.",
          "bodyHTML": "<p>Whenever I run a Google search on a TensorFlow functionality, say, <code>tf.reshape</code>, it gives me the entire documentation, not the specific documentation related to that functionality.</p>\n<p>Currently the way I use is  to run a search with <code>ctrl</code> + <code>f</code> to find specific documentation related to what I search for.</p>\n<p>Numpy has that property, i.e. when you run a Google search on <code>np.reshape</code>, you get the specific page.</p>\n<p>It would be a nice improvement for the documentation if someone fixes the Google indexing for the documentation page, especially for the users who frequently use Google search for the documentation.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "eli7310" },
          "number": 23208,
          "resourcePath": "/tensorflow/tensorflow/issues/23208",
          "state": "CLOSED",
          "publishedAt": "2018-10-24T08:50:07Z",
          "closedAt": "2018-11-18T01:05:38Z",
          "title": "TfLiteCameraDemo.apk with NNAPI",
          "bodyText": "Can someone please upload a version of TfLiteCameraDemo.apk that supports NNAPI?",
          "bodyHTML": "<p>Can someone please upload a version of TfLiteCameraDemo.apk that supports NNAPI?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "Franck-Dernoncourt" },
          "number": 7638,
          "resourcePath": "/tensorflow/tensorflow/issues/7638",
          "state": "CLOSED",
          "publishedAt": "2017-02-17T22:45:32Z",
          "closedAt": "2017-02-21T18:00:06Z",
          "title": "Links to Android nightly builds on README.md are broken",
          "bodyText": "Links to Android nightly builds on https://github.com/tensorflow/tensorflow/blob/master/README.md are broken.\n\n-->",
          "bodyHTML": "<p>Links to Android nightly builds on <a href=\"https://github.com/tensorflow/tensorflow/blob/master/README.md\">https://github.com/tensorflow/tensorflow/blob/master/README.md</a> are broken.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/15331/23086006/c80cc702-f538-11e6-927d-d44b1f5f80e2.png\"><img src=\"https://cloud.githubusercontent.com/assets/15331/23086006/c80cc702-f538-11e6-927d-d44b1f5f80e2.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>--&gt;<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/15331/23086019/d91aa082-f538-11e6-9d76-7e56cd879900.png\"><img src=\"https://cloud.githubusercontent.com/assets/15331/23086019/d91aa082-f538-11e6-9d76-7e56cd879900.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "rdeepc" },
          "number": 7558,
          "resourcePath": "/tensorflow/tensorflow/issues/7558",
          "state": "CLOSED",
          "publishedAt": "2017-02-16T06:54:25Z",
          "closedAt": "2017-02-16T21:31:45Z",
          "title": "tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.",
          "bodyText": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nEnvironment info\nOperating System:\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nWhat other attempted solutions have you tried?\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).",
          "bodyHTML": "<p>NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.</p>\n<p>For general support from the community, see <a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">StackOverflow</a>.<br>\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed<br>\nout of scope for GitHub Issues and point people to StackOverflow.</p>\n<p>For bugs or installation issues, please provide the following information.<br>\nThe more information you provide, the more easily we will be able to offer<br>\nhelp and advice.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<h3>Environment info</h3>\n<p>Operating System:</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed:</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)</li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<h3>What other attempted solutions have you tried?</h3>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "pwaller" },
          "number": 4397,
          "resourcePath": "/tensorflow/tensorflow/issues/4397",
          "state": "CLOSED",
          "publishedAt": "2016-09-15T16:04:12Z",
          "closedAt": "2016-10-19T01:46:19Z",
          "title": "Docker build devel-gpu build fails with 8.0-cudnn5-devel (Cannot find cudnn.h under .../lib)",
          "bodyText": "Summary: Why is it looking for the header file under the lib directory?\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\n#3989 (and references within)\n\nEnvironment info\nOperating System: Ubuntu 16.04. Docker 1.12.1.\nIf possible, provide a minimal reproducible example\n\nModify devel-gpu to read \"FROM nvidia/cuda:8.0-cudnn5-devel\".\nRun docker build -f Dockerfile.devel-gpu -t tf from the /tensorflow/tools/docker directory. (HEAD @ 4addf4b at time of posting)\n\nWhat other attempted solutions have you tried?\nNone yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.\nLogs or other output that would be helpful\n$ docker build -f Dockerfile.devel-gpu -t tf .\n\n[snip]\n\nStep 23 : RUN ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl\n ---> Running in 1f99527c7748\nNo Google Cloud Platform support will be enabled for TensorFlow\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nExtracting Bazel installation...\n____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.\nConfiguration finished\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/lib/x86_64-linux-gnu.\n____Elapsed time: 0.391s\nThe command '/bin/sh -c ./configure &&     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1",
          "bodyHTML": "<p><strong>Summary</strong>: Why is it looking for the header file under the lib directory?</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<ul>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"172765666\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3989\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3989/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3989\">#3989</a> (and references within)</li>\n</ul>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 16.04. Docker 1.12.1.</p>\n<h3>If possible, provide a minimal reproducible example</h3>\n<ol>\n<li>Modify <a href=\"https://github.com/tensorflow/tensorflow/blob/4addf4b5806cd731949c6582a83f5824599cd1ef/tensorflow/tools/docker/Dockerfile.devel-gpu\">devel-gpu</a> to read \"<code>FROM nvidia/cuda:8.0-cudnn5-devel</code>\".</li>\n<li>Run <code>docker build -f Dockerfile.devel-gpu -t tf</code> from the <code>/tensorflow/tools/docker</code> directory. (HEAD @ <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/4addf4b5806cd731949c6582a83f5824599cd1ef/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/4addf4b5806cd731949c6582a83f5824599cd1ef\"><tt>4addf4b</tt></a> at time of posting)</li>\n</ol>\n<h3>What other attempted solutions have you tried?</h3>\n<p>None yet, I'm not sure how to poke Bazel. I will continue poking around after filing the issue.</p>\n<h3>Logs or other output that would be helpful</h3>\n<pre><code>$ docker build -f Dockerfile.devel-gpu -t tf .\n\n[snip]\n\nStep 23 : RUN ./configure &amp;&amp;     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &amp;&amp;     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &amp;&amp;     pip install --upgrade /tmp/pip/tensorflow-*.whl\n ---&gt; Running in 1f99527c7748\nNo Google Cloud Platform support will be enabled for TensorFlow\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nExtracting Bazel installation...\n____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.\nConfiguration finished\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/lib/x86_64-linux-gnu.\n____Elapsed time: 0.391s\nThe command '/bin/sh -c ./configure &amp;&amp;     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &amp;&amp;     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &amp;&amp;     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "gunan" },
          "number": 4848,
          "resourcePath": "/tensorflow/tensorflow/issues/4848",
          "state": "CLOSED",
          "publishedAt": "2016-10-09T07:48:11Z",
          "closedAt": "2017-03-22T05:31:48Z",
          "title": "\"bazel clean\" will undo \"./configure\" when source configured to build for GPU.",
          "bodyText": "Environment info\nOperating System:\nubuntu 14.04\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\ncuda 8.0.44, cudnn 5.5\nIf installed from binary pip package, provide:\nNo\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nc8d4896\nThe output of bazel version\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n./configure   # configure with GPU support)\nbazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one should pass\nbazel clean\nbazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test  # this one fails, complaining no GPU support configured.\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@ae\n3629bd' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@1374ec1e', 'CONFIGURATION_FRAGMENT:com\n.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@3686b55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue\n$ConfigurationFragmentKey@a93d9174')\nat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)\nat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nDavid, Damien, looks like bazel clean undid some output of \"./configure\" at head.\nAny idea where things went wrong?",
          "bodyHTML": "<h3>Environment info</h3>\n<p>Operating System:<br>\nubuntu 14.04</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\ncuda 8.0.44, cudnn 5.5</p>\n<p>If installed from binary pip package, provide:<br>\nNo</p>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/c8d4896e3231c3ac32f174fd0af051867645fbb5/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/c8d4896e3231c3ac32f174fd0af051867645fbb5\"><tt>c8d4896</tt></a></li>\n<li>The output of <code>bazel version</code><br>\nBuild label: 0.3.1<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)<br>\nBuild timestamp: 1469783392<br>\nBuild timestamp as int: 1469783392</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>./configure   # configure with GPU support)<br>\nbazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test # this one should pass<br>\nbazel clean<br>\nbazel test -c opt --config=cuda tensorflow/core:common_runtime_direct_session_test  # this one fails, complaining no GPU support configured.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).<br>\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@ae<br>\n3629bd' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@1374ec1e', 'CONFIGURATION_FRAGMENT:com<br>\n.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@3686b55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue<br>\n$ConfigurationFragmentKey@a93d9174')<br>\nat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)<br>\nat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)<br>\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)<br>\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)<br>\nat java.lang.Thread.run(Thread.java:745)</p>\n<p>David, Damien, looks like bazel clean undid some output of \"./configure\" at head.<br>\nAny idea where things went wrong?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "edi-bice" },
          "number": 2291,
          "resourcePath": "/tensorflow/tensorflow/issues/2291",
          "state": "CLOSED",
          "publishedAt": "2016-05-09T13:34:03Z",
          "closedAt": "2018-02-07T23:24:41Z",
          "title": "Add -lm (Was: undefined reference to symbol 'ceil@@GLIBC_2.2.5)",
          "bodyText": "Environment info\nOperating System:\nepel-release-6-8.noarch\nredhat-release-server-6Server-6.7.0.3.el6.x86_64\nInstalled version of CUDA and cuDNN:\nNone\nIf installed from sources, provide the commit hash:\nf8eb1d7\nSteps to reproduce\n\nbazel clean\n./configure\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).\nERROR: /home/ebice/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command /opt/rh/devtoolset-2/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -no-canonical-prefixes -B/opt/rh/devtoolset-2/root/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 11 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/opt/rh/devtoolset-2/root/usr/bin/ld: /opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/libstdc++_nonshared.a(hashtable_c++0x44.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'\n/opt/rh/devtoolset-2/root/usr/bin/ld: note: 'ceil@@GLIBC_2.2.5' is defined in DSO /lib64/libm.so.6 so try adding it to the linker command line\n/lib64/libm.so.6: could not read symbols: Invalid operation\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build",
          "bodyHTML": "<h3>Environment info</h3>\n<p>Operating System:</p>\n<p>epel-release-6-8.noarch<br>\nredhat-release-server-6Server-6.7.0.3.el6.x86_64</p>\n<p>Installed version of CUDA and cuDNN:</p>\n<p>None</p>\n<p>If installed from sources, provide the commit hash:</p>\n<p><a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/f8eb1d70a7ea7dc2cd5e1eddde389395f88a6be9/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/f8eb1d70a7ea7dc2cd5e1eddde389395f88a6be9\"><tt>f8eb1d7</tt></a></p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>bazel clean</li>\n<li>./configure</li>\n<li>bazel build -c opt //tensorflow/tools/pip_package:build_pip_package</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment).</p>\n<p>ERROR: /home/ebice/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command /opt/rh/devtoolset-2/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -no-canonical-prefixes -B/opt/rh/devtoolset-2/root/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 11 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.<br>\n/opt/rh/devtoolset-2/root/usr/bin/ld: /opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/libstdc++_nonshared.a(hashtable_c++0x44.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'<br>\n/opt/rh/devtoolset-2/root/usr/bin/ld: note: 'ceil@@GLIBC_2.2.5' is defined in DSO /lib64/libm.so.6 so try adding it to the linker command line<br>\n/lib64/libm.so.6: could not read symbols: Invalid operation<br>\ncollect2: error: ld returned 1 exit status<br>\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "concretevitamin" },
          "number": 1828,
          "resourcePath": "/tensorflow/tensorflow/issues/1828",
          "state": "CLOSED",
          "publishedAt": "2016-04-08T22:59:50Z",
          "closedAt": "2016-07-08T16:59:15Z",
          "title": "SparseTensor: common unary ops",
          "bodyText": "Currently, we have a slew of common unary ops that work on Tensors, but not SparseTensors (ref):\ntf.pow()\ntf.exp()\ntf.log()\n\n# lower priority?\ntf.abs()\ntf.neg()\ntf.sign()\ntf.inv()\ntf.square()\ntf.round()\ntf.sqrt()\ntf.ceil()\ntf.floor()\n\nand so on.\nWe'd like these ops to work on SparseTensor. These do not change the indices nor shape of SparseTensors, so all that's needed is transform the .values field on Python side in O(1) line.",
          "bodyHTML": "<p>Currently, we have a slew of common unary ops that work on Tensors, but not SparseTensors (<a href=\"https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#basic-math-functions\" rel=\"nofollow\">ref</a>):</p>\n<pre><code>tf.pow()\ntf.exp()\ntf.log()\n\n# lower priority?\ntf.abs()\ntf.neg()\ntf.sign()\ntf.inv()\ntf.square()\ntf.round()\ntf.sqrt()\ntf.ceil()\ntf.floor()\n</code></pre>\n<p>and so on.</p>\n<p>We'd like these ops to work on SparseTensor. These do not change the indices nor shape of <code>SparseTensor</code>s, so all that's needed is transform the <code>.values</code> field on Python side in O(1) line.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "githubgsq" },
          "number": 19731,
          "resourcePath": "/tensorflow/tensorflow/issues/19731",
          "state": "CLOSED",
          "publishedAt": "2018-06-04T03:11:59Z",
          "closedAt": "2018-07-28T00:48:36Z",
          "title": "How to release GPU memory after sess.close()?",
          "bodyText": "hi, all:\nI'm training models iteratively. After each model trained, I run sess.close() and recreate a new session to run a new training process. But it seems that the GPU memory was not relseased and it's increasing constantly.\nI tried tf.reset_default_graph() before run session also typed gc.collect() after sess.close(), but takes no effect.\nHow could I release GPU memory timely to avoid OOM error please?\nThanks!",
          "bodyHTML": "<p>hi, all:<br>\nI'm training models iteratively. After each model trained, I run sess.close() and recreate a new session to run a new training process. But it seems that the GPU memory was not relseased and it's increasing constantly.<br>\nI tried tf.reset_default_graph() before run session also typed gc.collect() after sess.close(), but takes no effect.<br>\nHow could I release GPU memory timely to avoid OOM error please?<br>\nThanks!</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "xuancong84" },
          "number": 6111,
          "resourcePath": "/tensorflow/tensorflow/issues/6111",
          "state": "CLOSED",
          "publishedAt": "2016-12-06T03:56:26Z",
          "closedAt": "2017-06-16T23:41:48Z",
          "title": "Flawed memory management: allow_growth=True consumes more memory, causing out-of-memory",
          "bodyText": "To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\nHowever, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:\nTraining Epoch 0 ; learning_rate= 0.002 :                                                                                                \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \n\nHowever, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.\nIn principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.\nBelow are my system info:\nOperating System:\nUbuntu 14.04.5 LTS\n\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda-8.0\ncudnn-8.0-linux-x64-v5.1.tgz\n\nIt is installed from binary pip package\n\nA link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n\nThe output from python -c \"import tensorflow; print(tensorflow.version)\"\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0",
          "bodyHTML": "<p>To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:</p>\n<pre><code>config = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n</code></pre>\n<p>However, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:</p>\n<pre><code>Training Epoch 0 ; learning_rate= 0.002 :                                                                                                \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \n</code></pre>\n<p>However, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.</p>\n<p>In principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.</p>\n<p>Below are my system info:</p>\n<pre><code>Operating System:\nUbuntu 14.04.5 LTS\n\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda-8.0\ncudnn-8.0-linux-x64-v5.1.tgz\n\nIt is installed from binary pip package\n\nA link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n\nThe output from python -c \"import tensorflow; print(tensorflow.version)\"\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "yihong-chen" },
          "number": 5284,
          "resourcePath": "/tensorflow/tensorflow/issues/5284",
          "state": "CLOSED",
          "publishedAt": "2016-10-30T11:07:18Z",
          "closedAt": "2016-11-11T19:10:20Z",
          "title": "tf.contrib.learn output shapes : shapes (?, 1) and (?,) are incompatible",
          "bodyText": "I tried to train a binary DNNClassifier  similar as the example on https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart.\nI got the following traceback\ntraceback.txt\nI explored the tensorflow source code and found that it may relate to _get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\n\nWhen doing a binary classification,y_shape is something like[batch_size,1].Line 52 changes it into [1].Line 54,55 change it into [].And finally the output_shape is [batch_size,].However the correct ouput shape should be [batch_size,1].\nTo sum up,we do not need to skip 1st dimension if it is 1 and len(y_shape)=1.",
          "bodyHTML": "<p>I tried to train a binary DNNClassifier  similar as the example on <a rel=\"nofollow\" href=\"https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart\">https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart</a>.</p>\n<p>I got the following traceback<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/560406/traceback.txt\">traceback.txt</a></p>\n<p>I explored the tensorflow source code and found that it may relate to _get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/11754923/19836102/024ac634-9ed2-11e6-89cc-e9dc2623424b.png\"><img src=\"https://cloud.githubusercontent.com/assets/11754923/19836102/024ac634-9ed2-11e6-89cc-e9dc2623424b.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>When doing a binary classification,y_shape is something like[batch_size,1].Line 52 changes it into [1].Line 54,55 change it into [].And finally the output_shape is [batch_size,].However the correct ouput shape should be [batch_size,1].</p>\n<p>To sum up,we do not need to skip 1st dimension if it is 1 and len(y_shape)=1.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "maharjun" },
          "number": 22249,
          "resourcePath": "/tensorflow/tensorflow/issues/22249",
          "state": "CLOSED",
          "publishedAt": "2018-09-13T09:14:22Z",
          "closedAt": "2018-10-13T18:24:16Z",
          "title": "Inconsistency in supported integer types on GPU",
          "bodyText": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nnot relevant\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u4 (2018-08-21) x86_64 GNU/Linux\nVERSION_ID=\"9\"\nVERSION=\"9 (stretch)\"\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n\n\nTensorFlow installed from (source or binary):\n1.10.1 (from pip binary)\n\n\nTensorFlow version (use command below):\ntf.VERSION = 1.10.1\ntf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1\ntf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1\n\n\nPython version:\nPython 3.5.3\n\n\nBazel version (if compiling from source):\n\n\nGCC/Compiler version (if compiling from source):\n\n\nCUDA/cuDNN version:\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2017 NVIDIA Corporation\nBuilt on Fri_Nov__3_21:07:56_CDT_2017\nCuda compilation tools, release 9.1, V9.1.85\n\n\nGPU model and memory:\nTesla P100-PCIE-16GB\n\n\nExact command to reproduce:\n\n\nDescribe the problem\nIt appears that the kernel of tf.reduce_sum is not registerd for GPU's if the type of the tensor to be summed is int64 (only registered for tf.int32)\nMoreover the kernel of tf.tile is not registered for the case where the tensor to be tiled is of type tf.int32 (only registered for tf.int64)\nWhy is there this inconsistency?",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nnot relevant</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux 4.9.0-8-amd64 <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"115886302\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a> SMP Debian 4.9.110-3+deb9u4 (2018-08-21) x86_64 GNU/Linux<br>\nVERSION_ID=\"9\"<br>\nVERSION=\"9 (stretch)\"</p>\n</li>\n<li>\n<p><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\n1.10.1 (from pip binary)</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\ntf.VERSION = 1.10.1<br>\ntf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1<br>\ntf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\nPython 3.5.3</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>:</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nnvcc: NVIDIA (R) Cuda compiler driver<br>\nCopyright (c) 2005-2017 NVIDIA Corporation<br>\nBuilt on Fri_Nov__3_21:07:56_CDT_2017<br>\nCuda compilation tools, release 9.1, V9.1.85</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nTesla P100-PCIE-16GB</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>It appears that the kernel of <code>tf.reduce_sum</code> is not registerd for GPU's if the type of the tensor to be summed is <code>int64</code> (only registered for <code>tf.int32</code>)<br>\nMoreover the kernel of <code>tf.tile</code> is not registered for the case where the tensor to be tiled is of type <code>tf.int32</code> (only registered for <code>tf.int64</code>)</p>\n<p>Why is there this inconsistency?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "wenouyang" },
          "number": 8910,
          "resourcePath": "/tensorflow/tensorflow/issues/8910",
          "state": "CLOSED",
          "publishedAt": "2017-04-02T19:11:53Z",
          "closedAt": "2017-04-02T19:32:14Z",
          "title": "regarding the ValueError: inputs must be a list of at least one Tensor with the same dtype and shape",
          "bodyText": "There is a program that defines the loss function as follows:\nreg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\nweight_loss = tf.add_n(tf.get_collection(reg_loss_col),name='reg_loss')\n\n\nRunning the program raises the following error message\n\nFile \"/home/ decoder/kitti_multiloss.py\", line 86, in loss\nname='reg_loss')\nFile \"/devl /tensorflow/tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py\", line 1827, in add_n\nraise ValueError(\"inputs must be a list of at least one Tensor with the \"\nValueError: inputs must be a list of at least one Tensor with the same dtype and shape\n\nI am curious how to print out the tensor information of the first parameter tf.get_collection(reg_loss_col) in tf.add_n, so that I can figure out why this cause the error.",
          "bodyHTML": "<p>There is a program that defines the loss function as follows:</p>\n<pre><code>reg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\nweight_loss = tf.add_n(tf.get_collection(reg_loss_col),name='reg_loss')\n\n</code></pre>\n<p>Running the program raises the following error message</p>\n<blockquote>\n<p>File \"/home/ decoder/kitti_multiloss.py\", line 86, in loss<br>\nname='reg_loss')<br>\nFile \"/devl /tensorflow/tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py\", line 1827, in add_n<br>\nraise ValueError(\"inputs must be a list of at least one Tensor with the \"<br>\nValueError: inputs must be a list of at least one Tensor with the same dtype and shape</p>\n</blockquote>\n<p>I am curious how to print out the tensor information of the first parameter <code>tf.get_collection(reg_loss_col)</code> in <code>tf.add_n</code>, so that I can figure out why this cause the error.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "com9009" },
          "number": 16995,
          "resourcePath": "/tensorflow/tensorflow/issues/16995",
          "state": "CLOSED",
          "publishedAt": "2018-02-14T00:07:44Z",
          "closedAt": "2018-03-10T01:41:07Z",
          "title": "tf.fake_quant_with_min_max_vars returns wrong answer",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): pip install tensorflow-gpu\nTensorFlow version (use command below): 1.5.0\nPython version: 2.7.12\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: CUDA-8.0 CuDNN 6.0\nGPU model and memory: GTX 1080 8GB\nExact command to reproduce:\n\nDescribe the problem\ntf.fake_quant_with_min_max_vars returns wrong answer.\nSource code / logs\nimport tensorflow as tf\n\na =tf.Variable([ 0.09504107, 0.0748544, 0.09333218, 0.106306, 0.09921047, 0.0930253, 0.09277194, 0.08704954, 0.12734564, 0.11479893], dtype=tf.float32)\n\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_variables([a]))\n\nprint a.eval()\nprint tf.fake_quant_with_min_max_vars(inputs=a, min=tf.reduce_min(a), max=tf.reduce_max(a), num_bits=8).eval()\nprint tf.reduce_min(a).eval()\nprint tf.reduce_max(a).eval()\nit prints like below\n[ 0.09504107  0.0748544   0.09333218  0.106306    0.09921047  0.0930253\n  0.09277194  0.08704954  0.12734564  0.11479893]\n[ 0.05249124  0.05249124  0.05249124  0.05249124  0.05249124  0.05249124\n  0.05249124  0.05249124  0.05249124  0.05249124]\n0.0748544\n0.127346",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install tensorflow-gpu</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.5.0</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA-8.0 CuDNN 6.0</li>\n<li><strong>GPU model and memory</strong>: GTX 1080 8GB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>tf.fake_quant_with_min_max_vars returns wrong answer.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">tensorflow</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">tf</span>\n\n<span class=\"pl-s1\">a</span> <span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">Variable</span>([ <span class=\"pl-c1\">0.09504107</span>, <span class=\"pl-c1\">0.0748544</span>, <span class=\"pl-c1\">0.09333218</span>, <span class=\"pl-c1\">0.106306</span>, <span class=\"pl-c1\">0.09921047</span>, <span class=\"pl-c1\">0.0930253</span>, <span class=\"pl-c1\">0.09277194</span>, <span class=\"pl-c1\">0.08704954</span>, <span class=\"pl-c1\">0.12734564</span>, <span class=\"pl-c1\">0.11479893</span>], <span class=\"pl-s1\">dtype</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">float32</span>)\n\n\n<span class=\"pl-s1\">sess</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">InteractiveSession</span>()\n<span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">initialize_variables</span>([<span class=\"pl-s1\">a</span>]))\n\n<span class=\"pl-k\">print</span> <span class=\"pl-s1\">a</span>.<span class=\"pl-en\">eval</span>()\n<span class=\"pl-k\">print</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">fake_quant_with_min_max_vars</span>(<span class=\"pl-s1\">inputs</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">a</span>, <span class=\"pl-s1\">min</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">reduce_min</span>(<span class=\"pl-s1\">a</span>), <span class=\"pl-s1\">max</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">reduce_max</span>(<span class=\"pl-s1\">a</span>), <span class=\"pl-s1\">num_bits</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">8</span>).<span class=\"pl-en\">eval</span>()\n<span class=\"pl-k\">print</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">reduce_min</span>(<span class=\"pl-s1\">a</span>).<span class=\"pl-en\">eval</span>()\n<span class=\"pl-k\">print</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">reduce_max</span>(<span class=\"pl-s1\">a</span>).<span class=\"pl-en\">eval</span>()</pre></div>\n<p>it prints like below</p>\n<pre><code>[ 0.09504107  0.0748544   0.09333218  0.106306    0.09921047  0.0930253\n  0.09277194  0.08704954  0.12734564  0.11479893]\n[ 0.05249124  0.05249124  0.05249124  0.05249124  0.05249124  0.05249124\n  0.05249124  0.05249124  0.05249124  0.05249124]\n0.0748544\n0.127346\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "glhfgg1024" },
          "number": 17121,
          "resourcePath": "/tensorflow/tensorflow/issues/17121",
          "state": "CLOSED",
          "publishedAt": "2018-02-19T06:22:19Z",
          "closedAt": "2018-02-19T06:25:40Z",
          "title": "Feature Request for the back-propagated errors in intermediate layers",
          "bodyText": "After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:\n I->(W1)->C1->(W2)->C2->(W3)->O\n\nI is the input, O is the output, W1,W2,W3 is the weights for 3 layers. C1 and C2 are the outputs for the first two layers. With O and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to C1 and C2?\nI know we could get the parameter operators as follows:\nW1_op = tf.get_default_graph().get_tensor_by_name('W1')\nW1_op = ...\n\nMy final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).\nI know that we could use the tf.test.check_gradient to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.\nIn the Caffe framework, it seems those errors were saved in diff memory for each layer. I want to get these back-propagated errors in each layer. Does anybody know how to get that?",
          "bodyHTML": "<p>After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:</p>\n<pre><code> I-&gt;(W1)-&gt;C1-&gt;(W2)-&gt;C2-&gt;(W3)-&gt;O\n</code></pre>\n<p><code>I</code> is the input, <code>O</code> is the output, <code>W1,W2,W3</code> is the weights for 3 layers. <code>C1</code> and <code>C2</code> are the outputs for the first two layers. With <code>O</code> and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to <code>C1</code> and <code>C2</code>?</p>\n<p>I know we could get the parameter operators as follows:</p>\n<pre><code>W1_op = tf.get_default_graph().get_tensor_by_name('W1')\nW1_op = ...\n</code></pre>\n<p>My final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).</p>\n<p>I know that we could use the <code>tf.test.check_gradient</code> to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.</p>\n<p>In the Caffe framework, it seems those <code>errors</code> were saved in <code>diff</code> memory for each layer. I want to get these back-propagated <code>errors</code> in each layer. Does anybody know how to get that?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "jowagner" },
          "number": 6956,
          "resourcePath": "/tensorflow/tensorflow/issues/6956",
          "state": "CLOSED",
          "publishedAt": "2017-01-19T13:49:20Z",
          "closedAt": "2018-02-24T03:40:27Z",
          "title": "Error downloading nasm",
          "bodyText": "As noted in issue #6950 nasm-2.12.02.tar.bz2 is currently unavailable. (www.nasm.us does not accept connections.) @trsaunders observed this for head of r0.12 and I can confirm this for v0.12.0.\nWorkaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.",
          "bodyHTML": "<p>As noted in issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"201778904\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6950\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6950/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6950\">#6950</a> nasm-2.12.02.tar.bz2 is currently unavailable. (<a href=\"http://www.nasm.us\" rel=\"nofollow\">www.nasm.us</a> does not accept connections.) <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/trsaunders/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/trsaunders\">@trsaunders</a> observed this for head of r0.12 and I can confirm this for v0.12.0.</p>\n<p>Workaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "dbikard" },
          "number": 3439,
          "resourcePath": "/tensorflow/tensorflow/issues/3439",
          "state": "CLOSED",
          "publishedAt": "2016-07-21T09:47:39Z",
          "closedAt": "2016-07-27T19:26:18Z",
          "title": "\"tensorflow: Input iterator is exhausted\" when passing numpy arrays in validation monitor",
          "bodyText": "I'm not sure if this is a bug or whether I just don't understand the usage of monitors (in which case I apologize for posting here). I'm trying to use a validation monitor by passing my validation set as numpy array.\nval_monitor = learn.monitors.ValidationMonitor(X_val, Y_val, every_n_steps=100)\nreg.fit(X_train, Y_train, steps=1000, batch_size=200, monitors=[val_monitor])\n\n(X_val and Y_val are numpy arrays)\nThe code runs but only the first validation step is done properly, then I get the following message in the logs:\nINFO:tensorflow:Input iterator is exhausted.\nAny help is welcome!",
          "bodyHTML": "<p>I'm not sure if this is a bug or whether I just don't understand the usage of monitors (in which case I apologize for posting here). I'm trying to use a validation monitor by passing my validation set as numpy array.</p>\n<pre><code>val_monitor = learn.monitors.ValidationMonitor(X_val, Y_val, every_n_steps=100)\nreg.fit(X_train, Y_train, steps=1000, batch_size=200, monitors=[val_monitor])\n</code></pre>\n<p>(X_val and Y_val are numpy arrays)</p>\n<p>The code runs but only the first validation step is done properly, then I get the following message in the logs:<br>\n<code>INFO:tensorflow:Input iterator is exhausted.</code></p>\n<p>Any help is welcome!</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "tfboyd" },
          "number": 22357,
          "resourcePath": "/tensorflow/tensorflow/issues/22357",
          "state": "CLOSED",
          "publishedAt": "2018-09-18T20:51:54Z",
          "closedAt": "2019-05-11T02:25:21Z",
          "title": "Make NVIDIA library versions to TF Version matrix more visible.",
          "bodyText": "This request was a product of the TensorFlow Fall Symposium. Consider talking to the documentation team as well as maybe linking it to the top or near the top of all the release documents.",
          "bodyHTML": "<p>This request was a product of the TensorFlow Fall Symposium. Consider talking to the documentation team as well as maybe linking it to the top or near the top of all the release documents.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "laket" },
          "number": 22861,
          "resourcePath": "/tensorflow/tensorflow/issues/22861",
          "state": "CLOSED",
          "publishedAt": "2018-10-10T08:59:19Z",
          "closedAt": "2018-10-24T13:08:34Z",
          "title": "Variable names created by tf.kera.Model.build() is inconsistent with that by tf.keras.Model.call()",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):  v1.11.0-0-gc19e29306c 1.11.0\nPython version: Python 3.5.2\nBazel version (if compiling from source): No\nGCC/Compiler version (if compiling from source): No\nCUDA/cuDNN version: 7.2.1\nGPU model and memory: GTX 1060 6GB\nExact command to reproduce: Please see the below\n\nDescribe the problem\ntf.keras.Model makes Variables of its weights, when its build() or call() is called first.\nWhile I found call() makes Variables with the prefix \"MyModel/\", build() make Variables without any prefix.\nThat is inconvenient to manage non-object based checkpoint.\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nlayers = keras.layers\n\nclass Model(keras.Model):\n    def __init__(self, name=\"MyModel\"):\n        super().__init__(name=name)\n        self.conv1 = layers.Conv2D(32, [5,5], activation=tf.nn.relu, name=\"conv1\")\n        self.conv2 = layers.Conv2D(64, [5,5], activation=tf.nn.relu, name=\"conv2\")\n\n    def call(self, images):\n        featmap = self.conv1(images)\n        featmap = self.conv2(featmap)\n        return featmap\n\n    def inference(self, images):\n        return self.__call__(images)\n\n\nmodel = Model()\nflags = \"build\"\n\nif flags == \"build\":\n    model.build(input_shape=tf.TensorShape([None, 32, 32, 3]))\n\n    for w in model.weights:\n        print (w.op.name)\nelse:\n    dummy = tf.zeros([1, 32, 32, 3])\n    model(dummy)\n\n    for w in model.weights:\n        print (w.op.name)",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: 16.04.4 LTS</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: No</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:  v1.11.0-0-gc19e29306c 1.11.0</li>\n<li><strong>Python version</strong>: Python 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: No</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: No</li>\n<li><strong>CUDA/cuDNN version</strong>: 7.2.1</li>\n<li><strong>GPU model and memory</strong>: GTX 1060 6GB</li>\n<li><strong>Exact command to reproduce</strong>: Please see the below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>tf.keras.Model makes Variables of its weights, when its build() or call() is called first.</p>\n<p>While I found call() makes Variables with the prefix \"MyModel/\", build() make Variables without any prefix.</p>\n<p>That is inconvenient to manage non-object based checkpoint.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">tensorflow</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">tf</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">tensorflow</span>.<span class=\"pl-s1\">keras</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">keras</span>\n\n<span class=\"pl-s1\">layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">keras</span>.<span class=\"pl-s1\">layers</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-v\">Model</span>(<span class=\"pl-s1\">keras</span>.<span class=\"pl-v\">Model</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">__init__</span>(<span class=\"pl-s1\">self</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"MyModel\"</span>):\n        <span class=\"pl-en\">super</span>().<span class=\"pl-en\">__init__</span>(<span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">name</span>)\n        <span class=\"pl-s1\">self</span>.<span class=\"pl-s1\">conv1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">layers</span>.<span class=\"pl-v\">Conv2D</span>(<span class=\"pl-c1\">32</span>, [<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">5</span>], <span class=\"pl-s1\">activation</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">nn</span>.<span class=\"pl-s1\">relu</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"conv1\"</span>)\n        <span class=\"pl-s1\">self</span>.<span class=\"pl-s1\">conv2</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">layers</span>.<span class=\"pl-v\">Conv2D</span>(<span class=\"pl-c1\">64</span>, [<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">5</span>], <span class=\"pl-s1\">activation</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">nn</span>.<span class=\"pl-s1\">relu</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"conv2\"</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-s1\">self</span>, <span class=\"pl-s1\">images</span>):\n        <span class=\"pl-s1\">featmap</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">self</span>.<span class=\"pl-en\">conv1</span>(<span class=\"pl-s1\">images</span>)\n        <span class=\"pl-s1\">featmap</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">self</span>.<span class=\"pl-en\">conv2</span>(<span class=\"pl-s1\">featmap</span>)\n        <span class=\"pl-k\">return</span> <span class=\"pl-s1\">featmap</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">inference</span>(<span class=\"pl-s1\">self</span>, <span class=\"pl-s1\">images</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-s1\">self</span>.<span class=\"pl-en\">__call__</span>(<span class=\"pl-s1\">images</span>)\n\n\n<span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Model</span>()\n<span class=\"pl-s1\">flags</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"build\"</span>\n\n<span class=\"pl-k\">if</span> <span class=\"pl-s1\">flags</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s\">\"build\"</span>:\n    <span class=\"pl-s1\">model</span>.<span class=\"pl-en\">build</span>(<span class=\"pl-s1\">input_shape</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">TensorShape</span>([<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>]))\n\n    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">w</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-s1\">weights</span>:\n        <span class=\"pl-en\">print</span> (<span class=\"pl-s1\">w</span>.<span class=\"pl-s1\">op</span>.<span class=\"pl-s1\">name</span>)\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-s1\">dummy</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">zeros</span>([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>])\n    <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">dummy</span>)\n\n    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">w</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-s1\">weights</span>:\n        <span class=\"pl-en\">print</span> (<span class=\"pl-s1\">w</span>.<span class=\"pl-s1\">op</span>.<span class=\"pl-s1\">name</span>)</pre></div>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "Lldenaurois" },
          "number": 6644,
          "resourcePath": "/tensorflow/tensorflow/issues/6644",
          "state": "CLOSED",
          "publishedAt": "2017-01-04T21:21:42Z",
          "closedAt": "2017-02-13T17:44:20Z",
          "title": "Error: Data loss: file is too short to be an sstable",
          "bodyText": "Hi there,\nI'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm\nI am getting a \"Data loss\" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.\nCurrently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.\nW tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable\nTraceback (most recent call last):\n  File \"single_lm_train.py\", line 38, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 111, in run_eval\n    while ckpt_loader.load_checkpoint():\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 45, in load_checkpoint\n    if load_from_checkpoint(self.saver, self.logdir):\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 27, in load_from_checkpoint\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\n\ntensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]\nCaused by op u'save/RestoreV2_15', defined at:  File \"single_lm_train.py\", line 38, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 98, in run_eval\n    saver = tf.train.Saver(model.avg_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/repl\nica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha\npe_and_slices)]]",
          "bodyHTML": "<p>Hi there,</p>\n<p>I'm training the Language Model code available here: <a href=\"https://github.com/rafaljozefowicz/lm\">https://github.com/rafaljozefowicz/lm</a></p>\n<p>I am getting a \"Data loss\" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.</p>\n<p>Currently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.</p>\n<pre><code>W tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable\nTraceback (most recent call last):\n  File \"single_lm_train.py\", line 38, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 111, in run_eval\n    while ckpt_loader.load_checkpoint():\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 45, in load_checkpoint\n    if load_from_checkpoint(self.saver, self.logdir):\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 27, in load_from_checkpoint\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\n\ntensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]\nCaused by op u'save/RestoreV2_15', defined at:  File \"single_lm_train.py\", line 38, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 98, in run_eval\n    saver = tf.train.Saver(model.avg_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/repl\nica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha\npe_and_slices)]]\n\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ed-alertedh" },
          "number": 12345,
          "resourcePath": "/tensorflow/tensorflow/issues/12345",
          "state": "OPEN",
          "publishedAt": "2017-08-17T02:33:49Z",
          "closedAt": null,
          "title": "PEP 484 Type Annotations (feature request)",
          "bodyText": "System information\nN/A\nDescribe the problem\nBackground\nPEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.\nWhen adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the \"untyped\" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.\nBenefits of Adding Type Annotations\n\nThe expected inputs and outputs of functions become much clearer\nCode completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs\nStatic analysis can uncover latent bugs (case study here[5])\n\nDifficulties/Drawbacks\n\nPeople may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages \"Power Features\" [4] I would argue that striving towards code that is explicit is a similar philosophy\nThe protobuf compiler would need to be augmented to generate type annotations.\nThe Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.\nTensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.\n\nFinal thoughts\nI realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.\n[1] https://www.python.org/dev/peps/pep-0484/\n[2] http://mypy-lang.org/\n[3] https://github.com/python/typeshed\n[4] https://google.github.io/styleguide/pyguide.html#Power_Features\n[5] http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/",
          "bodyHTML": "<h3>System information</h3>\n<p>N/A</p>\n<h3>Describe the problem</h3>\n<h2>Background</h2>\n<p>PEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.</p>\n<p>When adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the \"untyped\" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.</p>\n<h2>Benefits of Adding Type Annotations</h2>\n<ul>\n<li>The expected inputs and outputs of functions become much clearer</li>\n<li>Code completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs</li>\n<li>Static analysis can uncover latent bugs (case study here[5])</li>\n</ul>\n<h2>Difficulties/Drawbacks</h2>\n<ul>\n<li>People may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages \"Power Features\" [4] I would argue that striving towards code that is explicit is a similar philosophy</li>\n<li>The protobuf compiler would need to be augmented to generate type annotations.</li>\n<li>The Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.</li>\n<li>Tensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.</li>\n</ul>\n<h2>Final thoughts</h2>\n<p>I realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.</p>\n<p>[1] <a rel=\"nofollow\" href=\"https://www.python.org/dev/peps/pep-0484/\">https://www.python.org/dev/peps/pep-0484/</a><br>\n[2] <a rel=\"nofollow\" href=\"http://mypy-lang.org/\">http://mypy-lang.org/</a><br>\n[3] <a href=\"https://github.com/python/typeshed\">https://github.com/python/typeshed</a><br>\n[4] <a rel=\"nofollow\" href=\"https://google.github.io/styleguide/pyguide.html#Power_Features\">https://google.github.io/styleguide/pyguide.html#Power_Features</a><br>\n[5] <a rel=\"nofollow\" href=\"http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/\">http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/</a></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "alanhdu" },
          "number": 20096,
          "resourcePath": "/tensorflow/tensorflow/issues/20096",
          "state": "CLOSED",
          "publishedAt": "2018-06-18T15:32:41Z",
          "closedAt": "2018-06-18T19:01:57Z",
          "title": "ModuleNotFoundError: No module named 'tensorflow.keras'",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary (CPU linux wheel)\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6.5\n\nDescribe the problem\nUsing Tensorflow 1.8.0, running:\nfrom tensorflow.keras.utils import Progbar\nraises an error:\nModuleNotFoundError: No module named 'tensorflow.keras'\n\nOf course, from tensorflow import keras works fine.\nThis is a minor nit since there's an obvious workaround, but IMO this is pretty unintuitive behavior for how modules work in Python. I'm not sure what kind of sorcery is going on to end up with this result \ud83d\ude06.",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary (CPU linux wheel)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using Tensorflow 1.8.0, running:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">tensorflow</span>.<span class=\"pl-s1\">keras</span>.<span class=\"pl-s1\">utils</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Progbar</span></pre></div>\n<p>raises an error:</p>\n<pre><code>ModuleNotFoundError: No module named 'tensorflow.keras'\n</code></pre>\n<p>Of course, <code>from tensorflow import keras</code> works fine.</p>\n<p>This is a minor nit since there's an obvious workaround, but IMO this is pretty unintuitive behavior for how modules work in Python. I'm not sure what kind of sorcery is going on to end up with this result <g-emoji class=\"g-emoji\" alias=\"laughing\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f606.png\">\ud83d\ude06</g-emoji>.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "taki0112" },
          "number": 14070,
          "resourcePath": "/tensorflow/tensorflow/issues/14070",
          "state": "CLOSED",
          "publishedAt": "2017-10-29T13:09:55Z",
          "closedAt": "2019-04-18T13:41:02Z",
          "title": "Feature request : add weight normalization",
          "bodyText": "can you implement weight norm ?\nI want to use it as follows.\n    x = tf.layers.conv2d(x, filter_size=32, kernel_size=[3,3], strides=2)\n    x = weight_norm(x)\nIs it possible?",
          "bodyHTML": "<p>can you implement <a href=\"https://arxiv.org/pdf/1602.07868.pdf\" rel=\"nofollow\">weight norm</a> ?</p>\n<p>I want to use it as follows.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">layers</span>.<span class=\"pl-en\">conv2d</span>(<span class=\"pl-s1\">x</span>, <span class=\"pl-s1\">filter_size</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">32</span>, <span class=\"pl-s1\">kernel_size</span><span class=\"pl-c1\">=</span>[<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>], <span class=\"pl-s1\">strides</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">2</span>)\n    <span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">weight_norm</span>(<span class=\"pl-s1\">x</span>)</pre></div>\n<p>Is it possible?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "wangli0519" },
          "number": 6769,
          "resourcePath": "/tensorflow/tensorflow/issues/6769",
          "state": "CLOSED",
          "publishedAt": "2017-01-10T15:02:29Z",
          "closedAt": "2017-01-10T19:08:31Z",
          "title": "TF Learn TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16",
          "bodyText": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nFor general support from the community, see StackOverflow.\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\ntf.cast()\nEnvironment info\nOperating System:\nmasOSSierra\njupyter notebook\ntensforflow v0.12.1\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nNo\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: pip install tensorflow\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".tensforflow v0.12.1\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nI try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py\nWhat other attempted solutions have you tried?\ntry to cast DataFrame into float32 with\nX_train = X_train.astype(np.float32)\ntry to cast each column with\ntf.cast(col, tf.float32)\nbut after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)\nfeature_columns dtype just turn to tf.float64\n(tried and didn't find attribute from source code that I can change dtype here)\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\n\nTypeError                                 Traceback (most recent call last)\n in ()\n----> 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])\n/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)\n189             _call_location(), decorator_utils.get_qualified_name(func),\n190             func.module, arg_name, date, instructions)\n--> 191       return func(*args, **kwargs)\n192     new_func.doc = _add_deprecated_arg_notice_to_docstring(\n193         func.doc, date, instructions)\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\n353                              steps=steps,\n354                              monitors=monitors,\n--> 355                              max_steps=max_steps)\n356     logging.info('Loss for final step: %s.', loss)\n357     return self\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\n697       # cases, but will soon be deleted after the subclasses are updated.\n698       # TODO(b/32664904): Update subclasses and delete the else-statement.\n--> 699       train_ops = self._get_train_ops(features, labels)\n700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature\n701         train_op = train_ops.train_op\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)\n1050       ModelFnOps object.\n1051     \"\"\"\n-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\n1053\n1054   def _get_eval_ops(self, features, labels, metrics):\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)\n1019                                           params=self.params)\n1020       else:\n-> 1021         model_fn_results = self._model_fn(features, labels, mode=mode)\n1022     else:\n1023       model_fn_results = self._model_fn(features, labels)\n in conv_model(feature, target, mode)\n10                                     activation_fn=tf.nn.relu)\n11\n---> 12         h_pool1 = max_pool_2x2(h_conv1)\n13\n14     with tf.variable_scope('conv_layer2'):\n in max_pool_2x2(tensor_in)\n1 def max_pool_2x2(tensor_in):\n----> 2     return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)\n1615                                 padding=padding,\n1616                                 data_format=data_format,\n-> 1617                                 name=name)\n1618\n1619\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)\n1596   result = _op_def_lib.apply_op(\"MaxPool\", input=input, ksize=ksize,\n1597                                 strides=strides, padding=padding,\n-> 1598                                 data_format=data_format, name=name)\n1599   return result\n1600\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n580             for base_type in base_types:\n581               _SatisfiesTypeConstraint(base_type,\n--> 582                                        _Attr(op_def, input_arg.type_attr))\n583             attrs[input_arg.type_attr] = attr_value\n584             inferred_from[input_arg.type_attr] = input_name\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)\n58           \"DataType %s for attr '%s' not in list of allowed values: %s\" %\n59           (dtypes.as_dtype(dtype).name, attr_def.name,\n---> 60            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n61\n62\nTypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16\nMany thanks.",
          "bodyHTML": "<p>NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.</p>\n<p>For general support from the community, see <a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">StackOverflow</a>.<br>\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed<br>\nout of scope for GitHub Issues and point people to StackOverflow.</p>\n<p>For bugs or installation issues, please provide the following information.<br>\nThe more information you provide, the more easily we will be able to offer<br>\nhelp and advice.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>tf.cast()</p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nmasOSSierra<br>\njupyter notebook<br>\ntensforflow v0.12.1</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\nNo</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed: pip install tensorflow</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.tensforflow v0.12.1</li>\n</ol>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)</li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>I try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py</a></p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>try to cast DataFrame into float32 with<br>\nX_train = X_train.astype(np.float32)<br>\ntry to cast each column with<br>\ntf.cast(col, tf.float32)<br>\nbut after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)<br>\nfeature_columns dtype just turn to tf.float64<br>\n(tried and didn't find attribute from source code that I can change dtype here)</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>\n<hr>\n<p>TypeError                                 Traceback (most recent call last)<br>\n in ()<br>\n----&gt; 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)<br>\n189             _call_location(), decorator_utils.get_qualified_name(func),<br>\n190             func.<strong>module</strong>, arg_name, date, instructions)<br>\n--&gt; 191       return func(*args, **kwargs)<br>\n192     new_func.<strong>doc</strong> = _add_deprecated_arg_notice_to_docstring(<br>\n193         func.<strong>doc</strong>, date, instructions)</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)<br>\n353                              steps=steps,<br>\n354                              monitors=monitors,<br>\n--&gt; 355                              max_steps=max_steps)<br>\n356     logging.info('Loss for final step: %s.', loss)<br>\n357     return self</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)<br>\n697       # cases, but will soon be deleted after the subclasses are updated.<br>\n698       # TODO(b/32664904): Update subclasses and delete the else-statement.<br>\n--&gt; 699       train_ops = self._get_train_ops(features, labels)<br>\n700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature<br>\n701         train_op = train_ops.train_op</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)<br>\n1050       <code>ModelFnOps</code> object.<br>\n1051     \"\"\"<br>\n-&gt; 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)<br>\n1053<br>\n1054   def _get_eval_ops(self, features, labels, metrics):</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)<br>\n1019                                           params=self.params)<br>\n1020       else:<br>\n-&gt; 1021         model_fn_results = self._model_fn(features, labels, mode=mode)<br>\n1022     else:<br>\n1023       model_fn_results = self._model_fn(features, labels)</p>\n<p> in conv_model(feature, target, mode)<br>\n10                                     activation_fn=tf.nn.relu)<br>\n11<br>\n---&gt; 12         h_pool1 = max_pool_2x2(h_conv1)<br>\n13<br>\n14     with tf.variable_scope('conv_layer2'):</p>\n<p> in max_pool_2x2(tensor_in)<br>\n1 def max_pool_2x2(tensor_in):<br>\n----&gt; 2     return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)<br>\n1615                                 padding=padding,<br>\n1616                                 data_format=data_format,<br>\n-&gt; 1617                                 name=name)<br>\n1618<br>\n1619</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)<br>\n1596   result = _op_def_lib.apply_op(\"MaxPool\", input=input, ksize=ksize,<br>\n1597                                 strides=strides, padding=padding,<br>\n-&gt; 1598                                 data_format=data_format, name=name)<br>\n1599   return result<br>\n1600</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)<br>\n580             for base_type in base_types:<br>\n581               _SatisfiesTypeConstraint(base_type,<br>\n--&gt; 582                                        _Attr(op_def, input_arg.type_attr))<br>\n583             attrs[input_arg.type_attr] = attr_value<br>\n584             inferred_from[input_arg.type_attr] = input_name</p>\n<p>/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)<br>\n58           \"DataType %s for attr '%s' not in list of allowed values: %s\" %<br>\n59           (dtypes.as_dtype(dtype).name, attr_def.name,<br>\n---&gt; 60            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))<br>\n61<br>\n62</p>\n<p>TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16</p>\n<p>Many thanks.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "nehardave" },
          "number": 8586,
          "resourcePath": "/tensorflow/tensorflow/issues/8586",
          "state": "CLOSED",
          "publishedAt": "2017-03-21T14:58:38Z",
          "closedAt": "2017-03-23T06:13:40Z",
          "title": "TensorFlow upgrade to 1.0.1 ",
          "bodyText": "I upgraded my server from 1.0.0 (ubuntu):\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\nBut I get the following discrepancy:\n(tensorflow)$ pip list | grep tensorflow\ntensorflow (1.0.0)\n(tensorflow)$ python -c 'import tensorflow as tf; print(tf.version)'\n1.0.1",
          "bodyHTML": "<p>I upgraded my server from 1.0.0 (ubuntu):</p>\n<p>pip install --upgrade <a rel=\"nofollow\" href=\"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl</a><br>\nBut I get the following discrepancy:</p>\n<p>(tensorflow)$ pip list | grep tensorflow<br>\ntensorflow (1.0.0)<br>\n(tensorflow)$ python -c 'import tensorflow as tf; print(tf.<strong>version</strong>)'<br>\n1.0.1</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "eaplatanios" },
          "number": 15323,
          "resourcePath": "/tensorflow/tensorflow/issues/15323",
          "state": "CLOSED",
          "publishedAt": "2017-12-12T21:45:03Z",
          "closedAt": "2018-01-30T01:55:50Z",
          "title": "Beam Search Decoder API",
          "bodyText": "@ebrevdo Why is it that the user needs to call tile_batch explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided initial_state in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.\nThank you!",
          "bodyHTML": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/ebrevdo/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> Why is it that the user needs to call <code>tile_batch</code> explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided <code>initial_state</code> in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.</p>\n<p>Thank you!</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "sakaia" },
          "number": 16400,
          "resourcePath": "/tensorflow/tensorflow/issues/16400",
          "state": "CLOSED",
          "publishedAt": "2018-01-25T10:17:07Z",
          "closedAt": "2018-01-25T16:06:01Z",
          "title": "[doc] link to \"How to Use t-SNE Effectively\" from embeddings is broken",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7:\nTensorFlow installed from (source or binary) binary:\n**TensorFlow version (use command below) 1.5.0rc0 **:\nPython version  3.5.1:\nBazel version (if compiling from source) NOT USED:\nGCC/Compiler version (if compiling from source) NOT USED:\nCUDA/cuDNN version NOT USED:\n**GPU model and memory NOT USED **:\nExact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/:\n\nDescribe the problem\n\nLink to to \"How to Use t-SNE Effectively\" is broken.\nThe page link is follows (before junmping)\n\nhttps://www.tensorflow.org/programmers_guide/embedding\n404 page is following URL\n\nhttps://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/\n\n\n\n\nOriginal page should be follows. (the URL in embedding.md should rewrite to follows)\n\nhttps://distill.pub/2016/misread-tsne/\n\n\n\nSource code / logs\n\nThe problem code is follows.\n\nhttps://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary) binary</strong>:</li>\n<li>**TensorFlow version (use command below) 1.5.0rc0 **:</li>\n<li><strong>Python version  3.5.1</strong>:</li>\n<li><strong>Bazel version (if compiling from source) NOT USED</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source) NOT USED</strong>:</li>\n<li><strong>CUDA/cuDNN version NOT USED</strong>:</li>\n<li>**GPU model and memory NOT USED **:</li>\n<li><strong>Exact command to reproduce DOC Problem. Just look <a href=\"https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/\" rel=\"nofollow\">https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/</a></strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<ul>\n<li>Link to to \"How to Use t-SNE Effectively\" is broken.</li>\n<li>The page link is follows (before junmping)\n<ul>\n<li><a href=\"https://www.tensorflow.org/programmers_guide/embedding\" rel=\"nofollow\">https://www.tensorflow.org/programmers_guide/embedding</a></li>\n<li>404 page is following URL\n<ul>\n<li><a href=\"https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/\" rel=\"nofollow\">https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Original page should be follows. (the URL in embedding.md should rewrite to follows)\n<ul>\n<li><a href=\"https://distill.pub/2016/misread-tsne/\" rel=\"nofollow\">https://distill.pub/2016/misread-tsne/</a></li>\n</ul>\n</li>\n</ul>\n<h3>Source code / logs</h3>\n<ul>\n<li>The problem code is follows.\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123\">https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123</a></li>\n</ul>\n</li>\n</ul>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "lizaigaoge550" },
          "number": 21985,
          "resourcePath": "/tensorflow/tensorflow/issues/21985",
          "state": "CLOSED",
          "publishedAt": "2018-08-31T02:46:10Z",
          "closedAt": "2018-08-31T22:40:02Z",
          "title": "While loop no gradients provided for any variable",
          "bodyText": "After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.\n\nwhile_loop\ndef dynamic_pointing_decoder(self, U, mask):\ndef _HMN(ut, h, us, ue):\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\nut_r = tf.concat([ut, r], axis=1) #batch,3d\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\nhmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\nhmn = tf.reduce_max(hmn, axis=2) #batch 1\nhmn = tf.reshape(hmn, [-1]) #batch\nreturn hmn\n    def body(time_step, p1s, p2s, alphas, betas, us, ue, state):\n        us_ue = tf.concat([us, ue], axis=1)  # batch 4d\n        h, state = cell(inputs=us_ue, state=state)  # batch * d\n\n        with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):\n            alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_start = tf.argmax(alpha, axis=1)  # batch\n        i_start = tf.cast(i_start, tf.int32)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        us = tf.gather_nd(U, s_idx)  # batch 2d\n\n        with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):\n            beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_end = tf.argmax(beta, axis=1)  # batch\n        i_end = tf.cast(i_end, tf.int32)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        ue = tf.gather_nd(U, e_idx)  # batch 2d\n\n        p1s.write(time_step, i_start)\n        p2s.write(time_step, i_end)\n        alphas.write(time_step, alpha)\n        betas.write(time_step, beta)\n        return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)\n\n    def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):\n        return tf.less(time_step, 4)\n\n\n    with tf.variable_scope('dynamic_pointing_decoder'):\n        cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\n        i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        idx = tf.range(0, tf.shape(U)[0], 1)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        us = tf.gather_nd(U, s_idx) #batch 2d\n        ue = tf.gather_nd(U, e_idx) #batch 2d\n        p1s = tf.TensorArray(tf.int32, size=4)\n        p2s = tf.TensorArray(tf.int32, size=4)\n        alphas = tf.TensorArray(tf.float32, size=4)\n        betas = tf.TensorArray(tf.float32, size=4)\n        state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\n                                          tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\n        U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\n        #time_step, p1s, p2s, us, ue, state\n        time_step = 0\n        time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,\n                                                                          loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),\n                                                                          maximum_iterations=4)\n        self.p1s = tf.transpose(p1s.stack()) #batch*4\n        self.p2s = tf.transpose(p2s.stack())\n        print(\n            \"p1s  shape : {0}\".format(np.shape(self.p1s))\n        )\n        self.p1 = tf.unstack(self.p1s,axis=0)[-1]\n        print(\"p1 shape : {0}\".format(np.shape(self.p1)))\n        self.p2 = tf.unstack(self.p2s, axis=0)[-1]\n        alphas = tf.unstack(alphas.stack(), axis=0)\n        betas = tf.unstack(betas.stack(), axis=0)\n        print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))\n        return alphas, betas\n\n#no while loop\ndef dynamic_pointing_decoder(self, U, mask):\ndef _HMN(ut, h, us, ue):\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\nut_r = tf.concat([ut, r], axis=1) #batch,3d\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\nhmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\nhmn = tf.reduce_max(hmn, axis=2) #batch 1\nhmn = tf.reshape(hmn, [-1]) #batch\nreturn hmn\nwith tf.variable_scope('dynamic_pointing_decoder'):\n#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)\n#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])\n#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)\ncell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\ni_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\ni_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\nidx = tf.range(0, tf.shape(U)[0], 1)\ns_idx = tf.stack([idx, i_start], axis=1)\ne_idx = tf.stack([idx, i_end], axis=1)\nus = tf.gather_nd(U, s_idx) #batch 2d\nue = tf.gather_nd(U, e_idx) #batch 2d\nalphas, betas = [], []\nstate = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\ntf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\nU_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\nfor time_step in range(4):\nif time_step >= 1:\ntf.get_variable_scope().reuse_variables()\nus_ue = tf.concat([us,ue], axis=1) #batch 4d\nh, state = cell(inputs=us_ue, state=state) #batch * d\n            with tf.variable_scope('alpha_HMN'):\n                if time_step >= 1:\n                    tf.get_variable_scope().reuse_variables()\n                alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_start = tf.argmax(alpha, axis=1) #batch\n            i_start = tf.cast(i_start, tf.int32)\n            s_idx = tf.stack([idx, i_start], axis=1)\n            us = tf.gather_nd(U, s_idx) #batch 2d\n\n            with tf.variable_scope('betas_HMN'):\n                if time_step >= 1:\n                    tf.get_variable_scope().reuse_variables()\n                beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_end = tf.argmax(alpha, axis=1) #batch\n            i_end = tf.cast(i_end, tf.int32)\n            e_idx = tf.stack([idx, i_end], axis=1)\n            ue = tf.gather_nd(U, e_idx) #batch 2d\n\n            alphas.append(alpha)\n            betas.append(beta)\n            #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):\n            #    break\n            #else:\n            #    pre_start = i_start\n            #    pre_end = i_end\n        return alpha, beta\n\nanyone can help me? Thank you very much",
          "bodyHTML": "<p>After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/12975526/44889979-d9ca6b80-ad0a-11e8-967b-0f31a2467c68.PNG\"><img src=\"https://user-images.githubusercontent.com/12975526/44889979-d9ca6b80-ad0a-11e8-967b-0f31a2467c68.PNG\" alt=\"default\" style=\"max-width:100%;\"></a></p>\n<h1>while_loop</h1>\n<p>def dynamic_pointing_decoder(self, U, mask):<br>\ndef _HMN(ut, h, us, ue):<br>\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d<br>\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())<br>\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d<br>\nut_r = tf.concat([ut, r], axis=1) #batch,3d<br>\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())<br>\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())<br>\nmt1 = tf.einsum('bt,top-&gt;bop', ut_r, W1) + b1<br>\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d<br>\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())<br>\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())<br>\nmt2 = tf.einsum('bi,ijp-&gt;bjp', mt1, W2) + b2<br>\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d<br>\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d<br>\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())<br>\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())<br>\nhmn = tf.einsum('bi,ijp-&gt;bjp', mt12, W3) + b3<br>\nhmn = tf.reduce_max(hmn, axis=2) #batch 1<br>\nhmn = tf.reshape(hmn, [-1]) #batch<br>\nreturn hmn</p>\n<pre><code>    def body(time_step, p1s, p2s, alphas, betas, us, ue, state):\n        us_ue = tf.concat([us, ue], axis=1)  # batch 4d\n        h, state = cell(inputs=us_ue, state=state)  # batch * d\n\n        with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):\n            alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_start = tf.argmax(alpha, axis=1)  # batch\n        i_start = tf.cast(i_start, tf.int32)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        us = tf.gather_nd(U, s_idx)  # batch 2d\n\n        with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):\n            beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_end = tf.argmax(beta, axis=1)  # batch\n        i_end = tf.cast(i_end, tf.int32)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        ue = tf.gather_nd(U, e_idx)  # batch 2d\n\n        p1s.write(time_step, i_start)\n        p2s.write(time_step, i_end)\n        alphas.write(time_step, alpha)\n        betas.write(time_step, beta)\n        return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)\n\n    def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):\n        return tf.less(time_step, 4)\n\n\n    with tf.variable_scope('dynamic_pointing_decoder'):\n        cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\n        i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        idx = tf.range(0, tf.shape(U)[0], 1)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        us = tf.gather_nd(U, s_idx) #batch 2d\n        ue = tf.gather_nd(U, e_idx) #batch 2d\n        p1s = tf.TensorArray(tf.int32, size=4)\n        p2s = tf.TensorArray(tf.int32, size=4)\n        alphas = tf.TensorArray(tf.float32, size=4)\n        betas = tf.TensorArray(tf.float32, size=4)\n        state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\n                                          tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\n        U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\n        #time_step, p1s, p2s, us, ue, state\n        time_step = 0\n        time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,\n                                                                          loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),\n                                                                          maximum_iterations=4)\n        self.p1s = tf.transpose(p1s.stack()) #batch*4\n        self.p2s = tf.transpose(p2s.stack())\n        print(\n            \"p1s  shape : {0}\".format(np.shape(self.p1s))\n        )\n        self.p1 = tf.unstack(self.p1s,axis=0)[-1]\n        print(\"p1 shape : {0}\".format(np.shape(self.p1)))\n        self.p2 = tf.unstack(self.p2s, axis=0)[-1]\n        alphas = tf.unstack(alphas.stack(), axis=0)\n        betas = tf.unstack(betas.stack(), axis=0)\n        print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))\n        return alphas, betas\n</code></pre>\n<p>#no while loop<br>\ndef dynamic_pointing_decoder(self, U, mask):<br>\ndef _HMN(ut, h, us, ue):<br>\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d<br>\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())<br>\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d<br>\nut_r = tf.concat([ut, r], axis=1) #batch,3d<br>\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())<br>\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())<br>\nmt1 = tf.einsum('bt,top-&gt;bop', ut_r, W1) + b1<br>\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d<br>\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())<br>\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())<br>\nmt2 = tf.einsum('bi,ijp-&gt;bjp', mt1, W2) + b2<br>\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d<br>\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d<br>\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())<br>\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())<br>\nhmn = tf.einsum('bi,ijp-&gt;bjp', mt12, W3) + b3<br>\nhmn = tf.reduce_max(hmn, axis=2) #batch 1<br>\nhmn = tf.reshape(hmn, [-1]) #batch<br>\nreturn hmn<br>\nwith tf.variable_scope('dynamic_pointing_decoder'):<br>\n#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)<br>\n#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])<br>\n#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)<br>\ncell = CudnnCompatibleLSTMCell(self._config.hidden_dim)<br>\ni_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)<br>\ni_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)<br>\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)<br>\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)<br>\nidx = tf.range(0, tf.shape(U)[0], 1)<br>\ns_idx = tf.stack([idx, i_start], axis=1)<br>\ne_idx = tf.stack([idx, i_end], axis=1)<br>\nus = tf.gather_nd(U, s_idx) #batch 2d<br>\nue = tf.gather_nd(U, e_idx) #batch 2d<br>\nalphas, betas = [], []<br>\nstate = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),<br>\ntf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN<br>\nU_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d<br>\nfor time_step in range(4):<br>\nif time_step &gt;= 1:<br>\ntf.get_variable_scope().reuse_variables()<br>\nus_ue = tf.concat([us,ue], axis=1) #batch 4d<br>\nh, state = cell(inputs=us_ue, state=state) #batch * d</p>\n<pre><code>            with tf.variable_scope('alpha_HMN'):\n                if time_step &gt;= 1:\n                    tf.get_variable_scope().reuse_variables()\n                alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_start = tf.argmax(alpha, axis=1) #batch\n            i_start = tf.cast(i_start, tf.int32)\n            s_idx = tf.stack([idx, i_start], axis=1)\n            us = tf.gather_nd(U, s_idx) #batch 2d\n\n            with tf.variable_scope('betas_HMN'):\n                if time_step &gt;= 1:\n                    tf.get_variable_scope().reuse_variables()\n                beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_end = tf.argmax(alpha, axis=1) #batch\n            i_end = tf.cast(i_end, tf.int32)\n            e_idx = tf.stack([idx, i_end], axis=1)\n            ue = tf.gather_nd(U, e_idx) #batch 2d\n\n            alphas.append(alpha)\n            betas.append(beta)\n            #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):\n            #    break\n            #else:\n            #    pre_start = i_start\n            #    pre_end = i_end\n        return alpha, beta\n</code></pre>\n<p>anyone can help me? Thank you very much</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "martinitus" },
          "number": 4027,
          "resourcePath": "/tensorflow/tensorflow/issues/4027",
          "state": "CLOSED",
          "publishedAt": "2016-08-24T23:17:50Z",
          "closedAt": "2016-09-06T23:04:01Z",
          "title": "[cmake] Build error in dependency re2",
          "bodyText": "Somehow the re2 dependency does not get build correctly.\nI think its because of re2_INCLUDE_DIR in re.cmake holding two directories. Then COMMAND ${CMAKE_COMMAND} -E make_directory ${re2_INCLUDE_DIR} fails since cmake -E make_directory only takes one arg. I'm just comiping and might add a PR if it works.",
          "bodyHTML": "<p>Somehow the re2 dependency does not get build correctly.<br>\nI think its because of <code>re2_INCLUDE_DIR</code> in re.cmake holding two directories. Then <code>COMMAND ${CMAKE_COMMAND} -E make_directory ${re2_INCLUDE_DIR}</code> fails since <code>cmake -E make_directory</code> only takes one arg. I'm just comiping and might add a PR if it works.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "carlthome" },
          "number": 14797,
          "resourcePath": "/tensorflow/tensorflow/issues/14797",
          "state": "CLOSED",
          "publishedAt": "2017-11-22T14:45:08Z",
          "closedAt": "2017-11-22T15:42:51Z",
          "title": "XLA AOT tfcompile failure due to undeclared inclusions in cc_binary rule",
          "bodyText": "This happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:\nERROR: tensorflow/BUILD:13:1: undeclared inclusion(s) in rule '//:model':\nthis rule is missing dependency declarations for the following files included by 'graph.cc':\n  'tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h'\n  'tensorflow/compiler/tf2xla/xla_local_runtime_context.h'\n  'tensorflow/core/platform/macros.h'\n  '/tensorflow/core/platform/types.h'\n  '/tensorflow/core/platform/platform.h'\n  '/tensorflow/core/platform/default/integral_types.h'\n  '/tensorflow/compiler/xla/executable_run_options.h'\ngraph.cc pretty much just does #include \"graph.h\" as per the tfcompile tutorial and it's weird because these headers seem to be included in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.\nThis is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):\nload(\"@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\n\ntf_library(\n  name = \"graph\",\n  cpp_class = \"Graph\",\n  graph = \"graph.pb\",\n  config = \"graph.config.pb\",\n)\n\ncc_binary(\n  name = \"model\",\n  srcs = [\"graph.cc\"],\n  deps = [\":graph\", \"//third_party/eigen3\"],\n  linkopts = [\"-lpthread\"]\n)\nI'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.",
          "bodyHTML": "<p>This happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:</p>\n<div class=\"highlight highlight-source-shell\"><pre>ERROR: tensorflow/BUILD:13:1: undeclared inclusion(s) <span class=\"pl-k\">in</span> rule <span class=\"pl-s\"><span class=\"pl-pds\">'</span>//:model<span class=\"pl-pds\">'</span></span>:\nthis rule is missing dependency declarations <span class=\"pl-k\">for</span> the following files included by <span class=\"pl-s\"><span class=\"pl-pds\">'</span>graph.cc<span class=\"pl-pds\">'</span></span>:\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h<span class=\"pl-pds\">'</span></span>\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tensorflow/compiler/tf2xla/xla_local_runtime_context.h<span class=\"pl-pds\">'</span></span>\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tensorflow/core/platform/macros.h<span class=\"pl-pds\">'</span></span>\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tensorflow/core/platform/types.h<span class=\"pl-pds\">'</span></span>\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tensorflow/core/platform/platform.h<span class=\"pl-pds\">'</span></span>\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tensorflow/core/platform/default/integral_types.h<span class=\"pl-pds\">'</span></span>\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tensorflow/compiler/xla/executable_run_options.h<span class=\"pl-pds\">'</span></span></pre></div>\n<p>graph.cc pretty much just does <code>#include \"graph.h\"</code> as per the tfcompile tutorial and it's weird because these headers <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile.bzl#L209-L230\">seem to be included</a> in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.</p>\n<p>This is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):</p>\n<div class=\"highlight highlight-source-shell\"><pre>load(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tf_library<span class=\"pl-pds\">\"</span></span>)\n\ntf_library(\n  name = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>graph<span class=\"pl-pds\">\"</span></span>,\n  cpp_class = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Graph<span class=\"pl-pds\">\"</span></span>,\n  graph = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>graph.pb<span class=\"pl-pds\">\"</span></span>,\n  config = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>graph.config.pb<span class=\"pl-pds\">\"</span></span>,\n)\n\ncc_binary(\n  name = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>,\n  srcs = [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>graph.cc<span class=\"pl-pds\">\"</span></span>],\n  deps = [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>:graph<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>//third_party/eigen3<span class=\"pl-pds\">\"</span></span>],\n  linkopts = [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-lpthread<span class=\"pl-pds\">\"</span></span>]\n)</pre></div>\n<p>I'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "drasmuss" },
          "number": 7251,
          "resourcePath": "/tensorflow/tensorflow/issues/7251",
          "state": "CLOSED",
          "publishedAt": "2017-02-03T21:48:30Z",
          "closedAt": "2018-02-08T01:06:26Z",
          "title": "Native GPU version of `tf.dynamic_stitch`",
          "bodyText": "Environment info\nOperating System: Windows 10\nInstalled version of CUDA and cuDNN: 8.0, 5105\ntensorflow release 0.12.1\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport tensorflow as tf\nfrom tensorflow.python.client.timeline import Timeline\n\nwith tf.device(\"/gpu:0\"):\n    x = tf.ones(100)\n    idxs = tf.range(100)\n\n    for _ in range(10):\n        y = tf.identity(x)\n        x = tf.dynamic_stitch([idxs, idxs], [x, y])\n        # x = tf.gather(y, idxs)\n\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    metadata = tf.RunMetadata()\n    sess.run(x, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n             run_metadata=metadata)\n\ntimeline = Timeline(metadata.step_stats)\nwith open(\"profile.json\", \"w\") as f:\n    f.write(timeline.generate_chrome_trace_format())\nThe log_device_placement output shows that everything is assigned to the GPU, as expected.  However, inspecting the trace output shows that data is being copied on and off the GPU for each call to dynamic_stitch.  This is something specific to the dynamic_stitch implementation, because using tf.gather (a similar indexed read operation, and functionally equivalent in this case), doesn't show this behaviour.\nIs this intended behaviour for dynamic_stitch (i.e., the copying to and from the GPU is necessary)?  Or is this a bug?  If it isn't a bug, is there some equivalent solution that doesn't require the data to be copied back and forth?",
          "bodyHTML": "<h3>Environment info</h3>\n<p>Operating System: Windows 10</p>\n<p>Installed version of CUDA and cuDNN: 8.0, 5105<br>\ntensorflow release 0.12.1</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">tensorflow</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">tf</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">tensorflow</span>.<span class=\"pl-s1\">python</span>.<span class=\"pl-s1\">client</span>.<span class=\"pl-s1\">timeline</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Timeline</span>\n\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">device</span>(<span class=\"pl-s\">\"/gpu:0\"</span>):\n    <span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">ones</span>(<span class=\"pl-c1\">100</span>)\n    <span class=\"pl-s1\">idxs</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">range</span>(<span class=\"pl-c1\">100</span>)\n\n    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">_</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"pl-c1\">10</span>):\n        <span class=\"pl-s1\">y</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">identity</span>(<span class=\"pl-s1\">x</span>)\n        <span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">dynamic_stitch</span>([<span class=\"pl-s1\">idxs</span>, <span class=\"pl-s1\">idxs</span>], [<span class=\"pl-s1\">x</span>, <span class=\"pl-s1\">y</span>])\n        <span class=\"pl-c\"># x = tf.gather(y, idxs)</span>\n\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">Session</span>(<span class=\"pl-s1\">config</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">ConfigProto</span>(<span class=\"pl-s1\">log_device_placement</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)) <span class=\"pl-k\">as</span> <span class=\"pl-s1\">sess</span>:\n    <span class=\"pl-s1\">metadata</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">RunMetadata</span>()\n    <span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">x</span>, <span class=\"pl-s1\">options</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">RunOptions</span>(<span class=\"pl-s1\">trace_level</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">RunOptions</span>.<span class=\"pl-v\">FULL_TRACE</span>),\n             <span class=\"pl-s1\">run_metadata</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">metadata</span>)\n\n<span class=\"pl-s1\">timeline</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Timeline</span>(<span class=\"pl-s1\">metadata</span>.<span class=\"pl-s1\">step_stats</span>)\n<span class=\"pl-k\">with</span> <span class=\"pl-en\">open</span>(<span class=\"pl-s\">\"profile.json\"</span>, <span class=\"pl-s\">\"w\"</span>) <span class=\"pl-k\">as</span> <span class=\"pl-s1\">f</span>:\n    <span class=\"pl-s1\">f</span>.<span class=\"pl-en\">write</span>(<span class=\"pl-s1\">timeline</span>.<span class=\"pl-en\">generate_chrome_trace_format</span>())</pre></div>\n<p>The <code>log_device_placement</code> output shows that everything is assigned to the GPU, as expected.  However, inspecting the trace output shows that data is being copied on and off the GPU for each call to <code>dynamic_stitch</code>.  This is something specific to the <code>dynamic_stitch</code> implementation, because using <code>tf.gather</code> (a similar indexed read operation, and functionally equivalent in this case), doesn't show this behaviour.</p>\n<p>Is this intended behaviour for <code>dynamic_stitch</code> (i.e., the copying to and from the GPU is necessary)?  Or is this a bug?  If it isn't a bug, is there some equivalent solution that doesn't require the data to be copied back and forth?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "dhasegan" },
          "number": 10019,
          "resourcePath": "/tensorflow/tensorflow/issues/10019",
          "state": "CLOSED",
          "publishedAt": "2017-05-19T01:30:21Z",
          "closedAt": "2017-05-19T03:09:35Z",
          "title": "Java Api String tensors support",
          "bodyText": "from Tensor.java:\nnon-scalar DataType.STRING tensors are not supported yet\nIs there a plan for adding them for the Java interface?",
          "bodyHTML": "<p>from <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L90\">Tensor.java</a>:<br>\n<strong>non-scalar DataType.STRING tensors are not supported yet</strong></p>\n<p>Is there a plan for adding them for the Java interface?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "worldforward" },
          "number": 19814,
          "resourcePath": "/tensorflow/tensorflow/issues/19814",
          "state": "CLOSED",
          "publishedAt": "2018-06-06T16:29:43Z",
          "closedAt": "2018-06-07T09:27:40Z",
          "title": "How to make statistics script using summary?",
          "bodyText": "Hi, all\nI want parameter distribution analysis script for pretrained models.\nI do not want special script for each model, just want single program to do it.\nSome person advised me to use the summary graph.\ntensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc\nI check the code and fell that the code does not support extracting parameters from pb file.\nI wrote a draft code to analyze;\nhttps://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py\nAny suggestion is welcome, and I am beginner, please explain softly.\nBest,\nSyouyu",
          "bodyHTML": "<p>Hi, all</p>\n<p>I want parameter distribution analysis script for pretrained models.<br>\nI do not want special script for each model, just want single program to do it.</p>\n<p>Some person advised me to use the summary graph.<br>\ntensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc</p>\n<p>I check the code and fell that the code does not support extracting parameters from pb file.</p>\n<p>I wrote a draft code to analyze;<br>\n<a href=\"https://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py\">https://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py</a></p>\n<p>Any suggestion is welcome, and I am beginner, please explain softly.</p>\n<p>Best,<br>\nSyouyu</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "Mikun4619" },
          "number": 16946,
          "resourcePath": "/tensorflow/tensorflow/issues/16946",
          "state": "CLOSED",
          "publishedAt": "2018-02-12T12:21:13Z",
          "closedAt": "2018-02-18T11:09:25Z",
          "title": "tensorflow lite converter(toco) build error ",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10(64bit)\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below): tensorflow 1.5.0\nPython version: Python 2.7/3.6\nBazel version (if compiling from source):  bazel 0.9.0\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: No GPU model\n\nDescribe the problem\nI try to build the toco that is tensorflow lite converter.\nBut I can not success to build. please see below for the details.\nSource code / logs\nC:\\tensorflow>bazel build //tensorflow/contrib/lite/toco:toco\nThe following error message appears.\n\nERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\nFile \"C:/tensorflow/third_party/repo.bzl\", line 88\n_apply_patch(ctx, ctx.attr.patch_file)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\n_execute_and_check_ret_code(ctx, cmd)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\nfail(\"Non-zero return code({1}) when ...))\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\nWARNING: Target pattern parsing failed.\nERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\nFile \"C:/tensorflow/third_party/repo.bzl\", line 88\n_apply_patch(ctx, ctx.attr.patch_file)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\n_execute_and_check_ret_code(ctx, cmd)\nFile \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\nfail(\"Non-zero return code({1}) when ...))\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\nINFO: Elapsed time: 27.852s\nFAILED: Build did NOT complete successfully (0 packages loaded)\ncurrently loading: tensorflow/contrib/lite/toco\n\n\nplus info.\nThe following message appears when I input like this in command line. (for test)\npatch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch\n\n\npatching file src/google/protobuf/compiler/cpp/cpp_file.cc\nAssertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information.\n\npatch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary\n\npatching file src/google/protobuf/compiler/cpp/cpp_file.cc\nHunk #1 succeeded at 750 with fuzz 1 (offset 193 lines).\nHunk #2 succeeded at 825 (offset 169 lines).\nHunk #3 succeeded at 906 with fuzz 2 (offset 169 lines).\n\nI don't know how to add --binary option to script...\nref. #10435",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Windows 10(64bit)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow 1.5.0</li>\n<li><strong>Python version</strong>: Python 2.7/3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:  bazel 0.9.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: No GPU model</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I try to build the toco that is tensorflow lite converter.<br>\nBut I can not success to build. please see below for the details.</p>\n<h3>Source code / logs</h3>\n<p><code>C:\\tensorflow&gt;bazel build //tensorflow/contrib/lite/toco:toco</code><br>\nThe following error message appears.</p>\n<blockquote>\n<p>ERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):<br>\nFile \"C:/tensorflow/third_party/repo.bzl\", line 88<br>\n_apply_patch(ctx, ctx.attr.patch_file)<br>\nFile \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch<br>\n_execute_and_check_ret_code(ctx, cmd)<br>\nFile \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code<br>\nfail(\"Non-zero return code({1}) when ...))<br>\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':<br>\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc<br>\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354<br>\nThis application has requested the Runtime to terminate it in an unusual way.<br>\nPlease contact the application's support team for more information.<br>\nWARNING: Target pattern parsing failed.<br>\nERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):<br>\nFile \"C:/tensorflow/third_party/repo.bzl\", line 88<br>\n_apply_patch(ctx, ctx.attr.patch_file)<br>\nFile \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch<br>\n_execute_and_check_ret_code(ctx, cmd)<br>\nFile \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code<br>\nfail(\"Non-zero return code({1}) when ...))<br>\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':<br>\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc<br>\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354<br>\nThis application has requested the Runtime to terminate it in an unusual way.<br>\nPlease contact the application's support team for more information.<br>\nINFO: Elapsed time: 27.852s<br>\nFAILED: Build did NOT complete successfully (0 packages loaded)<br>\ncurrently loading: tensorflow/contrib/lite/toco</p>\n</blockquote>\n<ul>\n<li>plus info.<br>\nThe following message appears when I input like this in command line. (for test)<br>\n<code>patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch</code></li>\n</ul>\n<blockquote>\n<p>patching file src/google/protobuf/compiler/cpp/cpp_file.cc<br>\nAssertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354<br>\nThis application has requested the Runtime to terminate it in an unusual way.<br>\nPlease contact the application's support team for more information.</p>\n</blockquote>\n<p><code>patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary</code></p>\n<blockquote>\n<p>patching file src/google/protobuf/compiler/cpp/cpp_file.cc<br>\nHunk <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"115886302\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a> succeeded at 750 with fuzz 1 (offset 193 lines).<br>\nHunk <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"115894138\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2\">#2</a> succeeded at 825 (offset 169 lines).<br>\nHunk <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"115896656\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3\">#3</a> succeeded at 906 with fuzz 2 (offset 169 lines).</p>\n</blockquote>\n<p>I don't know how to add --binary option to script...<br>\nref. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"233524134\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/10435\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/10435/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/10435\">#10435</a></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "blueevangelion" },
          "number": 7959,
          "resourcePath": "/tensorflow/tensorflow/issues/7959",
          "state": "CLOSED",
          "publishedAt": "2017-03-01T04:06:42Z",
          "closedAt": "2017-03-10T20:03:11Z",
          "title": "ImportError: cannot import name model_fn",
          "bodyText": "I tried run cnn_mnist.py and I got the following error.\nTraceback (most recent call last):\nFile \" cnn_mnist.py\", line 13, in \nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\nImportError: cannot import name model_fn\ncuda veriosn\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Sun_Sep__4_22:14:01_CDT_2016\nCuda compilation tools, release 8.0, V8.0.44\ntensorflow version\ntensorflow (0.10.0)\npython version\nPython 2.7.12",
          "bodyHTML": "<p>I tried run cnn_mnist.py and I got the following error.</p>\n<p>Traceback (most recent call last):<br>\nFile \" cnn_mnist.py\", line 13, in <br>\nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib<br>\nImportError: cannot import name model_fn</p>\n<p>cuda veriosn<br>\nnvcc: NVIDIA (R) Cuda compiler driver<br>\nCopyright (c) 2005-2016 NVIDIA Corporation<br>\nBuilt on Sun_Sep__4_22:14:01_CDT_2016<br>\nCuda compilation tools, release 8.0, V8.0.44</p>\n<p>tensorflow version<br>\ntensorflow (0.10.0)</p>\n<p>python version<br>\nPython 2.7.12</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "Jinxit" },
          "number": 1962,
          "resourcePath": "/tensorflow/tensorflow/issues/1962",
          "state": "CLOSED",
          "publishedAt": "2016-04-15T08:11:37Z",
          "closedAt": "2016-05-13T21:03:25Z",
          "title": "Killed during checkpoint save (v0.8)",
          "bodyText": "Environment info\nOperating System: Ubuntu 15.10\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root   322936 aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a lrwxrwxrwx 1 root root       16 aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5 lrwxrwxrwx 1 root root       19 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18 -rwxr-xr-x 1 root root   383336 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18 -rw-r--r-- 1 root root   720192 aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4 -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48 -rw-r--r-- 1 root root 62025862 mar  6 15:08 /usr/local/cuda/lib64/libcudnn_static.a\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\nThe output from python -c \"import tensorflow; print(tensorflow.version)\".\n\n0.8.0rc0\nRelevant code:\nUnfortunately I can't upload everything, but the code is based on your CIFAR-10 example. Here is the structure of my network:\nhttp://pastebin.com/5QEJWqtm\nAfter running for some time, I save a checkpoint:\nsaver.save(sess, checkpoint_path, global_step=step)\nWhich sometimes allocates all memory on my system and gets killed. I have 8gb RAM and 8gb swap. For the first few checkpoints it seems fine, it allocates 2gb RAM (equal to checkpoint file size), but after some time it locks up my entire system and gets killed automatically.\nDidn't have any issues in 0.7.",
          "bodyHTML": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu 15.10</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<p><code>-rw-r--r-- 1 root root   322936 aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a lrwxrwxrwx 1 root root       16 aug 15  2015 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.7.5 lrwxrwxrwx 1 root root       19 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -&gt; libcudart.so.7.5.18 -rwxr-xr-x 1 root root   383336 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18 -rw-r--r-- 1 root root   720192 aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4 -rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5 -rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48 -rw-r--r-- 1 root root 62025862 mar  6 15:08 /usr/local/cuda/lib64/libcudnn_static.a</code></p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.</li>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\".</li>\n</ol>\n<p><code>0.8.0rc0</code></p>\n<h3>Relevant code:</h3>\n<p>Unfortunately I can't upload everything, but the code is based on your CIFAR-10 example. Here is the structure of my network:<br>\n<a rel=\"nofollow\" href=\"http://pastebin.com/5QEJWqtm\">http://pastebin.com/5QEJWqtm</a></p>\n<p>After running for some time, I save a checkpoint:</p>\n<p><code>saver.save(sess, checkpoint_path, global_step=step)</code></p>\n<p>Which <em>sometimes</em> allocates all memory on my system and gets killed. I have 8gb RAM and 8gb swap. For the first few checkpoints it seems fine, it allocates 2gb RAM (equal to checkpoint file size), but after some time it locks up my entire system and gets killed automatically.</p>\n<p>Didn't have any issues in 0.7.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "alex04309" },
          "number": 2062,
          "resourcePath": "/tensorflow/tensorflow/issues/2062",
          "state": "CLOSED",
          "publishedAt": "2016-04-22T09:09:04Z",
          "closedAt": "2016-04-22T20:24:45Z",
          "title": "tf.cond not working with depedencies",
          "bodyText": "tf.cond seems to have a bug if one of the condition have a dependency. (Dependencies are run, whatever tf.cond arg is True or False).\nTo illustrate:\nimport tensorflow as tf\n\na = tf.Variable(0)\nincr = a.count_up_to(1)\n\ndef todo_if_true():\n  with tf.control_dependencies([incr]):\n    return tf.identity(a)\ndef todo_if_false():\n  return tf.identity(a)\n\ng = tf.cond(tf.constant(False), todo_if_true, todo_if_false)\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  sess.run(init)\n  print(sess.run(g))\n\nOutput:\n1 #But should be 0",
          "bodyHTML": "<p>tf.cond seems to have a bug if one of the condition have a dependency. (Dependencies are run, whatever tf.cond arg is True or False).</p>\n<p>To illustrate:</p>\n<pre><code>import tensorflow as tf\n\na = tf.Variable(0)\nincr = a.count_up_to(1)\n\ndef todo_if_true():\n  with tf.control_dependencies([incr]):\n    return tf.identity(a)\ndef todo_if_false():\n  return tf.identity(a)\n\ng = tf.cond(tf.constant(False), todo_if_true, todo_if_false)\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  sess.run(init)\n  print(sess.run(g))\n</code></pre>\n<p>Output:</p>\n<pre><code>1 #But should be 0\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "jswelch" },
          "number": 16226,
          "resourcePath": "/tensorflow/tensorflow/issues/16226",
          "state": "OPEN",
          "publishedAt": "2018-01-18T19:20:32Z",
          "closedAt": null,
          "title": "Include netstat in the tensorflow docker container",
          "bodyText": "Describe the problem\nThis is a feature request to add net-tools to the Tensorflow docker containers.  Having netstat in the Tensorflow container will make it easier to find open ports in a multi-tenant environment when launching Tensorflow Distributed or Tensorboard.\nNote, I have found how to add netstat (see URL below), but would prefer not having to change or maintain a modified version of the Tensorflow container.\nhttps://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):NA\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):tensorflow/tensorflow:1.3.0 docker container\nTensorFlow installed from (source or binary):docker container\nTensorFlow version (use command below):1.3.0\nPython version: 2.7.12\nBazel version (if compiling from source):NA\nGCC/Compiler version (if compiling from source):NA\nCUDA/cuDNN version:NA\nGPU model and memory:NA\nExact command to reproduce:netstat",
          "bodyHTML": "<h3>Describe the problem</h3>\n<p>This is a feature request to add net-tools to the Tensorflow docker containers.  Having netstat in the Tensorflow container will make it easier to find open ports in a multi-tenant environment when launching Tensorflow Distributed or Tensorboard.</p>\n<p>Note, I have found how to add netstat (see URL below), but would prefer not having to change or maintain a modified version of the Tensorflow container.</p>\n<p><a rel=\"nofollow\" href=\"https://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container\">https://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container</a></p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:NA</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:tensorflow/tensorflow:1.3.0 docker container</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:docker container</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.3.0</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>:NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:NA</li>\n<li><strong>CUDA/cuDNN version</strong>:NA</li>\n<li><strong>GPU model and memory</strong>:NA</li>\n<li><strong>Exact command to reproduce</strong>:netstat</li>\n</ul>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "Steve7878" },
          "number": 8633,
          "resourcePath": "/tensorflow/tensorflow/issues/8633",
          "state": "CLOSED",
          "publishedAt": "2017-03-22T22:05:53Z",
          "closedAt": "2017-04-28T21:40:22Z",
          "title": "Please provide an example how to use a model trained from scratch for image classification",
          "bodyText": "The following documentation of TensorFlow-Slim contains how to train a model from scratch, but it's not explained how to use the resulting checkpoint files for image classification.\nhttps://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch\nAlthough it's possible to load the pre-trained models (https://github.com/tensorflow/models/tree/master/slim#pre-trained-models) and use it for image classification with the example given in https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb, it seems not possible to simply use the checkpoint files generated by \"training from scratch\" in the same way.\nAny example on how to use the newly generated checkpoints for image classification (for example with inception) would be appreciated.",
          "bodyHTML": "<p>The following documentation of TensorFlow-Slim contains how to train a model from scratch, but it's not explained how to use the resulting checkpoint files for image classification.<br>\n<a href=\"https://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch\">https://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch</a></p>\n<p>Although it's possible to load the pre-trained models (<a href=\"https://github.com/tensorflow/models/tree/master/slim#pre-trained-models\">https://github.com/tensorflow/models/tree/master/slim#pre-trained-models</a>) and use it for image classification with the example given in <a href=\"https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb\">https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb</a>, it seems not possible to simply use the checkpoint files generated by \"training from scratch\" in the same way.</p>\n<p>Any example on how to use the newly generated checkpoints for image classification (for example with inception) would be appreciated.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "volvador" },
          "number": 16584,
          "resourcePath": "/tensorflow/tensorflow/issues/16584",
          "state": "CLOSED",
          "publishedAt": "2018-01-30T13:00:44Z",
          "closedAt": "2018-02-01T12:38:21Z",
          "title": "TensorFlow op to copy weights of Keras model",
          "bodyText": "I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)\nwith tf.device(tf.train.replica_device_setter(...):\n      model = ##create model by keras\n      clone_model = ## create the same model by keras but now a stateful one\n\nafter calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling\n clone_model.set_weights(model.get_weights())\n\ndoes not work.\nI understand I need to define this weight copy as an op and then call session(run) of that op\nCan you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?",
          "bodyHTML": "<p>I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)</p>\n<pre><code>with tf.device(tf.train.replica_device_setter(...):\n      model = ##create model by keras\n      clone_model = ## create the same model by keras but now a stateful one\n</code></pre>\n<p>after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling</p>\n<pre><code> clone_model.set_weights(model.get_weights())\n</code></pre>\n<p>does not work.<br>\nI understand I need to define this weight copy as an op and then call session(run) of that op</p>\n<p>Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "Davidnet" },
          "number": 23165,
          "resourcePath": "/tensorflow/tensorflow/issues/23165",
          "state": "CLOSED",
          "publishedAt": "2018-10-22T16:18:50Z",
          "closedAt": "2018-12-06T19:18:15Z",
          "title": "TensorRT engine binding error",
          "bodyText": "System information*\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson TX2, Linux4Tegra Xenial 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.8.0\nPython version: 2.7.12\nBazel version (if compiling from source): 0.18.0\nGCC/Compiler version (if compiling from source): gcc5\nCUDA/cuDNN version: 9.0/7.1.5\nGPU model and memory: Jetson tx2 8GB\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the current behavior:\nI have frozen a graph that contains the following operations:\n\ntf.layers.conv2d(*args, **kwargs)\ntf.layers.batch_normalization(net, **batch_norm)\n\nI will try to provide a minimal graph that have this problem.\nDescribe the expected behavior:\nI create a trt_graph by using the function trt.create_inference_graph and it creates a graph successfully  but whenever I try to make an inference I encounter:\n2018-10-22 15:29:17.537843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with\u2502\n 363 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)                                            \u2502\n2018-10-22 15:29:20.426303: F tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:76] TensorRT engine cannot find binding: ModelBase/Conv2dBatchNorm/Relu     \nAborted (core dumped)\n\nAny guideline on how to provide a better log issue?\nThanks",
          "bodyHTML": "<p><em>System information</em>*</p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson TX2, Linux4Tegra Xenial 16.04</li>\n<li>TensorFlow installed from (source or binary): Source</li>\n<li>TensorFlow version (use command below): 1.8.0</li>\n<li>Python version: 2.7.12</li>\n<li>Bazel version (if compiling from source): 0.18.0</li>\n<li>GCC/Compiler version (if compiling from source): gcc5</li>\n<li>CUDA/cuDNN version: 9.0/7.1.5</li>\n<li>GPU model and memory: Jetson tx2 8GB</li>\n</ul>\n<p>You can collect some of this information using our environment capture <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">script</a><br>\nYou can also obtain the TensorFlow version with<br>\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<p><strong>Describe the current behavior</strong>:</p>\n<p>I have frozen a graph that contains the following operations:</p>\n<ul>\n<li>tf.layers.conv2d(*args, **kwargs)</li>\n<li>tf.layers.batch_normalization(net, **batch_norm)</li>\n</ul>\n<p>I will try to provide a minimal graph that have this problem.<br>\n<strong>Describe the expected behavior</strong>:<br>\nI create a trt_graph by using the function trt.create_inference_graph and it creates a graph successfully  but whenever I try to make an inference I encounter:</p>\n<pre><code>2018-10-22 15:29:17.537843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with\u2502\n 363 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)                                            \u2502\n2018-10-22 15:29:20.426303: F tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:76] TensorRT engine cannot find binding: ModelBase/Conv2dBatchNorm/Relu     \nAborted (core dumped)\n</code></pre>\n<p>Any guideline on how to provide a better log issue?</p>\n<p>Thanks</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "Cogitans" },
          "number": 8138,
          "resourcePath": "/tensorflow/tensorflow/issues/8138",
          "state": "CLOSED",
          "publishedAt": "2017-03-06T20:10:51Z",
          "closedAt": "2017-03-07T22:16:28Z",
          "title": "gradient_override_map should raise an error if passed invalid gradient names.",
          "bodyText": "Currently, gradient_override_map does not complain if passed in nonsensical information (apart from the simple check to make sure the map is a map from strings to strings).\nFor instance, the following lines of code run without issue:\n with graph.gradient_override_map({\"nonsense\": \"more_nonsense\"}): input = tf.sign(input)\nA more subtle point (that happened with me), when attempting to override sign's gradient, the following typo ran without problem:\n with graph.gradient_override_map({\"sign\": \"Identity\"}): input = tf.sign(input)\n(\"sign\" should be \"Sign\").\nSeems like a fairly simple issue, but I am not quite versed enough in the Tensorflow backend to suggest a fix to this problem.",
          "bodyHTML": "<p>Currently, gradient_override_map does not complain if passed in nonsensical information (apart from the simple check to make sure the map is a map from strings to strings).</p>\n<p>For instance, the following lines of code run without issue:</p>\n<p><code> with graph.gradient_override_map({\"nonsense\": \"more_nonsense\"}): input = tf.sign(input)</code></p>\n<p>A more subtle point (that happened with me), when attempting to override sign's gradient, the following typo ran without problem:</p>\n<p><code> with graph.gradient_override_map({\"sign\": \"Identity\"}): input = tf.sign(input)</code><br>\n(\"sign\" should be \"Sign\").</p>\n<p>Seems like a fairly simple issue, but I am not quite versed enough in the Tensorflow backend to suggest a fix to this problem.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ZhuFengdaaa" },
          "number": 2106,
          "resourcePath": "/tensorflow/tensorflow/issues/2106",
          "state": "CLOSED",
          "publishedAt": "2016-04-26T07:22:53Z",
          "closedAt": "2016-04-26T14:58:49Z",
          "title": "[ distribution ] How to use multiple GPU on each replica ?",
          "bodyText": "The Code Here shows how to set each replica which has a single tower that uses one GPU. I'm wondering if there is a way changing this code a little bit to make use of multiple GPU on one machine like that example.\nThe way I currently used for using all GPU on a worker machine is starting the number of workers that equal to the number of GPUs. then the workers can communicate to each other as if they are not on one machine. That is slower than if I can start a woker that control more than one GPU.",
          "bodyHTML": "<p>The <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/imagenet_distributed_train.py\">Code Here</a> shows how to set each replica which has a single tower that uses one GPU. I'm wondering if there is a way changing this code a little bit to make use of multiple GPU on one machine like <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py\">that example</a>.</p>\n<p>The way I currently used for using all GPU on a worker machine is starting the number of workers that equal to the number of GPUs. then the workers can communicate to each other as if they are not on one machine. That is slower than if I can start a woker that control more than one GPU.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "AndreasMadsen" },
          "number": 5122,
          "resourcePath": "/tensorflow/tensorflow/issues/5122",
          "state": "CLOSED",
          "publishedAt": "2016-10-21T20:48:58Z",
          "closedAt": "2016-10-23T19:07:22Z",
          "title": "custom CUDA op example returns random values",
          "bodyText": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNothing\nEnvironment info\nOperating System:\n$ uname -a\nLinux n-62-18-47 2.6.32-642.6.1.el6.x86_64 #1 SMP Wed Oct 5 08:48:31 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: CUDA: 8, cuDNN 5.1\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$ ls -l /appl/cuda/8.0/lib64/libcud*\n-rw-r--r-- 1 sebo root 560184 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 sebo root     16 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 sebo root     19 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 sebo root 394472 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 sebo root 737516 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart_static.a\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD) 5a5a25e\nThe output of bazel version\n\n$ bazel version\nBuild label: 0.3.2- (@non-git)\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 21 15:09:04 2016 (1477062544)\nBuild timestamp: 1477062544\nBuild timestamp as int: 1477062544\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nUsing the CUDA example from: https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op\n\ncompile example\n\nexport TF_INC=/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/include\n\nnvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \\\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\n\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \\\ncuda_op_kernel.cu.o -I $TF_INC -fPIC -L /appl/cuda/8.0/lib64 -L /appl/cudnn/v5.1-prod/lib64 -lcudart\n\n\nedit tensorflow.g3doc.how_tos.adding_an_op import cuda_op to import cuda_op in cuda_op_test.py.\n\nWhat other attempted solutions have you tried?\n\nI tried a non CUDA example, worked fine.\nI tried a diffrent cuda kernel (square operator) also failed.\nI added printf to the kernel launcher and made sure it was executed.\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\nCUDA_VISIBLE_DEVICES=3 python3 cuda_op_test.py\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:944] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:02:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:965] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\nnot equal where =  (array([0, 1, 2, 3, 4]),)\nnot equal lhs =  [ 280541332  143397048 2031878174 1533025280 1612453930]\nnot equal rhs =  [6 5 4 3 2]\nF.\n======================================================================\nFAIL: test (__main__.AddOneTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"cuda_op_test.py\", line 31, in test\n    self.assertAllEqual(result.eval(), [6, 5, 4, 3, 2])\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 499, in assertAllEqual\n    np.testing.assert_array_equal(a, b)\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 813, in assert_array_equal\n    verbose=verbose, header='Arrays are not equal')\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError:\nArrays are not equal\n\n(mismatch 100.0%)\n x: array([ 280541332,  143397048, 2031878174, 1533025280, 1612453930], dtype=int32)\n y: array([6, 5, 4, 3, 2])\n\n----------------------------------------------------------------------\nRan 2 tests in 0.213s\n\nFAILED (failures=1)\n\n\nIt looks like the output just contains random memory. Perhaps the GPU memory isn't copied back to the host memory.",
          "bodyHTML": "<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>Nothing</p>\n<h3>Environment info</h3>\n<p>Operating System:</p>\n<pre><code>$ uname -a\nLinux n-62-18-47 2.6.32-642.6.1.el6.x86_64 #1 SMP Wed Oct 5 08:48:31 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre>\n<p>Installed version of CUDA and cuDNN: CUDA: 8, cuDNN 5.1<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<pre><code>$ ls -l /appl/cuda/8.0/lib64/libcud*\n-rw-r--r-- 1 sebo root 560184 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 sebo root     16 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so -&gt; libcudart.so.8.0\nlrwxrwxrwx 1 sebo root     19 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.27\n-rwxr-xr-x 1 sebo root 394472 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 sebo root 737516 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart_static.a\n</code></pre>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>) <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/5a5a25ea3ebef623e07fb9a46419a9df377a37a5/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/5a5a25ea3ebef623e07fb9a46419a9df377a37a5\"><tt>5a5a25e</tt></a></li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<pre><code>$ bazel version\nBuild label: 0.3.2- (@non-git)\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 21 15:09:04 2016 (1477062544)\nBuild timestamp: 1477062544\nBuild timestamp as int: 1477062544\n</code></pre>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>Using the CUDA example from: <a href=\"https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op\">https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op</a></p>\n<ol>\n<li>compile example</li>\n</ol>\n<pre><code>export TF_INC=/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/include\n\nnvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \\\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\n\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \\\ncuda_op_kernel.cu.o -I $TF_INC -fPIC -L /appl/cuda/8.0/lib64 -L /appl/cudnn/v5.1-prod/lib64 -lcudart\n</code></pre>\n<ol start=\"2\">\n<li>edit <code>tensorflow.g3doc.how_tos.adding_an_op import cuda_op</code> to <code>import cuda_op</code> in <code>cuda_op_test.py</code>.</li>\n</ol>\n<h3>What other attempted solutions have you tried?</h3>\n<ul>\n<li>I tried a non CUDA example, worked fine.</li>\n<li>I tried a diffrent cuda kernel (square operator) also failed.</li>\n<li>I added <code>printf</code> to the kernel launcher and made sure it was executed.</li>\n</ul>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>\n<pre><code>CUDA_VISIBLE_DEVICES=3 python3 cuda_op_test.py\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:944] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:02:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:965] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\nnot equal where =  (array([0, 1, 2, 3, 4]),)\nnot equal lhs =  [ 280541332  143397048 2031878174 1533025280 1612453930]\nnot equal rhs =  [6 5 4 3 2]\nF.\n======================================================================\nFAIL: test (__main__.AddOneTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"cuda_op_test.py\", line 31, in test\n    self.assertAllEqual(result.eval(), [6, 5, 4, 3, 2])\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 499, in assertAllEqual\n    np.testing.assert_array_equal(a, b)\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 813, in assert_array_equal\n    verbose=verbose, header='Arrays are not equal')\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError:\nArrays are not equal\n\n(mismatch 100.0%)\n x: array([ 280541332,  143397048, 2031878174, 1533025280, 1612453930], dtype=int32)\n y: array([6, 5, 4, 3, 2])\n\n----------------------------------------------------------------------\nRan 2 tests in 0.213s\n\nFAILED (failures=1)\n</code></pre>\n<hr>\n<p>It looks like the output just contains random memory. Perhaps the GPU memory isn't copied back to the host memory.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "manuelreis" },
          "number": 12078,
          "resourcePath": "/tensorflow/tensorflow/issues/12078",
          "state": "CLOSED",
          "publishedAt": "2017-08-07T13:11:05Z",
          "closedAt": "2017-08-09T15:20:30Z",
          "title": "Assert randomly fails when training with multiple threads",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04.2 LTS\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0-rc2\nPython version:  2.7.12\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: no\nGPU model and memory: no\nExact command to reproduce: for ((n=0;n<100;n++)); do python mnist_softmax_parallel_issue.py; done\n\nDescribe the problem\nThe following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads.\nSource code / logs\nmnist_softmax_device_issue.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\nimport threading\nimport numpy as np\nimport json\nimport os\nimport time\n\nFLAGS = None\n\nINTER_OP_PARALLELISM = 76\nINTRA_OP_PARALLELISM = 1\nBATCH_SIZE = 100\nITERATIONS = 1000\nTRAINING_THREADS = 46\n\nthreads = [None] * TRAINING_THREADS\n\ndef train_function(thread_idx, mnist, sess, train_step, x, y_, y):\n  iterations = int(ITERATIONS/TRAINING_THREADS)\n  for i in range(iterations):\n    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\ndef main(_):\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n  x = tf.placeholder(tf.float32, [None, 784])\n  W = tf.Variable(tf.zeros([784, 10]))\n  b = tf.Variable(tf.zeros([10]))\n  y = tf.matmul(x, W) + b\n\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  cross_entropy = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n  train_step = tf.train.GradientDescentOptimizer(0.5, use_locking=True).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads = INTRA_OP_PARALLELISM, inter_op_parallelism_threads= INTER_OP_PARALLELISM))\n  sess.run(tf.global_variables_initializer())\n\n  for i in range(TRAINING_THREADS):\n      threads[i] = threading.Thread(target=train_function, args=[i, mnist, sess, train_step, x, y_, y])\n\n  for thread in threads:\n      thread.start()\n  for thread in threads:\n      thread.join()\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str, default='mnist-data',\n                      help='Directory for storing input data')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\n\nTraceback\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::T ensorEvaluator(const XprType&, const Device&) [with Broadcast = const Eigen::IndexList<Eigen::type2index<1l>, int>; ArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::XprType = Eigen::TensorBroadcastingOp<const Eigen::IndexList<Eigen::type2index<1l>, int>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer> >]: Assertion input_dims[i] > $' failed.",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Ubuntu 16.04.2 LTS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0-rc2</li>\n<li><strong>Python version</strong>:  2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: no</li>\n<li><strong>GPU model and memory</strong>: no</li>\n<li><strong>Exact command to reproduce</strong>: <code>for ((n=0;n&lt;100;n++)); do python mnist_softmax_parallel_issue.py; done</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads.</p>\n<h3>Source code / logs</h3>\n<p><em>mnist_softmax_device_issue.py</em></p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\nimport threading\nimport numpy as np\nimport json\nimport os\nimport time\n\nFLAGS = None\n\nINTER_OP_PARALLELISM = 76\nINTRA_OP_PARALLELISM = 1\nBATCH_SIZE = 100\nITERATIONS = 1000\nTRAINING_THREADS = 46\n\nthreads = [None] * TRAINING_THREADS\n\ndef train_function(thread_idx, mnist, sess, train_step, x, y_, y):\n  iterations = int(ITERATIONS/TRAINING_THREADS)\n  for i in range(iterations):\n    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\ndef main(_):\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n  x = tf.placeholder(tf.float32, [None, 784])\n  W = tf.Variable(tf.zeros([784, 10]))\n  b = tf.Variable(tf.zeros([10]))\n  y = tf.matmul(x, W) + b\n\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  cross_entropy = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n  train_step = tf.train.GradientDescentOptimizer(0.5, use_locking=True).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads = INTRA_OP_PARALLELISM, inter_op_parallelism_threads= INTER_OP_PARALLELISM))\n  sess.run(tf.global_variables_initializer())\n\n  for i in range(TRAINING_THREADS):\n      threads[i] = threading.Thread(target=train_function, args=[i, mnist, sess, train_step, x, y_, y])\n\n  for thread in threads:\n      thread.start()\n  for thread in threads:\n      thread.join()\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str, default='mnist-data',\n                      help='Directory for storing input data')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\n</code></pre>\n<p><em>Traceback</em></p>\n<p><code>external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator&lt;const Eigen::TensorBroadcastingOp&lt;Broadcast, XprType&gt;, Device&gt;::T ensorEvaluator(const XprType&amp;, const Device&amp;) [with Broadcast = const Eigen::IndexList&lt;Eigen::type2index&lt;1l&gt;, int&gt;; ArgType = const Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 2, 1, long int&gt;, 16, Eigen::MakePointer&gt;; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator&lt;const Eigen::TensorBroadcastingOp&lt;Broadcast, XprType&gt;, Device&gt;::XprType = Eigen::TensorBroadcastingOp&lt;const Eigen::IndexList&lt;Eigen::type2index&lt;1l&gt;, int&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 2, 1, long int&gt;, 16, Eigen::MakePointer&gt; &gt;]: Assertion input_dims[i] &gt; $' failed.</code></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ztwe" },
          "number": 21200,
          "resourcePath": "/tensorflow/tensorflow/issues/21200",
          "state": "CLOSED",
          "publishedAt": "2018-07-28T04:14:14Z",
          "closedAt": "2018-08-05T11:21:43Z",
          "title": "the weights of tf.contrib.rnn.BasicLSTMCell can't be updated",
          "bodyText": "Environment\nOS: Ubuntu 16.04\nTensorflow-gpu: 1.8\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n        num_units=self.config.num_lstm_units, state_is_tuple=True)\n    if self.mode == \"train\":\n      lstm_cell = tf.contrib.rnn.DropoutWrapper(\n          lstm_cell,\n          input_keep_prob=self.config.lstm_dropout_keep_prob,\n          output_keep_prob=self.config.lstm_dropout_keep_prob)\n\n    with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n      # Feed the image embeddings to set the initial LSTM state.\n      zero_state = lstm_cell.zero_state(\n          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\n\n      # Allow the LSTM variables to be reused.\n      #lstm_scope.reuse_variables()\n\n      ........\n\n      scores = tf.Variable(tf.random_normal(shape=[K, self.config.batch_size, C]), name=\"scores\")\n\n      M = tf.Variable(tf.random_normal(shape=[K+1, self.config.batch_size, 2, 3]), name=\"M\")\n      tf.assign(M[0], tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.]]))\n\n      lstm_input_size = 14\n      zk_size = 4096\n\n      hidden = zero_state\n\n      for k in range(0, K+1):\n          .......\n\n          lstm_outputs, hidden = lstm_cell(f_k, hidden) \n\nM and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module\uff1f",
          "bodyHTML": "<p>Environment<br>\nOS: Ubuntu 16.04<br>\nTensorflow-gpu: 1.8</p>\n<pre><code>    lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n        num_units=self.config.num_lstm_units, state_is_tuple=True)\n    if self.mode == \"train\":\n      lstm_cell = tf.contrib.rnn.DropoutWrapper(\n          lstm_cell,\n          input_keep_prob=self.config.lstm_dropout_keep_prob,\n          output_keep_prob=self.config.lstm_dropout_keep_prob)\n\n    with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n      # Feed the image embeddings to set the initial LSTM state.\n      zero_state = lstm_cell.zero_state(\n          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\n\n      # Allow the LSTM variables to be reused.\n      #lstm_scope.reuse_variables()\n\n      ........\n\n      scores = tf.Variable(tf.random_normal(shape=[K, self.config.batch_size, C]), name=\"scores\")\n\n      M = tf.Variable(tf.random_normal(shape=[K+1, self.config.batch_size, 2, 3]), name=\"M\")\n      tf.assign(M[0], tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.]]))\n\n      lstm_input_size = 14\n      zk_size = 4096\n\n      hidden = zero_state\n\n      for k in range(0, K+1):\n          .......\n\n          lstm_outputs, hidden = lstm_cell(f_k, hidden) \n</code></pre>\n<p>M and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module\uff1f</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7899459/43352992-ce472a2a-925f-11e8-9f48-08c9d64fa503.png\"><img src=\"https://user-images.githubusercontent.com/7899459/43352992-ce472a2a-925f-11e8-9f48-08c9d64fa503.png\" alt=\"opimizeloss\" style=\"max-width:100%;\"></a></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "luchensk" },
          "number": 13207,
          "resourcePath": "/tensorflow/tensorflow/issues/13207",
          "state": "CLOSED",
          "publishedAt": "2017-09-21T09:40:50Z",
          "closedAt": "2017-09-26T03:55:40Z",
          "title": "Failed to compile tensorflow offline with '--fetch=false' after all external dependencies fetched by bazel",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.2\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0 from master branch\nPython version: Python 2.7.12\nBazel version (if compiling from source): 0.5.4\nCUDA/cuDNN version: null\nGPU model and memory: null\nExact command to reproduce:\n\n\nFetch all external dependencies by docker image with internet access.\n\nbazel fetch //tensorflow/tools/pip_package:build_pip_package\n\n\nComplie tensorflow offline without internet access with --fetch=false.\n\nbazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\n\nDescribe the problem\nI fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub.\n#bazel fetch //tensorflow/tools/pip_package:build_pip_package\nINFO: All external dependencies fetched successfully.\n\nAfter that, I tried to compile tensorflow offline by using the image with bazel build --fetch=false since there is no internet access on my server, but it failed with the error \"no such package '@xxx'\", as follows:\n#bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:323:3: External repository 'six_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:173:3: External repository 'eigen_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\nERROR: /home/admin/src/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': BUILD file not found on package path and referenced by '//third_party/eigen3:eigen3'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 0.349s\n\nIn fact, be sure that the package eigen_archive was existing in the container, as below:\n#ls /root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/external/ | grep eigen_archive\neigen_archive\n\nAnd the ps stdout of bazel process was as follows:\n#ps aux | grep bazel\nroot        664  0.8  0.8 20850492 570628 ?     Ssl  16:39   0:29 bazel(src) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/A-server.jar --max_idle_secs=10800 --connect_timeout_secs=10 --install_base=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f --install_md5=2211725bdc2c34f807246fe9fb601a7f --output_base=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a --workspace_directory=/home/admin/src/tensorflow --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --client_debug=false --product_name=Bazel --option_sources=\n\nSo, please help for that and let me know if something wrong with my operation, thanks.",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04.2</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0 from master branch</li>\n<li><strong>Python version</strong>: Python 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.4</li>\n<li><strong>CUDA/cuDNN version</strong>: null</li>\n<li><strong>GPU model and memory</strong>: null</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<ol>\n<li>Fetch all external dependencies by docker image with internet access.</li>\n</ol>\n<pre><code>bazel fetch //tensorflow/tools/pip_package:build_pip_package\n</code></pre>\n<ol start=\"2\">\n<li>Complie tensorflow offline without internet access with <code>--fetch=false</code>.</li>\n</ol>\n<pre><code>bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub.</p>\n<pre><code>#bazel fetch //tensorflow/tools/pip_package:build_pip_package\nINFO: All external dependencies fetched successfully.\n</code></pre>\n<p>After that, I tried to compile tensorflow offline by using the image with <code>bazel build --fetch=false</code> since there is no internet access on my server, but it failed with the error \"no such package '<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/xxx/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xxx\">@xxx</a>'\", as follows:</p>\n<pre><code>#bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:323:3: External repository 'six_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\nWARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:173:3: External repository 'eigen_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.\nERROR: /home/admin/src/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': BUILD file not found on package path and referenced by '//third_party/eigen3:eigen3'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 0.349s\n</code></pre>\n<p>In fact, be sure that the package <code>eigen_archive</code> was existing in the container, as below:</p>\n<pre><code>#ls /root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/external/ | grep eigen_archive\neigen_archive\n</code></pre>\n<p>And the <code>ps</code> stdout of bazel process was as follows:</p>\n<pre><code>#ps aux | grep bazel\nroot        664  0.8  0.8 20850492 570628 ?     Ssl  16:39   0:29 bazel(src) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/A-server.jar --max_idle_secs=10800 --connect_timeout_secs=10 --install_base=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f --install_md5=2211725bdc2c34f807246fe9fb601a7f --output_base=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a --workspace_directory=/home/admin/src/tensorflow --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --client_debug=false --product_name=Bazel --option_sources=\n</code></pre>\n<p>So, please help for that and let me know if something wrong with my operation, thanks.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ivopivo" },
          "number": 22954,
          "resourcePath": "/tensorflow/tensorflow/issues/22954",
          "state": "CLOSED",
          "publishedAt": "2018-10-12T21:25:16Z",
          "closedAt": "2018-10-14T13:42:12Z",
          "title": "Error Building from source on Windows / my CPU doesn't have AVX ",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below):1.11\nPython version:3.6.6\nBazel version (if compiling from source): 0.17.2\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\n\nDescribe the problem\nI can't build from source as it gives me the error\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command\nPerhaps because my CPU doesnt have AVX instructions set\non my CPU Supported Instructions sets\tMMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T\nLack of AVX don't allow me to pip install tf>1.5\nMy question is how to install from source without AVX instructions set?\nSource code / logs\n\nC:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\nStarting local Bazel server and connecting to it...\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\nWARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan\ud83d\ude8b target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\nWARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of tf.distributions to tfp.distributions.\nWARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of tf.contrib.distributions to tfp.distributions.\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).\nINFO: Found 1 target...\nINFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:\ncl : Command line warning D9025 : overriding '/w' with '/W3'\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command\ncd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin\nSET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe\nSET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages\nSET TF_DOWNLOAD_CLANG=0\nSET TF_NEED_CUDA=0\nSET TF_NEED_OPENCL_SYCL=0\nSET TF_NEED_ROCM=0\nC:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command\ncd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin\nSET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe\nSET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages\nSET TF_DOWNLOAD_CLANG=0\nSET TF_NEED_CUDA=0\nSET TF_NEED_OPENCL_SYCL=0\nSET TF_NEED_ROCM=0\nC:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\n/usr/bin/bash: line 1:  7128 Illegal instruction     bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 74.199s, Critical Path: 2.64s\nINFO: 42 processes: 42 local.\nFAILED: Build did NOT complete successfully",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Windows 10</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:source</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.11</li>\n<li><strong>Python version</strong>:3.6.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.17.2</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I can't build from source as it gives me the error<br>\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command<br>\nPerhaps because my CPU doesnt have AVX instructions set</p>\n<p>on my CPU Supported Instructions sets\tMMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T<br>\nLack of AVX don't allow me to pip install tf&gt;1.5<br>\nMy question is how to install from source without AVX instructions set?</p>\n<h3>Source code / logs</h3>\n<blockquote>\n<p>C:\\tensorflow&gt;bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package<br>\nStarting local Bazel server and connecting to it...<br>\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated<br>\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12<br>\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12<br>\nWARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12<br>\nWARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of <code>tf.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of <code>tf.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of <code>tf.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of <code>tf.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan<g-emoji class=\"g-emoji\" alias=\"train\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f68b.png\">\ud83d\ude8b</g-emoji> target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of <code>tf.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of <code>tf.contrib.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of <code>tf.contrib.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of <code>tf.contrib.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of <code>tf.contrib.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of <code>tf.contrib.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of <code>tf.distributions</code> to <code>tfp.distributions</code>.<br>\nWARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (<a href=\"https://github.com/tensorflow/probability\">https://github.com/tensorflow/probability</a>). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of <code>tf.contrib.distributions</code> to <code>tfp.distributions</code>.<br>\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).<br>\nINFO: Found 1 target...<br>\nINFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:<br>\ncl : Command line warning D9025 : overriding '/w' with '/W3'<br>\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command<br>\ncd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow<br>\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin<br>\nSET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe<br>\nSET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages<br>\nSET TF_DOWNLOAD_CLANG=0<br>\nSET TF_NEED_CUDA=0<br>\nSET TF_NEED_OPENCL_SYCL=0<br>\nSET TF_NEED_ROCM=0<br>\nC:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js &gt; bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command<br>\ncd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow<br>\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin<br>\nSET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe<br>\nSET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages<br>\nSET TF_DOWNLOAD_CLANG=0<br>\nSET TF_NEED_CUDA=0<br>\nSET TF_NEED_OPENCL_SYCL=0<br>\nSET TF_NEED_ROCM=0<br>\nC:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js &gt; bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc<br>\n/usr/bin/bash: line 1:  7128 Illegal instruction     bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js &gt; bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc<br>\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build<br>\nINFO: Elapsed time: 74.199s, Critical Path: 2.64s<br>\nINFO: 42 processes: 42 local.<br>\nFAILED: Build did NOT complete successfully</p>\n</blockquote>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ophiry" },
          "number": 21429,
          "resourcePath": "/tensorflow/tensorflow/issues/21429",
          "state": "CLOSED",
          "publishedAt": "2018-08-07T06:08:42Z",
          "closedAt": "2018-09-11T18:40:34Z",
          "title": "TocoConvertor: converting keras models to tflite doesn't support custom objects",
          "bodyText": "Have I written custom code: No\nOS Platform and Distribution: Mac\nTensorFlow installed from: pip\nTensorFlow version: 1.10-rc1\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)\nMobile device: N/A\n\nThis is required to convert models that use custom layers or loss functions\nthis is the fix (will submit a PR soon):\nFrom 2c2179765cc9006762cf75c6a1b587e06895b869 Mon Sep 17 00:00:00 2001\nFrom: Ophir Yoktan <ophir@ziprecruiter.com>\nDate: Wed, 1 Aug 2018 10:16:10 +0300\nSubject: [PATCH] add support for custom_objects when loading keras model for\n conversion\n\n---\n tensorflow/contrib/lite/python/lite.py | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/tensorflow/contrib/lite/python/lite.py b/tensorflow/contrib/lite/python/lite.py\nindex 2f9b9d469a2..c12617df061 100644\n--- a/tensorflow/contrib/lite/python/lite.py\n+++ b/tensorflow/contrib/lite/python/lite.py\n@@ -274,7 +274,8 @@ def from_keras_model_file(cls,\n                             model_file,\n                             input_arrays=None,\n                             input_shapes=None,\n-                            output_arrays=None):\n+                            output_arrays=None,\n+                            custom_objects=None):\n     \"\"\"Creates a TocoConverter class from a tf.keras model file.\n \n     Args:\n@@ -293,7 +294,7 @@ def from_keras_model_file(cls,\n     \"\"\"\n     _keras.backend.clear_session()\n     _keras.backend.set_learning_phase(False)\n-    keras_model = _keras.models.load_model(model_file)\n+    keras_model = _keras.models.load_model(model_file, custom_objects=custom_objects)\n     sess = _keras.backend.get_session()\n \n     # Get input and output tensors.",
          "bodyHTML": "<p>Have I written custom code: No<br>\nOS Platform and Distribution: Mac<br>\nTensorFlow installed from: pip<br>\nTensorFlow version: 1.10-rc1<br>\nBazel version: N/A<br>\nCUDA/cuDNN version: N/A<br>\nGPU model and memory: N/A<br>\nExact command to reproduce: <code>tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)</code><br>\nMobile device: N/A</p>\n<hr>\n<p>This is required to convert models that use custom layers or loss functions</p>\n<p>this is the fix (will submit a PR soon):</p>\n<pre><code>From 2c2179765cc9006762cf75c6a1b587e06895b869 Mon Sep 17 00:00:00 2001\nFrom: Ophir Yoktan &lt;ophir@ziprecruiter.com&gt;\nDate: Wed, 1 Aug 2018 10:16:10 +0300\nSubject: [PATCH] add support for custom_objects when loading keras model for\n conversion\n\n---\n tensorflow/contrib/lite/python/lite.py | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/tensorflow/contrib/lite/python/lite.py b/tensorflow/contrib/lite/python/lite.py\nindex 2f9b9d469a2..c12617df061 100644\n--- a/tensorflow/contrib/lite/python/lite.py\n+++ b/tensorflow/contrib/lite/python/lite.py\n@@ -274,7 +274,8 @@ def from_keras_model_file(cls,\n                             model_file,\n                             input_arrays=None,\n                             input_shapes=None,\n-                            output_arrays=None):\n+                            output_arrays=None,\n+                            custom_objects=None):\n     \"\"\"Creates a TocoConverter class from a tf.keras model file.\n \n     Args:\n@@ -293,7 +294,7 @@ def from_keras_model_file(cls,\n     \"\"\"\n     _keras.backend.clear_session()\n     _keras.backend.set_learning_phase(False)\n-    keras_model = _keras.models.load_model(model_file)\n+    keras_model = _keras.models.load_model(model_file, custom_objects=custom_objects)\n     sess = _keras.backend.get_session()\n \n     # Get input and output tensors.\n</code></pre>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "hemalshahX" },
          "number": 5179,
          "resourcePath": "/tensorflow/tensorflow/issues/5179",
          "state": "CLOSED",
          "publishedAt": "2016-10-25T00:25:45Z",
          "closedAt": "2016-11-11T17:45:36Z",
          "title": "Undefined reference to CheckOpMessageBuilder::NewString() when linking libtensorflow_cc.so",
          "bodyText": "I am trying to use the TensorFlow Session C++ API (Python-free) to load a pre-trained model for inference. For build-time considerations, I am trying to deploy TensorFlow as a \"system\" package by linking against libtensorflow_cc.so and including headers into my Bazel-based workspace which has its own copies of protobuf and Eigen. I am almost there except that I have run into linker errors for missing implementations of tensorflow::internal::CheckOpMessageBuilder::NewString(). The symbols appear to be exported by libtensorflow_cc.so and it does seem to all be linking correctly, just not this symbol.\nAny help fixing this issue or suggestions for a better way of doing this would be greatly appreciated.\nThanks,\nHemal\nMy setup is the following:\nDocker image from ubuntu:16.04 using gcc5.\nBazel 0.3.1 (needed to upgrade from 0.3.0 because of other Tensorflow build issues)\nI matched the Eigen version but the protobuf used to build the Tensorflow wheel below is installed via apt-get and there is another copy (3.0.0) within my workspace's third_party directory.\nThe following is in my Dockerfile to build and \"deploy\" Tensorflow:\nRUN git clone https://github.com/tensorflow/tensorflow.git /tmp/tensorflow \\\n&& cd /tmp/tensorflow && git checkout r0.11 \\\n&& yes '' | ./configure \\\n&& bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \\\n&& /tmp/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \\\n&& pip2 install --quiet --upgrade /tmp/tensorflow_pkg/*.whl \\\n&& bazel build -c opt //tensorflow:libtensorflow_cc.so \\\n&& cp /tmp/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so /usr/lib/libtensorflow_cc.so \\\n&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow /usr/include/tensorflow \\\n&& ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/third_party /usr/include/third_party\n164:1: Linking of rule '//estimation/detection:playback_ground_truth' failed: clang-3.6 failed: error executing command\n(cd /code/.cache/bazel/_bazel_hemalshah/6fa7a91faa1abdfbb41bc875fa66f0f6/execroot/robotics && \nexec env - \n/usr/bin/clang-3.6 -o bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth -L/usr/lib/python2.7/config-x86_64-linux-gnu -L/usr/lib -Wl,-O1 -Wl,-Bsymbolic-functions -pthread -B/usr/bin/ -Wl,@bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference totensorflow::internal::CheckOpMessageBuilder::NewString()'",
          "bodyHTML": "<p>I am trying to use the TensorFlow Session C++ API (Python-free) to load a pre-trained model for inference. For build-time considerations, I am trying to deploy TensorFlow as a \"system\" package by linking against libtensorflow_cc.so and including headers into my Bazel-based workspace which has its own copies of protobuf and Eigen. I am almost there except that I have run into linker errors for missing implementations of tensorflow::internal::CheckOpMessageBuilder::NewString(). The symbols appear to be exported by libtensorflow_cc.so and it does seem to all be linking correctly, just not this symbol.</p>\n<p>Any help fixing this issue or suggestions for a better way of doing this would be greatly appreciated.</p>\n<p>Thanks,<br>\nHemal</p>\n<p>My setup is the following:<br>\nDocker image from ubuntu:16.04 using gcc5.<br>\nBazel 0.3.1 (needed to upgrade from 0.3.0 because of other Tensorflow build issues)<br>\nI matched the Eigen version but the protobuf used to build the Tensorflow wheel below is installed via apt-get and there is another copy (3.0.0) within my workspace's third_party directory.</p>\n<p>The following is in my Dockerfile to build and \"deploy\" Tensorflow:<br>\nRUN git clone <a href=\"https://github.com/tensorflow/tensorflow.git\">https://github.com/tensorflow/tensorflow.git</a> /tmp/tensorflow \\<br>\n&amp;&amp; cd /tmp/tensorflow &amp;&amp; git checkout r0.11 \\<br>\n&amp;&amp; yes '' | ./configure \\<br>\n&amp;&amp; bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \\<br>\n&amp;&amp; /tmp/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \\<br>\n&amp;&amp; pip2 install --quiet --upgrade /tmp/tensorflow_pkg/*.whl \\<br>\n&amp;&amp; bazel build -c opt //tensorflow:libtensorflow_cc.so \\<br>\n&amp;&amp; cp /tmp/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so /usr/lib/libtensorflow_cc.so \\<br>\n&amp;&amp; ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow /usr/include/tensorflow \\<br>\n&amp;&amp; ln -sv /usr/local/lib/python2.7/dist-packages/tensorflow/include/third_party /usr/include/third_party</p>\n<p>164:1: Linking of rule '//estimation/detection:playback_ground_truth' failed: clang-3.6 failed: error executing command<br>\n(cd /code/.cache/bazel/_bazel_hemalshah/6fa7a91faa1abdfbb41bc875fa66f0f6/execroot/robotics &amp;&amp; <br>\nexec env - <br>\n/usr/bin/clang-3.6 -o bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth -L/usr/lib/python2.7/config-x86_64-linux-gnu -L/usr/lib -Wl,-O1 -Wl,-Bsymbolic-functions -pthread -B/usr/bin/ -Wl,@bazel-out/local_linux-opt/bin/estimation/detection/playback_ground_truth-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.</p>\n<p>bazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function <code>std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;* tensorflow::internal::MakeCheckOpString&lt;long long, long long&gt;(long long const&amp;, long long const&amp;, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference to</code>tensorflow::internal::CheckOpMessageBuilder::NewString()'<br>\nbazel-out/local_linux-opt/bin/estimation/detection/libtof_pose_estimator.lo(tof_pose_estimator.o): In function <code>std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;* tensorflow::internal::MakeCheckOpString&lt;unsigned long, unsigned long&gt;(unsigned long const&amp;, unsigned long const&amp;, char const*)': /usr/include/tensorflow/core/platform/default/logging.h:170: undefined reference to</code>tensorflow::internal::CheckOpMessageBuilder::NewString()'</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "smartcat2010" },
          "number": 2233,
          "resourcePath": "/tensorflow/tensorflow/issues/2233",
          "state": "CLOSED",
          "publishedAt": "2016-05-05T12:09:26Z",
          "closedAt": "2016-06-03T23:45:55Z",
          "title": "protobuf message overflow on trying distributed ",
          "bodyText": "I'm trying to build an RNN on multi-machines following the Distributed Tensorflow.\nwhen I use \"with sv.managed_session(server.target) as sess:\", it shows error:\nAttributeError: 'Supervisor' object has no attribute 'managed_session'\nSo I follow the code of \"Inception\":\nwith sv.prepare_or_wait_for_session(server.target, config = sess_config) as sess :\nThen it starts to run, but hangs immediately after reporting the following error:\n[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:569] Reading dangerously large protocol message.  If the message turns out to be larger than 67108864 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 67108864\nE tensorflow/core/framework/tensor.cc:105] Input size was 67108839 and expected 72000800\nWould you please help me on this?\nThanks a lot in advance!",
          "bodyHTML": "<p>I'm trying to build an RNN on multi-machines following the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md\">Distributed Tensorflow</a>.<br>\nwhen I use \"with sv.managed_session(server.target) as sess:\", it shows error:<br>\nAttributeError: 'Supervisor' object has no attribute 'managed_session'<br>\nSo I follow the code of \"Inception\":<br>\nwith sv.prepare_or_wait_for_session(server.target, config = sess_config) as sess :</p>\n<p>Then it starts to run, but hangs immediately after reporting the following error:</p>\n<p>[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:569] Reading dangerously large protocol message.  If the message turns out to be larger than 67108864 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.<br>\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.<br>\n[libprotobuf WARNING google/protobuf/src/google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 67108864<br>\nE tensorflow/core/framework/tensor.cc:105] Input size was 67108839 and expected 72000800</p>\n<p>Would you please help me on this?<br>\nThanks a lot in advance!</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "cgorman" },
          "number": 2510,
          "resourcePath": "/tensorflow/tensorflow/issues/2510",
          "state": "CLOSED",
          "publishedAt": "2016-05-26T00:38:21Z",
          "closedAt": "2017-06-16T19:38:15Z",
          "title": "Tensorboard feature request - Text summary",
          "bodyText": "Would it be reasonable to add a basic text summary feature to Tensorboard? Personally I've run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where I could put some arbitrary text where I just wrote the key differences in my runs.\nFor example, on the Events page (or somewhere else) there would be a dropdown, similar to the summaries on the Events and Histograms page, with text I added (either hardcoded or as a script argument) that says what I did differently this run. Maybe I would print out the argument values for each run as well, that would be pretty useful, but basically something where I can say \"What did I do with this run again? Why was it different than the one before? Oh yeah I changed the batch size\" or \"Oh yeah I used my other dataset instead.\"\nObviously if it's arbitrary text you could maybe use it to write up a description of the network or whatever you want.",
          "bodyHTML": "<p>Would it be reasonable to add a basic text summary feature to Tensorboard? Personally I've run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where I could put some arbitrary text where I just wrote the key differences in my runs.</p>\n<p>For example, on the Events page (or somewhere else) there would be a dropdown, similar to the summaries on the Events and Histograms page, with text I added (either hardcoded or as a script argument) that says what I did differently this run. Maybe I would print out the argument values for each run as well, that would be pretty useful, but basically something where I can say \"What did I do with this run again? Why was it different than the one before? Oh yeah I changed the batch size\" or \"Oh yeah I used my other dataset instead.\"</p>\n<p>Obviously if it's arbitrary text you could maybe use it to write up a description of the network or whatever you want.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "olesalscheider" },
          "number": 16669,
          "resourcePath": "/tensorflow/tensorflow/issues/16669",
          "state": "CLOSED",
          "publishedAt": "2018-02-01T17:53:48Z",
          "closedAt": "2018-03-03T18:23:13Z",
          "title": "grappler: memory optimizer fails with: No output shape in Conv2DBackpropInput op / Conv2DBackpropFilter op.",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below):  v1.5.0-0-g37aa430d84\nPython version: 3.5\nBazel version (if compiling from source): 0.10.0\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: 9.1 / 7\nGPU model and memory: TITAN Xp, 12196MiB\nExact command to reproduce: -\n\nDescribe the problem\nWhen I enable the memory optimizer in grappler, it fails with the following errors:\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\n\nMy network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.\nIs this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?",
          "bodyHTML": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux, Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>:  v1.5.0-0-g37aa430d84</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.10.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.1 / 7</li>\n<li><strong>GPU model and memory</strong>: TITAN Xp, 12196MiB</li>\n<li><strong>Exact command to reproduce</strong>: -</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When I enable the memory optimizer in grappler, it fails with the following errors:</p>\n<pre><code>E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\n</code></pre>\n<p>My network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.</p>\n<p>Is this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "chihyuwang" },
          "number": 13458,
          "resourcePath": "/tensorflow/tensorflow/issues/13458",
          "state": "CLOSED",
          "publishedAt": "2017-10-03T05:20:50Z",
          "closedAt": "2017-10-03T06:18:15Z",
          "title": "How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)",
          "bodyText": "I want to limit the total memory of each GPU in mnist,\nhttps://www.tensorflow.org/tutorials/using_gpu\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nsession = tf.Session(config=config, ...)\nand I added the above code to the mnis.py\nhttps://github.com/tensorflow/models/tree/master/official/mnist\nhere is the modified code in mnis.py :\ndef main(unused_argv):\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nmnist_classifier = tf.estimator.Estimator(\nmodel_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)\nbut I get the below error:\nTraceback (most recent call last):\nFile \"mnist.py\", line 231, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\nFile \"mnist.py\", line 206, in main\nmodel_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 142, in init\nconfig)\nValueError: config must be an instance of RunConfig, but provided gpu_options {\nper_process_gpu_memory_fraction: 0.4\n}\n.\nMy question is :\nHow to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)",
          "bodyHTML": "<p>I want to limit the total memory of each GPU in mnist,<br>\n<a rel=\"nofollow\" href=\"https://www.tensorflow.org/tutorials/using_gpu\">https://www.tensorflow.org/tutorials/using_gpu</a></p>\n<p>config = tf.ConfigProto()<br>\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4<br>\nsession = tf.Session(config=config, ...)</p>\n<p>and I added the above code to the mnis.py<br>\n<a href=\"https://github.com/tensorflow/models/tree/master/official/mnist\">https://github.com/tensorflow/models/tree/master/official/mnist</a></p>\n<p>here is the modified code in mnis.py :<br>\ndef main(unused_argv):</p>\n<p>config = tf.ConfigProto()<br>\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4</p>\n<p>mnist_classifier = tf.estimator.Estimator(<br>\nmodel_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)</p>\n<p>but I get the below error:</p>\n<p>Traceback (most recent call last):<br>\nFile \"mnist.py\", line 231, in <br>\ntf.app.run()<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run<br>\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))<br>\nFile \"mnist.py\", line 206, in main<br>\nmodel_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 142, in <strong>init</strong><br>\nconfig)<br>\nValueError: config must be an instance of RunConfig, but provided gpu_options {<br>\nper_process_gpu_memory_fraction: 0.4<br>\n}<br>\n.</p>\n<p>My question is :<br>\nHow to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "Azorlogh" },
          "number": 6314,
          "resourcePath": "/tensorflow/tensorflow/issues/6314",
          "state": "CLOSED",
          "publishedAt": "2016-12-14T15:05:31Z",
          "closedAt": "2016-12-14T18:05:05Z",
          "title": "Error 404 when downloading Tensorflow on Windows",
          "bodyText": "The links provided on the website to the .whl used to install Tensorflow on Windows seem to be broken : https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows\nI'm getting an HTTP Error 404.\nI found the CPU Build elsewhere but I can't find the GPU build which I would like tu use.",
          "bodyHTML": "<p>The links provided on the website to the .whl used to install Tensorflow on Windows seem to be broken : <a rel=\"nofollow\" href=\"https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows\">https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows</a><br>\nI'm getting an HTTP Error 404.<br>\nI found the CPU Build elsewhere but I can't find the GPU build which I would like tu use.</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "ShownX" },
          "number": 4474,
          "resourcePath": "/tensorflow/tensorflow/issues/4474",
          "state": "CLOSED",
          "publishedAt": "2016-09-19T21:01:47Z",
          "closedAt": "2017-04-07T20:47:38Z",
          "title": "Feature request: Patch extracting given location implementation needed.",
          "bodyText": "Hello there,\nFeature\nI consider a lot but finally decide put this request here.\nI know there are some matrix operation and image operations like image_patch_extract and image.extract_glimpse().\nBut what I want is extract the patches given several locations.\nInput Tensor:\n         batch image: [batch_size, image_height, image_width, image_channel]\n         batch locations: [batch_size, num_patches, 2]\nOutput Tensor:\n        [batch_size, num_patches, patch_height, patch_width, image_channel]\nor \n        [batch_size * num_patches, patch_height, patch_width, image_channel]\nReference:\nGeorgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.\nhttps://github.com/trigeorgis/tensorflow\n\nBut when I use it in v0.10.0, it requires me to define a shape function.\nI really want to use it in the future, so I am glad it can be added as a new feature.\nOther solution I tried\nI have tried use extract_glimpse() instead.\noutput_list = [[] for _ in range(batch_size)]   # create n_patch list\nlocations = locations/image_size  # normalize the location\nfor j in range(n_patch):\n    patch_one = tf.image.glimpse(batch_image, tf.constant([20, 20]), locations[:, j, :], centered=False)\n    for i in range(batch_size):\n          output_list[i].append(patch_one[j])  # add tensor to each list\npatches = tf.pack(output_list)  # pack the list to tensor, size = [batch_size, n_patch, patch_size, patch_height, image_channel]",
          "bodyHTML": "<p>Hello there,</p>\n<h2>Feature</h2>\n<p>I consider a lot but finally decide put this request here.<br>\nI know there are some matrix operation and image operations like <code>image_patch_extract</code> and <code>image.extract_glimpse()</code>.<br>\nBut what I want is extract the patches given several locations.</p>\n<p>Input Tensor:</p>\n<div class=\"highlight highlight-source-python\"><pre>         <span class=\"pl-s1\">batch</span> <span class=\"pl-s1\">image</span>: [<span class=\"pl-s1\">batch_size</span>, <span class=\"pl-s1\">image_height</span>, <span class=\"pl-s1\">image_width</span>, <span class=\"pl-s1\">image_channel</span>]\n         <span class=\"pl-s1\">batch</span> <span class=\"pl-s1\">locations</span>: [<span class=\"pl-s1\">batch_size</span>, <span class=\"pl-s1\">num_patches</span>, <span class=\"pl-c1\">2</span>]</pre></div>\n<p>Output Tensor:</p>\n<div class=\"highlight highlight-source-python\"><pre>        [<span class=\"pl-s1\">batch_size</span>, <span class=\"pl-s1\">num_patches</span>, <span class=\"pl-s1\">patch_height</span>, <span class=\"pl-s1\">patch_width</span>, <span class=\"pl-s1\">image_channel</span>]\n<span class=\"pl-s1\">or</span> \n        [<span class=\"pl-s1\">batch_size</span> <span class=\"pl-c1\">*</span> <span class=\"pl-s1\">num_patches</span>, <span class=\"pl-s1\">patch_height</span>, <span class=\"pl-s1\">patch_width</span>, <span class=\"pl-s1\">image_channel</span>]</pre></div>\n<h2>Reference:</h2>\n<p>Georgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.</p>\n<pre><code>https://github.com/trigeorgis/tensorflow\n</code></pre>\n<p>But when I use it in v0.10.0, it requires me to define a shape function.<br>\nI really want to use it in the future, so I am glad it can be added as a new feature.</p>\n<h2>Other solution I tried</h2>\n<p>I have tried use <code>extract_glimpse()</code> instead.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">output_list</span> <span class=\"pl-c1\">=</span> [[] <span class=\"pl-k\">for</span> <span class=\"pl-s1\">_</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"pl-s1\">batch_size</span>)]   <span class=\"pl-c\"># create n_patch list</span>\n<span class=\"pl-s1\">locations</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">locations</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">image_size</span>  <span class=\"pl-c\"># normalize the location</span>\n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">j</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"pl-s1\">n_patch</span>):\n    <span class=\"pl-s1\">patch_one</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">image</span>.<span class=\"pl-en\">glimpse</span>(<span class=\"pl-s1\">batch_image</span>, <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">constant</span>([<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">20</span>]), <span class=\"pl-s1\">locations</span>[:, <span class=\"pl-s1\">j</span>, :], <span class=\"pl-s1\">centered</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">False</span>)\n    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">i</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"pl-s1\">batch_size</span>):\n          <span class=\"pl-s1\">output_list</span>[<span class=\"pl-s1\">i</span>].<span class=\"pl-en\">append</span>(<span class=\"pl-s1\">patch_one</span>[<span class=\"pl-s1\">j</span>])  <span class=\"pl-c\"># add tensor to each list</span>\n<span class=\"pl-s1\">patches</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">pack</span>(<span class=\"pl-s1\">output_list</span>)  <span class=\"pl-c\"># pack the list to tensor, size = [batch_size, n_patch, patch_size, patch_height, image_channel]</span></pre></div>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "benvand" },
          "number": 4675,
          "resourcePath": "/tensorflow/tensorflow/issues/4675",
          "state": "CLOSED",
          "publishedAt": "2016-09-30T09:51:54Z",
          "closedAt": "2017-06-16T23:35:01Z",
          "title": "LinearClassifier feature_columns overwritten in LinearClassifier.fit",
          "bodyText": "tensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit\neffectively returns\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit\nEstimator.fit calls:\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model\nwhich has these lines:\n      features, targets = input_fn()\n      self._check_inputs(features, targets)\n      train_op, loss_op = self._get_train_ops(features, targets)\n\ntensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs\n    if self._features_info is not None:\n      ...\n    else:\n      self._features_info = tensor_signature.create_signatures(features)\n\nSo we get to a point where features, as derived from the input_fn, is treated as our feature columns set.\nIn pseudo code:\ndef input_function()\n    return [foo, bar, baz], quux\n\nlc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be\nlc.fit(input_function) # run fit on out input function\nlc._feature_columns # repr _feature_columns on the instantiated classifier\n    [foo]\nlc.estimator._features_info # _features_info on the instantiated classifiers instantiated estimator\n    [foo, bar, baz]\n\nThe issue is:\nAlthough this line appears to indicate that we will be making an estimation based on the feature columns supplied:\nlc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be\nWhat happens is that the passed in feature_columns is unused and instead the return from the input_function supplied to fit are used.\nAm I correct in thinking that if the feature_columns arg is supplied that only those columns should be used by the classifiers estimator?\nThat when we instantiate the classifier we are setting the feature_columns we expect to be used?\nThe work around for this is simply to only return the columns you need from your input function however I found this misleading.\nPoint in the tutorial:\nEither the code or the tutorial need to be changed.\nhttps://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model\nError raised:\nWARNING:tensorflow:Setting feature info to\nas per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613",
          "bodyHTML": "<p><code>tensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit</code><br>\neffectively returns<br>\n<code>tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit</code><br>\n<code>Estimator.fit</code> calls:<br>\n<code>tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model</code><br>\nwhich has these lines:</p>\n<pre><code>      features, targets = input_fn()\n      self._check_inputs(features, targets)\n      train_op, loss_op = self._get_train_ops(features, targets)\n</code></pre>\n<p><code>tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs</code></p>\n<pre><code>    if self._features_info is not None:\n      ...\n    else:\n      self._features_info = tensor_signature.create_signatures(features)\n</code></pre>\n<p>So we get to a point where features, as derived from the input_fn, is treated as our feature columns set.</p>\n<p>In pseudo code:</p>\n<pre><code>def input_function()\n    return [foo, bar, baz], quux\n\nlc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be\nlc.fit(input_function) # run fit on out input function\nlc._feature_columns # repr _feature_columns on the instantiated classifier\n    [foo]\nlc.estimator._features_info # _features_info on the instantiated classifiers instantiated estimator\n    [foo, bar, baz]\n</code></pre>\n<h3>The issue is:</h3>\n<p>Although this line appears to indicate that we will be making an estimation based on the feature columns supplied:<br>\n<code>lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be</code><br>\nWhat happens is that the passed in <code>feature_columns</code> is unused and instead the return from the input_function supplied to fit are used.</p>\n<p>Am I correct in thinking that if the <code>feature_columns</code> arg is supplied that only those columns should be used by the classifiers estimator?<br>\nThat when we instantiate the classifier we are setting the feature_columns we expect to be used?</p>\n<p>The work around for this is simply to only return the columns you need from your input function however I found this misleading.</p>\n<p>Point in the tutorial:<br>\nEither the code or the tutorial need to be changed.<br>\n<a rel=\"nofollow\" href=\"https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model\">https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model</a></p>\n<p>Error raised:<br>\n<code>WARNING:tensorflow:Setting feature info to</code><br>\nas per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613</p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "FBerendsen" },
          "number": 23344,
          "resourcePath": "/tensorflow/tensorflow/issues/23344",
          "state": "CLOSED",
          "publishedAt": "2018-10-29T09:37:04Z",
          "closedAt": "2018-11-21T18:08:25Z",
          "title": "LayoutOptimizer optimizes to unsupported data_format for max_pool on CPU",
          "bodyText": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):  v1.11.0-rc2-4-gc19e29306c 1.11.0\nPython version: 3.6.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0.176/7.1.3\nGPU model and memory: NVIDIA Quadro M2200\n\nDescribe the current behavior\nTensorFlow automatically replaces my MaxPoolingOp by one using another data format which is not supported, subsequently.\nThis throws the InvalidArgumentError as seen below.\nThe problem arose when upgrading from TensorFlow 1.8 to 1.11 and from the error it seems to be caused by a TransposeNHWCToNCHW-LayoutOptimizer. When isolating the issue to reproduce it, it seems that max_pool, dataset and squeeze are involved to raise the error.  The only related (closed) issue I could find: #19497 \"NHWC convolution sometimes incorrectly considered NCHW\"\n InvalidArgumentError (see above for traceback): Default MaxPoolingOp only supports NHWC on device type CPU [[{{node label_image_dilated}} = MaxPool[T=DT_INT32, data_format=\"NCHW\", ksize=[1, 1, 3, 3], padding=\"SAME\", strides=[1, 1, 1, 1]](label_image_dilated-0-TransposeNHWCToNCHW-LayoutOptimizer)]] [[{{node OneShotIterator_2}} = OneShotIterator[container=\"\", dataset_factory=_make_dataset_UaZD9hBkHvg[], output_shapes=[[?,?]], output_types=[DT_INT32], shared_name=\"\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nDescribe the expected behavior\nTo not throw the error as was the case for TensorFlow 1.8.\nCode to reproduce the issue\nimport tensorflow as tf\nimport numpy as np\n\ndataset_np = {'height_or_width': 512, \n              'indices': np.random.randint(low=0, high=512, size=(1000,2),dtype=np.int64),\n              'values' : np.random.randint(low=0, high=1000, size=(1000,),dtype=np.int32)}\ndataset = tf.data.Dataset.from_tensors(dataset_np)\n\ndef densify(element):\n    label_image_sparse = tf.SparseTensor(indices = element['indices'], \n                                         values = element['values'], \n                                         dense_shape = tf.cast(tf.stack([element['height_or_width'],\n                                                                         element['height_or_width']]),tf.int64))\n    label_image = tf.sparse_tensor_to_dense(label_image_sparse, \n                                            default_value=-1, \n                                            validate_indices=False, \n                                            name='label_image')\n    label_image_dilated = tf.squeeze(tf.nn.max_pool([tf.expand_dims(label_image, axis=-1)], \n                                                     data_format=\"NHWC\", \n                                                     ksize= [1,3,3,1], \n                                                     strides = [1,1,1,1], \n                                                     padding='SAME', \n                                                     name='label_image_dilated'),[0,-1])\n    return {'label_image_dilated':label_image_dilated}\n\ndataset = dataset.map(densify)\nelement = dataset.make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n    result = sess.run(element)\n    print(result)\nOther info / logs",
          "bodyHTML": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:</li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version (use command below):  v1.11.0-rc2-4-gc19e29306c 1.11.0</li>\n<li>Python version: 3.6.5</li>\n<li>Bazel version (if compiling from source):</li>\n<li>GCC/Compiler version (if compiling from source):</li>\n<li>CUDA/cuDNN version: 9.0.176/7.1.3</li>\n<li>GPU model and memory: NVIDIA Quadro M2200</li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>TensorFlow automatically replaces my MaxPoolingOp by one using another data format which is not supported, subsequently.<br>\nThis throws the <code>InvalidArgumentError</code> as seen below.<br>\nThe problem arose when upgrading from TensorFlow 1.8 to 1.11 and from the error it seems to be caused by a <code>TransposeNHWCToNCHW-LayoutOptimizer</code>. When isolating the issue to reproduce it, it seems that <code>max_pool</code>, <code>dataset</code> and <code>squeeze</code> are involved to raise the error.  The only related (closed) issue I could find: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"325666823\" data-permission-text=\"Title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/19497\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/19497/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/19497\">#19497</a> \"NHWC convolution sometimes incorrectly considered NCHW\"</p>\n<p><code> InvalidArgumentError (see above for traceback): Default MaxPoolingOp only supports NHWC on device type CPU [[{{node label_image_dilated}} = MaxPool[T=DT_INT32, data_format=\"NCHW\", ksize=[1, 1, 3, 3], padding=\"SAME\", strides=[1, 1, 1, 1]](label_image_dilated-0-TransposeNHWCToNCHW-LayoutOptimizer)]] [[{{node OneShotIterator_2}} = OneShotIterator[container=\"\", dataset_factory=_make_dataset_UaZD9hBkHvg[], output_shapes=[[?,?]], output_types=[DT_INT32], shared_name=\"\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]</code></p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>To not throw the error as was the case for TensorFlow 1.8.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">tensorflow</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">tf</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">numpy</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">np</span>\n\n<span class=\"pl-s1\">dataset_np</span> <span class=\"pl-c1\">=</span> {<span class=\"pl-s\">'height_or_width'</span>: <span class=\"pl-c1\">512</span>, \n              <span class=\"pl-s\">'indices'</span>: <span class=\"pl-s1\">np</span>.<span class=\"pl-s1\">random</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-s1\">low</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-s1\">high</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">512</span>, <span class=\"pl-s1\">size</span><span class=\"pl-c1\">=</span>(<span class=\"pl-c1\">1000</span>,<span class=\"pl-c1\">2</span>),<span class=\"pl-s1\">dtype</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">np</span>.<span class=\"pl-s1\">int64</span>),\n              <span class=\"pl-s\">'values'</span> : <span class=\"pl-s1\">np</span>.<span class=\"pl-s1\">random</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-s1\">low</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-s1\">high</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1000</span>, <span class=\"pl-s1\">size</span><span class=\"pl-c1\">=</span>(<span class=\"pl-c1\">1000</span>,),<span class=\"pl-s1\">dtype</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">np</span>.<span class=\"pl-s1\">int32</span>)}\n<span class=\"pl-s1\">dataset</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">data</span>.<span class=\"pl-v\">Dataset</span>.<span class=\"pl-en\">from_tensors</span>(<span class=\"pl-s1\">dataset_np</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">densify</span>(<span class=\"pl-s1\">element</span>):\n    <span class=\"pl-s1\">label_image_sparse</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">SparseTensor</span>(<span class=\"pl-s1\">indices</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">element</span>[<span class=\"pl-s\">'indices'</span>], \n                                         <span class=\"pl-s1\">values</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">element</span>[<span class=\"pl-s\">'values'</span>], \n                                         <span class=\"pl-s1\">dense_shape</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">cast</span>(<span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">stack</span>([<span class=\"pl-s1\">element</span>[<span class=\"pl-s\">'height_or_width'</span>],\n                                                                         <span class=\"pl-s1\">element</span>[<span class=\"pl-s\">'height_or_width'</span>]]),<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">int64</span>))\n    <span class=\"pl-s1\">label_image</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">sparse_tensor_to_dense</span>(<span class=\"pl-s1\">label_image_sparse</span>, \n                                            <span class=\"pl-s1\">default_value</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">-</span><span class=\"pl-c1\">1</span>, \n                                            <span class=\"pl-s1\">validate_indices</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">False</span>, \n                                            <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'label_image'</span>)\n    <span class=\"pl-s1\">label_image_dilated</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">squeeze</span>(<span class=\"pl-s1\">tf</span>.<span class=\"pl-s1\">nn</span>.<span class=\"pl-en\">max_pool</span>([<span class=\"pl-s1\">tf</span>.<span class=\"pl-en\">expand_dims</span>(<span class=\"pl-s1\">label_image</span>, <span class=\"pl-s1\">axis</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">-</span><span class=\"pl-c1\">1</span>)], \n                                                     <span class=\"pl-s1\">data_format</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"NHWC\"</span>, \n                                                     <span class=\"pl-s1\">ksize</span><span class=\"pl-c1\">=</span> [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">1</span>], \n                                                     <span class=\"pl-s1\">strides</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], \n                                                     <span class=\"pl-s1\">padding</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'SAME'</span>, \n                                                     <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'label_image_dilated'</span>),[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">-</span><span class=\"pl-c1\">1</span>])\n    <span class=\"pl-k\">return</span> {<span class=\"pl-s\">'label_image_dilated'</span>:<span class=\"pl-s1\">label_image_dilated</span>}\n\n<span class=\"pl-s1\">dataset</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">dataset</span>.<span class=\"pl-en\">map</span>(<span class=\"pl-s1\">densify</span>)\n<span class=\"pl-s1\">element</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">dataset</span>.<span class=\"pl-en\">make_one_shot_iterator</span>().<span class=\"pl-en\">get_next</span>()\n\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">tf</span>.<span class=\"pl-v\">Session</span>() <span class=\"pl-k\">as</span> <span class=\"pl-s1\">sess</span>:\n    <span class=\"pl-s1\">result</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sess</span>.<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">element</span>)\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">result</span>)</pre></div>\n<p><strong>Other info / logs</strong></p>"
        }
      }
    },
    {
      "repository": {
        "issue": {
          "author": { "login": "VinceMarron" },
          "number": 15103,
          "resourcePath": "/tensorflow/tensorflow/issues/15103",
          "state": "CLOSED",
          "publishedAt": "2017-12-04T19:43:46Z",
          "closedAt": "2017-12-15T19:54:38Z",
          "title": "No GPU OpKernel for tf.exp() operation for Complex64",
          "bodyText": "I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea'  on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:\nIt seems that there is no OpKernel on device='GPU'  for the tf.exp() operation applied to complex numbers in eager mode.  This can be reproduced with the below code:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution()\n\nwith tf.device('/gpu:0'):\n  g = tf.spectral.rfft(tf.ones(64))\n  \n  tf.exp(g)\n\nwhich results in\n\t (OpKernel was found, but attributes didn't match)\n\t.  Registered:  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_HALF]\n  device='GPU'; T in [DT_FLOAT]\n [Op:Exp]\n\na more practical example that would lead to this same error:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n\nframe_length=256\nframe_step=64\nn_mels = 64\nsr=16000\nfilename = 'path/to/a.wav'\n\nsome_signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(filename), \n                                     file_format='wav', \n                                     samples_per_second=16000, \n                                     channel_count=1)\n\nwith tf.device('/gpu:0'):\n  stft = tf.contrib.signal.stft(tf.transpose(some_signal), frame_length=frame_length, \n                                  frame_step=frame_step, fft_length=frame_length)\n\n  linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n      n_mels, 1+frame_length//2, sr)\n\n  magnitude_spectrograms = tf.abs(stft)\n  log_mel_spec = tf.log(1e-6+ tf.tensordot(magnitude_spectrograms,\n                                           linear_to_mel_weight_matrix, \n                                           axes = [[2], [0]]))\n\n  mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spec)\n\nKeeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks",
          "bodyHTML": "<p>I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea'  on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:</p>\n<p>It seems that there is no OpKernel on device='GPU'  for the tf.exp() operation applied to complex numbers in eager mode.  This can be reproduced with the below code:</p>\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution()\n\nwith tf.device('/gpu:0'):\n  g = tf.spectral.rfft(tf.ones(64))\n  \n  tf.exp(g)\n</code></pre>\n<p>which results in</p>\n<pre lang=\"NotFoundError:\" data-meta=\"No registered 'Exp' OpKernel for GPU devices compatible with node Exp = Exp[T=DT_COMPLEX64](dummy_input)\"><code>\t (OpKernel was found, but attributes didn't match)\n\t.  Registered:  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_HALF]\n  device='GPU'; T in [DT_FLOAT]\n [Op:Exp]\n</code></pre>\n<p>a more practical example that would lead to this same error:</p>\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n\nframe_length=256\nframe_step=64\nn_mels = 64\nsr=16000\nfilename = 'path/to/a.wav'\n\nsome_signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(filename), \n                                     file_format='wav', \n                                     samples_per_second=16000, \n                                     channel_count=1)\n\nwith tf.device('/gpu:0'):\n  stft = tf.contrib.signal.stft(tf.transpose(some_signal), frame_length=frame_length, \n                                  frame_step=frame_step, fft_length=frame_length)\n\n  linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n      n_mels, 1+frame_length//2, sr)\n\n  magnitude_spectrograms = tf.abs(stft)\n  log_mel_spec = tf.log(1e-6+ tf.tensordot(magnitude_spectrograms,\n                                           linear_to_mel_weight_matrix, \n                                           axes = [[2], [0]]))\n\n  mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spec)\n</code></pre>\n<p>Keeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks</p>"
        }
      }
    }
  ]
  